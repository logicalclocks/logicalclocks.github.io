{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"Enterprise Data Feature Engineering Frameworks Pandas Flink Spark SQL Data Sources JDBC BigQuery Object Store Snowflake RedShift RDS Enterprise Feature Store Govern &amp; Monitor Serve Share &amp; Re-use Create Project Based Collaboration Write API Feature Groups External Feature Groups Read API Feature Views Training Data Feature Vectors Search, Versioning, Statistics, Monitoring Provenance &amp; Lineage Azure AWS Google Cloud On-premise Enterprise AI MLOps Experiments &amp; Model Training Model Registry Model Serving Operational ML Analytical ML BI Tools Vector DBOpenSearch <p>Hopsworks is a data platform for ML with a Python-centric Feature Store and MLOps capabilities. Hopsworks is a modular platform. You can use it as a standalone Feature Store, you can use it to manage, govern, and serve your models, and you can even use it to develop and operate feature, training and inference pipelines. Hopsworks brings collaboration for ML teams, providing a secure, governed platform for developing, managing, and sharing ML assets - features, models, training data, batch scoring data, logs, and more.</p>"},{"location":"#python-centric-feature-store","title":"Python-Centric Feature Store","text":"<p>Hopsworks is widely used as a standalone Feature Store. Hopsworks breaks the monolithic model development pipeline into separate feature and training pipelines, enabling both feature reuse and better tested ML assets. You can develop features by building feature pipelines in any Python (or Spark or Flink) environment, either inside or outside Hopsworks. You can use the Python frameworks you are familiar with to build production feature pipelines. You can compute aggregations in Pandas, validate feature data with Great Expectations, reduce your data dimensionality with embeddings and PCA, test your feature logic and features end-to-end with PyTest, and transform your categorical and numerical features with Scikit-Learn, TensorFlow, and PyTorch. You can orchestrate your feature pipelines with your Python framework of choice, including Hopsworks' own Airflow support.</p>"},{"location":"#the-widest-feature-store-capabilities","title":"The Widest Feature Store Capabilities","text":"<p>Hopsworks Feature Store also supports feature pipelines in PySpark, Spark, Flink, and SQL. Offline features can either be stored in Hopsworks, as Hudi tables on object storage, or in external data lakehouses (Snowflake, Databricks, Redshift, BigQuery, any JDBC-enabled platform) via External Feature Groups. Online features are served by RonDB, developed by Hopsworks as the lowest latency, highest throughput, highest availability data store for your features.</p>"},{"location":"#mlops-on-hopsworks","title":"MLOps on Hopsworks","text":"<p>Hopsworks provides model serving capabilities through KServe, with additional support for feature/prediction logging to Kafka (also part of Hopsworks), and secure, low-latency model deployments via Istio. Hopsworks also has a Model Registry for KServe, with support for versioning both models and model assets (such as KServe transformers). Hopsworks also includes a vector database to provide similarity search capabilities for embeddings, based on OpenSearch.</p>"},{"location":"#project-based-multi-tenancy-and-team-collaboration","title":"Project-based Multi-Tenancy and Team Collaboration","text":"<p>Hopsworks provides projects as a secure sandbox in which teams can collaborate and share ML assets. Hopsworks' unique multi-tenant project model even enables sensitive data to be stored in a shared cluster, while still providing fine-grained sharing capabilities for ML assets across project boundaries.  Projects can be used to structure teams so that they have end-to-end responsibility from raw data to managed features and models. Projects can also be used to create development, staging, and production environments for data teams. All ML assets support versioning, lineage, and provenance provide all Hopsworks users with a complete view of the MLOps life cycle, from feature engineering through model serving.</p>"},{"location":"#development-and-operations","title":"Development and Operations","text":"<p>Hopsworks provides a FTI (feature/training/inference) pipeline architecture for ML systems. Each part of the pipeline is defined in a Hopsworks job which corresponds to a Jupyter notebook, a python script or a jar. The production pipelines are then orchestrated with Airflow which is bundled in Hopsworks. Hopsworks provides several python environments that can be used and customized for each part of the FTI pipeline, for example switching between using PyTorch or TensorFlow in the training pipeline. You can train models on as many GPUs as are installed in a Hopsworks cluster and easily share them among users. You can also run Spark, Spark Streaming, or Flink programs on Hopsworks. JupyterLab is also bundled which can be used to run Python and Spark interactively.</p>"},{"location":"#available-on-any-platform","title":"Available on any Platform","text":"<p>Hopsworks is available to be installed on a kubernetes cluster in the cloud on AWS, Azure, and GCP, and On-Prem (Ubuntu/Redhat compatible), even in air-gapped data centers. Hopsworks is also available as a serverless platform that manages and serves both your features and models.</p>"},{"location":"#join-the-community","title":"Join the community","text":"<ul> <li>Ask questions and give us feedback in the Hopsworks Community</li> <li>Follow us on Twitter</li> <li>Check out all our latest product releases</li> <li>Join our public slack-channel</li> </ul>"},{"location":"#contribute","title":"Contribute","text":"<p>We are building the most complete and modular ML platform available in the market, and we count on your support to continuously improve Hopsworks. Feel free to add features to our library and report bugs anytime.</p>"},{"location":"#open-source","title":"Open-Source","text":"<p>Hopsworks Python API is available under the Apache License 2.0.</p>"},{"location":"concepts/hopsworks/","title":"Hopsworks Platform","text":"<p>Hopsworks is a modular MLOps platform with:</p> <ul> <li>a feature store (available as standalone)</li> <li>model registry and model serving based on KServe</li> <li>vector database based on OpenSearch</li> <li>a data science and data engineering platform</li> </ul> <p></p>"},{"location":"concepts/hopsworks/#standalone-feature-store","title":"Standalone Feature Store","text":"<p>Hopsworks was the first open-source and first enterprise feature store for ML.  You can use Hopsworks as a standalone feature store with the HSFS API.</p>"},{"location":"concepts/hopsworks/#model-management","title":"Model Management","text":"<p>Hopsworks includes support for model management, with model deployments using the KServe framework and a model registry designed for KServe. Hopsworks logs all inference requests to Kafka to enable easy monitoring of deployed models, and provides model metrics with grafana/prometheus.</p>"},{"location":"concepts/hopsworks/#vector-db","title":"Vector DB","text":"<p>Hopsworks provides a vector database (or embedding store) based on OpenSearch kNN (FAISS and nmslib). Hopsworks Vector DB includes out-of-the-box support for authentication, access control, filtering, backup-and-restore, and horizontal scalability. Hopsworks' Feature Store and vector DB are often used together to build scalable recommender systems, such as ranking-and-retrieval for real-time recommendations.</p>"},{"location":"concepts/hopsworks/#governance","title":"Governance","text":"<p>Hopsworks provides a data-mesh architecture for managing ML assets and teams, with multi-tenant projects. Not unlike a GitHub repository, a project is a sandbox containing team members, data, and ML assets. In Hopsworks, all ML assets (features, models, training data) are versioned, taggable, lineage-tracked, and support free-text search. Data can be also be securely shared between projects.</p>"},{"location":"concepts/hopsworks/#data-science-platform","title":"Data Science Platform","text":"<p>You can develop feature engineering, model training and inference pipelines in Hopsworks. There is support for version control (GitHub, GitLab, BitBucket), Jupyter notebooks, a shared distributed file system, many bundled modular project python environments for managing python dependencies without needing to write Dockerfiles, jobs (Python, Spark, Flink), and workflow orchestration with Airflow.</p>"},{"location":"concepts/dev/inside/","title":"Inside Hopsworks","text":"<p>Hopsworks provides a complete self-service development environment for feature engineering and model training. You can develop programs as Jupyter notebooks or jobs, customize the bundled FTI (feature, training and inference pipeline) python environments, you can manage your source code with Git, and you can orchestrate jobs with Airflow.</p> <p></p>"},{"location":"concepts/dev/inside/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<p>Hopsworks provides a Jupyter notebook development environment for programs written in Python, Spark, Flink, and SparkSQL. You can also develop in your IDE (PyCharm, IntelliJ, etc), test locally, and then run your programs as Jobs in Hopsworks. Jupyter notebooks can also be run as Jobs.</p>"},{"location":"concepts/dev/inside/#source-code-control","title":"Source Code Control","text":"<p>Hopsworks provides source code control support using Git (GitHub, GitLab or BitBucket). You can securely check out code into your project and commit and push updates to your code to your source code repository.</p>"},{"location":"concepts/dev/inside/#fti-pipeline-environments","title":"FTI Pipeline Environments","text":"<p>Hopsworks postulates that building ML systems following the FTI pipeline architecture is best practice. This architecture consists of three independently developed and operated ML pipelines:</p> <ul> <li>Feature pipeline: takes as input raw data that it transforms into features (and labels)</li> <li>Training pipeline: takes as input features (and labels) and outputs a trained model</li> <li>Inference pipeline: takes new feature data and a trained model and makes predictions</li> </ul> <p>In order to facilitate the development of these pipelines Hopsworks bundles several python environments containing necessary dependencies. Each of these environments may then also be customized further by cloning it and installing additional dependencies from PyPi, Conda channels, Wheel files, GitHub repos or a custom Dockerfile. Internal compute such as Jobs and Jupyter is run in one of these environments and changes are applied transparently when you install new libraries using our APIs. That is, there is no need to write a Dockerfile, users install libraries directly in one or more of the environments. You can setup custom development and production environments by creating separate projects or creating multiple clones of an environment within the same project.</p>"},{"location":"concepts/dev/inside/#jobs","title":"Jobs","text":"<p>In Hopsworks, a Job is a schedulable program that is allocated compute and memory resources. You can run a Job in Hopsworks:</p> <ul> <li>From the UI</li> <li>Programmatically with the Hopsworks SDK (Python, Java) or REST API</li> <li>From Airflow programs (either inside our outside Hopsworks)</li> <li>From your IDE using a plugin (PyCharm/IntelliJ plugin)</li> </ul>"},{"location":"concepts/dev/inside/#orchestration","title":"Orchestration","text":"<p>Airflow comes out-of-the box with Hopsworks, but you can also use an external Airflow cluster (with the Hopsworks Job operator) if you have one. Airflow can be used to schedule the execution of Jobs, individually or as part of Airflow DAGs.</p>"},{"location":"concepts/dev/outside/","title":"Outside Hopsworks","text":"<p>You can write programs that use Hopsworks in any Python, Spark, PySpark, or Flink environment. Hopsworks also running SQL queries to compute features in external data warehouses. The Feature Store can also be queried with SQL.</p> <p>There is REST API for Hopsworks that can be used with a valid API key, generated in Hopsworks. However, it is often easier to develop your programs against SDKs available in Python and Java/Scala for HSFS, in Python for HSML, and in Python for the Hopsworks API.</p> <p></p>"},{"location":"concepts/fs/","title":"Architecture","text":""},{"location":"concepts/fs/#what-is-hopsworks-feature-store","title":"What is Hopsworks Feature Store?","text":"<p>Hopsworks and its Feature Store are an open source data-intensive AI platform used for the development and operation of machine learning models at scale. The Hopsworks Feature Store provides the HSFS API to enable clients to write features to feature groups in the feature store, and to read features from feature views - either through a low latency Online API to retrieve pre-computed features for operational models or through a high throughput, latency insensitive Offline API, used to create training data and to retrieve batch data for scoring.</p> <p></p>"},{"location":"concepts/fs/#hsfs-api","title":"HSFS API","text":"<p>The HSFS (Hopsworks Feature Store) API is how you, as a developer, will use the feature store. The HSFS API helps simplify some of the problems that feature stores address including:</p> <ul> <li>consistent features for training and serving</li> <li>centralized, secure access to features</li> <li>point-in-time JOINs of features to create training data with no data leakage</li> <li>easier connection and backfilling of features from external data sources</li> <li>use of external tables as features</li> <li>transparent computation of statistics and usage data for features.</li> </ul>"},{"location":"concepts/fs/#write-to-feature-groups-read-from-feature-views","title":"Write to feature groups, read from feature views","text":"<p>You write to feature groups with a feature pipeline program. The program can be written in Python, Spark, Flink, or SQL.</p> <p>You read from views on top of the feature groups, called feature views. That is, a feature view does not store feature data, but is a logical grouping of features. Typically, you define a feature view because you want to train/deploy a model with exactly those features in the feature view. Feature views enable the reuse of feature data from different feature groups across different models.</p>"},{"location":"concepts/fs/feature_group/external_fg/","title":"External Feature Groups","text":"<p>External feature groups are offline feature groups where their data is stored in an external table. An external table requires a data source, defined with the Connector API (or more typically in the user interface), to enable HSFS to retrieve data from the external table. An external feature group doesn't allow for offline data ingestion or modification; instead, it includes a user-defined SQL string for retrieving data. You can also perform SQL operations, including projections, aggregations, and so on. The SQL query is executed on-demand when HSFS retrieves data from the external Feature Group, for example, when creating training data using features in the external table.</p> <p>In the image below, we can see that HSFS currently supports a large number of data sources, including any JDBC-enabled source, Snowflake, Data Lake, Redshift, BigQuery, S3, ADLS, GCS, RDS, and Kafka</p> <p></p>"},{"location":"concepts/fs/feature_group/feature_monitoring/","title":"Feature Monitoring","text":"<p>Feature Monitoring complements data validation capabilities by allowing you to monitor your feature data after it has been ingested into the Feature Store.</p> <p>HSFS supports monitoring features on your Feature Group by:</p> <ul> <li>transparently computing statistics on the whole or a subset of feature data defined by a detection window.</li> <li>comparing statistics against a reference window of feature data, and configuring thresholds to identify anomalous data.</li> <li>configuring alerts based on the statistics comparison results.</li> </ul>"},{"location":"concepts/fs/feature_group/feature_monitoring/#scheduled-statistics","title":"Scheduled Statistics","text":"<p>After creating a Feature Group in HSFS, you can setup statistics monitoring to compute statistics over one or more features on a scheduled basis. Statistics are computed on the whole or a subset of feature data (i.e., detection window) already inserted into the Feature Group.</p>"},{"location":"concepts/fs/feature_group/feature_monitoring/#statistics-comparison","title":"Statistics Comparison","text":"<p>In addition to scheduled statistics, you can enable the comparison of statistics against a reference subset of feature data (i.e., reference window) and define the criteria for this comparison including the statistics metric to compare and a threshold to identify anomalous values.</p> <p>Feature Monitoring Guide</p> <p>More information can be found in the Feature monitoring guide.</p>"},{"location":"concepts/fs/feature_group/feature_pipelines/","title":"Feature Pipelines","text":"<p>A feature pipeline is a program that orchestrates the execution of a dataflow graph of data validation, aggregation, dimensionality reduction, transformation, and other feature engineering steps on input data to create and/or update feature data. With HSFS, you can write feature pipelines in different languages as shown in the figure below.</p> <p></p>"},{"location":"concepts/fs/feature_group/feature_pipelines/#data-sources","title":"Data Sources","text":"<p>Your feature pipeline needs to connect to some (external) data source to read the data to be processed. Python, Spark, and Flink have connectors to a huge number of different data sources, while SQL feature pipelines are often restricted to a single data source (for example, your connector to SnowFlake only runs SQL on SnowFlake). SparkSQL, in contrast, can be used over tables that originate in different  data sources.</p>"},{"location":"concepts/fs/feature_group/feature_pipelines/#data-validation","title":"Data Validation","text":"<p>In order to be able to train and serve models that you can rely on, you need clean, high quality features. Data validation operations include removing bad data, removing or imputing missing values, and identifying problems such as feature shift. HSFS supports Great Expectations to specify data validation rules that are executed in the client before features are written to the Feature Store. The validation results are collected and shown in Hopsworks.</p>"},{"location":"concepts/fs/feature_group/feature_pipelines/#aggregations","title":"Aggregations","text":"<p>Aggregations are used to summarize large datasets into more concise, signal-rich features. Popular aggregations include count(), sum(), mean(), median(), stddev(), min(), and max(). These aggregations produce a single number (a numerical feature) that captures information about a potentially large dataset. Both numerical and categorical features are often transformed before being used to train or serve models.</p>"},{"location":"concepts/fs/feature_group/feature_pipelines/#dimensionality-reduction","title":"Dimensionality Reduction","text":"<p>If input data is impractically large or if it has a significant amount of redundancy, it can often be transformed into a reduced set of features with dimensionality reduction (often called feature extraction). Popular dimensionality algorithms include embedding algorithms, PCA, and TSNE.</p>"},{"location":"concepts/fs/feature_group/feature_pipelines/#transformations","title":"Transformations","text":"<p>Transformations are covered in more detail in training/inference pipelines, as transformations typically happen after the feature store. If you store transformed features in feature groups, the feature data is no longer useful for EDA (as it near to impossible for Data Scientists to understand the transformed values). It also makes it impossible for inference pipelines to log untransformed feature values and predictions for an operational model. There is one use case for storing transformed features in feature groups - when you need to have ultra low latency when reading precomputed features (and online transformations when reading features add too much latency for your use case). The figure below shows to include transformations in your feature pipelines.</p> <p></p>"},{"location":"concepts/fs/feature_group/feature_pipelines/#feature-engineering-in-python","title":"Feature Engineering in Python","text":"<p>Python is the most widely used framework for feature engineering due to its extensive library support for aggregations (Pandas/Polars), data validation (Great Expectations), and dimensionality reduction (embeddings, PCA), and transformations (in Scikit-Learn, TensorFlow, PyTorch). Python also supports open-source feature engineering frameworks used for automated feature engineering, such as featuretools that supports relational and temporal sources.</p>"},{"location":"concepts/fs/feature_group/feature_pipelines/#feature-engineering-in-sparkpyspark","title":"Feature Engineering in Spark/PySpark","text":"<p>Spark is popular as a feature engineering framework as it can scale to process larger volumes of data than Python, and provides native support for aggregations, and it supports many of the same data validation (Great Expectations), and dimensionality reduction algorithms (embeddings, PCA) as Python. Spark also has native support for transformations, which are useful for analytical models (batch scoring), but less useful for operational models, where online transformations are required, and Spark environments are less common. Online model serving environments typically only support online transformations in Python.</p>"},{"location":"concepts/fs/feature_group/feature_pipelines/#feature-engineering-in-sql","title":"Feature Engineering in SQL","text":"<p>SQL has grown in popularity for performing heavy lifting in feature pipelines - computing aggregates on data - when the input data already resides in a data warehouse. Data warehouses also support data validation, for example, through Great Expectations in DBT. However, SQL is not mature as a platform for transformations and dimensionality reductions, where UDFs are applied row-wise.</p> <p>You can do aggregation in SQL for data in your data warehouse or database.</p>"},{"location":"concepts/fs/feature_group/feature_pipelines/#feature-engineering-in-flink","title":"Feature Engineering in Flink","text":"<p>Apache Flink is a powerful and flexible framework for stateful feature computation operations over unbounded and bounded data streams. It is used for feature engineering when you need very fresh features computed in real-time. Flink provides a rich set of operators and functions such as time windows and aggregation operations that can be applied to keyed and/or global window streams. Flink\u2019s stateful operations allow users to maintain and update state across multiple data records or events, which is particularly useful for feature engineering tasks such as sessionization and/or maintaining rolling aggregates over a sliding window of data.</p> <p>Flink feature engineering pipelines are supported in Java/Scala only.</p>"},{"location":"concepts/fs/feature_group/feature_pipelines/#feature-engineering-in-beam","title":"Feature Engineering in Beam","text":"<p>Beam feature engineering pipelines are supported in Java/Scala only.</p>"},{"location":"concepts/fs/feature_group/fg_overview/","title":"Overview","text":"<p>As a programmer, you can consider a feature, in machine learning, to be a variable associated with some entity that contains a value that is useful for helping train a model to solve a prediction problem. That is, the feature is just a variable with predictive power for a machine learning problem, or task.</p> <p>A feature group is a table of features, where each feature group has a primary key, and optionally an event_time column (indicating when the features in that row were observed), and a partition key. Collectively, they are referred to as columns. The partition key determines how to layout the feature group rows on disk such that you can efficiently query the data using queries with the partition key. For example, if your partition key is the day and you have hundreds of days worth of data, with a partition key, you can query the day for only a given day or a range of days, and only the data for those days will be read from disk.</p> <p></p>"},{"location":"concepts/fs/feature_group/fg_overview/#online-and-offline-storage","title":"Online and offline Storage","text":"<p>Feature groups can be stored in a low-latency \"online\" database and/or in low cost, high throughput \"offline\" storage, typically a data lake or data warehouse.</p> <p></p>"},{"location":"concepts/fs/feature_group/fg_overview/#online-storage","title":"Online Storage","text":"<p>The online store stores only the latest values of features for a feature group. It is used to serve pre-computed features to models at runtime.</p>"},{"location":"concepts/fs/feature_group/fg_overview/#offline-storage","title":"Offline Storage","text":"<p>The offline store stores the historical values of features for a feature group so that it may store much more data than the online store. Offline feature groups are used, typically, to create training data for models, but also to retrieve data for batch scoring of models.</p> <p>In most cases, offline data is stored in Hopsworks, but through the implementation of data sources, it can reside in an external file system. The externally stored data can be managed by Hopsworks by defining ordinary feature groups or it can be used for reading only by defining External Feature Group.</p>"},{"location":"concepts/fs/feature_group/fg_statistics/","title":"Data Validation/Stats/Alerts","text":"<p>HSFS supports monitoring, validation, and alerting for features:</p> <ul> <li>transparently compute statistics over features on writing to a feature group;</li> <li>validation of data written to feature groups using Great Expectations</li> <li>alerting users when there was a problem writing or update features.</li> </ul>"},{"location":"concepts/fs/feature_group/fg_statistics/#statistics","title":"Statistics","text":"<p>When you create a Feature Group in HSFS, you can configure it to compute statistics over the features inserted into the Feature Group by setting the <code>statistics_config</code> dict parameter, see Feature Group Statistics for details. Every time you write to the Feature Group, new statistics will be computed over all of the data in the Feature Group.</p>"},{"location":"concepts/fs/feature_group/fg_statistics/#data-validation","title":"Data Validation","text":"<p>You can define expectation suites in Great Expectations and associate them with feature groups. When you write to a feature group, the expectations are executed, then you can define a policy on the feature group for what to do if any expectation fails.</p> <p></p>"},{"location":"concepts/fs/feature_group/fg_statistics/#alerting","title":"Alerting","text":"<p>HSFS also supports alerts, that can be triggered when there are problems in your feature pipelines, for example, when a write fails due to an error or a failed expectation. You can send alerts to different alerting endpoints, such as email or Slack, that can be configured in the Hopsworks UI. For example, you can send a slack message if features being written to a feature group are missing some input data.</p>"},{"location":"concepts/fs/feature_group/on_demand_feature/","title":"On-demand features","text":"<p>Features are defined as on-demand when their value cannot be pre-computed beforehand, rather they need to be computed in real-time during inference. This is achieved by implementing the on-demand features as a Python function in a Python module. Also ensure that the same version of the Python module is installed in both the feature and inference pipelines.</p> <p>In the image below shows an example of a housing price model that demonstrates how to implement an on-demand feature, a zip code (or post code) that is computed using longitude/latitude parameters. In your online application, longitude and latitude are provided as parameters to the application, and the same python function used to calculate the zip code in the feature pipeline is used to compute the zip code in the Online Inference pipeline.</p> <p></p>"},{"location":"concepts/fs/feature_group/spine_group/","title":"Spine Group","text":"<p>It is possible to maintain labels or prediction events among the regular features in a regular feature group with a feature pipeline updating the labels at a specific cadence.</p> <p>Often times, however, it is more convenient to provide the training events or entities in a Dataframe when reading feature data from the feature store through a feature view. We call such a Dataframe a Spine as it is the structure around which the training data or batch data is built. In order to retrieve the correct feature values for the entities in the Dataframe, using a point-in-time correct join, some additional metadata apart from the Dataframe schema is necessary. Namely, the information about which columns define the primary key, and which column indicates the event time at which the label was valid. The spine Dataframe together with this additional metadata is what we call a Spine Group.</p> <p>For example, in the following spine, we want to retrieve the features for the three locations, no later than the event time of each of the rainfall measurements, which is our prediction target:</p> location_id event_time rainfall (label) 1 2022-06-01 13:11 44 2 2022-06-01 09:14 5 3 2022-06-01 06:36 2 <p>A Spine Group does not materialize any data to the feature store itself, and always needs to be provided when retrieving features from the offline API. You can think of it as a place holder or a temporary feature group, to be replaced by a Dataframe in point-in-time joins.</p> <p>When using the online API, it is not necessary to provide the spine, since the online feature store contains only the latest feature values, and therefore no point in time join is required, the label is not required, as the inference pipeline is going to compute the prediction and the primary key values are specified when calling the online API.</p>"},{"location":"concepts/fs/feature_group/versioning/","title":"Versioning","text":"<p>See here for information about version of feature views.</p>"},{"location":"concepts/fs/feature_group/versioning/#schema-versioning","title":"Schema Versioning","text":"<p>The schema of feature groups is versioned. If you make a breaking change to the schema of a feature group, you need to increment the version of the feature group, and then backfill the new feature group. A breaking schema change is when you:</p> <ul> <li>drop a column from the schema</li> <li>add a new feature without any default value for the new feature</li> <li>change how a feature is computed, such that, for training models, the data for the old feature is not compatible with the data for the new feature.   For example, if you have an embedding as a feature and change the algorithm to compute that embedding, you probably should not mix feature values computed with the old embedding model with feature values computed with the new embedding model.</li> </ul> <p></p>"},{"location":"concepts/fs/feature_group/versioning/#data-versioning-for-feature-groups","title":"Data Versioning for Feature Groups","text":"<p>Data Versioning of a feature group involves tracking updates to the feature group, so that you can recover the state of the feature group at a given point-in-time in the past.</p> <p></p>"},{"location":"concepts/fs/feature_group/write_apis/","title":"Write APIs","text":"<p>You write to feature groups, and read from feature views.</p> <p>There are 3 APIs for writing to feature groups, as shown in the table below:</p> Stream API Batch API Connector API Python X - - Spark X X - Flink X - - External Table - - X"},{"location":"concepts/fs/feature_group/write_apis/#stream-api","title":"Stream API","text":"<p>The Stream API is the only API for Python and Flink clients, and is the preferred API for Spark, as it ensures consistent features between offline and online feature stores. The Stream API first writes data to be ingested to a Kafka topic, and then Hopsworks ensures that the data is synchronized to the Online and Offline Feature Groups through the OnlineFS service and Hudi DeltaStreamer jobs, respectively. The data in the feature groups is guaranteed to arrive at-most-once, through idempotent writes to the online feature group (only the latest values of features are stored there, and duplicates in Kafka only cause idempotent updates) and duplicate removal by Apache Hudi for the offline feature group.</p> <p></p>"},{"location":"concepts/fs/feature_group/write_apis/#batch-api","title":"Batch API","text":"<p>For very large updates to feature groups, such as when you are backfilling large amounts of data to an offline feature group, it is often preferential to write directly to the Hudi tables in Hopsworks, instead of via Kafka - thus reducing write amplification. Spark clients can write directly to Hudi tables on Hopsworks with Hopsworks libraries and certificates using a HDFS API. This requires network connectivity between the Spark clients and the datanodes in Hopsworks.</p> <p></p>"},{"location":"concepts/fs/feature_group/write_apis/#connector-api","title":"Connector API","text":"<p>Hopsworks supports external tables as feature groups. You can mount a table from an external database as an offline feature group using the Connector API - you create an external table using the connector. This enables you to use features from your external data source (Snowflake, Redshift, Delta Lake, etc) as you would any feature in an offline feature group in Hopsworks. You can, for example, join features from different feature groups (external or not) together to create feature views and training data for models.</p> <p></p>"},{"location":"concepts/fs/feature_view/feature_monitoring/","title":"Feature Monitoring","text":"<p>Feature Monitoring complements data validation capabilities by allowing you to monitor your feature data once they have been ingested into the Feature Store.</p> <p>HSFS supports monitoring features on your Feature View by:</p> <ul> <li>transparently computing statistics on the whole or a subset of feature data defined by a detection window.</li> <li>comparing statistics against a reference window of feature data (e.g., training dataset), and configuring thresholds to identify anomalous data.</li> <li>configuring alerts based on the statistics comparison results.</li> </ul>"},{"location":"concepts/fs/feature_view/feature_monitoring/#scheduled-statistics","title":"Scheduled Statistics","text":"<p>After creating a Feature View in HSFS, you can setup statistics monitoring to compute statistics over one or more features on a scheduled basis. Statistics are computed on the whole or a subset of feature data (i.e., detection window) using the Feature View query.</p>"},{"location":"concepts/fs/feature_view/feature_monitoring/#statistics-comparison","title":"Statistics Comparison","text":"<p>In addition to scheduled statistics, you can enable the comparison of statistics against a reference subset of feature data (i.e., reference window), typically a training dataset, and define the criteria for this comparison including the statistics metric to compare and a threshold to identify anomalous values.</p> <p>Feature Monitoring Guide</p> <p>More information can be found in the Feature monitoring guide.</p>"},{"location":"concepts/fs/feature_view/fv_overview/","title":"Overview","text":"<p>A feature view is a logical view over (or interface to) a set of features that may come from different feature groups. You create a feature view by joining together features from existing feature groups. In the illustration below, we can see that features are joined together from the two feature groups: seller_delivery_time_monthly and the seller_reviews_quarterly. You can also see that features in the feature view inherit not only the feature type from their feature groups, but also whether they are the primary key and/or the event_time. The image also includes transformation functions that are applied to individual features. Transformation functions are a part of the feature types included in the feature view. That is, a feature in a feature view is not only defined by its data type (int, string, etc) or its feature type (categorical, numerical, embedding), but also by its transformation.</p> <p></p> <p>Feature views can also include:</p> <ul> <li>the label for the supervised ML problem</li> <li>transformation functions that should be applied to specified features consistently between training and serving</li> <li>the ability to create training data</li> <li>the ability to retrieve a feature vector with the most recent feature values</li> </ul> <p>In the flow chart below, we can see the decisions that can be taken when creating (1) a feature view, and (2) creating training data with the feature view.</p> <p></p> <p>We can see here how the feature view is a representation for a model in the feature store - the same feature view is used to retrieve feature vectors for operational model that was created with training data from this feature view. As such, you can see that the most common use case for creating a feature view is to define the features that will be used in a model. In this way, feature views enable features from different feature groups to be reused across different models, and if features are stored untransformed in feature groups, they become even more reusable, as different feature views can apply different transformations to the same feature.</p>"},{"location":"concepts/fs/feature_view/offline_api/","title":"Offline API","text":"<p>The feature view provides an Offline API for</p> <ul> <li>creating training data</li> <li>creating batch (scoring) data</li> </ul>"},{"location":"concepts/fs/feature_view/offline_api/#training-data","title":"Training Data","text":"<p>Training data is created using a feature view. You can create training data as either:</p> <ul> <li>in-memory Pandas/Polars DataFrames, useful when you have a small amount of training data;</li> <li>materialized training data in files, in a file format of your choice (such as .tfrecord, .csv, or .parquet).</li> </ul> <p>You can apply filters when creating training data from a feature view:</p> <ul> <li>start-time and end-time, for example, to create the train-set from an earlier time range, and the test-set from a later (unseen) time range;</li> <li>feature value features, for example, only train a model on customers from a particular country.</li> </ul> <p>Note that filters are not applied when retrieving feature vectors using feature views, as we only look up features for a specific entity, like a customer. In this case, the application should know that predictions for this customer should be made on the model trained on customers in USA, for example.</p>"},{"location":"concepts/fs/feature_view/offline_api/#point-in-time-correct-training-data","title":"Point-in-time Correct Training Data","text":"<p>When you create training data from features in different feature groups, it is possible that the feature groups are updated at different cadences. For example, maybe one feature group is updated hourly, while another feature group is updated daily. It is very complex to write code that joins features together from such feature groups and ensures there is no data leakage in the resultant training data. HSFS hides this complexity by performing the point-in-time JOIN transparently, similar to the illustration below:</p> <p></p> <p>HSFS uses the event_time columns on both feature groups to determine the most recent (but not newer) feature values that are joined together with the feature values from the feature group containing the label. That is, the features in the feature group containing the label are the observation times for the features in the resulting training data, and we want feature values from the other feature groups that have the most recent timestamps, but not newer than the timestamp in the label-containing feature group.</p>"},{"location":"concepts/fs/feature_view/offline_api/#spine-groups","title":"Spine Groups","text":"<p>The left side of the point-in-time join is typically the set of training entities/primary key values for which the relevant features need to be retrieved. This left side of the join can also be replaced by a spine group. When using feature groups also so save labels/prediction targets, it can happen that you end up with the same entity multiple times in the training dataset depending on the cadence at which the label group was updated and the length of the event time interval that is being used to generate the training dataset. This can lead to bias in the training dataset and should be avoided. To avoid this kind of situation, users can either narrow down the event time interval during training dataset creation or use a spine in order to precisely define the entities to be included in the training dataset. This is just one example where spines are helpful.</p>"},{"location":"concepts/fs/feature_view/offline_api/#splitting-training-data","title":"Splitting Training Data","text":"<p>You can create random train/validation/test splits of your training data using the HSFS API. You can also time-based splits with the HSFS API.</p>"},{"location":"concepts/fs/feature_view/offline_api/#evaluation-sets","title":"Evaluation Sets","text":"<p>Test data can also be split into evaluation sets to help evaluate a model for potential bias. First, you have to identify the classes of samples that could be at risk of bias, and generate evaluation sets from your unseen test set - one evaluation set for each group of samples at risk of bias. For example, if you have a feature group of users, where one of the features is gender, and you want to evaluate the risk of bias due to gender, you can use filters to generate 3 evaluation sets from your test set - one for male, female, and non-binary. Then you score your model against all 3 evaluation sets to ensure that the prediction performance is comparable and non-biased across all 3 gender.</p>"},{"location":"concepts/fs/feature_view/offline_api/#batch-scoring-data","title":"Batch (Scoring) Data","text":"<p>Batch data for scoring models is created using a feature view. Similar to training data, you can create batch data as either:</p> <ul> <li>in-memory Pandas/Polars DataFrames, useful when you have a small amount of data to score;</li> <li>materialized data in files, in a file format of your choice (such as .tfrecord, .csv, or .parquet)</li> </ul> <p>Batch data requires specification of a <code>start_time</code> for the start of the batch scoring data. You can also specify the <code>end_time</code> (default is the current date).</p> <p></p>"},{"location":"concepts/fs/feature_view/offline_api/#spine-dataframes","title":"Spine Dataframes","text":"<p>Similar to training dataset generation, it might be helpful to specify a spine when retrieving features for batch inference. The only difference in this case is that the spine dataframe doesn't need to contain the label, as this will be the output of the inference pipeline. A typical use case is the handling of opt-ins, where certain customers have to be excluded from an inference pipeline due to a missing marketing opt-in.</p>"},{"location":"concepts/fs/feature_view/online_api/","title":"Online API","text":"<p>The Feature View provides an Online API to return an individual feature vector, or a batch of feature vectors, containing the latest feature values. To retrieve a feature vector, a client needs to provide the primary key(s) for the feature groups backing the feature view. For example, if you have <code>customer_profile</code> and <code>customer_purchases</code> Feature Groups both with <code>customer_id</code> as a primary key, and a Feature View made up from features from both Feature Groups, then, you would use <code>customer_id</code> to retrieve a feature vector using the Feature View object.</p>"},{"location":"concepts/fs/feature_view/online_api/#feature-vectors","title":"Feature Vectors","text":"<p>A feature vector is a row of features (without the primary key(s) and event timestamp):</p> <p></p> <p>It may be the case that for any given feature vector, not all features will come pre-engineered from the feature store. Some features will be provided by the client (or at least the raw data to compute the feature will come from the client). We call these 'passed' features and, similar to precomputed features from the feature store, they can also be transformed by the HSFS client in the method:</p> <ul> <li>feature_view.get_feature_vector(entry, passed_features={...})</li> </ul>"},{"location":"concepts/fs/feature_view/statistics/","title":"Statistics","text":"<p>The feature view does not contain any statistics, as it is simply an interface consisting of a number of features and any transformation functions applied to those features.</p> <p>However, training data can have descriptive statistics over it computed by HSFS. Descriptive statistics for training data is important for model monitoring, as it can enable model monitoring. If you compute the same descriptive statistics over windows of input features to models, you can help determine when there is a significant change in the distribution of an input feature, so-called feature shift.</p>"},{"location":"concepts/fs/feature_view/training_inference_pipelines/","title":"Consistent Transformations","text":"<p>A training pipeline is a program that orchestrates the training of a machine learning model. For supervised machine learning, a training pipeline requires both features and labels, and these can typically be retrieved from the feature store as either in-memory Pandas/Polars DataFrames or read as training data files, created from the feature store. An inference pipeline is a program that takes user input, optionally enriches it with features from the feature store, and builds a feature vector (or batch of feature vectors) with with it uses a model to make a prediction.</p>"},{"location":"concepts/fs/feature_view/training_inference_pipelines/#transformations","title":"Transformations","text":"<p>Feature transformations are mathematical operations that change feature values with the goal of improving model convergence or performance properties. Transformation functions take as input a single value (or small number of values), they often require state (such as the mean value of a feature to normalize the input), and they output a single value or a list of values.</p>"},{"location":"concepts/fs/feature_view/training_inference_pipelines/#training-serving-skew","title":"Training Serving Skew","text":"<p>It is crucial that the transformations performed when creating features (for training or serving) are consistent - use the same code - to avoid training/serving skew. In the image below, you can see that transformations happen after the Feature Store, but that the implementation of the transformation functions need to be consistent between the training and inference pipelines.</p> <p></p> <p>There are 3 main approaches to prevent training/serving skew that we support in Hopsworks. These are (1) perform transformations in models, (2) perform transformations in pipelines (sklearn, TF, PyTorch) and use the model registry to save the transformation pipeline so that the same transformation is used in your inference pipeline, and (3) use HSFS transformations, defined as UDFs in Python.</p>"},{"location":"concepts/fs/feature_view/training_inference_pipelines/#transformations-as-pre-processing-layers-in-models","title":"Transformations as Pre-Processing Layers in Models","text":"<p>Transformation functions can be implemented as preprocessing steps within a model. For example, you can write a transformation function as a pre-processing layer in Keras/TensorFlow. When you save the model, the preprocessing steps will also be saved as part of the model. Any state required to compute the transformation, such as the arithmetic mean of a numerical feature in the train set, is also stored with the function, enabling consistent transformations during inference.  When data preprocessing is part of the model, users can just send the untransformed feature values to the model and the model itself will apply any transformation functions as preprocessing layers (such as encoding categorical variables or normalizing numerical variables).</p>"},{"location":"concepts/fs/feature_view/training_inference_pipelines/#transformation-pipelines-in-scikit-learntensorflowpytorch","title":"Transformation Pipelines in Scikit-Learn/TensorFlow/PyTorch","text":"<p>You have to save your transformation pipeline (serialize the object or the parameters) and make sure you apply exactly the same transformations in your inference pipeline. This means you should version the transformations. In Hopsworks, you can store the transformations with your versioned models in the Model Registry, helping you to ensure the same transformation pipeline is applied to both training/serving for the same model version.</p>"},{"location":"concepts/fs/feature_view/training_inference_pipelines/#transformations-as-python-udfs-in-hsfs","title":"Transformations as Python UDFs in HSFS","text":"<p>Hopsworks feature store also supports consistent transformation functions by enabling a Python UDF, that implements a transformation, to be attached a to feature in a feature view. When training data is created with a feature view or when a feature vector is retrieved from a feature view, HSFS ensures that any transformation functions defined over any features will be applied before returning feature values. You can use built-in transformation objects in HSFS or write your own custom transformation functions as Python UDFs. The benefit of this approach is that transformations are applied consistently when creating training data and when retrieving feature data from the online feature store. Transformations no longer need to be included in either your training pipeline or inference pipeline, as they are applied transparently when creating training data and retrieving feature vectors. Hopsworks uses Spark to create training data as files, and any transformation functions for features are executed as Python UDFs in Spark - enabling transformation functions to be applied on large volumes of data and removing potentially CPU-intensive transformations from training pipelines.</p>"},{"location":"concepts/fs/feature_view/versioning/","title":"Versioning","text":"<p>Feature views are interfaces, and if there is a change in the interface (the types of the features, the transformations applied to the features), then you need to change the version, to prevent breaking existing clients.</p> <p>Training datasets are associated with a specific feature view version. Training data also has its own version number (along with the version of its parent feature view). For example, online transformation functions often need training data statistics (e.g., normalizing a numerical feature requires you to divide the feature value by the mean value for that feature in the training dataset). As many training datasets can be created from a feature view, when you initialize the feature view you need to tell it which version of the training data to use - <code>feature_view.init(1)</code> means use version 1 of the training data for this feature view.</p>"},{"location":"concepts/mlops/bi_tools/","title":"BI Tools","text":"<p>The Hopsworks Feature Store is based on an offline data store, queryable via an Apache Hive API, and an online data store, queryable via a MySQL Server API.</p> <p>Given that Feature Groups in Hopsworks have well-defined schemas, features in the Hopsworks Feature Store can be analyzed and reports can be generated from them using any BI Tools that include connectors for MySQL (JDBC) and Apache Hive (2-way TLS required). One platform we use with customers is Apache Superset, as it can be configured alongside Hopsworks to provide BI Tooling capabilities.</p>"},{"location":"concepts/mlops/data_transformations/","title":"Data Transformations","text":"<p>Data transformations are integral to all AI applications. Data transformations produce new features that can enhance the performance of an AI application. However, not all transformations in an AI application are equivalent.</p> <p>Transformations like binning and aggregations typically create reusable features, while transformations like one-hot encoding, scaling and normalization often produce model-specific features. Additionally, in real-time AI systems, some features can only be computed during inference when the request is received, as they need request-time parameters to be computed.</p> <p></p> <p>This classification of features can be used to create a taxonomy for data transformation that would apply to any scalable and modular AI system that aims to reuse features. The taxonomy helps identify which classes of data transformation can cause online-offline skews in AI systems, allowing for their prevention. Hopsworks provides support for a feature view abstraction as well as model-dependent transformations and on-demand transformations to prevent online-offline skew.</p>"},{"location":"concepts/mlops/data_transformations/#data-transformation-taxonomy-for-ai-systems","title":"Data Transformation Taxonomy for AI Systems","text":"<p>Transformation functions in an AI system can be classified into three types based on the nature of the input features they generate: model-independent, model-dependent, and on-demand transformations.</p> <p></p> <p>Model-independent transformations create reusable features that can be utilized across one or more machine-learning models. These transformations include techniques such as grouped aggregations (e.g., minimum, maximum, or average of a variable), windowed aggregations (e.g., the number of clicks per day), and binning to generate categorical variables. Since the data produced by model-independent transformations are reusable, these features can be stored in a feature store.</p> <p>Model-dependent transformations generate features specific to one model. These include transformations that are unique to a particular model or are parameterized by the training dataset, making them model-specific. For instance, text tokenization is a transformation required by all large language models (LLMs) but each LLM has their own (unique) tokenizer. Other transformations, such as encoding categorical variables in a numerical representation or scaling/normalizing/standardizing numerical variables to enhance the performance of gradient-based models, are parameterized by the training dataset. Consequently, the features produced are applicable only to the model trained using that specific training dataset. Since these features are not reusable, there is no need to store them in a feature store. Also, storing encoded features in a feature store leads to write amplification, as every time feature values are written to a feature group, all existing rows in the feature group have to be re-encoded (and creation of a training dataset using a subset or rows in the feature group becomes impossible as they cannot be re-encoded).</p> <p>On-demand transformations are exclusive to real-time AI systems, where predictions must be generated in real time based on incoming prediction requests. On-demand transformations compute on-demand features, which usually require at least one input parameter that is only available in a prediction request for their computation. These transformations can also combine request-time parameters with precomputed features from feature stores. Some examples include generating zip_codes from latitude and longitude received in the prediction request or calculating the time_since_last_transaction from a transaction request. The on-demand features produced can also be computed and backfilled into a feature store when the necessary historical data required for their computation becomes available. Backfilling on-demand features into the feature store eliminates the need to recompute them when creating training data. On-demand transformations are typically also model-independent transformations (model-dependent transformations can be applied after the on-demand transformation).</p> <p>Each of these transformations is employed within specific areas in a modular AI system and can be illustrated using the figure below. </p> <p>Model-independent transformations are utilized exclusively in areas where new and historical data arrives, typically within feature pipelines. Model-dependent transformations are necessary during the creation of training data, in training programs and must also be consistently applied in inference programs prior to making predictions. On-demand transformations are primarily employed in online inference programs, though they can also be integrated into feature engineering programs to backfill data into the feature store.</p> <p>The presence of model-dependent and on-demand transformations across different modules in a modular AI system introduces the potential for online-offline skew. Hopsworks provides support for  model-dependent transformations and on-demand transformations to easily create modular skew-free AI pipelines.</p>"},{"location":"concepts/mlops/data_transformations/#hopsworks-and-the-data-transformation-taxonomy","title":"Hopsworks and the Data Transformation Taxonomy","text":"<p>In Hopsworks, an AI system is typically decomposed into different AI pipelines and usually falls into either a feature pipeline, a training pipeline, or an inference pipeline.</p> <p>Hopsworks stores reusable feature data, created by model-independent transformations within the feature pipeline, into feature groups (tables containing feature data in both offline and online stores). Model-independent transformations in Hopsworks can be performed using a wide range of commonly used data engineering tools and the generated features can be seamlessly inserted into feature groups. The figure below illustrates the different software tools supported by Hopsworks for creating reusable features through model-independent transformations.</p> <p></p> <p>Additionally, Hopsworks provides a simple Python API to create custom transformation functions as either Python or Pandas User-Defined Functions (UDFs). Pandas UDFs enable the vectorized execution of transformation functions, offering significantly higher throughput compared to Python UDFs for large volumes of data. They can also be scaled out across workers in a Spark program, allowing for scalability from gigabytes (GBs) to terabytes (TBs) or more. However, Python UDFs can be much faster for small volumes of data, such as in the case of online inference.</p> <p>Transformation functions defined in Hopsworks can then be attached to feature groups to create on-demand transformation. On-demand transformations in feature groups are executed automatically whenever data is inserted into them to compute and backfill the on-demand features into the feature group. Backfilling on-demand features removes the need to recompute them while creating training and batch data.</p> <p>Hopsworks also provides a powerful abstraction known as feature views, which enables feature reuse and prevents skew between training and inference pipelines. A feature view is a meta-data-only selection of features, created from potentially different feature groups. It includes the input and output schema required for a model. This means that a feature view describes not only the input features but also the output targets, along with any helper columns necessary for training or inference of the model. This allows feature views to create consistent snapshots of data for both training and inference of a model. Additionally feature views, also compute and save statistics for the training datasets they create.</p> <p>Hopsworks supports attaching transformations functions to feature views to create model-dependent transformations that have no online-offline skew. These transformations get access to the same training dataset statistics during both training and inference ensuring their consistency. Additionally, feature views through lineage get access to the on-demand transformation used to create on-demand features if any are selected during the creation of the feature view. This allows for the computation of on-demand features in real-time during online-inference.</p>"},{"location":"concepts/mlops/opensearch/","title":"Vector Database","text":"<p>Hopsworks includes OpenSearch as a multi-tenant service in projects. OpenSearch provides vector database capabilities through its k-NN plugin, that supports the FAISS and nsmlib embedding indexes. Through Hopsworks, OpenSearch also provides enterprise capabilities, including authentication and access control to indexes (an index can be private to a Hopsworks project), filtering, scalability, high availability, and disaster recovery support. To learn how Opensearch empowers vector similar search in Hopsworks, you can see this guide.</p> <p></p>"},{"location":"concepts/mlops/prediction_services/","title":"Prediction Services","text":"<p>A prediction service is an end-to-end analytical or operational machine learning system that takes in data and outputs predictions that are consumed by users of the prediction service.</p> <p>A prediction service consists of the following components:</p> <ul> <li>feature pipeline(s),</li> <li>training pipeline,</li> <li>inference pipeline (for either batch predictions or online predictions)</li> <li>a sink for predictions - either a store or a user-interface.</li> </ul>"},{"location":"concepts/mlops/prediction_services/#analytical-ml","title":"Analytical ML","text":"<p>In the analytical ML figure below, we can see an analytical prediction service, where feature pipelines update the feature store with new feature data, running at some schedule (e.g., hourly, daily), and a batch program also runs on a schedule, reading batch scoring data from the feature store, computing predictions for that data with an embedded model, and writing those predictions to a database sink, from where the predictions are used for (predictive, prescriptive) analytical reports and/or to AI-enable operational services.</p> <p></p>"},{"location":"concepts/mlops/prediction_services/#operational-ml","title":"Operational ML","text":"<p>In the operational ML figure below, we can see an operational prediction service, where feature pipelines update the feature store with new feature data, running at some schedule (e.g., streaming, hourly, daily), and the operational service sends prediction requests to a model deployed on KServe via its secured Istio endpoint. A deployed model on KServer handles the prediction request by first retrieving pre-computed features from the feature store for the given request, and then building a feature vector that is scored by the model. The prediction result is returned to the client (the operational service). KServe logs both the feature values and the prediction results back to Hopsworks for further analysis and to help create new training data.</p> <p></p>"},{"location":"concepts/mlops/prediction_services/#mlops-flywheel","title":"MLOps Flywheel","text":"<p>Once you have built your analytical or operational ML system, the MLOps flywheel is the path to building a self-managing system that automatically collects and processes feature logs, prediction logs, and outcomes to help create new training data for models. This enables a ML flywheel where new training data and insights are generated from your operational or analytical ML service, by feeding logs back into the feature store. More training data enables the training of better models, and with better models, you should hopefully improve you operational/batch services, so that you attract more clients, who in turn produce more data for training models. And, thus, the ML flywheel is bootstrapped and leads to a virtuous cycle of more data leading to better models and more models leading to more users, who produce more data, and so on.</p> <p></p>"},{"location":"concepts/mlops/registry/","title":"Model Registry","text":"<p>Hopsworks Model Registry is designed with specific support for KServe and MLOps, through versioning. It enables developers to publish, test, monitor, govern and share models for collaboration with other teams. The model registry is where developers publish their models during the experimentation phase. The model registry can also be used to share models with the team and stakeholders.</p> <p>Like other project-based multi-tenant services in Hopsworks, a model registry is private to a project. That means you can easily add a development, staging, and production model registry to a cluster, and implement CI/CD processes for transitioning a model from development to staging to production.</p> <p>The model registry for KServe's capability are shown in the diagram below:</p> <p></p> <p>The model registry centralizes model management, enabling models to be securely accessed and governed. Models are more than just the model itself - the registry also stores sample data for testing, configuration information, provenance information, environment variables, links to the code used to generate the model, the model version, and tags/descriptions). When you save a model, you can also save model metrics with the model, enabling users to understand, for example, performance of the model on test (or unseen) data.</p>"},{"location":"concepts/mlops/registry/#model-package","title":"Model Package","text":"<p>A ML model consists of a number of different components in a model package:</p> <ul> <li>Model Input/Output Schema</li> <li>Model artifacts</li> <li>Model version information</li> <li>Model format (based on the ML framework used to train the model - e.g., .pkl or .tb files)</li> </ul> <p>You can also optionally include in your packaged model:</p> <ul> <li>Sample data (used to test the model  in KServe)</li> <li>The source notebook/program/experiment used to create the model</li> </ul>"},{"location":"concepts/mlops/serving/","title":"Model Serving","text":"<p>In Hopsworks, you can easily deploy models from the model registry in KServe or in Docker containers (for Hopsworks Community). KServe is the defacto open-source framework for model serving on Kubernetes. You can deploy models in either programs, using the HSML library, or in the UI. A KServe model deployment can include the following components:</p> <code>Transformer</code> <p>A pre-processing and post-processing component that can transform model inputs before predictions are made, and predictions before these are delivered back to the client.</p> <code>Predictor</code> <p>A predictor is a ML model in a Python object that takes a feature vector as input and returns a prediction as output.</p> <code>Inference Logger</code> <p>Hopsworks logs inputs and outputs of transformers and predictors to a Kafka topic that is part of the same project as the model.</p> <code>Inference Batcher</code> <p>Inference requests can be batched to improve throughput (at the cost of slightly higher latency).</p> <code>Istio Model Endpoint</code> <p>You can publish a model over REST(HTTP) or gRPC using a Hopsworks API key. API keys have scopes to ensure the principle of least privilege access control to resources managed by Hopsworks.</p> <p>Models deployed on KServe in Hopsworks can be easily integrated with the Hopsworks Feature Store using either a Transformer or Predictor Python script, that builds the predictor's input feature vector using the application input and pre-computed features from the Feature Store.</p> <p></p> <p>Model Serving Guide</p> <p>More information can be found in the Model Serving guide.</p>"},{"location":"concepts/mlops/training/","title":"Model Training","text":"<p>Hopsworks supports running model training pipelines on any Python environment, whether on an external Python client or on a Hopsworks cluster. The outputs of a training pipeline are typically experiment results, including logs, and possibly a trained model. You can plugin your own experimentation tracking platform or model registry, or you can use Hopsworks.</p>"},{"location":"concepts/mlops/training/#training-pipelines-on-hopsworks","title":"Training Pipelines on Hopsworks","text":"<p>If you train models with Hopsworks, you can setup CI/CD pipelines as shown below, where the experiments are tracked by Hopsworks, and any model created is published to a model registry. Each project has its own private model registry, so when you are working in a development project, you typically publish models to your project's private development registry, and if all model validation tests pass, and the model performance is good enough, the same training pipeline can be submitted via a CI/CD pipeline (e.g., GitHub push request) to a staging project, and the same procedure can be repeated to push the training pipeline to a production project.</p> <p></p> <p>Hopsworks Model Registry and Model Serving capabilities can then be used to build a batch or online prediction service using the model.</p>"},{"location":"concepts/projects/cicd/","title":"CI/CD","text":"<p>You can setup traditional development, staging, and production environment in Hopsworks using Projects. A project enables you provide access control for the different environments - just like a GitHub repository, owners of projects can add and remove members of projects and assign different roles to project members - the \"data owner\" role can write to feature store, while a \"data scientist\" can only read from the feature store and create training data.</p>"},{"location":"concepts/projects/cicd/#dev-staging-prod","title":"Dev, Staging, Prod","text":"<p>You can create dev, staging, and prod projects - either on the same cluster, but mostly commonly, with production on its own cluster:</p> <p></p>"},{"location":"concepts/projects/cicd/#versioning","title":"Versioning","text":"<p>Hopsworks supports the versioning of ML assets, including:</p> <ul> <li>Feature Groups: the version of its schema - breaking schema changes require a new version and backfilling the new version;</li> <li>Feature Views:  the version of its schema, and breaking schema changes only require a new version;</li> <li>Models: the version of a model;</li> <li>Deployments: the version of the deployment of a model - a model with the same version can be found in &gt;1 deployment.</li> </ul>"},{"location":"concepts/projects/cicd/#pytest-for-feature-logic-and-feature-pipeline-tests","title":"Pytest for feature logic and feature pipeline tests","text":"<p>Pytest and Great Expectations can be used for testing feature pipelines. Pytest is used to test feature logic and for end-to-end feature pipeline tests, while Great Expectations is used for data validation tests. Here, we can see how a feature pipeline test uses sample data to compute features and validate they have been written successfully, first to a development feature store, and then they can be pushed to a staging feature store, before finally being promoted to production. </p>"},{"location":"concepts/projects/governance/","title":"Governance","text":"<p>Hopsworks provides project-level multi-tenancy, a data mesh enabling technology. Think of it as a GitHub repository for your teams and ML assets. More specifically, a project is a sandbox for team members, ML assets (features, training data, models, vector database, model deployments), and optionally feature pipelines and training pipelines. The ML assets can only be accessed by project members, and there is role-based access control (RBAC) for project members within a project.</p> <p></p>"},{"location":"concepts/projects/governance/#devstagingprod-for-data","title":"Dev/Staging/Prod for Data","text":"<p>Projects enable you to define development, staging, and even production projects on the same cluster. Often, companies deploy production projects on dedicated clusters, but development projects and staging projects on a shared cluster. This way, projects can be easily used to implement CI/CD workflows.</p>"},{"location":"concepts/projects/governance/#data-mesh-of-feature-stores","title":"Data Mesh of Feature Stores","text":"<p>Projects enable you to move beyond the traditional dev/staging/prod ownership model for data. Different teams or lines of business can have their own private feature stores, you can mix them with a group-wide feature store, and feature stores can be securely shared between teams/organizations. Effectively, you can have decentralized ownership of feature stores, with domain-specific projects, and each project managing its own feature pipelines. Hopsworks provides data/feature sharing support between these self-service projects.</p>"},{"location":"concepts/projects/governance/#audit-logs-with-rest-api","title":"Audit Logs with REST API","text":"<p>Hopsworks stores audit logs for all calls on its REST API in its file system, HopsFS. The audit log can be used to analyze the historical usage of services by users.</p>"},{"location":"concepts/projects/search/","title":"Tags/Search/Lineage","text":""},{"location":"concepts/projects/search/#search","title":"Search","text":"<p>Hopsworks supports free-text search to discover machine-learning assets:</p> <ul> <li>features</li> <li>feature groups</li> <li>feature views</li> <li>training data</li> </ul> <p>You can use the search bar at the top of your project to free-text search for the names or descriptions of any ML asset. You can also search using keywords or tags that are attached to an ML asset.</p> <p>You can search for assets within a specific project or across all projects in a Hopsworks deployment, including those you are not a member of. This allows for easier discoverability and reusability of assets within an organization. To avoid users gaining unauthorized access to data, if a search result is in a project you are not a member of, the information displayed is limited to: names, descriptions, tags, asset creator and create date. If the search result is within a project you are a member of, you are also able to inspect recent activities on the asset as well as statistics.</p>"},{"location":"concepts/projects/search/#tags","title":"Tags","text":"<p>A keyword is a single user-defined word attached to an ML asset. Keywords can be used to help it make it easier to find ML assets or understand the context in which they should be used, for example, PII could be used to indicate that the ML asset is based on personally identifiable information.</p> <p>However, it may be preferable to have a stronger governance framework for ML assets than keywords alone. For this, you can define a schematized tag, defining a list of key/value tags along with a type for a value. In the figure below, you can see an example of a schematized tag with two key/value pairs: pii of type boolean (indicating if this feature group contains PII data), and owner of type string (indicating who the owner of the data in this feature group is). Note there is also a keyword defined for this feature group called eu_region, indicating the data has its origins in the EU.</p> <p></p>"},{"location":"concepts/projects/search/#lineage","title":"Lineage","text":"<p>Hopsworks tracks the lineage (or provenance) of ML assets automatically for you. You can see what features are used in which feature view or training dataset. You can see what training dataset was used to train a given model. For assets that are managed outside of Hopsworks, there is support for the explicit definition of lineage dependencies.</p> <p></p>"},{"location":"concepts/projects/storage/","title":"Data Storage/Sharing","text":"<p>Every project in Hopsworks has its own private assets:</p> <ul> <li>a Feature Store (including both Online and Offline Stores)</li> <li>a Filesystem subtree (all directory and files under /Projects//) <li>a Model Registry</li> <li>Model Deployments</li> <li>Kafka topics</li> <li>OpenSearch indexes (including KNN indexes - the vector DB)</li> <li>a Hive Database</li> <p>Access control to these assets is controlled using project membership ACLs (access-control lists). Users in a project who have a Data Owner role have read/write access to these assets.  Users in a project who have a Data Scientist role have mostly read-only access to these assets, with the exception of the ability to write to well-known directories (Resources, Jupyter, Logs).</p> <p>However, it is often desirable to share assets between projects, with read-only, read/write privileges, and to restrict the privileges to specific role (e.g., Data Owners) in the target project. In Hopsworks, you can explicitly share assets between projects without copying the assets. Sharing is managed by ACLs in Hopsworks, see example below: </p>"},{"location":"setup_installation/","title":"Setup and Administration","text":"<p>This section contains installation guides for the Hopsworks Platform using kubernetes, on</p> <ul> <li>AWS</li> <li>Azure</li> <li>GCP</li> <li>On-Prem environments</li> </ul> <p>and common administration instructions.</p> <p>For instructions on installing the Hopsworks Client libraries, see the Client Installation guide.</p>"},{"location":"setup_installation/admin/","title":"Cluster Administration","text":"<p>Hopsworks has a cluster management page that allows you, the administrator, to perform management actions, monitor and control Hopsworks.</p> <p>To access the cluster management page you should log in into Hopsworks using your administrator account. In the top right corner, click on your name in the top right corner of the navigation bar and choose Cluster Settings from the dropdown menu.</p>"},{"location":"setup_installation/admin/alert/","title":"Configure Alerts","text":""},{"location":"setup_installation/admin/alert/#introduction","title":"Introduction","text":"<p>Alerts are sent from Hopsworks using Prometheus' Alert manager. In order to send alerts we first need to configure the Alert manager.</p>"},{"location":"setup_installation/admin/alert/#prerequisites","title":"Prerequisites","text":"<p>Administrator account on a Hopsworks cluster.</p>"},{"location":"setup_installation/admin/alert/#step-1-go-to-alerts-configuration","title":"Step 1: Go to alerts configuration","text":"<p>To configure the Alert manager click on your name in the top right corner of the navigation bar and choose Cluster Settings from the dropdown menu. In the Cluster Settings' Alerts tab you can configure the alert manager to send alerts via email, slack or pagerduty.</p> Configure alerts"},{"location":"setup_installation/admin/alert/#step-2-configure-email-alerts","title":"Step 2: Configure Email Alerts","text":"<p>To send alerts via email you need to configure an SMTP server. Click on the Configure button on the left side of the email row and fill out the form that pops up.</p> Configure Email Alerts <ul> <li>Default from: the address used as sender in the alert email.</li> <li>SMTP smarthost: the Simple Mail Transfer Protocol (SMTP) host through which emails are sent.</li> <li>Default hostname (optional): hostname to identify to the SMTP server.</li> <li>Authentication method: how to authenticate to the SMTP server.   CRAM-MD5, LOGIN or PLAIN.</li> </ul> <p>Optionally cluster wide Email alert receivers can be added in Default receiver emails. These receivers will be available to all users when they create event triggered alerts.</p>"},{"location":"setup_installation/admin/alert/#step-3-configure-slack-alerts","title":"Step 3: Configure Slack Alerts","text":"<p>Alerts can also be sent via Slack messages. To be able to send Slack messages you first need to configure a Slack webhook. Click on the Configure button on the left side of the slack row and past in your Slack webhook in Webhook.</p> Configure slack Alerts <p>Optionally cluster wide Slack alert receivers can be added in Slack channel/user. These receivers will be available to all users when they create event triggered alerts.</p>"},{"location":"setup_installation/admin/alert/#step-4-configure-pagerduty-alerts","title":"Step 4: Configure Pagerduty Alerts","text":"<p>Pagerduty is another way you can send alerts from Hopsworks. Click on the Configure button on the left side of the pagerduty row and fill out the form that pops up.</p> Configure Pagerduty Alerts <p>Fill in Pagerduty URL: the URL to send API requests to.</p> <p>Optionally cluster wide Pagerduty alert receivers can be added in Service key/Routing key. By first choosing the PagerDuty integration type:</p> <ul> <li>global event routing (routing_key): when using PagerDuty integration type <code>Events API v2</code>.</li> <li>service (service_key): when using PagerDuty integration type <code>Prometheus</code>.</li> </ul> <p>Then adding the Service key/Routing key of the receiver(s). PagerDuty provides documentation on how to integrate with Prometheus' Alert manager.</p>"},{"location":"setup_installation/admin/alert/#step-5-configure-webhook-alerts","title":"Step 5: Configure Webhook Alerts","text":"<p>You can also use webhooks to send alerts. A Webhook Alert is sent as an HTTP POST command with a JSON-encoded parameter payload. Click on the Configure button on the left side of the webhook row and fill out the form that pops up.</p> Configure Webhook Alerts <p>Fill in the unique URL of your Webhook: the endpoint to send HTTP POST requests to.</p> <p>A global receiver is created when a webhook is configured and can be used by any project in the cluster.</p>"},{"location":"setup_installation/admin/alert/#step-6-advanced-configuration","title":"Step 6: Advanced configuration","text":"<p>If you are familiar with Prometheus' Alert manager you can also configure alerts by editing the yaml/json file directly by going to the advaced page and clicking the edit button.</p> <p>The advanced page shows the configuration currently loaded on the alert manager. After editing the configuration it takes some time to propagate changes to the alertmanager.</p> <p>The reload button can be used to validate the changes made to the configuration. It will try to load the new configuration to the alertmanager and show any errors that might prevent the configuration from being loaded.</p> Advanced configuration <p>Warning</p> <p>If you make any changes to the configuration ensure that the changes are valid by reloading the configuration until the changes are loaded and visible in the advanced page.</p> <p>Example: Adding the yaml snippet shown below in the global section of the alert manager configuration will have the same effect as creating the SMTP configuration as shown in section 1 above.</p> <pre><code>global:\n    smtp_smarthost: smtp.gmail.com:587\n    smtp_from: hopsworks@gmail.com\n    smtp_auth_username: hopsworks@gmail.com\n    smtp_auth_password: XXXXXXXXX\n    smtp_auth_identity: hopsworks@gmail.com\n ...\n</code></pre> <p>To test the alerts by creating triggers from Jobs and Feature group validations see Alerts.</p> <p>The yaml syntax in the UI is slightly different in that it does not allow double quotes (it will ignore the values but give no error). Below is an example configuration, that can be used in the UI, with both email and slack receivers configured for system alerts.</p> <pre><code>global:\n    smtp_smarthost: smtp.gmail.com:587\n    smtp_from: hopsworks@gmail.com\n    smtp_auth_username: hopsworks@gmail.com\n    smtp_auth_password: XXXXXXXXX\n    smtp_auth_identity: hopsworks@gmail.com\n    resolveTimeout: 5m\ntemplates:\n  - /srv/hops/alertmanager/alertmanager-0.17.0.linux-amd64/template/*.tmpl\nroute:\n  receiver: default\n  routes:\n    - receiver: email\n      continue: true\n      match:\n        type: system-alert\n    - receiver: slack\n      continue: true\n      match:\n        type: system-alert\n  groupBy:\n    - alertname\n  groupWait: 10s\n  groupInterval: 10s\nreceivers:\n  - name: default\n  - name: email\n    emailConfigs:\n      - to: someone@logicalclocks.com\n        from: hopsworks@logicalclocks.com\n        smarthost: mail.hello.com\n        text: &gt;-\n          summary: {{ .CommonAnnotations.summary }} description: {{\n          .CommonAnnotations.description }}\n  - name: slack\n    slackConfigs:\n      - apiUrl: &gt;-\n          https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX\n        channel: '#general'\n        text: &gt;-\n          &lt;!channel&gt; summary: {{ .Annotations.summary }} description: {{\n          .Annotations.description }}\n</code></pre>"},{"location":"setup_installation/admin/auth/","title":"Authentication Methods","text":""},{"location":"setup_installation/admin/auth/#introduction","title":"Introduction","text":"<p>Hopsworks can be configured to use different types of authentication methods. In this guide we will look at the different authentication methods available in Hopsworks.</p>"},{"location":"setup_installation/admin/auth/#prerequisites","title":"Prerequisites","text":"<p>Administrator account on a Hopsworks cluster.</p>"},{"location":"setup_installation/admin/auth/#step-1-go-to-authentication-methods-page","title":"Step 1: Go to Authentication methods page","text":"<p>To configure Authentication methods click on your name in the top right corner of the navigation bar and choose Cluster Settings from the dropdown menu.</p>"},{"location":"setup_installation/admin/auth/#step-2-configure-authentication-methods","title":"Step 2: Configure Authentication methods","text":"<p>In the Cluster Settings Authentication tab you can configure how users authenticate.</p> <ol> <li>TOTP Two-factor Authentication: can be disabled, optional or mandatory.    If set to mandatory all users are required to set up two-factor authentication when registering.</li> </ol> <p>!!! note        If two-factor is set to mandatory on a cluster with preexisting users all users will need to go through lost device recovery step to enable two-factor.        So consider setting it to optional first and allow users to enable it before setting it to mandatory.</p> <ol> <li>OAuth2: if your organization already have an identity management system compatible with OpenID Connect (OIDC) you can configure Hopsworks to use your identity provider by enabling OAuth as shown in the figure below.    After enabling OAuth you can register your identity provider by clicking on Add Identity Provider button.    See Create client for details.</li> <li>LDAP/Kerberos: if your organization is using LDAP or Kerberos to manage users and services you can configure Hopsworks to use it as the user management system.    You can enable LDAP/Kerberos by clicking on the checkbox, as shown in the figure below, and choosing LDAP or Kerberos.    For more information on how to configure LDAP and Kerberos see Configure LDAP and Configure Kerberos.</li> </ol> Setup Authentication Methods <p>In the figure above we see a cluster with Two-factor authentication disabled, OAuth enabled with one registered identity provider and LDAP authentication enabled.</p>"},{"location":"setup_installation/admin/configure-project-mapping/","title":"Configure group to project mapping","text":""},{"location":"setup_installation/admin/configure-project-mapping/#introduction","title":"Introduction","text":"<p>A group-to-project mapping lets you automatically add all members of a Hopsworks group to a project, eliminating the need to add each user individually. To create a mapping, you simply select a Hopsworks group, choose the project it should be linked to, and assign the role that its members will have within that project.</p> <p>Once a mapping is created, project membership is controlled through Hopsworks group membership. Any updates made to the Hopsworks group\u2014such as adding or removing users\u2014will automatically be reflected in the project membership. For example, if a user is removed from the Hopsworks group, they will also be removed from the corresponding project.</p>"},{"location":"setup_installation/admin/configure-project-mapping/#prerequisites","title":"Prerequisites","text":"<ol> <li>Hopsworks group mapping sync enabled. This can be done by setting the variable <code>hw_group_mapping_sync_enabled=true</code>. See Cluster Configuration on how to change variable values in Hopsworks.</li> </ol> Enable Hopsworks mapping <p>If you can not find the variable <code>hw_group_mapping_sync_enabled</code> create it by clicking on New variable.</p> Create Hopsworks group mapping enabled variable"},{"location":"setup_installation/admin/configure-project-mapping/#step-1-create-a-mapping","title":"Step 1: Create a mapping","text":"<p>To create a mapping go to Cluster Settings by clicking on your name in the top right corner of the navigation bar and choosing Cluster Settings from the dropdown menu. In the Project mapping tab, you can create a new mapping by clicking on Create new mapping.</p> Project mapping <p>This will take you to the create mapping page shown below</p> Create mapping <p>Here you can enter your Hopsworks group and map it to a project from the Project drop down list. You can also choose the Project role users will be assigned when they are added to the project.</p> <p>Finally, click on Create mapping and go back to mappings. You should see the newly created mapping(s) as shown below.</p> Project mappings"},{"location":"setup_installation/admin/configure-project-mapping/#step-2-edit-a-mapping","title":"Step 2: Edit a mapping","text":"<p>From the list of mappings click on the edit button (). This will open a popup that will allow you to change the group, project name, and project role of a mapping.</p> Edit mapping <p>Warning</p> <p>Updating a mapping's group or project name will remove all members of the previous group from the project.</p>"},{"location":"setup_installation/admin/configure-project-mapping/#step-3-delete-a-mapping","title":"Step 3: Delete a mapping","text":"<p>To delete a mapping click on the delete button.</p> <p>Warning</p> <p>Deleting a mapping will remove all members of that group from the project.</p>"},{"location":"setup_installation/admin/project/","title":"Manage Projects","text":"<p>Hopsworks provides an administrator with a view of the projects in a Hopsworks cluster.</p> <p>A Hopsworks administrator is not automatically a member of all the projects in a cluster. However, they can see which projects exist, who is the project owner, and they can limit the storage quota and compute quota for each project.</p>"},{"location":"setup_installation/admin/project/#prerequisites","title":"Prerequisites","text":"<p>You need to be an administrator on a Hopsworks cluster.</p>"},{"location":"setup_installation/admin/project/#changing-project-quotas","title":"Changing project quotas","text":"<p>You can find the Project management page by clicking on your name, in the top right corner of the navigation bar, and choosing Cluster Settings from the dropdown menu and going to the Project tab.</p> Project page <p>This page will list all the projects in a cluster, their name, owner and when its quota was last updated. By clicking on the edit configuration link of a project you will be able to edit the quotas of that project.</p> Project quotas"},{"location":"setup_installation/admin/project/#storage","title":"Storage","text":"<p>Storage quota represents the amount of data a project can store. The storage quota is broken down in three different areas:</p> <ul> <li>Feature Store: This represents the storage quota for files and directories stored in the <code>_featurestore.db</code> dataset in the project. This dataset contains all the feature group offline data for the project.</li> <li>Hive DB: This represents the storage quota for files and directories stored in the <code>[projectName].db</code> dataset in the project. This is a general purpose Hive database for the project that can be used for analytics.</li> <li>Project: This represents the storage quota for all the data stored on any other dataset.</li> </ul> <p>Each storage quota is divided into space quota, i.e., how much space the files can consume, and namespace quota, i.e., how many files and directories there can be. If Hopsworks is deployed on-premise using hard drives to store the data, i.e., Hopsworks is not configured to store its data in a S3-compliant storage system, the data is replicated across multiple nodes (by default 3) and the space quota takes the replication factor into consideration. As an example, a 100MB file stored with a replication factor of 3, will consume 300MB of space quota.</p> <p>By default, all storage quotas are disabled and not enforced. Administrators can change this default by changing the following configuration in the Configuration UI and/or the cluster definition:</p> <pre><code>hopsworks:\n    featurestore_default_quota: [default quota in bytes, -1 to disable]\n    hdfs_default_quota: [default quota in bytes, -1 to disable]\n    hive_default_quota: [default quota in bytes, -1 to disable]\n</code></pre> <p>The values specified will be set during project creation and administrators will be able to customize each project using this UI.</p>"},{"location":"setup_installation/admin/project/#compute","title":"Compute","text":"<p>Compute quotas represents the amount of compute a project can use to run Spark and Flink applications as well as Tez queries. Quota is expressed as number of seconds a container of size 1 CPU and 1GB of RAM can run for.</p> <p>If the Hopsworks cluster is connected to a Kubernetes cluster, Python jobs, Jupyter notebooks and KServe models are not subject to the compute quota. Currently, Hopsworks does not support defining quotas for compute scheduled on the connected Kubernetes cluster.</p> <p>By default, the compute quota is disabled. Administrators can change this default by changing the following configuration in the Configuration UI and/or the cluster definition:</p> <pre><code>hopsworks:\n    yarn_default_payment_type: [NOLIMIT to disable the quota, PREPAID to enable it]\n    yarn_default_quota: [default quota in seconds]\n</code></pre> <p>The values specified will be set during project creation and administrators will be able to customize each project using this UI.</p>"},{"location":"setup_installation/admin/project/#kafka-topics","title":"Kafka Topics","text":"<p>Kafka is used within Hopsworks to enable users to write data to the feature store in Real-Time and from a variety of different frameworks. If a user creates a feature group with the stream APIs enabled, then a Kafka topic will be created for that feature group. By default, a project can have up to 100 Kafka topics. Administrators can increase the number of Kafka topics a project is allowed to create by increasing the quota in the project admin UI.</p>"},{"location":"setup_installation/admin/project/#force-deleting-a-project","title":"Force deleting a project","text":"<p>Administrators have the option to force delete a project. This is useful if the project was not created or deleted properly, e.g., because of an error.</p>"},{"location":"setup_installation/admin/project/#controlling-who-can-create-projects","title":"Controlling who can create projects","text":"<p>Every user on Hopsworks can create projects. By default, each user can create up to 10 projects. For production environments, the number of projects should be limited and controlled for resource allocation purposes as well as closer control over the data. Administrators can control how many projects a user can provision by setting the following configuration in the Configuration UI and/or cluster definition:</p> <pre><code>hopsworks:\n    max_num_proj_per_user: [Maximum number of projects each user can create]\n</code></pre> <p>This value will be set when the user is provisioned. Administrators can grant additional projects to a specific user through the User Administration UI.</p>"},{"location":"setup_installation/admin/roleChaining/","title":"AWS IAM Role Chaining","text":""},{"location":"setup_installation/admin/roleChaining/#introduction","title":"Introduction","text":"<p>When running Hopsworks in Amazon EKS you have several options to give the Hopsworks user access to AWS resources. The simplest is to assign Amazon EKS node IAM role access to the resources. But, this will make these resources accessible by all users. To manage access to resources on a project base you need to use Role chaining.</p> <p>In this document we will see how to configure AWS and Hopsworks to use Role chaining in your Hopsworks projects.</p>"},{"location":"setup_installation/admin/roleChaining/#prerequisites","title":"Prerequisites","text":"<p>Before you begin this guide you'll need the following:</p> <ul> <li>A Hopsworks cluster running on EKS.</li> <li>Enabled IAM OpenID Connect (OIDC) provider for your cluster.</li> <li>Administrator account on the Hopsworks cluster.</li> </ul>"},{"location":"setup_installation/admin/roleChaining/#step-1-create-an-iam-role-and-associate-it-with-a-kubernetes-service-account","title":"Step 1: Create an IAM role and associate it with a Kubernetes service account","text":"<p>To use role chaining the hopsworks instance pods need to be able to impersonate the roles you want to be linked to your project. For this you need to create an IAM role and associate it with your Kubernetes service accounts with assume role permissions and attach it to your hopsworks instance pods. For more details on how to create an IAM roles for Kubernetes service accounts see the aws documentation.</p> <p>Note</p> <p>To ensure that users can't use the service account role and impersonate the roles by their own means, you need to ensure that the service account is only attached to the hopsworks instance pods.</p> <pre><code>account_id=$(aws sts get-caller-identity --query \"Account\" --output text)\noidc_provider=$(aws eks describe-cluster --name my-cluster --region $AWS_REGION --query \"cluster.identity.oidc.issuer\" --output text | sed -e \"s/^https:\\/\\///\")\n</code></pre> <pre><code>export namespace=hopsworks\nexport service_account=my-service-account\n</code></pre> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Federated\": \"arn:aws:iam::$account_id:oidc-provider/$oidc_provider\"\n      },\n      \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n      \"Condition\": {\n        \"StringEquals\": {\n          \"$oidc_provider:aud\": \"sts.amazonaws.com\",\n          \"$oidc_provider:sub\": \"system:serviceaccount:$namespace:$service_account\"\n        }\n      }\n    }\n  ]\n}\n</code></pre> Example trust policy for a service account. <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AssumeDataRoles\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"sts:AssumeRole\",\n            \"Resource\": [\n                \"arn:aws:iam::123456789011:role/my-role\",\n                \"arn:aws:iam::xxxxxxxxxxxx:role/s3-role\",\n                \"arn:aws:iam::xxxxxxxxxxxx:role/dev-s3-role\",\n                \"arn:aws:iam::xxxxxxxxxxxx:role/redshift\"\n            ]\n        }\n    ]\n}\n</code></pre> Example policy for assuming four roles. <p>The IAM role will need to add a trust policy to allow the service account to assume the role, and permissions to assume the different roles that will be used to access resources.</p> <p>To associate the IAM role with your Kubernetes service account you will need to annotate your service account with the Amazon Resource Name (ARN) of the IAM role that you want the service account to assume.</p> <pre><code>kubectl annotate serviceaccount -n $namespace $service_account eks.amazonaws.com/role-arn=arn:aws:iam::$account_id:role/my-role\n</code></pre>"},{"location":"setup_installation/admin/roleChaining/#step-2-create-the-resource-roles","title":"Step 2: Create the resource roles","text":"<p>For the service account role to be able to impersonate the roles you also need to configure the roles themselves to allow it. This is done by adding the service account role to the role's Trust relationships.</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n        \"Effect\": \"Allow\",\n        \"Principal\": {\n            \"AWS\": \"arn:aws:iam::xxxxxxxxxxxx:role/service-account-role\"\n        },\n        \"Action\": \"sts:AssumeRole\"\n        }\n    ]\n}\n</code></pre> Example resource roles."},{"location":"setup_installation/admin/roleChaining/#step-3-create-mappings","title":"Step 3: Create mappings","text":"<p>Now that the service account IAM role can assume the roles we need to configure Hopsworks to delegate access to the roles on a project base.</p> <p>In Hopsworks, click on your name in the top right corner of the navigation bar and choose Cluster Settings from the dropdown menu. In the Cluster Settings' IAM Role Chaining tab you can configure the mappings between projects and IAM roles.</p> Role Chaining <p>Add mappings by clicking on New role chaining. Enter the project name. Select the type of user that can assume the role. Enter the role ARN. And click on Create new role chaining</p> Create Role Chaining <p>Project member can now create connectors using temporary credentials to assume the role you configured. More details about using temporary credentials can be found in the Temporary Credentials section of the S3 datasource creation guide.</p> <p>Project member can see the list of role they can assume by going the Project Settings -&gt; Assuming IAM Roles page.</p>"},{"location":"setup_installation/admin/user/","title":"User Management","text":""},{"location":"setup_installation/admin/user/#introduction","title":"Introduction","text":"<p>Whether you run Hopsworks on-premise, or on the cloud using kubernetes, you have a Hopsworks cluster which contains all users and projects.</p>"},{"location":"setup_installation/admin/user/#prerequisites","title":"Prerequisites","text":"<p>Administrator account on a Hopsworks cluster.</p>"},{"location":"setup_installation/admin/user/#step-1-go-to-user-management","title":"Step 1: Go to user management","text":"<p>All the users of your Hopsworks instance have access to your cluster with different access rights. You can find them by clicking on your name in the top right corner of the navigation bar and choosing Cluster Settings from the dropdown menu and going to the Users tab (You need to have Admin role to get access to the Cluster Settings page).</p> Active Users"},{"location":"setup_installation/admin/user/#step-2-manage-user-roles","title":"Step 2: Manage user roles","text":"<p>Roles let you manage the access rights of a user to the cluster.</p> <ul> <li>User: users with this role are only allowed to use the cluster by creating a limited number of projects.</li> <li>Admin: users with this role are allowed to manage the cluster.   This includes accepting new users to the cluster or blocking them, managing user quota, configure alerts and setting up authentication methods.</li> </ul> <p>You can change the role of a user by clicking on the select dropdown that shows the current role of the user.</p>"},{"location":"setup_installation/admin/user/#step-3-validating-and-blocking-users","title":"Step 3: Validating and blocking users","text":"<p>By default, a user who register on Hopsworks using their own credentials are not granted access to the cluster. First, a user with an admin role needs to validate their account.</p> <p>By clicking on the Review Requests button you can open a user request review popup as shown in the image below.</p> Review user request <p>On the user request review popup you can activate or block users. Users with a validated email address will have a check mark on their email.</p> <p>Similarly, if a user is no longer allowed access to the cluster you can block them. To keep consistency with the history of your datasets, a user can not be deleted but only blocked. If necessary a user can be deleted manually in the cluster using the command line. You can block a user by clicking on the block icon on the right side of the user in the list.</p> Blocked Users <p>Blocked users will appear on the lower section of the page. Click on display blocked users to show all the blocked users in your cluster. If a user is blocked by mistake you can reactivate it by clicking on the check mark icon that corresponds to that user in the blocked users list.</p> <p>If there are too many users in your cluster, use the search box (available for blocked users too) to filter users by name or email. It is also possible to filter activated users by role. For example to see all administrators in you cluster click on the select dropdown to the right of the search box and choose Admin.</p>"},{"location":"setup_installation/admin/user/#step-4-create-a-new-users","title":"Step 4: Create a new users","text":"<p>If you want to allow users to login without registering you can pre-create them by clicking on New user.</p> Create new user <p>After setting the user's name and email chose the type of user you want to create (Hopsworks, Kerberos or LDAP). To create a Kerberos or LDAP user you need to get the users UUID from the Kerberos or LDAP server. Hopsworks user can also be assigned a Role. Kerberos and LDAP users on the other hand can only be assigned a role through group mapping.</p> <p>A temporary password will be generated and displayed when you click on Create new user. Copy the password and pass it securely to the user.</p> Copy temporary password"},{"location":"setup_installation/admin/user/#step-5-reset-user-password","title":"Step 5: Reset user password","text":"<p>In the case where a user loses her/his password and can not recover it with the password recovery, an administrator can reset it for them.</p> <p>On the bottom of the Users page click on the Reset a user password link. A popup window with a dropdown for searching users by name or email will open. Find the user and click on Reset new password.</p> Reset user password <p>A temporary password will be displayed. Copy the password and pass it to the user securely.</p> Copy temporary password <p>A user with a temporary password will see a warning message when going to Account settings Authentication tab.</p> Change password <p>Note</p> <p>A temporary password should be changed as soon as possible.</p>"},{"location":"setup_installation/admin/variables/","title":"Cluster Configuration","text":""},{"location":"setup_installation/admin/variables/#introduction","title":"Introduction","text":"<p>Whether you run Hopsworks on-premise, or on the cloud using kubernetes, it is possible to change a variety of configurations on the cluster, changing its default behaviour. This section is not going into detail for every setting, since every Hopsworks cluster comes with a robust default setup. However, this guide is to explain where to find the configurations and if necessary, how to change them.</p> <p>Note</p> <p>In most cases you will be only be prompted to change these configurations by a Hopsworks Solutions Engineer or similar.</p>"},{"location":"setup_installation/admin/variables/#prerequisites","title":"Prerequisites","text":"<p>An administrator account on a Hopsworks cluster.</p>"},{"location":"setup_installation/admin/variables/#step-1-the-configuration-page","title":"Step 1: The configuration page","text":"<p>You can find the configuration page by navigating in the UI:</p> <ol> <li>Click on your user name in the top right corner, then select Cluster Settings.</li> <li>Among the cluster settings, you will find a tab Configuration</li> </ol> Configuration settings"},{"location":"setup_installation/admin/variables/#step-2-editing-existing-configurations","title":"Step 2: Editing existing configurations","text":"<p>To edit an existing configuration, simply find the property using the search field, then click the edit button to change the value of the setting or its visibility. Once you have made the change, don't forget to click save to persist the changes.</p>"},{"location":"setup_installation/admin/variables/#visibility","title":"Visibility","text":"<p>The visibility setting indicates whether a setting can be read only by Hops Admins or also by simple Hops Users, that is everyone. Additionally, you can also allow to read the setting even when not authenticated. If the setting contains a password or sensitive information, you can also hide the value so it's not shown in the UI.</p>"},{"location":"setup_installation/admin/variables/#step-3-adding-a-new-configuration","title":"Step 3: Adding a new configuration","text":"<p>In rare cases it might be necessary to add additional configurations.</p> <p>To do so, click on New Variable, where you can then configure the new setting with a key, value and visibility. Once you have set the desired properties, you can persist them by clicking Create Configuration</p> Adding a new configuration property"},{"location":"setup_installation/admin/audit/audit-logs/","title":"Access Audit Logs","text":""},{"location":"setup_installation/admin/audit/audit-logs/#introduction","title":"Introduction","text":"<p>Hopsworks collects audit logs on all URL requests to the application server. These logs are saved in Payara log directory under <code>&lt;payara-log-dir&gt;/audit</code> by default.</p>"},{"location":"setup_installation/admin/audit/audit-logs/#prerequisites","title":"Prerequisites","text":"<p>In order to access the audit logs you need the following:</p> <ul> <li>Administrator account on the Hopsworks cluster.</li> <li>SSH access to the Hopsworks cluster with a user in the <code>glassfish</code> group.</li> </ul>"},{"location":"setup_installation/admin/audit/audit-logs/#step-1-configure-audit-logs","title":"Step 1: Configure Audit logs","text":"<p>Audit logs can be configured from the Cluster Settings Configuration tab. You can access the Configuration page of your Hopsworks cluster by clicking on your name, in the top right corner, and choosing Cluster Settings from the dropdown menu.</p> Audit log configuration <p>Type audit in the search box to see the configuration variables associated with audit logs. To edit a configuration variable, you can click on the edit button (), insert the new value and save changes clicking on the check mark ().</p> <p>Audit logs configuration variables</p> Name Description audit_log_count the number of files to keep when rotating logs (java.util.logging.FileHandler count) audit_log_file_format log file name pattern. (java.util.logging.FileHandler.pattern) audit_log_file_type the output format of the log file. Can be one of java.util.logging.SimpleFormatter (default), io.hops.hopsworks.audit.helper.JSONLogFormatter, or io.hops.hopsworks.audit.helper.HtmlLogFormatter. audit_log_size_limit the maximum number of bytes to write to any one file. (java.util.logging.FileHandler.limit) audit_log_date_format if io.hops.hopsworks.audit.helper.JSONLogFormatter is used as audit log file type, this will set the date format of the output JSON. The format should be java.text.SimpleDateFormat compatible string. <p>Warning</p> <p>Hopsworks application needs to be reloaded for any changes to be applied. For doing that, go to the Payara admin panel (<code>https://&lt;your-domain&gt;:4848</code>), click on Applications on the side menu and reload the hopsworks-ear application.</p>"},{"location":"setup_installation/admin/audit/audit-logs/#step-2-access-the-logs","title":"Step 2: Access the Logs","text":"<p>To access the audit logs, SSH into the instance pod of your Hopsworks cluster and navigate to the path <code>/opt/payara/appserver/glassfish/nodes/&lt;node name&gt;/&lt;instance name&gt;/logs/audit</code>.</p> <p>Audit logs follow the format set in the audit_log_file_type configuration variable.</p> <p>Example of audit logs using JSONLogFormatter</p> <pre><code>{\"className\":\"io.hops.hopsworks.api.user.AuthService\",\"methodName\":\"login\",\"parameters\":\"[admin@hopsworks.ai, org.apache.catalina.connector.ResponseFacade@2de6dd0b, org.apache.catalina.connector.RequestFacade@7a82f674]\",\"outcome\":\"200\",\"caller\":{\"username\":null,\"email\":\"admin@hopsworks.ai\",\"userId\":null},\"clientIp\":\"10.0.2.2\",\"userAgent\":\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36\",\"pathInfo\":\"/auth/login\",\"dateTime\":\"2022-11-09 12:00:08\"}\n</code></pre> <p>Regardless the format, each line in the audit logs can contain the following variables:</p> <p>Audit log variables</p> Name Description className the class called by the request methodName the method called by the request parameters parameters sent from the client outcome response code sent from the server caller the logged in user that made the request. Can be username, email, or userId clientIp the IP address of the client userAgent the browser used by the client pathInfo the URL path called by the client dateTime time of the request"},{"location":"setup_installation/admin/audit/audit-logs/#going-further","title":"Going Further","text":"<p>You can export audit logs to use them outside Hopsworks.</p>"},{"location":"setup_installation/admin/audit/export-audit-logs/","title":"Export Audit Logs","text":""},{"location":"setup_installation/admin/audit/export-audit-logs/#introduction","title":"Introduction","text":"<p>Audit logs can be exported to your storage of preference. In case audit logs have not been configured yet in your Hopsworks cluster, please see Access Audit Logs.</p> <p>Note</p> <pre><code>As an example, in this guide we will show how to export audit logs to BigQuery using the ```bq``` command-line tool.\n</code></pre>"},{"location":"setup_installation/admin/audit/export-audit-logs/#prerequisites","title":"Prerequisites","text":"<p>In order to export audit logs you need SSH access to the Hopsworks cluster.</p>"},{"location":"setup_installation/admin/audit/export-audit-logs/#step-1-create-a-bigquery-table","title":"Step 1: Create a BigQuery Table","text":"<p>Create a dataset and a table in BigQuery.</p> <p>The table schema is shown below.</p> <pre><code>fullname        mode     type      description\npathInfo        NULLABLE STRING\nmethodName      NULLABLE STRING\ncaller          NULLABLE RECORD\ndateTime        NULLABLE TIMESTAMP bq-datetime\nuserAgent       NULLABLE STRING\nclientIp        NULLABLE STRING\noutcome         NULLABLE STRING\nparameters      NULLABLE STRING\nclassName       NULLABLE STRING\ncaller.userId   NULLABLE STRING\ncaller.email    NULLABLE STRING\ncaller.username NULLABLE STRING\n</code></pre>"},{"location":"setup_installation/admin/audit/export-audit-logs/#step-2-export-audit-logs-to-the-bigquery-table","title":"Step 2: Export Audit Logs to the BigQuery Table","text":"<p>Audit logs can be exported in different formats. For instance, to export audit logs in JSON format set <code>audit_log_file_type=io.hops.hopsworks.audit.helper.JSONLogFormatter</code>.</p> <p>Info</p> <pre><code>For more information on how to configure the audit log file type see the ```audit_log_file_type``` configuration variable in [Audit logs](../audit/audit-logs.md#step-1-configure-audit-logs).\n</code></pre> <p>To export the audit logs to the BigQuery table created in the previous step, run the following command.</p> <pre><code>bq load --project_id &lt;projectId&gt; \\\n        --source_format=NEWLINE_DELIMITED_JSON \\\n        &lt;DATASET.TABLE&gt; \\\n        /srv/hops/domains/domain1/logs/audit/server_audit_log0.log\n</code></pre> <p>Tip</p> <pre><code>This command can be configured to run periodically on a given schedule by setting up a cronjob.\n</code></pre>"},{"location":"setup_installation/admin/ha-dr/dr/","title":"Disaster Recovery","text":""},{"location":"setup_installation/admin/ha-dr/dr/#backup","title":"Backup","text":"<p>The state of a Hopsworks cluster is split between data and metadata and distributed across multiple services. This section explains how to take consistent backups for the offline and online feature stores as well as cluster metadata.</p> <p>In Hopsworks, a consistent backup should back up the following services:</p> <ul> <li>RonDB: cluster metadata and the online feature store data.</li> <li>HopsFS: offline feature store data plus checkpoints and logs for feature engineering applications.</li> <li>Opensearch: search metadata, logs, dashboards, and user embeddings.</li> <li>Kubernetes objects: cluster credentials, backup metadata, serving metadata, and project namespaces with service accounts, roles, secrets, and configmaps.</li> <li>Python environments: custom project environments are stored in your configured container registry. Back up the registry separately. If a project and its environment are deleted, you must recreate the environment after restore.</li> </ul> <p>Besides the above services, Hopsworks uses also Apache Kafka which carries in-flight data heading to the online feature store. In the event of a total cluster loss, running jobs with in-flight data must be replayed.</p>"},{"location":"setup_installation/admin/ha-dr/dr/#prerequisites","title":"Prerequisites","text":"<p>When enabling backup in Hopsworks, cron jobs are configured for RonDB and Opensearch. For HopsFS, backups rely on versioning in the object store. For Kubernetes objects, Hopsworks uses Velero to snapshot the required resources. Before enabling backups:</p> <ul> <li>Enable versioning on the S3-compatible bucket used for HopsFS.</li> <li>Install and configure Velero with the AWS plugin (S3).</li> </ul>"},{"location":"setup_installation/admin/ha-dr/dr/#install-velero","title":"Install Velero","text":"<p>Velero provides backup and restore for Kubernetes resources. Install it with either the Velero CLI or Helm (Velero docs: Velero basic install guide).</p> <ul> <li>Using the Velero CLI, set up the CRDs and deployment:</li> </ul> <pre><code>velero install \\\n    --image velero/velero:v1.17.1 \\\n    --plugins velero/velero-plugin-for-aws:v1.13.0 \\\n    --no-default-backup-location \\\n    --no-secret \\\n    --use-volume-snapshots=false \\\n    --wait\n</code></pre> <ul> <li>Using the Velero Helm chart:</li> </ul> <pre><code>helm repo add vmware-tanzu https://vmware-tanzu.github.io/helm-charts\nhelm repo update\n\nhelm install velero vmware-tanzu/velero \\\n  --namespace velero \\\n  --version 11.2.0 \\\n  --create-namespace \\\n  --set \"initContainers[0].name=velero-plugin-for-aws\" \\\n  --set \"initContainers[0].image=velero/velero-plugin-for-aws:v1.13.0\" \\\n  --set \"initContainers[0].volumeMounts[0].mountPath=/target\" \\\n  --set \"initContainers[0].volumeMounts[0].name=plugins\" \\\n  --set-json configuration.backupStorageLocation='[]' \\\n  --set \"credentials.useSecret=false\" \\\n  --set \"snapshotsEnabled=false\" \\\n  --wait\n</code></pre>"},{"location":"setup_installation/admin/ha-dr/dr/#configuring-backup","title":"Configuring Backup","text":"<p>Note</p> <p>Backup is only supported for clusters that use S3-compatible object storage.</p> <p>You can enable backups during installation or a later upgrade. Set the schedule with a cron expression in the values file:</p> <pre><code>global:\n  _hopsworks:\n    backups:\n      enabled: true\n      schedule: \"@weekly\"\n</code></pre> <p>After configuring backups, go to the cluster settings and open the Backup tab. You should see <code>enabled</code> at the top level and for all services if everything is configured correctly.</p> Backup overview page <p>If any service is misconfigured, the backup status shows as <code>partial</code>. In the example below, Velero is disabled because it was not configured correctly. Fix partial backups before relying on them for recovery.</p> Backup overview page (partial setup)"},{"location":"setup_installation/admin/ha-dr/dr/#cleanup","title":"Cleanup","text":"<p>Use the backup time-to-live (<code>ttl</code>) flag to automatically prune backups older than the configured duration.</p> <pre><code>global:\n  _hopsworks:\n    backups:\n      enabled: true\n      schedule: \"@weekly\"\n      ttl: 60d\n</code></pre> <p>For S3 object storage, you can also configure a bucket lifecycle policy to expire old object versions. Example for AWS S3:</p> <pre><code>{\n  \"Rules\": [\n    {\n      \"ID\": \"HopsFSBlocksRetentionPolicy\",\n      \"Status\": \"Enabled\",\n      \"Filter\": {},\n      \"Expiration\": {\n        \"ExpiredObjectDeleteMarker\": true\n      },\n      \"NoncurrentVersionExpiration\": {\n        \"NoncurrentDays\": 60\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"setup_installation/admin/ha-dr/dr/#restore","title":"Restore","text":"<p>Note</p> <p>Restore is only supported in a newly created cluster; in-place restore is not supported. Use the exact Hopsworks version that was used to create the backup.</p> <p>The restore process has two phases:</p> <ul> <li>Restore Kubernetes objects required for the cluster restore.</li> <li>Install the cluster with Helm using the correct backup IDs.</li> </ul>"},{"location":"setup_installation/admin/ha-dr/dr/#restore-kubernetes-objects","title":"Restore Kubernetes objects","text":"<p>Restore the Kubernetes objects that were backed up using Velero.</p> <ul> <li>Ensure that Velero is installed and configured with the AWS plugin as described in the prerequisites.</li> <li> <p>Set up a Velero backup storage location to point to the S3 bucket.</p> </li> <li> <p>If you are using AWS S3 and access is controlled by an IAM role:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: velero.io/v1\nkind: BackupStorageLocation\nmetadata:\nname: hopsworks-bsl\nnamespace: velero\nspec:\nprovider: aws\nconfig:\n    region: REGION\nobjectStorage:\n    bucket: BUCKET_NAME\n    prefix: k8s_backup\nEOF\n</code></pre> </li> <li> <p>If you are using an S3-compatible object storage, provide credentials and endpoint:</p> <pre><code>cat &lt;&lt; EOF &gt; hopsworks-bsl-credentials\n[default]\naws_access_key_id=YOUR_ACCESS_KEY\naws_secret_access_key=YOUR_SECRET_KEY\nEOF\n\nkubectl create secret generic -n velero hopsworks-bsl-credentials --from-file=cloud=hopsworks-bsl-credentials\n\nkubectl apply -f - &lt;&lt;EOF\napiVersion: velero.io/v1\nkind: BackupStorageLocation\nmetadata:\nname: hopsworks-bsl\nnamespace: velero\nspec:\nprovider: aws\nconfig:\n    region: REGION\n    s3Url: ENDPOINT\ncredential:\n    key: cloud\n    name: hopsworks-bsl-credentials\nobjectStorage:\n    bucket: BUCKET_NAME\n    prefix: k8s_backup\nEOF\n</code></pre> </li> <li> <p>After the backup storage location becomes available, restore the backups. The following script restores the latest available backup. To restore a specific backup, set <code>backupName</code> instead of <code>scheduleName</code>.</p> </li> </ul> <pre><code>echo \"=== Waiting for Velero BackupStorageLocation  hopsworks-bsl to become Available ===\"\nuntil [ \"$(kubectl get backupstoragelocations hopsworks-bsl -n velero -o jsonpath='{.status.phase}' 2&gt;/dev/null)\" = \"Available\" ]; do\n  echo \"Still waiting...\"; sleep 5;\ndone\n\necho \"=== Waiting for Velero to sync the backups from hopsworks-bsl ===\"\nuntil [ \"$(kubectl get backups -n velero -ojson | jq -r '[.items[] | select(.spec.storageLocation == \"hopsworks-bsl\")] | length' 2&gt;/dev/null)\" != \"0\" ]; do\n  echo \"Still waiting...\"; sleep 5;\ndone\n\n\n# Restores the latest - if specific backup is needed then backupName instead\necho \"=== Creating Velero Restore object for k8s-backups-main ===\"\nRESTORE_SUFFIX=$(date +%s)\nkubectl apply -f - &lt;&lt;EOF\napiVersion: velero.io/v1\nkind: Restore\nmetadata:\n  name: k8s-backups-main-restore-$RESTORE_SUFFIX\n  namespace: velero\nspec:\n  scheduleName: k8s-backups-main\nEOF\n\necho \"=== Waiting for Velero restore to finish ===\"\nuntil [ \"$(kubectl get restore k8s-backups-main-restore-$RESTORE_SUFFIX -n velero -o jsonpath='{.status.phase}' 2&gt;/dev/null)\" = \"Completed\" ]; do\n  echo \"Still waiting...\"; sleep 5;\ndone\n\n# Restores the latest - if specific backup is needed then backupName instead\necho \"=== Creating Velero Restore object for k8s-backups-users-resources ===\"\nkubectl apply -f - &lt;&lt;EOF\napiVersion: velero.io/v1\nkind: Restore\nmetadata:\n  name: k8s-backups-users-resources-restore-$RESTORE_SUFFIX\n  namespace: velero\nspec:\n  scheduleName: k8s-backups-users-resources\nEOF\n\necho \"=== Waiting for Velero restore to finish ===\"\nuntil [ \"$(kubectl get restore k8s-backups-users-resources-restore-$RESTORE_SUFFIX -n velero -o jsonpath='{.status.phase}' 2&gt;/dev/null)\" = \"Completed\" ]; do\n  echo \"Still waiting...\"; sleep 5;\ndone\n</code></pre> <p>After the restore completes, verify the restored resources in Kubernetes. RonDB and Opensearch store their backup metadata in the <code>rondb-backups-metadata</code> and <code>opensearch-backups-metadata</code> configmaps. Use the commands below to list successful backup IDs (newest first) that can be referenced during cluster installation.</p> <pre><code>kubectl get configmap rondb-backups-metadata -n hopsworks -o json \\\n| jq -r '.data | to_entries[] | select(.value | fromjson | .state == \"SUCCESS\") | .key' \\\n| sort -nr\n\nkubectl get configmap opensearch-backups-metadata -n hopsworks -o json \\\n| jq -r '.data | to_entries[] | select(.value | fromjson | .state == \"SUCCESS\") | .key' \\\n| sort -nr\n</code></pre>"},{"location":"setup_installation/admin/ha-dr/dr/#restore-on-cluster-installation","title":"Restore on Cluster installation","text":"<p>To restore a cluster during installation, configure the backup ID in the values YAML file:</p> <pre><code>global:\n  _hopsworks:\n    backups:\n      enabled: true\n      schedule: \"@weekly\"\n    restoreFromBackup:\n      backupId: \"254811200\"\n</code></pre>"},{"location":"setup_installation/admin/ha-dr/dr/#customizations","title":"Customizations","text":"<p>Warning</p> <p>Even if you override the backup IDs for RonDB and Opensearch, you must still set <code>.global._hopsworks.restoreFromBackup.backupId</code> to ensure HopsFS is restored.</p> <p>To restore a different backup ID for RonDB:</p> <pre><code>global:\n  _hopsworks:\n    backups:\n      enabled: true\n      schedule: \"@weekly\"\n    restoreFromBackup:\n      backupId: \"254811200\"\n\nrondb:\n  rondb:\n    restoreFromBackup:\n      backupId: \"254811140\"\n</code></pre> <p>To restore a different backup for Opensearch:</p> <pre><code>global:\n  _hopsworks:\n    backups:\n      enabled: true\n      schedule: \"@weekly\"\n    restoreFromBackup:\n      backupId: \"254811200\"\n\nolk:\n  opensearch:\n    restore:\n      repositories:\n        default:\n          snapshots:\n            default:\n              snapshot_name: \"254811140\"\n</code></pre> <p>You can also customize the Opensearch restore process to skip specific indices:</p> <pre><code>global:\n  _hopsworks:\n    backups:\n      enabled: true\n      schedule: \"@weekly\"\n    restoreFromBackup:\n      backupId: \"254811200\"\n\nolk:\n  opensearch:\n    restore:\n      repositories:\n        default:\n          snapshots:\n            default:\n              snapshot_name: \"254811140\"\n              payload:\n                indices: \"-myindex\"\n</code></pre>"},{"location":"setup_installation/admin/ha-dr/ha/","title":"High Availability","text":"<p>At a high level a Hopsworks cluster can be divided into 4 groups of nodes. Each node group should be deployed according to the requirements (e.g., 3/5/7 nodes for the head node group) to guarantee the availability of the components.</p> <ul> <li>Head nodes: The head node is responsible for running all the metadata, public API, and user interface services that are required for Hopsworks to provide its functionality.   They need to be deployed in an odd number (1, 3, 5) as the head nodes run services like Zookeeper and OpenSearch which enforce consistency through quorum based protocols.   The head nodes are also responsible for managing the services running on the remaining group of nodes.</li> <li>Worker nodes: The worker node is responsible for executing the feature engineering pipeline code as well as storing the data for the offline feature store (HopsFS).   In an on-prem deployment, the data is stored and replicated on the workers\u2019 local hard drives.   By default the data is replicated across 3 workers.   In a cloud deployment, HopsFS\u2019 data is persisted in a cloud object store (Amazon S3, Azure Blob Storage, Google Cloud Blob Storage) and the HopsFS datanodes are responsible for persisting, retrieving and caching of blocks from the object store.</li> <li>RonDB Data nodes:   These nodes are responsible for storing the services\u2019 metadata (Hopsworks, HopsFS, Hive Metastore, Airflow) as well as the data for the online feature store.   For high availability, at least two data nodes should be deployed and RonDB is typically configured with a replication factor of 2, as it uses synchronous replication with 2-phase commit, not a quorum-based replication protocol.   More advanced deployment patterns and best practices are covered in the RonDB documentation.</li> <li>Query brokers: The query brokers are the entry point for querying the online feature store.   They handle authentication, authorization and execution of the requests for online feature data being submitted from the feature store APIs.   At least two query brokers should be deployed to achieve high availability.   Query brokers are stateless.   Additional query brokers should be deployed to handle additional load and clients.</li> </ul> <p>Example deployment:</p> Example High Available deployment <p>For higher availability, a Hopsworks cluster should be deployed across multiple availability zones, however, a single cluster cannot be deployed across multiple regions. Multiple region deployments are out of the scope of this guide.</p> <p>A different service placement is also possible, e.g., separating RonDB data nodes between metadata and online feature store or adding more replicas of a metadata service without necessarily adding a whole new head node, however, this is outside the scope of this guide.</p>"},{"location":"setup_installation/admin/ha-dr/intro/","title":"Hopsworks High Availability and Disaster Recovery Documentation","text":"<p>The Hopsworks Feature Store is the underlying component powering enterprise ML pipelines as well as serving feature data to model making user facing predictions. Sometimes the Hopsworks cluster can experience hardware failures or power loss, to help you plan for these occasions and avoid Hopsworks Feature Store downtime, we put together this guide. This guide is divided into three sections:</p> <ul> <li>High availability: deployment patterns and best practices to make sure individual component failures do not impact the availability of the Hopsworks cluster.</li> <li>Backup: configuration policies and best practices to make sure you have have fresh copy of the data and metadata in case of necessity</li> <li>Restore: procedures and best practices to restore a previous backup if needed.</li> </ul>"},{"location":"setup_installation/admin/ldap/configure-krb/","title":"Configure Kerberos","text":""},{"location":"setup_installation/admin/ldap/configure-krb/#introduction","title":"Introduction","text":"<p>Kerberos is a network authentication protocol that allow nodes to communicating over a non-secure network to prove their identity to one another in a secure manner. This tutorial shows an administrator how to configure Kerberos authentication.</p> <p>Kerberos need some server configuration before you can enable it from the UI.</p>"},{"location":"setup_installation/admin/ldap/configure-krb/#prerequisites","title":"Prerequisites","text":"<p>A server configured with Kerberos. See Server Configuration for Kerberos for instruction on how to do this.</p>"},{"location":"setup_installation/admin/ldap/configure-krb/#step-1-enable-kerberos","title":"Step 1: Enable Kerberos","text":"<p>After configuring the server you can configure Authentication methods by clicking on your name in the top right corner of the navigation bar and choosing Cluster Settings from the dropdown menu. In the Authentication tab you can find in Cluster Settings, you can enable Kerberos by clicking on the Kerberos checkbox.</p> <p>If LDAP/Kerberos checkbox is not checked, make sure that you configured your application server and enable it by clicking on the checkbox.</p> Setup Authentication Methods"},{"location":"setup_installation/admin/ldap/configure-krb/#step-2-edit-configuration","title":"Step 2: Edit configuration","text":"<p>Finally, click on edit configuration and fill in the attributes.</p> Configure Kerberos <ul> <li>Account status: the status a user will be assigned when logging in for the first time.   If a user is assigned a status different from Activated an admin needs to manually activate each user from the User management.</li> <li>Group mapping: allows you to specify a mapping between LDAP groups and Hopsworks groups.   The mapping is a semicolon separated string in the form <code>Directory Administrators-&gt;HOPS_ADMIN;IT People-&gt; HOPS_USER</code>.   Default is empty.   If no mapping is specified, users need to be assigned a role by an admin before they can log in.</li> <li>User id: the id field in LDAP with a string placeholder.   Default <code>uid=%s</code>.</li> <li>User given name: the given name field in LDAP.   Default <code>givenName</code>.</li> <li>User surname: the surname field in LDAP.   Default <code>sn</code>.</li> <li>User email: the email field in LDAP.   Default <code>mail</code>.</li> <li>User search filter: the search filter for user.   Default <code>uid=%s</code>.</li> <li>Principal search filter: the search filter for principal name.   Default <code>krbPrincipalName=%s</code>.</li> <li>Group search filter: the search filter for groups.   Default <code>member=%d</code>.</li> <li>Group target: the target to search for groups in the LDAP directory tree.   Default <code>cn</code>.</li> <li>Dynamic group target: the target to search for dynamic groups in the LDAP directory tree.   Default <code>memberOf</code>.</li> <li>User dn: specify the distinguished name (DN) of the container or base point where the users are stored.   Default is empty.</li> <li>Group dn: specify the DN of the container or base point where the groups are stored.   Default is empty.</li> </ul> <p>All defaults are taken from OpenLDAP.</p> <p>Note</p> <p>Group mapping can be disabled by setting <code>ldap_group_mapping_enabled=false</code> in the Configuration UI. When group mapping is disabled an administrator needs to activate each user from the User Management page.</p> <p>If group mapping is disabled then Account status in the Kerberos configuration above should be set to <code>Verified</code>.</p> <p>The login page will now have the choice to use Kerberos for authentication.</p> Log in using Kerberos <p>Note</p> <p>Make sure that you have Kerberos properly configured on your computer and you are logged in. Kerberos support must also be configured on the browser to use Kerberos for authentication.</p>"},{"location":"setup_installation/admin/ldap/configure-ldap/","title":"Configure LDAP/Kerberos","text":""},{"location":"setup_installation/admin/ldap/configure-ldap/#introduction","title":"Introduction","text":"<p>LDAP (Lightweight Directory Access Protocol) is a software protocol for enabling anyone in a network to gain access to resources such as files and devices. This tutorial shows an administrator how to configure LDAP authentication.</p> <p>LDAP need some server configuration before you can enable it from the UI.</p>"},{"location":"setup_installation/admin/ldap/configure-ldap/#prerequisites","title":"Prerequisites","text":"<p>A server configured with LDAP. See Server Configuration for LDAP for instruction on how to do this.</p>"},{"location":"setup_installation/admin/ldap/configure-ldap/#step-1-enable-ldap","title":"Step 1: Enable LDAP","text":"<p>After configuring the server you can configure Authentication methods by clicking on your name in the top right corner of the navigation bar and choosing Cluster Settings from the dropdown menu. In the Authentication tab you can find in Cluster Settings, you can enable LDAP by clicking on the LDAP checkbox.</p> <p>If LDAP/Kerberos checkbox is not checked make sure that you configured your application server and enable it by clicking on the checkbox.</p> Setup Authentication Methods"},{"location":"setup_installation/admin/ldap/configure-ldap/#step-2-edit-configuration","title":"Step 2: Edit configuration","text":"<p>Finally, click on edit configuration and fill in the attributes.</p> Configure LDAP <ul> <li>Account status: the status a user will be assigned when logging in for the first time.   If a user is assigned a status different from Activated an admin needs to manually activate each user from the User management.</li> <li>Group mapping: allows you to specify a mapping between LDAP groups and Hopsworks groups.   The mapping is a semicolon separated string in the form <code>Directory Administrators-&gt;HOPS_ADMIN;IT People-&gt; HOPS_USER</code>.   Default is empty.   If no mapping is specified, users need to be assigned a role by an admin before they can log in.</li> <li>User id: the id field in LDAP with a string placeholder.   Default <code>uid=%s</code>.</li> <li>User given name: the given name field in LDAP.   Default <code>givenName</code>.</li> <li>User surname: the surname field in LDAP.   Default <code>sn</code>.</li> <li>User email: the email field in LDAP.   Default <code>mail</code>.</li> <li>User search filter: the search filter for user.   Default <code>uid=%s</code>.</li> <li>Group search filter: the search filter for groups.   Default <code>member=%d</code>.</li> <li>Group target: the target to search for groups in the LDAP directory tree.   Default <code>cn</code>.</li> <li>Dynamic group target: the target to search for dynamic groups in the LDAP directory tree.   Default <code>memberOf</code>.</li> <li>User dn: specify the distinguished name (DN) of the container or base point where the users are stored.   Default is empty.</li> <li>Group dn: specify the DN of the container or base point where the groups are stored.   Default is empty.</li> </ul> <p>All defaults are taken from OpenLDAP.</p> <p>The login page will now have the choice to use LDAP for authentication.</p> Log in using LDAP <p>Note</p> <p>Group mapping can be disabled by setting <code>ldap_group_mapping_enabled=false</code> in the Configuration UI. When group mapping is disabled an administrator needs to activate each user from the User Management page.</p> <p>If group mapping is disabled then Account status in LDAP configuration above should be set to <code>Verified</code>.</p>"},{"location":"setup_installation/admin/ldap/configure-project-mapping/","title":"Configure LDAP/Kerberos group to project mapping","text":""},{"location":"setup_installation/admin/ldap/configure-project-mapping/#introduction","title":"Introduction","text":"<p>A group-to-project mapping lets you automatically add all members of an LDAP group to a project, eliminating the need to add each user individually. To create a mapping, you simply select the LDAP group, choose the project it should be linked to, and assign the role that its members will have within that project.</p> <p>Once a mapping is created, project membership is controlled through LDAP group membership. Any updates made to the LDAP group\u2014such as adding or removing users\u2014will automatically be reflected in Hopsworks. For example, if a user is removed from the LDAP group, they will also be removed from the corresponding project.</p>"},{"location":"setup_installation/admin/ldap/configure-project-mapping/#prerequisites","title":"Prerequisites","text":"<ol> <li>A server configured with LDAP or Kerberos. See Server Configuration for Kerberos and Server Configuration for LDAP for instructions on how to do this.</li> <li>LDAP group mapping sync enabled. This can be done by setting the variable <code>ldap_group_mapping_sync_enabled=true</code>.</li> </ol> <p>See Cluster Configuration on how to change variable values in Hopsworks.</p> Enable ldap mapping <p>If you can not find the variable <code>ldap_group_mapping_sync_enabled</code> create it by clicking on New variable.</p> Create ldap mapping enabled variable"},{"location":"setup_installation/admin/ldap/configure-project-mapping/#step-1-create-a-mapping","title":"Step 1: Create a mapping","text":"<p>To create a mapping go to Cluster Settings by clicking on your name in the top right corner of the navigation bar and choosing Cluster Settings from the dropdown menu. In the Project mapping tab, you can create a new mapping by clicking on Create new mapping.</p> Project mapping <p>This will take you to the create mapping page shown below</p> Create mapping <p>Here you can choose from your LDAP groups and map them to a project from the Project drop down list. You can also choose the Project role users will be assigned when they are added to the project.</p> <p>Finally, click on Create mapping and go back to mappings. You should see the newly created mapping(s) as shown below.</p> Project mappings <p>Note</p> <p>If there are no groups in the Remote group drop down list check if ldap_groups_search_filter is correct by using the value in <code>ldapsearch</code> replacing <code>%c</code> with <code>*</code>, as shown in the example below.</p> <pre><code>ldapsearch -LLL -H ldap:/// -b '&lt;base dn&gt;' -D '&lt;user dn&gt;' -w &lt;password&gt; '(&amp;(objectClass=groupOfNames)(cn=*))'\n</code></pre> <p>This should return all the groups in your LDAP. See Cluster Configuration on how to find and update the value of this variable.</p>"},{"location":"setup_installation/admin/ldap/configure-project-mapping/#step-2-edit-a-mapping","title":"Step 2: Edit a mapping","text":"<p>From the list of mappings click on the edit button (). This will open a popup that will allow you to change the remote group, project name, and project role of a mapping.</p> Edit mapping <p>Warning</p> <p>Updating a mapping's remote group or project name will remove all members of the previous group from the project.</p>"},{"location":"setup_installation/admin/ldap/configure-project-mapping/#step-3-delete-a-mapping","title":"Step 3: Delete a mapping","text":"<p>To delete a mapping click on the delete button.</p> <p>Warning</p> <p>Deleting a mapping will remove all members of that group from the project.</p>"},{"location":"setup_installation/admin/ldap/configure-project-mapping/#step-4-configure-sync-interval","title":"Step 4: Configure sync interval","text":"<p>After configuring all the group mappings users will be added to or removed from the projects in the mapping when they login to Hopsworks. It is also possible to synchronize mappings without requiring users to log out. This can be done by setting <code>ldap_group_mapping_sync_interval</code> to an interval greater or equal to 2 minutes. If <code>ldap_group_mapping_sync_interval</code> is set group mapping sync will run periodically based on the interval and add or remove users from projects.</p>"},{"location":"setup_installation/admin/ldap/configure-server/","title":"Configure Server for LDAP and Kerberos","text":""},{"location":"setup_installation/admin/ldap/configure-server/#introduction","title":"Introduction","text":"<p>LDAP and Kerberos integration need some configuration in the helm charts for your cluster definition used to deploy your Hopsworks cluster. This tutorial shows an administrator how to configure the application server for LDAP and Kerberos integration.</p>"},{"location":"setup_installation/admin/ldap/configure-server/#prerequisites","title":"Prerequisites","text":"<p>An accessible LDAP domain. A Kerberos Key Distribution Center (KDC) running on the same domain as Hopsworks (Only for Kerberos).</p>"},{"location":"setup_installation/admin/ldap/configure-server/#step-1-server-configuration-for-ldap","title":"Step 1: Server Configuration for LDAP","text":"<p>The LDAP attributes below are used to configure JNDI external resource in Payara. The JNDI resource will communicate with your LDAP server to perform the authentication.</p> <pre><code>ldap:\n    enabled: true\n    jndilookupname: \"dc=hopsworks,dc=ai\"\n    provider_url: \"ldap://193.10.66.104:1389\"\n    attr_binary_val: \"entryUUID\"\n    security_auth: \"none\"\n    security_principal: \"\"\n    security_credentials: \"\"\n    referral: \"ignore\"\n    additional_props: \"\"\n</code></pre> <ul> <li>jndilookupname: should contain the LDAP domain.</li> <li>attr_binary_val: is the binary unique identifier that will be used in subsequent logins to identify the user.</li> <li>security_auth: how to authenticate to the LDAP server.</li> <li>security_principal: contains the username of the user that will be used to query LDAP.</li> <li>security_credentials: contains the password of the user that will be used to query LDAP.</li> <li>referral: whether to follow or ignore an alternate location in which an LDAP Request may be processed.</li> </ul> <p>An already deployed instance can be configured to connect to LDAP. Go to the payara admin UI and create a new JNDI external resource. The name of the resource should be ldap/LdapResource.</p> LDAP Resource <p>This can also be achieved by running the below asadmin command.</p> <pre><code>asadmin create-jndi-resource \\\n --restype javax.naming.ldap.LdapContext \\\n --factoryclass com.sun.jndi.ldap.LdapCtxFactory \\\n --jndilookupname dc\\=hopsworks\\,dc\\=ai \\\n --property java.naming.provider.url=ldap\\\\://193\\.10\\.66\\.104\\\\:1389:\\\n hopsworks.ldap.basedn=dc\\\\\\=hopsworks\\,dc\\\\\\=ai:\\\n java.naming.ldap.attributes.binary=entryUUID:\\\n java.naming.security.authentication=simple:\\\n java.naming.security.principal=&lt;username&gt;:\\\n java.naming.security.credentials=&lt;password&gt;:\\\n java.naming.referral=ignore \\\n ldap/LdapResource\n</code></pre>"},{"location":"setup_installation/admin/ldap/configure-server/#step-2-server-configuration-for-kerberos","title":"Step 2: Server Configuration for Kerberos","text":"<p>The Kerberos attributes are used to configure SPNEGO. SPNEGO is used to establish a secure context between the requester and the application server when using Kerberos authentication.</p> <pre><code>kerberos:\n    enabled: true\n    krb_conf_path: \"/etc/krb5.conf\"\n    krb_server_key_tab_path: \"/etc/security/keytabs/service.keytab\"\n    krb_server_key_tab_name: \"service.keytab\"\n    spnego_server_conf: '\\nuseKeyTab=true\\nprincipal=\\\"HTTP/server.hopsworks.ai@HOPSWORKS.AI\\\"\\nstoreKey=true\\nisInitiator=false'\nldap:\n    jndilookupname: \"dc=hopsworks,dc=ai\"\n    provider_url: \"ldap://193.10.66.104:1389\"\n    attr_binary_val: \"objectGUID\"\n    security_auth: \"none\"\n    security_principal: \"\"\n    security_credentials: \"\"\n    referral: \"ignore\"\n    additional_props: \"\"\n</code></pre> <p>Both Kerberos and LDAP attributes need to be specified to configure Kerberos. The LDAP attributes are explained above.</p> <ul> <li>krb_conf_path: contains the path to the krb5.conf used by SPNEGO to get information about the default domain and the location of the Kerberos KDC.   The file is copied by the recipe in to /srv/hops/domains/domain1/config.</li> <li>krb_server_key_tab_path: contains the path to the Kerberos service keytab.   The keytab is copied by the recipe in to /srv/hops/domains/domain/config with the name set in the krb_server_key_tab_name attribute.</li> <li>spnego_server_conf: contains the configuration that will be appended to Payara's (application serve used to host hopsworks) login.conf.   In particular, it should contain useKeyTab=true, and the principal name to be used in the authentication phase.   Initiator should be set to false.</li> </ul>"},{"location":"setup_installation/admin/monitoring/export-metrics/","title":"Exporting Hopsworks metrics","text":""},{"location":"setup_installation/admin/monitoring/export-metrics/#introduction","title":"Introduction","text":"<p>Hopsworks services produce metrics which are centrally gathered by Prometheus and visualized in Grafana. Although the system is self-contained, it is possible for another federated Prometheus instance to scrape these metrics or directly push them to another system. This is useful if you have a centralized monitoring system with already configured alerts.</p>"},{"location":"setup_installation/admin/monitoring/export-metrics/#prerequisites","title":"Prerequisites","text":"<p>In order to configure Prometheus to export metrics you need to have the right to change the remote Prometheus configuration.</p>"},{"location":"setup_installation/admin/monitoring/export-metrics/#exporting-metrics","title":"Exporting metrics","text":"<p>Prometheus can be configured to export metrics to another Prometheus instance (cross-service federation) or to a custom service which knows how to handle them.</p>"},{"location":"setup_installation/admin/monitoring/export-metrics/#prometheus-federation","title":"Prometheus federation","text":"<p>Prometheus servers can be federated to scale better or to just clone all metrics (cross-service federation).</p> <p>In the guide below we assume Prometheus A is the service running in Hopsworks and Prometheus B is the server you want to clone metrics to.</p>"},{"location":"setup_installation/admin/monitoring/export-metrics/#step-1","title":"Step 1","text":"<p>Prometheus B needs to be able to connect to TCP port <code>9090</code> of Prometheus A to scrape metrics. If you have any firewall (or Security Group) in place, allow ingress for that port.</p>"},{"location":"setup_installation/admin/monitoring/export-metrics/#step-2","title":"Step 2","text":"<p>The next step is to expose Prometheus A running inside Hopsworks Kubernetes cluster. If Prometheus B has direct access to Prometheus A then you can skip this step.</p> <p>We will create a Kubernetes Service of type LoadBalancer to expose port <code>9090</code></p> <p>Warning</p> <p>If you need to apply custom annotations, then modify the Manifest below. The example below assumes Hopsworks is installed at Namespace hopsworks</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Service\nmetadata:\n  name: prometheus-external\n  namespace: hopsworks\n  labels:\n    app: prometheus\nspec:\n  type: LoadBalancer\n  selector:\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/component: server\n  ports:\n    - protocol: TCP\n      port: 9090\n      targetPort: 9090\nEOF\n</code></pre> <p>Then we need to find the External IP address of the newly created Service:</p> <pre><code>export NAMESPACE=hopsworks\nkubectl -n $NAMESPACE get svc prometheus-external -ojsonpath='{.status.loadBalancer.ingress[0].ip}'\n</code></pre> <p>Warning</p> <p>It will take a few seconds until an IP address is assigned to the Service.</p> <p>We will use this IP address in Step 3.</p>"},{"location":"setup_installation/admin/monitoring/export-metrics/#step-3","title":"Step 3","text":"<p>Edit the configuration file of Prometheus B server and append the following Job under <code>scrape_configs</code>:</p> <p>Note</p> <p>Replace IP_ADDRESS with the IP address from Step 2 or the IP address of Prometheus service if it is directly accessible. The snippet below assumes Hopsworks services runs at Namespace hopsworks.</p> <pre><code>- job_name: 'federate'\n  scrape_interval: 15s\n\n  honor_labels: true\n  metrics_path: '/federate'\n\n  params:\n    'match[]':\n      - '{namespace=\"hopsworks\"}'\n\n  static_configs:\n    - targets:\n      - 'IP_ADDRESS:9090'\n</code></pre> <p>The configuration above will scrape for services metrics under the hopsworks Namespace. If you want to additionally scrape user application metrics then append <code>'{job=\"pushgateway\"}'</code> to the matchers, for example:</p> <pre><code>  params:\n    'match[]':\n      - '{namespace=\"hopsworks\"}'\n      - '{job=\"pushgateway\"}'\n</code></pre> <p>Depending on the Prometheus setup you might need to restart Prometheus B service to pick up the new configuration. For more details on federation visit Prometheus documentation.</p>"},{"location":"setup_installation/admin/monitoring/export-metrics/#custom-service","title":"Custom service","text":"<p>Prometheus can push metrics to another custom resource via HTTP. The custom service is responsible for handling the received metrics. To push metrics with this method we use the <code>remote_write</code> configuration.</p> <p>We will only give a sample configuration as <code>remote_write</code> is extensively documented in Prometheus documentation. In the example below we push metrics to a custom service listening on port 9096 which transforms the metrics and forwards them.</p> <p>In order to configure Prometheus to push metrics to a remote HTTP service we need to customize our Helm chart values file with the following snippet after changing the url accordingly. You can also tweak other configuration parameters to your needs.</p> <pre><code>prometheus:\n  prometheus:\n    server:\n      remoteWrite:\n      - url: \"http://localhost:9096\"\n        queue_config:\n          capacity: 10000\n          max_samples_per_send: 5000\n          batch_send_deadline: 60s\n</code></pre> <p>If the section already exists, then append the <code>remoteWrite</code> section.</p> <p>Run <code>helm install</code> or <code>helm upgrade</code> if it's the first time you install Hopsworks or you want to apply the change to an existing cluster respectively.</p>"},{"location":"setup_installation/admin/monitoring/grafana/","title":"Services Dashboards","text":""},{"location":"setup_installation/admin/monitoring/grafana/#introduction","title":"Introduction","text":"<p>The Hopsworks platform is composed of different services. Hopsworks uses Prometheus to collect health and performance metrics from the different services and Grafana to display them to the Hopsworks administrators.</p> <p>In this guide you will learn how to access the Grafana dashboards to monitor the health of the cluster or to troubleshoot performance issues.</p>"},{"location":"setup_installation/admin/monitoring/grafana/#prerequisites","title":"Prerequisites","text":"<p>To access the services dashboards in Grafana, you need to have an administrator account on the Hopsworks cluster.</p>"},{"location":"setup_installation/admin/monitoring/grafana/#step-1-access-grafana","title":"Step 1: Access Grafana","text":"<p>You can access the admin page of your Hopsworks cluster by clicking on your name, in the top right corner, and choosing Cluster Settings from the dropdown menu.</p> <p>You can then navigate to the Monitoring tab. The Monitoring tab gives you access to several of the observability tools that are already deployed to help you manage the health of the cluster.</p> Monitoring tab <p>Click on the Grafana link to open the Grafana web application and navigate through the dashboards.</p>"},{"location":"setup_installation/admin/monitoring/grafana/#step-2-navigate-through-the-dashboards","title":"Step 2: Navigate through the dashboards","text":"<p>In the Grafana web application, you can click on the Home button on the top left corner and navigate through the available dashboards.</p> <p>Dashboards are organized into three folders:</p> <ul> <li> <p>Hops: This folder contains all the dashboards of the Hopsworks services (e.g., the web application, the file system, resource manager) as well as the dashboards of the hosts (e.g., EC2 instances, virtual machines, servers) on which the cluster is deployed.</p> </li> <li> <p>RonDB: This folder contains all the dashboard related to the database. The Database dashboard contains a general overview of the RonDB cluster, while the remaining dashboards focus on specific items (e.g., thread activity, memory management, etc).</p> </li> <li> <p>Kubernetes: If you have integrated Hopsworks with a Kubernetes cluster, this folder contains the dashboards to monitor the health of the Kubernetes cluster.</p> </li> </ul> Grafana view <p>The default dashboards are read only and cannot be edited. Additional dashboards can be created by logging in to Grafana. You can log in into Grafana using the username and password specified in the cluster definition.</p> <p>Warning</p> <p>By default Hopsworks keeps metrics information only for the past 15 days. This means that, by default, you will not be able to access health and performance metrics which are older than 15 days.</p>"},{"location":"setup_installation/admin/monitoring/grafana/#going-further","title":"Going Further","text":"<p>You can read Grafana Documentation to learn how to use it advancedly.</p>"},{"location":"setup_installation/admin/monitoring/services-logs/","title":"Services Logs","text":""},{"location":"setup_installation/admin/monitoring/services-logs/#introduction","title":"Introduction","text":"<p>The Hopsworks platform is composed of different services running on different nodes. Hopsworks uses Filebeat, Logstash and OpenSearch to collect, parse, index and present the logs to the Hopsworks administrators.</p> <p>In this guide you will learn how to access the Hopsworks logs using OpenSearch Dashboards.</p>"},{"location":"setup_installation/admin/monitoring/services-logs/#prerequisites","title":"Prerequisites","text":"<p>To access the services logs, you need to have an administrator account on the Hopsworks cluster.</p>"},{"location":"setup_installation/admin/monitoring/services-logs/#step-1-access-the-logs","title":"Step 1: Access the Logs","text":"<p>You can access the admin page of your Hopsworks cluster by clicking on your name, in the top right corner, and choosing Cluster Settings from the dropdown menu.</p> <p>You can then navigate to the Monitoring tab. The Monitoring tab gives you access to several of the observability tools that are already deployed to help you manage the health of the cluster.</p> Monitoring tab <p>Click on the Service Logs link to open the OpenSearch Dashboards web application and navigate through the logs.</p>"},{"location":"setup_installation/admin/monitoring/services-logs/#step-2-search-the-logs","title":"Step 2: Search the logs","text":"<p>In the OpenSearch dashboard web application you will see by default all the logs generated by all monitored services in the last 15 minutes.</p> <p>You can filter the logs of a specific service by searching for the term <code>service:[service name]</code>. As shown in the picture below, you can search for the namenode logs by querying <code>service:namenode</code>.</p> <p>Currently only the logs of the following services are collected and indexed: Hopsworks web application (called <code>domain1</code> in the log entries), namenodes, resource managers, datanodes, nodemanagers, Kafka brokers, Hive services and RonDB. These are the core component of the platform, additional logs will be added in the future.</p> OpenSearch Dashboards displaying the logs <p>Warning</p> <p>By default, logs are rotated automatically after 7 days. This means that by default, you will not be able to access logs through OpenSearch Dashboards which are older than 7 days. Depending on the service and on the Hopsworks configuration, you can still access the logs by SSH directly into the machines of the cluster.</p>"},{"location":"setup_installation/admin/monitoring/services-logs/#going-further","title":"Going Further","text":"<p>You can read OpenSearch Dashboards Documentation to learn how to use them advancedly.</p>"},{"location":"setup_installation/admin/oauth2/configure-project-mapping/","title":"Configure OAuth2 group to project mapping","text":""},{"location":"setup_installation/admin/oauth2/configure-project-mapping/#introduction","title":"Introduction","text":"<p>A group-to-project mapping lets you automatically add all members of an OAuth2 group to a project, eliminating the need to add each user individually. To create a mapping, you simply select an OAuth2 group, choose the project it should be linked to, and assign the role that its members will have within that project.</p> <p>Once a mapping is created, project membership is controlled through OAuth2 group membership. Any updates made to the OAuth2 group\u2014such as adding or removing users\u2014will automatically be reflected in Hopsworks. For example, if a user is removed from the OAuth2 group, they will also be removed from the corresponding project.</p>"},{"location":"setup_installation/admin/oauth2/configure-project-mapping/#prerequisites","title":"Prerequisites","text":"<ol> <li>A server configured with OAuth2. See Register Identity Provider in Hopsworks for instructions on how to do this.</li> <li>OAuth2 group mapping sync enabled. This can be done by setting the variable <code>oauth_group_mapping_sync_enabled=true</code>. See Cluster Configuration on how to change variable values in Hopsworks.</li> </ol> Enable OAuth2 mapping <p>If you can not find the variable <code>oauth_group_mapping_sync_enabled</code> create it by clicking on New variable.</p> Create OAuth2 mapping enabled variable"},{"location":"setup_installation/admin/oauth2/configure-project-mapping/#step-1-create-a-mapping","title":"Step 1: Create a mapping","text":"<p>To create a mapping go to Cluster Settings by clicking on your name in the top right corner of the navigation bar and choosing Cluster Settings from the dropdown menu. In the Project mapping tab, you can create a new mapping by clicking on Create new mapping.</p> Project mapping <p>This will take you to the create mapping page shown below</p> Create mapping <p>Here you can enter your OAuth2 group and map it to a project from the Project drop down list. You can also choose the Project role users will be assigned when they are added to the project.</p> <p>Finally, click on Create mapping and go back to mappings. You should see the newly created mapping(s) as shown below.</p> Project mappings <p>Note</p> <p>Make sure the group names from your OAuth2 provider match the one you entered above.</p> <p>If your identity provider uses a claim name other than <code>groups</code> or <code>roles</code> to represent group information, be sure to specify that claim name in the Group Claim field when setting up your identity provider.</p>"},{"location":"setup_installation/admin/oauth2/configure-project-mapping/#step-2-edit-a-mapping","title":"Step 2: Edit a mapping","text":"<p>From the list of mappings click on the edit button (). This will open a popup that will allow you to change the remote group, project name, and project role of a mapping.</p> Edit mapping <p>Warning</p> <p>Updating a mapping's remote group or project name will remove all members of the previous group from the project.</p>"},{"location":"setup_installation/admin/oauth2/configure-project-mapping/#step-3-delete-a-mapping","title":"Step 3: Delete a mapping","text":"<p>To delete a mapping click on the delete button.</p> <p>Warning</p> <p>Deleting a mapping will remove all members of that group from the project.</p>"},{"location":"setup_installation/admin/oauth2/create-azure-client/","title":"Create An Application in Azure Active Directory","text":""},{"location":"setup_installation/admin/oauth2/create-azure-client/#introduction","title":"Introduction","text":"<p>This example uses Azure Active Directory as the identity provider, but the same can be done with any identity provider supporting OAuth2 OpenID Connect protocol.</p>"},{"location":"setup_installation/admin/oauth2/create-azure-client/#prerequisites","title":"Prerequisites","text":"<p>Azure account.</p>"},{"location":"setup_installation/admin/oauth2/create-azure-client/#step-1-register-hopsworks-as-an-application-in-your-identity-provider","title":"Step 1: Register Hopsworks as an application in your identity provider","text":"<p>To use OAuth2 in Hopsworks you first need to create and configure an OAuth client in your identity provider. We will take the example of Azure AD for the remaining of this documentation, but equivalent steps can be taken on other identity providers.</p> <p>Navigate to the Microsoft Azure Portal and authenticate. Navigate to Azure Active Directory. Click on App Registrations. Click on New Registration.</p> <p> Create application </p> <p>Enter a name for the client such as hopsworks_oauth_client. Verify the Supported account type is set to Accounts in this organizational directory only. Click Register.</p> <p> Name application </p>"},{"location":"setup_installation/admin/oauth2/create-azure-client/#step-2-get-the-necessary-fields-for-client-registration","title":"Step 2: Get the necessary fields for client registration","text":"<p>In the Overview section, copy the Application (client) ID field. We will use it in Identity Provider registration under the name Client id.</p> <p> Copy client ID </p> <p>Click on Endpoints and copy the OpenId Connect metadata document endpoint excluding the .well-known/openid-configuration part. We will use it in Identity Provider registration under the name Connection URL.</p> <p> Endpoint </p> <p>Note</p> <p>If you have multiple tenants in your Azure Active Directory, the <code>OpenID Connect metadata document</code> endpoint might use <code>organizations</code>  instead of a specific tenant ID. In such cases, replace <code>organizations</code>  with your actual tenant ID to target a specific directory.</p> <p>example:</p> <pre><code>https://login.microsoftonline.com/organizations/oauth2/v2.0 --&gt; https://login.microsoftonline.com/&lt;YOUR_TENANT_ID&gt;/oauth2/v2.0\n</code></pre> <p>Click on Certificates &amp; secrets, then Click on New client secret.</p> <p> New client secret </p> <p>Add a description of the secret. Select an expiration period. Click Add.</p> <p> Client secret creation </p> <p>Copy the secret. This will be used in Identity Provider registration under the name Client Secret.</p> <p> Client secret creation </p> <p>Click on Authentication. Then click on Add a platform.</p> <p> Add a platform </p> <p>In Configure platforms click on Web.</p> <p> Configure platform: Web </p> <p>Enter the Redirect URI and click on Configure. The redirect URI is HOPSWORKS-URI/callback with HOPSWORKS-URI the URI of your Hopsworks cluster.</p> <p> Configure platform: Redirect </p>"},{"location":"setup_installation/admin/oauth2/create-client/","title":"Register Identity Provider in Hopsworks","text":""},{"location":"setup_installation/admin/oauth2/create-client/#introduction","title":"Introduction","text":"<p>Before registering your identity provider in Hopsworks you need to create a client application in your identity provider and acquire a client id and a client secret. An example on how to create a client using Okta and Azure Active Directory identity providers can be found in the following guides: Create Okta Client and Create Azure Client.</p>"},{"location":"setup_installation/admin/oauth2/create-client/#prerequisites","title":"Prerequisites","text":"<p>Acquired a client id and a client secret from your identity provider.</p>"},{"location":"setup_installation/admin/oauth2/create-client/#step-1-register-a-client","title":"Step 1: Register a client","text":"<p>After acquiring the client id and client secret create the client in Hopsworks by enabling OAuth2 and clicking on add another identity provider in the Authentication configuration page. Then set base uri of your identity provider in Connection URL give a name to your identity provider (the name will be used in the login page as an alternative login method) and set the client id and client secret in their respective fields, as shown in the figure below.</p> Application overview <ul> <li>Connection URL: (provider Uri) is the base uri of the identity provider's API (URI should contain scheme http:// or https://).</li> </ul> <p>Additional configuration can be set here:</p> <ul> <li>Verify email: if checked only users with verified email address (in the identity provider) can log in to Hopsworks.</li> <li>Code challenge: if your identity provider requires code challenge for authorization request check the code challenge check box.   This will allow you to choose code challenge method that can be either plain or S256.</li> <li>Logo URL: optionally a logo URL to an image can be added.   The logo will be shown on the login page with the name as shown in the figure below.</li> <li>Claim names for given name, family name, email and group can also be set here.   If left empty the default openid claim names will be used.</li> </ul>"},{"location":"setup_installation/admin/oauth2/create-client/#step-2-add-group-mappings","title":"Step 2: Add Group mappings","text":"<p>Optionally you can add a group mapping from your identity provider to Hopsworks groups, by clicking on your name in the top right corner of the navigation bar and choosing Cluster Settings from the dropdown menu. In the Cluster Settings Configuration tab search for oauth_group_mapping and click on the edit button.</p> Set Configuration variables <p>Note</p> <p>Setting <code>oauth_group_mapping</code> to <code>ANY_GROUP-&gt;HOPS_USER</code> will assign the role user to any user from any group in your identity provider when they log into Hopsworks with OAuth for the first time. You can replace ANY_GROUP with the group of your choice in the identity provider. You can replace HOPS_USER by HOPS_ADMIN if you want the users of that group to be admins in Hopsworks. You can do several mappings by separating them with a semicolon.</p> <p>Group mapping can be disabled by setting <code>oauth_group_mapping_enabled=false</code> in the Configuration UI. When group mapping is disabled an administrator needs to activate each user from the User Management page.</p> <p>If group mapping is disabled then <code>oauth_account_status</code> in the Configuration UI should be set to 1 (Verified).</p> <p>Users will now see a new button on the login page. The button has the name you set above for Name and will redirect to your identity provider.</p> Login with OAuth2 <p>Note</p> <p>When creating a client make sure you can access the provider metadata by making a GET request on the well known endpoint of the provider. The well-known URL, will typically be the Connection URL plus <code>.well-known/openid-configuration</code>. For the above client it would be <code>https://dev-86723251.okta.com/.well-known/openid-configuration</code>.</p>"},{"location":"setup_installation/admin/oauth2/create-okta-client/","title":"Create An Application in Okta","text":""},{"location":"setup_installation/admin/oauth2/create-okta-client/#introduction","title":"Introduction","text":"<p>This example uses an Okta development account to create an application that will represent a Hopsworks client in the identity provider.</p>"},{"location":"setup_installation/admin/oauth2/create-okta-client/#prerequisites","title":"Prerequisites","text":"<p>Okta development account. To create a developer account go to Okta developer.</p>"},{"location":"setup_installation/admin/oauth2/create-okta-client/#step-1-register-hopsworks-as-an-application-in-your-identity-provider","title":"Step 1: Register Hopsworks as an application in your identity provider","text":"<p>After creating a developer account register a client by going to Applications and click on Create App Integration.</p> Okta Applications <p>This will open a popup as shown in the figure below. Select OIDC as Sign-in-method and Web Application as Application type and click next.  Create new Application </p> <p>Give your application a name and select Client credential as Grant Type. Then add a Sign-in redirect URI that is your Hopsworks cluster domain name (including the port number if needed) with path /callback, and a Sign-out redirect URI that is Hopsworks cluster domain name (including the port number if needed) with no path.</p> New Application <p>If you want to limit who can access your Hopsworks cluster select Limit access to selected groups and select group(s) you want to give access to. Here we will allow everyone in the organization to access the cluster.</p> Group assignment"},{"location":"setup_installation/admin/oauth2/create-okta-client/#group-mapping","title":"Group mapping","text":"<p>You can also create mappings from groups in Okta to groups in Hopsworks. To achieve this you need to configure Okta to send Groups with user information. To do this go to Applications and select your application name. In the Sign On tab click edit OpenID Connect ID Token and select Filter for Groups claim type, then for Groups claim filter add groups as the claim name, select Match Regex from the dropdown and .* (dot star) as Regex to match all groups. See Group mapping on how to do the mapping in Hopsworks.</p> Group claim"},{"location":"setup_installation/admin/oauth2/create-okta-client/#step-2-get-the-necessary-fields-for-client-registration","title":"Step 2: Get the necessary fields for client registration","text":"<p>After the application is created go back to Applications and click on the application you just created. Use the Okta domain (Connection URL), client id and client secret generated for your app in the Identity Provider registration in Hopsworks.</p> Application overview <p>Note</p> <p>When copying the domain in the figure above make sure to add the url scheme (http:// or https://) when using it in the Connection URL in the Identity Provider registration form.</p>"},{"location":"setup_installation/aws/getting_started/","title":"AWS - Getting started","text":"<p>Kubernetes and Helm are used to install &amp; run Hopsworks and the Feature Store in the cloud. They both integrate seamlessly with third-party platforms such as Databricks, SageMaker and KubeFlow. This guide shows how to set up the Hopsworks platform in your organization's AWS account.</p>"},{"location":"setup_installation/aws/getting_started/#prerequisites","title":"Prerequisites","text":"<p>To follow the instruction on this page you will need the following:</p> <ul> <li>Kubernetes Version: Hopsworks can be deployed on EKS clusters running Kubernetes &gt;= 1.27.0.</li> <li>aws-cli to provision the AWS resources</li> <li>eksctl to interact with the AWS APIs and provision the EKS cluster</li> <li>helm to deploy Hopsworks</li> </ul>"},{"location":"setup_installation/aws/getting_started/#ecr-registry","title":"ECR Registry","text":"<p>Hopsworks allows users to customize the images used by Python jobs, Jupyter Notebooks and (Py)Spark applications running in their projects. The images are stored in ECR. Hopsworks needs access to an ECR repository to push the project images.</p>"},{"location":"setup_installation/aws/getting_started/#permissions","title":"Permissions","text":"<ul> <li> <p>The deployment requires cluster admin access to create ClusterRoles, ServiceAccounts, and ClusterRoleBindings.</p> </li> <li> <p>A namespace is required to deploy the Hopsworks stack. If you don\u2019t have permissions to create a namespace, ask your EKS administrator to provision one.</p> </li> </ul>"},{"location":"setup_installation/aws/getting_started/#eks-deployment","title":"EKS Deployment","text":"<p>The following steps describe how to deploy an EKS cluster and related resources so that it\u2019s compatible with Hopsworks.</p>"},{"location":"setup_installation/aws/getting_started/#step-1-aws-eks-setup","title":"Step 1: AWS EKS Setup","text":""},{"location":"setup_installation/aws/getting_started/#step-11-create-s3-bucket","title":"Step 1.1: Create S3 Bucket","text":"<pre><code>aws s3 mb s3://BUCKET_NAME --region REGION --profile PROFILE\n</code></pre>"},{"location":"setup_installation/aws/getting_started/#step-12-create-ecr-repository","title":"Step 1.2: Create ECR Repository","text":"<p>Create the repository to host the projects images.</p> <pre><code>aws --profile PROFILE ecr create-repository --repository-name NAMESPACE/hopsworks-base --region REGION\n</code></pre>"},{"location":"setup_installation/aws/getting_started/#step-13-create-iam-policies","title":"Step 1.3: Create IAM Policies","text":"<pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"hopsworksaiInstanceProfile\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"S3:PutObject\",\n        \"S3:ListBucket\",\n        \"S3:GetObject\",\n        \"S3:DeleteObject\",\n        \"S3:AbortMultipartUpload\",\n        \"S3:ListBucketMultipartUploads\",\n        \"S3:PutLifecycleConfiguration\",\n        \"S3:GetLifecycleConfiguration\",\n        \"S3:PutBucketVersioning\",\n        \"S3:GetBucketVersioning\",\n        \"S3:ListBucketVersions\",\n        \"S3:DeleteObjectVersion\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::BUCKET_NAME/*\",\n        \"arn:aws:s3:::BUCKET_NAME\"\n      ]\n    },\n    {\n      \"Sid\": \"AllowPushandPullImagesToUserRepo\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ecr:GetDownloadUrlForLayer\",\n        \"ecr:BatchGetImage\",\n        \"ecr:CompleteLayerUpload\",\n        \"ecr:UploadLayerPart\",\n        \"ecr:InitiateLayerUpload\",\n        \"ecr:BatchCheckLayerAvailability\",\n        \"ecr:PutImage\",\n        \"ecr:ListImages\",\n        \"ecr:BatchDeleteImage\",\n        \"ecr:GetLifecyclePolicy\",\n        \"ecr:PutLifecyclePolicy\",\n        \"ecr:TagResource\"\n      ],\n      \"Resource\": [\n        \"arn:aws:ecr:REGION:ECR_AWS_ACCOUNT_ID:repository/*/hopsworks-base\"\n      ]\n    }\n  ]\n}\n</code></pre> <pre><code>aws --profile PROFILE iam create-policy --policy-name POLICY_NAME --policy-document file://policy.json\n</code></pre>"},{"location":"setup_installation/aws/getting_started/#step-14-create-eks-cluster-using-eksctl","title":"Step 1.4: Create EKS cluster using eksctl","text":"<p>When creating the cluster using eksctl the following parameters are required in the cluster configuration YAML file (eksctl.yaml):</p> <ul> <li> <p>amiFamily should either be AmazonLinux2023 or Ubuntu2404</p> </li> <li> <p>Instance type should be Intel based or AMD (i.e not ARM)</p> </li> <li> <p>The following policies are required: IAM policies - eksctl</p> </li> </ul> <pre><code>- arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy\n- arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy\n- arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly\n</code></pre> <p>The following is required if you are using the EKS AWS Load Balancer Controller to grant permissions to the controller to provision the necessary load balancers Welcome: AWS Load Balancer Controller</p> <pre><code>      withAddonPolicies:\n        awsLoadBalancerController: true\n</code></pre> <p>You need to update the CLUSTER NAME and the POLICY ARN generated above</p> <pre><code>apiVersion: eksctl.io/v1alpha5\nkind: ClusterConfig\n\nmetadata:\n  name: CLUSTER_NAME\n  region: REGION\n  version: \"1.29\"\n\niam:\n  withOIDC: true\n\nmanagedNodeGroups:\n  - name: ng-1\n    amiFamily: AmazonLinux2023\n    instanceType: m6i.2xlarge\n    minSize: 1\n    maxSize: 5\n    desiredCapacity: 5\n    volumeSize: 100\n    ssh:\n      allow: true # will use ~/.ssh/id_rsa.pub as the default ssh key\n    iam:\n      attachPolicyARNs:\n        - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy\n        - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy\n        - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly\n        - arn:aws:iam::ECR_AWS_ACCOUNT_ID:policy/POLICYNAME\n      withAddonPolicies:\n        awsLoadBalancerController: true\naddons:\n  - name: aws-ebs-csi-driver\n    wellKnownPolicies:      # add IAM and service account\n      ebsCSIController: true\n</code></pre> <p>You can create the EKS cluster using the following eksctl command:</p> <pre><code>eksctl create cluster -f eksctl.yaml --profile PROFILE\n</code></pre> <p>Once the creation process is completed, you should be able to access the cluster using the kubectl CLI tool:</p> <pre><code>kubectl get nodes\n</code></pre> <p>You should see the list of nodes provisioned for the cluster.</p>"},{"location":"setup_installation/aws/getting_started/#step-14-install-the-aws-loadbalancer-addon","title":"Step 1.4: Install the AWS LoadBalancer Addon","text":"<p>For Hopsworks to provision the necessary network and application load balancers, we need to install the AWS LoadBalancer plugin (See AWS Documentation )</p> <pre><code>helm repo add eks https://aws.github.io/eks-charts\nhelm repo update eks\nhelm install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system --set clusterName=CLUSTER_NAME\n</code></pre>"},{"location":"setup_installation/aws/getting_started/#step-15-optional-create-gp3-storage-class","title":"Step 1.5: (Optional) Create GP3 Storage Class","text":"<p>By default EKS comes with GP2 as storage class. GP3 is more cost effective, we can use it with Hopsworks by creating the storage class</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: ebs-gp3\nprovisioner: ebs.csi.aws.com\nparameters:\n  csi.storage.k8s.io/fstype: xfs\n  type: gp3\nreclaimPolicy: Delete\nvolumeBindingMode: WaitForFirstConsumer\nallowVolumeExpansion: true\nEOF\n</code></pre>"},{"location":"setup_installation/aws/getting_started/#step-2-hopsworks-deployment","title":"Step 2: Hopsworks Deployment","text":"<p>This section describes the steps required to deploy the Hopsworks stack using Helm.</p>"},{"location":"setup_installation/aws/getting_started/#step-21-add-the-hopsworks-helm-repository","title":"Step 2.1: Add the Hopsworks Helm repository","text":"<ul> <li>Configure Repo</li> </ul> <p>To obtain access to the Hopsworks helm chart repository, please obtain an evaluation/startup licence.</p> <p>Once you have the helm chart repository URL, replace the environment variable $HOPSWORKS_REPO in the following command with this URL.</p> <pre><code>helm repo add hopsworks $HOPSWORKS_REPO\nhelm repo update hopsworks\n</code></pre> <ul> <li>Create Hopsworks namespace</li> </ul> <pre><code>kubectl create namespace hopsworks\n</code></pre> <ul> <li>Update values.aws.yml</li> </ul> <pre><code>global:\n  _hopsworks:\n    storageClassName: &amp;storageClass ebs-gp3\n    cloudProvider: \"AWS\"\n\n    managedDockerRegistery:\n      enabled: true\n      domain: \"ECR_AWS_ACCOUNT_ID.dkr.ecr.REGION.amazonaws.com\"\n      namespace: \"NAMESPACE\"\n      credHelper:\n        enabled: true\n        secretName: &amp;awsregcred \"awsregcred\"\n\n    managedObjectStorage:\n      enabled: true\n      s3:\n        bucket: \n          name: \"BUCKET_NAME\"\n        region: \"REGION\"\n\n    minio:\n      enabled: false\n\n    externalLoadBalancers:\n      enabled: true\n      class: null\n      annotations:\n        service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing\n\nhopsworks:\n  variables:\n    # awsregcred Secret contains the docker configuration to use Cloud\n    # specific docker login helper method instead of username/password\n    # Currently only AWS and GCP support this method\n    # Azure has deprecated it\n    docker_operations_managed_docker_secrets: *awsregcred\n    # We *need* to put awsregcred here because this is the list of\n    # Secrets that are copied from hopsworks namespace to Projects namespace\n    # during project creation.\n    docker_operations_image_pull_secrets: \"awsregcred\"\n  dockerRegistry:\n    preset:\n      usePullPush: false\n      secrets:\n        - *awsregcred\n  ingress:\n    enabled: true\n    ingressClassName: alb\n    annotations:\n      alb.ingress.kubernetes.io/scheme: internet-facing\n      alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-west-2:xxxxx:certificate/xxxxxxx\n      alb.ingress.kubernetes.io/target-type: ip\n      alb.ingress.kubernetes.io/target-group-attributes: stickiness.enabled=true,stickiness.lb_cookie.duration_seconds=300\n\n    host: &amp;host HOST_DOMAIN\n    hosts:\n      - *host\n    tls:\n      - hosts:\n        - *host\n\nrondb:\n  rondb:\n    resources:\n      requests:\n        storage:\n          classes:\n            default: *storageClass\n\nconsul:\n  consul:\n    server:\n      storageClass: *storageClass\n\nprometheus:\n  prometheus:\n    server:\n      persistentVolume:\n        storageClass: *storageClass\n</code></pre> <ul> <li>Run the Helm install</li> </ul> <pre><code>helm install hopsworks hopsworks/hopsworks --namespace hopsworks --values values.aws.yaml --timeout=600s\n</code></pre> <p>Using the kubectl CLI tool, you can track the deployment process. You can use the command below to track which pods are running and which ones are in the process of being provisioned. You can also use the command below to detect any failure.</p> <pre><code>kubectl -n hopsworks get pods\n</code></pre>"},{"location":"setup_installation/aws/getting_started/#step-3-resources-created","title":"Step 3: Resources Created","text":"<p>Using the Helm chart and the values files the following resources are created:</p> <p>Load Balancers:</p> <pre><code>    externalLoadBalancers:\n      enabled: true\n      class: null\n      annotations:\n        service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing\n</code></pre> <p>Enabling the external load balancer in the values.yml file provisions the following load balancers for the following services:</p> <ul> <li> <p>arrowflight : This load balancer is used to send queries from external clients to the Hopsworks Query Service</p> </li> <li> <p>kafka : This load balancer is used to send data to the Apache Kafka brokers for ingestion to the online feature store.</p> </li> <li> <p>rdrs: This load balancer is used to query online feature store data using the REST APIs</p> </li> <li> <p>mysql: This load balancer is used to query online feature store data using the MySQL APIs</p> </li> <li> <p>opensearch : This load balancer is used to query the Hopsworks vector database</p> </li> </ul> <p>On EKS using the AWS Load Balancers, the AWS controller deployed above will be responsible to provision the necessary load balancers. You can configure the load balancers using the annotations documented in the AWS Load Balancer controller guide</p> <p>You can enable/disable individual load balancers provisioning using the following values in the values.yml file:</p> <ul> <li> <p>kafka.externalLoadBalancer.enabled</p> </li> <li> <p>opensearch.externalLoadBalancer.enabled</p> </li> <li> <p>rdrs.externalLoadBalancer.enabled</p> </li> <li> <p>mysql.externalLoadBalancer.enabled</p> </li> </ul> <p>Other load balancer providers are also supported by providing the appropriate controller, class and annotations.</p> <p>Ingress:</p> <pre><code>  ingress:\n    enabled: true\n    ingressClassName: alb\n    annotations:\n      alb.ingress.kubernetes.io/scheme: internet-facing\n</code></pre> <p>Hopsworks UI and REST interface is available outside the K8s cluster using an Ingress. On AWS this is implemented by provisioning an application load balancer using the AWS load balancer controller. As per the load balancer above, the controller checks for the following annotations: Annotations - AWS Load Balancer Controller</p> <p>HTTPS is required to access the Hopsworks UI, therefore you need to add the following annotation:</p> <pre><code>alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-west-2:xxxxx:certificate/xxxxxxx\n</code></pre> <p>To configure the TLS certificate the Application Load Balancer should use to terminate the connection. The certificate should be available in the AWS Certificate Manager</p> <p>Cluster Roles and Cluster Role Bindings:</p> <p>By default a set of cluster roles are provisioned, if you don\u2019t have permissions to provision cluster roles or cluster role bindings, you should reach out to your K8s administrator. You should then provide the appropriate resource names as value in the values.yml file.</p>"},{"location":"setup_installation/aws/getting_started/#step-4-next-steps","title":"Step 4: Next steps","text":"<p>Check out our other guides for how to get started with Hopsworks and the Feature Store:</p> <ul> <li>Get started with the Hopsworks Feature Store</li> <li>Follow one of our tutorials</li> <li>Follow one of our Guide</li> </ul>"},{"location":"setup_installation/azure/getting_started/","title":"Azure - Getting started with AKS","text":"<p>Kubernetes and Helm are used to install &amp; run Hopsworks and the Feature Store in the cloud. They both integrate seamlessly with third-party platforms such as Databricks, SageMaker and KubeFlow. This guide shows how to set up the Hopsworks platform in your organization's Azure account.</p>"},{"location":"setup_installation/azure/getting_started/#prerequisites","title":"Prerequisites","text":"<p>To follow the instruction on this page you will need the following:</p> <ul> <li>Kubernetes Version: Hopsworks can be deployed on AKS clusters running Kubernetes &gt;= 1.27.0.</li> <li>An Azure resource group in which the Hopsworks cluster will be deployed.</li> <li>The azure CLI installed and logged in.</li> <li>kubectl (to manage the AKS cluster)</li> <li>helm (to deploy Hopsworks)</li> </ul>"},{"location":"setup_installation/azure/getting_started/#permissions","title":"Permissions","text":"<p>The deployment requires cluster admin access to create ClusterRoles, ServiceAccounts, and ClusterRoleBindings in AKS.</p> <p>A namespace is also required for deploying the Hopsworks stack. If you don\u2019t have permissions to create a namespace, ask your AKS administrator to provision one for you.</p> <p>To run all the commands on this page the user needs to have at least the following permissions on the Azure resource group:</p> <p>You will also need to have a role such as Application Administrator on the Azure Active Directory to be able to create the hopsworks.ai service principal.</p>"},{"location":"setup_installation/azure/getting_started/#step-1-azure-kubernetes-service-aks-setup","title":"Step 1: Azure Kubernetes Service (AKS) Setup","text":""},{"location":"setup_installation/azure/getting_started/#step-11-create-an-azure-blob-storage-account","title":"Step 1.1: Create an Azure Blob Storage Account","text":"<p>Create a storage account to host project data. Ensure that the storage account is in the same region as the AKS cluster for performance and cost reasons:</p> <pre><code>az storage account create --name $STORAGE_ACCOUNT_NAME --resource-group $RESOURCE_GROUP --location $REGION\n</code></pre> <p>Also, create the corresponding container:</p> <pre><code>az storage container create --account-name $STORAGE_ACCOUNT_NAME --name $CONTAINER_NAME\n</code></pre>"},{"location":"setup_installation/azure/getting_started/#step-12-create-an-azure-container-registry-acr","title":"Step 1.2: Create an Azure Container Registry (ACR)","text":"<p>Create an ACR to store the images used by Hopsworks:</p> <pre><code>az acr create --resource-group $RESOURCE_GROUP --name $CONTAINER_REGISTRY_NAME --sku Basic --location $REGION\n\nexport ACR_ID=`az acr show --name $CONTAINER_REGISTRY_NAME --resource-group $RESOURCE_GROUP --query \"id\" --output tsv`\n</code></pre>"},{"location":"setup_installation/azure/getting_started/#step-13-create-a-user-assigned-managed-identity","title":"Step 1.3: Create a User-Assigned Managed Identity","text":"<p>Create a user-assigned managed identity to grant AKS access to the storage account and container registry:</p> <pre><code>az identity create --name $UA_IDENTITY_NAME --resource-group $RESOURCE_GROUP\n\nexport UA_IDENTITY_PRINCIPAL_ID=`az identity show --name $UA_IDENTITY_NAME --resource-group $RESOURCE_GROUP --query principalId --output tsv`\nexport UA_IDENTITY_CLIENT_ID=`az identity show --name $UA_IDENTITY_NAME --resource-group $RESOURCE_GROUP --query clientId --output tsv`\nexport UA_IDENTITY_RESOURCE_ID=`az identity show --name $UA_IDENTITY_NAME --resource-group $RESOURCE_GROUP --query id --output tsv`\n</code></pre>"},{"location":"setup_installation/azure/getting_started/#step-14-grant-permissions-to-the-user-assigned-managed-identity","title":"Step 1.4: Grant permissions to the User-Assigned Managed Identity","text":"<p>Create a custom role definition with the minimum permissions needed to read and write to the storage account:</p> <pre><code>export STORAGE_ID=`az storage account show --name $STORAGE_ACCOUNT_NAME --resource-group $RESOURCE_GROUP --query \"id\" --output tsv`\n\naz role definition create --role-definition '{\n  \"Name\": \"hopsfs-storage-permissions\",\n  \"IsCustom\": true,\n  \"Description\": \"Allow HopsFS to access the storage container\",\n  \"Actions\": [\n    \"Microsoft.Storage/storageAccounts/blobServices/containers/write\",\n    \"Microsoft.Storage/storageAccounts/blobServices/containers/read\",\n    \"Microsoft.Storage/storageAccounts/blobServices/write\",\n    \"Microsoft.Storage/storageAccounts/blobServices/read\",\n    \"Microsoft.Storage/storageAccounts/listKeys/action\"\n  ],\n  \"NotActions\": [],\n  \"DataActions\": [\n    \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/delete\",\n    \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/read\",\n    \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/move/action\",\n    \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/write\"\n  ],\n  \"AssignableScopes\": [\n    \"'$STORAGE_ID'\"\n  ]\n}'\n\naz role assignment create --role hopsfs-storage-permissions --assignee-object-id $UA_IDENTITY_PRINCIPAL_ID --assignee-principal-type ServicePrincipal --scope $STORAGE_ID\n</code></pre>"},{"location":"setup_installation/azure/getting_started/#step-15-create-service-principal-for-hopsworks-services","title":"Step 1.5: Create Service Principal for Hopsworks services","text":"<p>Create a service principal to grant Hopsworks applications with access to the container registry. For example, Hopsworks uses this service principal to push new Python environments created via the Hopsworks UI.</p> <pre><code>export SP_PASSWORD=`az ad sp create-for-rbac --name $SP_NAME --scopes $ACR_ID --role AcrPush --years 1 --query \"password\" --output tsv`\nexport SP_USER_NAME=`az ad sp list --display-name $SP_NAME --query \"[].appId\" --output tsv`\nexport SP_RESOURCE_ID=`az ad sp list --display-name $SP_NAME --query \"[].id\" --output tsv`\n\naz role assignment create --role AcrDelete --assignee-object-id $SP_RESOURCE_ID --assignee-principal-type ServicePrincipal --scope $ACR_ID\n</code></pre>"},{"location":"setup_installation/azure/getting_started/#step-16-create-an-aks-kubernetes-cluster","title":"Step 1.6: Create an AKS Kubernetes Cluster","text":"<p>Provision an AKS cluster with a number of nodes:</p> <pre><code>az aks create --resource-group $RESOURCE_GROUP --name $KUBERNETES_CLUSTER_NAME --network-plugin azure \\\n    --enable-cluster-autoscaler --min-count 1 --max-count 4 --node-count 3 --node-vm-size Standard_D8_v4 \\\n    --attach-acr $CONTAINER_REGISTRY_NAME \\\n    --assign-identity $UA_IDENTITY_RESOURCE_ID --assign-kubelet-identity $UA_IDENTITY_RESOURCE_ID \\\n    --enable-managed-identity --generate-ssh-keys\n</code></pre>"},{"location":"setup_installation/azure/getting_started/#step-2-configure-kubectl","title":"Step 2: Configure kubectl","text":"<pre><code>az aks get-credentials --resource-group $RESOURCE_GROUP --name $KUBERNETES_CLUSTER_NAME --file ~/my-aks-kubeconfig.yaml\nexport KUBECONFIG=~/my-aks-kubeconfig.yaml\nkubectl config current-context\n</code></pre>"},{"location":"setup_installation/azure/getting_started/#step-3-create-secret-for-the-service-principal","title":"Step 3: Create Secret for the Service Principal","text":""},{"location":"setup_installation/azure/getting_started/#step-31-create-hopsworks-namespace","title":"Step 3.1: Create Hopsworks namespace","text":"<pre><code>kubectl create namespace hopsworks\n</code></pre>"},{"location":"setup_installation/azure/getting_started/#step-32-create-secret","title":"Step 3.2: Create secret","text":"<pre><code>kubectl create secret docker-registry azregcred \\\n    --namespace hopsworks \\\n    --docker-server=$CONTAINER_REGISTRY_NAME.azurecr.io \\\n    --docker-username=$SP_USER_NAME \\\n    --docker-password=$SP_PASSWORD\n</code></pre>"},{"location":"setup_installation/azure/getting_started/#step-4-setup-hopsworks-for-deployment","title":"Step 4: Setup Hopsworks for Deployment","text":""},{"location":"setup_installation/azure/getting_started/#step-41-add-the-hopsworks-helm-repository","title":"Step 4.1: Add the Hopsworks Helm repository","text":"<p>To obtain access to the Hopsworks helm chart repository, please obtain an evaluation/startup licence.</p> <p>Once you have the helm chart repository URL, replace the environment variable $HOPSWORKS_REPO in the following command with this URL.</p> <pre><code>helm repo add hopsworks $HOPSWORKS_REPO\nhelm repo update hopsworks\n</code></pre>"},{"location":"setup_installation/azure/getting_started/#step-42-create-helm-values-file","title":"Step 4.2: Create helm values file","text":"<p>Below is a simplifield values.azure.yaml file to get started which can be updated for improved performance and further customisation.</p> <pre><code>global:\n  _hopsworks:\n    storageClassName: null\n    cloudProvider: \"AZURE\"\n    managedDockerRegistery:\n      enabled: true\n      domain: \"CONTAINER_REGISTRY_NAME.azurecr.io\"\n      namespace: \"hopsworks\"\n      credHelper:\n        enabled: false\n        secretName: \"\"\n\n    minio:\n      enabled: false\n\nhopsworks:\n  variables:\n    docker_operations_managed_docker_secrets: &amp;azregcred \"azregcred\"\n    docker_operations_image_pull_secrets: *azregcred\n  dockerRegistry:\n    preset:\n      usePullPush: false\n      secrets:\n        - *azregcred\n\nhopsfs:\n  objectStorage:\n    enabled: true\n    provider: \"AZURE\"\n    azure:\n      storage:\n        account: \"STORAGE_ACCOUNT_NAME\"\n        container: \"STORAGE_ACCOUNT_CONTAINER_NAME\"\n        identityClientId: \"UA_IDENTITY_CLIENT_ID\"\n</code></pre>"},{"location":"setup_installation/azure/getting_started/#step-5-deploy-hopsworks","title":"Step 5: Deploy Hopsworks","text":"<p>Deploy Hopsworks in the created namespace.</p> <pre><code>helm install hopsworks hopsworks/hopsworks --namespace hopsworks --values values.azure.yaml --timeout=600s\n</code></pre> <p>Check that Hopsworks is installing on your provisioned AKS cluster.</p> <pre><code>kubectl get pods --namespace=hopsworks\n\nkubectl get svc -n hopsworks -o wide\n</code></pre> <p>Upon completion (circa 20 minutes), setup a load balancer to access Hopsworks:</p> <pre><code>kubectl expose deployment hopsworks --type=LoadBalancer --name=hopsworks-service --namespace &lt;namespace&gt;\n</code></pre>"},{"location":"setup_installation/azure/getting_started/#step-6-next-steps","title":"Step 6: Next steps","text":"<p>Check out our other guides for how to get started with Hopsworks and the Feature Store:</p> <ul> <li>Get started with the Hopsworks Feature Store</li> <li>Follow one of our tutorials</li> <li>Follow one of our Guide</li> </ul>"},{"location":"setup_installation/common/arrow_flight_duckdb/","title":"ArrowFlight Server with DuckDB","text":"<p>By default, Hopsworks uses big data technologies (Spark or Hive) to create training data and read data for Python clients. This is great for large datasets, but for small or moderately sized datasets (think of the size of data that would fit in a Pandas DataFrame in your local Python environment), the overhead of starting a Spark or Hive job and doing distributed data processing can be significant.</p> <p>ArrowFlight Server with DuckDB significantly reduces the time that Python clients need to read feature groups and batch inference data from the Feature Store, as well as creating moderately-sized in-memory training datasets.</p> <p>When the service is enabled, clients will automatically use it for the following operations:</p> <ul> <li>reading Feature Groups</li> <li>reading Queries</li> <li>reading Training Datasets</li> <li>creating In-Memory Training Datasets</li> <li>reading Batch Inference Data For larger datasets, clients can still make use of the Spark/Hive backend by explicitly setting <code>read_options={\"use_hive\": True}</code>.</li> </ul>"},{"location":"setup_installation/common/arrow_flight_duckdb/#service-configuration","title":"Service configuration","text":"<p>Note</p> <p>Supported only on AWS at the moment.</p> <p>The ArrowFlight Server is co-located with RonDB in the Hopsworks cluster. If the ArrowFlight Server is activated, RonDB and ArrowFlight Server can each use up to 50% of the available resources on the node, so they can co-exist without impacting each other. Just like RonDB, the ArrowFlight Server can be replicated across multiple nodes to serve more clients at lower latency. To guarantee high performance, each individual ArrowFlight Server instance processes client requests sequentially. Requests will be queued for up to 10 minutes before they are rejected.</p> <p> Activate ArrowFlight Server with DuckDB on a RonDB cluster </p> <p>To deploy ArrowFlight Server on a cluster:</p> <ol> <li>Select \"RonDB cluster\"</li> <li>Select an instance type with at least 16GB of memory and 4 cores. (*)</li> <li>Tick the checkbox <code>Enable ArrowFlight Server</code>.</li> </ol> <p>(*) The service should have at least the 2x the amount of memory available that a typical Python client would have.   Because RonDB and ArrowFlight Server share the same node we recommend selecting an instance type with at least 4x the client memory.   For example, if the service serves Python clients with typically 4GB of memory, an instance with at least 16GB of memory should be selected.   An instance with 16GB of memory will be able to read feature groups and training datasets of up to 10-100M rows, depending on the number of columns and size of the features (~2GB in parquet).   The same instance will be able to create point-in-time correct training datasets with 1-10M rows, also depending on the number and the size of the features.   Larger instances are able to handle larger datasets.   The numbers scale roughly linearly with the instance size.</p>"},{"location":"setup_installation/gcp/getting_started/","title":"GCP - Getting started with GKE","text":"<p>Kubernetes and Helm are used to install &amp; run Hopsworks and the Feature Store in the cloud. They both integrate seamlessly with third-party platforms such as Databricks, SageMaker and KubeFlow. This guide shows how to set up the Hopsworks platform in your organization's Google Cloud Platform's (GCP) account.</p>"},{"location":"setup_installation/gcp/getting_started/#prerequisites","title":"Prerequisites","text":"<p>To follow the instruction on this page you will need the following:</p> <ul> <li>Kubernetes Version: Hopsworks can be deployed on GKE clusters running Kubernetes &gt;= 1.27.0.</li> <li>gcloud CLI to provision the GCP resources</li> <li>gke-gcloud-auth-plugin to manage authentication with the GKE cluster</li> <li>helm to deploy Hopsworks</li> </ul>"},{"location":"setup_installation/gcp/getting_started/#permissions","title":"Permissions","text":"<ul> <li> <p>The deployment requires cluster admin access to create ClusterRoles, ServiceAccounts, and ClusterRoleBindings.</p> </li> <li> <p>A namespace is required to deploy the Hopsworks stack. If you don\u2019t have permissions to create a namespace, ask your GKE administrator to provision one.</p> </li> </ul>"},{"location":"setup_installation/gcp/getting_started/#step-1-gcp-gke-setup","title":"Step 1: GCP GKE Setup","text":""},{"location":"setup_installation/gcp/getting_started/#step-11-create-a-google-cloud-storage-gcs-bucket","title":"Step 1.1: Create a Google Cloud Storage (GCS) bucket","text":"<p>Create a bucket to store project data. Ensure the bucket is in the same region as your GKE cluster for performance and cost optimization.</p> <pre><code>gsutil mb -l $region gs://$bucket_name\n</code></pre>"},{"location":"setup_installation/gcp/getting_started/#step-12-create-service-account","title":"Step 1.2: Create Service Account","text":"<p>Create a file named <code>hopsworksai_role.yaml</code> with the following content:</p> <pre><code>title: Hopsworks AI Instances\ndescription: Role that allows Hopsworks AI Instances to access resources\nstage: GA\nincludedPermissions:\n- storage.buckets.get\n- storage.buckets.update\n- storage.multipartUploads.abort\n- storage.multipartUploads.create\n- storage.multipartUploads.list\n- storage.multipartUploads.listParts\n- storage.objects.create\n- storage.objects.delete\n- storage.objects.get\n- storage.objects.list\n- storage.objects.update\n- artifactregistry.repositories.create\n- artifactregistry.repositories.get\n- artifactregistry.repositories.uploadArtifacts\n- artifactregistry.repositories.downloadArtifacts\n- artifactregistry.tags.list\n- artifactregistry.tags.delete\n</code></pre> <p>Execute the following gcloud command to create a custom role from the file. Replace $PROJECT_ID with your GCP project id:</p> <pre><code>gcloud iam roles create hopsworksai_instances \\\n  --project=$PROJECT_ID \\\n  --file=hopsworksai_role.yaml\n</code></pre> <p>Execute the following gcloud command to create a service account for Hopsworks AI instances. Replace $PROJECT_ID with your GCP project id:</p> <pre><code>gcloud iam service-accounts create hopsworksai_instances \\\n  --project=$PROJECT_ID \\\n  --description=\"Service account for Hopsworks AI instances\" \\\n  --display-name=\"Hopsworks AI instances\"\n</code></pre> <p>Execute the following gcloud command to bind the custom role to the service account. Replace all occurrences $PROJECT_ID with your GCP project id:</p> <pre><code>gcloud projects add-iam-policy-binding $PROJECT_ID \\\n  --member=\"serviceAccount:hopsworks-ai-instances@$PROJECT_ID.iam.gserviceaccount.com\" \\\n  --role=\"projects/$PROJECT_ID/roles/hopsworksai_instances\"\n</code></pre>"},{"location":"setup_installation/gcp/getting_started/#step-13-create-a-gke-cluster","title":"Step 1.3: Create a GKE Cluster","text":"<pre><code>gcloud container clusters create &lt;cluster-name&gt; \\\n  --zone &lt;zone&gt; \\\n  --machine-type n2-standard-8 \\\n  --num-nodes 1 \\\n  --enable-ip-alias \\\n  --service-account my-service-account@my-project.iam.gserviceaccount.com\n</code></pre> <p>Once the creation process is completed, you should be able to access the cluster using the kubectl CLI tool:</p> <pre><code>kubectl get nodes\n</code></pre>"},{"location":"setup_installation/gcp/getting_started/#step-14-create-gcr-repository","title":"Step 1.4: Create GCR repository","text":"<p>Hopsworks allows users to customize images for Python jobs, Jupyter Notebooks, and (Py)Spark applications. These images should be stored in Google Container Registry (GCR). The GKE cluster needs access to a GCR repository to push project images.</p> <p>Enable Artifact Registry and create a GCR repository to store images:</p> <pre><code>gcloud artifacts repositories create &lt;repo-name&gt; \\\n  --repository-format=docker \\\n  --location=&lt;region&gt;\n</code></pre>"},{"location":"setup_installation/gcp/getting_started/#step-3-setup-hopsworks-for-deployment","title":"Step 3: Setup Hopsworks for Deployment","text":""},{"location":"setup_installation/gcp/getting_started/#step-31-add-the-hopsworks-helm-repository","title":"Step 3.1: Add the Hopsworks Helm repository","text":"<p>To obtain access to the Hopsworks helm chart repository, please obtain an evaluation/startup licence.</p> <p>Once you have the helm chart repository URL, replace the environment variable $HOPSWORKS_REPO in the following command with this URL.</p> <pre><code>helm repo add hopsworks $HOPSWORKS_REPO\nhelm repo update hopsworks\n</code></pre>"},{"location":"setup_installation/gcp/getting_started/#step-32-create-hopsworks-namespace","title":"Step 3.2: Create Hopsworks namespace","text":"<pre><code>kubectl create namespace hopsworks\n</code></pre>"},{"location":"setup_installation/gcp/getting_started/#step-33-create-helm-values-file","title":"Step 3.3: Create helm values file","text":"<p>Below is a simplifield values.gcp.yaml file to get started which can be updated for improved performance and further customisation.</p> <pre><code>global:\n  _hopsworks:\n    storageClassName: null\n    cloudProvider: \"GCP\"\n    managedDockerRegistery:\n      enabled: true\n      domain: \"europe-north1-docker.pkg.dev\"\n      namespace: \"PROJECT_ID/hopsworks\"\n      credHelper:\n        enabled: true\n        secretName: &amp;gcpregcred \"gcpregcred\"\n\n    managedObjectStorage:\n      enabled: true\n      s3:\n        bucket:\n          name: &amp;bucket \"hopsworks\"\n        region: &amp;region \"europe-north1\"\n        endpoint: &amp;gcpendpoint \"https://storage.cloud.google.com\"\n        secret:\n          name: &amp;gcpcredentials \"gcp-credentials\"\n          access_key_id: &amp;gcpaccesskey \"access-key-id\"\n          secret_key_id: &amp;gcpsecretkey \"secret-access-key\"\n    minio:\n      enabled: false\n</code></pre>"},{"location":"setup_installation/gcp/getting_started/#step-4-deploy-hopsworks","title":"Step 4: Deploy Hopsworks","text":"<p>Deploy Hopsworks in the created namespace.</p> <pre><code>helm install hopsworks hopsworks/hopsworks \\\n  --namespace hopsworks \\\n  --values values.gcp.yaml \\\n  --timeout=600s\n</code></pre> <p>Check that Hopsworks is installing on your provisioned AKS cluster.</p> <pre><code>kubectl get pods --namespace=hopsworks\n\nkubectl get svc -n hopsworks -o wide\n</code></pre> <p>Upon completion (circa 20 minutes), setup a load balancer to access Hopsworks:</p> <pre><code>kubectl expose deployment hopsworks --type=LoadBalancer --name=hopsworks-service --namespace &lt;namespace&gt;\n</code></pre>"},{"location":"setup_installation/gcp/getting_started/#step-5-next-steps","title":"Step 5: Next steps","text":"<p>Check out our other guides for how to get started with Hopsworks and the Feature Store:</p> <ul> <li>Get started with the Hopsworks Feature Store</li> <li>Follow one of our tutorials</li> <li>Follow one of our Guide</li> </ul>"},{"location":"setup_installation/on_prem/contact_hopsworks/","title":"Hopsworks On-Premise Installation","text":"<p>It is possible to use Hopsworks on-premises, which means that companies can run their machine learning workloads on their own hardware and infrastructure, rather than relying on a cloud provider. This can provide greater flexibility, control, and cost savings, as well as enabling companies to meet specific compliance and security requirements.</p> <p>Working on-premises with Hopsworks typically involves collaboration with the Hopsworks engineering teams, as each infrastructure is unique and requires a tailored approach to deployment and configuration. The process begins with an assessment of the company's existing infrastructure and requirements, including network topology, security policies, and hardware specifications.</p> <p>For further details about on-premise installations; contact us.</p>"},{"location":"setup_installation/on_prem/external_kafka_cluster/","title":"External Kafka cluster","text":"<p>Hopsworks uses Apache Kafka to ingest data to the feature store. Streaming applications and external clients send data to the Kafka cluster for ingestion to the online and offline feature store. By default, Hopsworks comes with an embedded Kafka cluster managed by Hopsworks itself, however, users can configure Hopsworks to leverage an existing external cluster. This guide will cover how to configure an Hopsworks cluster to leverage an external Kafka cluster.</p>"},{"location":"setup_installation/on_prem/external_kafka_cluster/#configure-the-external-kafka-cluster-integration","title":"Configure the external Kafka cluster integration","text":"<p>To enable the integration with an external Kafka cluster, you should set the <code>enable_bring_your_own_kafka</code> configuration option to <code>true</code>. This can also be achieved in the cluster definition by setting the following attribute:</p> <pre><code>hopsworks:\n  enable_bring_your_own_kafka: \"true\"\n</code></pre>"},{"location":"setup_installation/on_prem/external_kafka_cluster/#online-feature-store-service-configuration","title":"Online Feature Store service configuration","text":"<p>In addition to the configuration changes above, you should also configure the Online Feature Store service (OnlineFS in short) to connect to the external Kafka cluster. This can be achieved by provisioning the necessary credentials for OnlineFS to subscribe and consume messages from Kafka topics used by the Hopsworks feature store.</p> <p>OnlineFs can be configured to use these credentials by adding the following configurations to the cluster definition used to deploy Hopsworks:</p> <pre><code>  onlinefs:\n    config_dir: \"/home/ubuntu/cluster-definitions/byok\"\n    kafka_consumers:\n      topic_list: \"comma separated list of kafka topics to subscribe to\"\n</code></pre> <p>In particular, the <code>onlinefs/config_dir</code> should contain the credentials necessary for the Kafka consumers to authenticate. Additionally the directory should contain a file name <code>onlinefs-kafka.properties</code> with the Kafka consumer configuration. The following is an example of the <code>onlinefs-kafka.properties</code> file:</p> <pre><code>bootstrap.servers=cluster_identifier.us-east-2.aws.confluent.cloud:9092\nsecurity.protocol=SASL_SSL\nsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"username\" password=\"password\";\nsasl.mechanism=PLAIN\n</code></pre> <p>Hopsworks will not provision topics</p> <p>Please note that when using an external Kafka cluster, Hopsworks will not provision the topics for the different projects. Users are responsible for provisioning the necessary topics and configure the projects accordingly (see next section). Users should also specify the list of topics OnlineFS should subscribe to by providing the <code>onlinefs/kafka_consumers/topic_list</code> option in the cluster definition.</p>"},{"location":"setup_installation/on_prem/external_kafka_cluster/#project-configuration","title":"Project configuration","text":""},{"location":"setup_installation/on_prem/external_kafka_cluster/#topic-configuration","title":"Topic configuration","text":"<p>As mentioned above, when configuring Hopsworks to use an external Kafka cluster, Hopsworks will not provision the topics for the different projects. Instead, when creating a project, users will be asked to provide the topic name to use for the feature store operations.</p> <p> Example project creation when using an external Kafka cluster </p>"},{"location":"setup_installation/on_prem/external_kafka_cluster/#data-source-configuration","title":"Data Source configuration","text":"<p>Users should create a Kafka Data Source named <code>kafka_connector</code> which is going to be used by the feature store clients to configure the necessary Kafka producers to send data. The configuration is done for each project to ensure its members have the necessary authentication/authorization. If the data source is not found in the project, default values referring to Hopsworks managed Kafka will be used.</p>"},{"location":"tutorials/","title":"Tutorials","text":"<p>We are happy to welcome you to our collection of tutorials dedicated to exploring the fundamentals of Hopsworks and Machine Learning development. In addition to offering different types of use cases and common subjects in the field, it facilitates navigation and use of models in a production environment using Hopsworks Feature Store.</p>"},{"location":"tutorials/#how-to-run-the-tutorials","title":"How to run the tutorials","text":"<p>In order to run the tutorials, you will need a Hopsworks account. To do so, go to app.hopsworks.ai and create one. With a managed account, just run the Jupyter notebook from within Hopsworks. Generally the notebooks contain the information you will need on how to interact with the Hopsworks Platform.</p> <p>The easiest way to get started is by using Google Colab to run the notebooks. However, you can also run them in your local Python environment with Jupyter. You can find the raw notebook files in our tutorials repository.</p>"},{"location":"tutorials/#fraud-tutorial","title":"Fraud Tutorial","text":"<p>This is a quick-start of the Hopsworks Feature Store; using a fraud use case we will load data into the feature store, create two feature groups from which we will make a training dataset and train a model.</p>"},{"location":"tutorials/#batch","title":"Batch","text":"<p>This is a batch use case variant of the fraud tutorial, it will give you a high level view on how to use our python APIs and the UI to navigate the feature groups.</p> Notebooks 1. How to load, engineer and create feature groups 2. How to create training datasets 3. How to train a model from the feature store"},{"location":"tutorials/#online","title":"Online","text":"<p>This is a online use case variant of the fraud tutorial, it is similar to the batch use case, however, in this tutorial you will get introduced to the usage of Feature Groups which are kept in online storage, and how to access single feature vectors from the online storage at low latency. Additionally, the model will be deployed as a model serving instance, to provide a REST endpoint for real time serving.</p> Notebooks 1. How to load, engineer and create feature groups 2. How to create training datasets 3. How to train a model from the feature store and deploying it as a serving instance together with the online feature store"},{"location":"tutorials/#churn-tutorial","title":"Churn Tutorial","text":"<p>This is a churn tutorial with the Hopsworks feature store and model serving to build a prediction service. In this tutorial you will get introduced to the usage of Feature Groups which are kept in online storage, and how to access single feature vectors from the online storage at low latency. Additionally, the model will be deployed as a model serving instance, to provide a REST endpoint for real time serving.</p> Notebooks 1. How to load, engineer and create feature groups 2. How to create training datasets 3. How to train a model from the feature store and deploying it as a serving instance together with the online feature store"},{"location":"tutorials/#integration-tutorials","title":"Integration Tutorials","text":"<p>Hopsworks is easily integrated with many tools, especially from the Python world. In this section you will find examples for some popular libraries and services.</p>"},{"location":"tutorials/#great-expectations","title":"Great Expectations","text":"<p>Great Expectations is a library for data validation. You can use Great Expectations within Hopsworks to validate data which is to be inserted into the feature store, in order to ensure that only high-quality features end up in the feature store.</p> Notebooks 1. A brief introduction to Great Expectations concepts which are relevant for integration with the Hopsworks MLOps platform 2. How to integrate Great Expectations seamlessly with your Hopsworks feature pipelines"},{"location":"tutorials/#weights-and-biases","title":"Weights and Biases","text":"<p>Weights and Biases is a developer tool for machine learning model training that with a couple of lines of code let you keep track of hyperparameters, system metrics, and outputs so you can compare experiments, and easily share your findings with colleagues.</p> <p>This tutorial is a variant of the batch fraud tutorial using Weights and Biases for model training, tracking and as model registry.</p> Notebooks 1. How to load, engineer and create feature groups 2. How to create training datasets 3. How to train a model from the feature store and use Weights and Biases to track the process"},{"location":"user_guides/","title":"How-To Guides","text":"<p>This section serves to provide guides and examples for the common usage of abstractions and functionality of the Hopsworks platform through the Hopsworks UI and APIs.</p> <ul> <li>Client Installation: How to get started with the Hopsworks Client libraries.</li> <li>Feature Store: Learn about the common usage of the core Hopsworks Feature Store abstractions, such as Feature Groups, Feature Views, Data Validation and Data Sources.   Also, learn from the Client Integrations guides how to connect to the Feature Store from external environments such as a local Python environment, Databricks, or AWS Sagemaker</li> <li>MLOps: Learn about the common usage of Hopsworks MLOps abstractions, such as the Model Registry or Model Serving.</li> <li>Projects: The core abstraction on Hopsworks are Projects.   Learn in this section how to manage your projects and the services therein.</li> <li>Migration: Learn how to migrate to newer versions of Hopsworks.</li> </ul>"},{"location":"user_guides/client_installation/","title":"Client Installation Guide","text":""},{"location":"user_guides/client_installation/#hopsworks-python-library","title":"Hopsworks Python library","text":"<p>The Hopsworks Python client library is required to connect to Hopsworks from your local machine or any other Python environment such as Google Colab or AWS Sagemaker. Execute the following command to install the Hopsworks client library in your Python environment:</p> <p>Virtual environment</p> <p>It is recommended to use a virtual python environment instead of the system environment used by your operating system, in order to avoid any side effects regarding interfering dependencies.</p> <p>Windows/Conda Installation</p> <p>On Windows systems you might need to install twofish manually before installing hopsworks, if you don't have the Microsoft Visual C++ Build Tools installed.</p> <p>In that case, it is recommended to use a conda environment and run the following commands:</p> <pre><code>```bash\nconda install twofish\npip install hopsworks[python]\n```\n</code></pre> <pre><code>pip install hopsworks[python]\n</code></pre> <p>Supported versions of Python: 3.9, 3.10, 3.11, 3.12, 3.13 (PyPI \u2197)</p>"},{"location":"user_guides/client_installation/#profiles","title":"Profiles","text":"<p>The Hopsworks library has several profiles that bring additional dependencies and enable additional functionalities:</p> Profile Name Description No Profile This is the base installation. Supports interacting with the feature store metadata, model registry and deployments. It also supports reading and writing from the feature store from PySpark environments. <code>python</code> This profile enables reading and writing from/to the feature store from a Python environment <code>great-expectations</code> This profile installs the Great Expectations Python library and enables data validation on feature pipelines <code>polars</code> This profile installs the Polars library and enables reading and writing Polars DataFrames <p>You can install all the above profiles with the following command:</p> <pre><code>pip install hopsworks[python,great-expectations,polars]\n</code></pre>"},{"location":"user_guides/client_installation/#hsfs-java-library","title":"HSFS Java Library","text":"<p>If you want to interact with the Hopsworks Feature Store from environments such as Spark, Flink or Beam, you can use the Hopsworks Feature Store (HSFS) Java library.</p> <p>Feature Store Only</p> <p>The Java library only allows interaction with the Feature Store component of the Hopsworks platform. Additionally each environment might restrict the supported API operation. You can see which API operation is supported by which environment here</p> <p>The HSFS library is available on the Hopsworks' Maven repository. If you are using Maven as build tool, you can add the following in your <code>pom.xml</code> file:</p> <pre><code>&lt;repositories&gt;\n    &lt;repository&gt;\n        &lt;id&gt;Hops&lt;/id&gt;\n        &lt;name&gt;Hops Repository&lt;/name&gt;\n        &lt;url&gt;https://archiva.hops.works/repository/Hops/&lt;/url&gt;\n        &lt;releases&gt;\n            &lt;enabled&gt;true&lt;/enabled&gt;\n        &lt;/releases&gt;\n        &lt;snapshots&gt;\n            &lt;enabled&gt;true&lt;/enabled&gt;\n        &lt;/snapshots&gt;\n    &lt;/repository&gt;\n&lt;/repositories&gt;\n</code></pre> <p>The library has different builds targeting different environments:</p>"},{"location":"user_guides/client_installation/#hsfs-java","title":"HSFS Java","text":"<p>The <code>artifactId</code> for the HSFS Java build is <code>hsfs</code>, if you are using Maven as build tool, you can add the following dependency:</p> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;com.logicalclocks&lt;/groupId&gt;\n    &lt;artifactId&gt;hsfs&lt;/artifactId&gt;\n    &lt;version&gt;${hsfs.version}&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"user_guides/client_installation/#spark","title":"Spark","text":"<p>The <code>artifactId</code> for the Spark build is <code>hsfs-spark-spark{spark.version}</code>, if you are using Maven as build tool, you can add the following dependency:</p> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;com.logicalclocks&lt;/groupId&gt;\n    &lt;artifactId&gt;hsfs-spark-spark3.1&lt;/artifactId&gt;\n    &lt;version&gt;${hsfs.version}&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Hopsworks provides builds for Spark 3.1, 3.3 and 3.5. The builds are also provided as JAR files which can be downloaded from Hopsworks repository</p>"},{"location":"user_guides/client_installation/#flink","title":"Flink","text":"<p>The <code>artifactId</code> for the Flink build is <code>hsfs-flink</code>, if you are using Maven as build tool, you can add the following dependency:</p> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;com.logicalclocks&lt;/groupId&gt;\n    &lt;artifactId&gt;hsfs-flink&lt;/artifactId&gt;\n    &lt;version&gt;${hsfs.version}&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"user_guides/client_installation/#beam","title":"Beam","text":"<p>The <code>artifactId</code> for the Beam build is <code>hsfs-beam</code>, if you are using Maven as build tool, you can add the following dependency:</p> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;com.logicalclocks&lt;/groupId&gt;\n    &lt;artifactId&gt;hsfs-beam&lt;/artifactId&gt;\n    &lt;version&gt;${hsfs.version}&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"user_guides/client_installation/#next-steps","title":"Next Steps","text":"<p>If you are using a local python environment and want to connect to Hopsworks, you can follow the Python Guide section to create an API Key and to get started.</p>"},{"location":"user_guides/client_installation/#other-environments","title":"Other environments","text":"<p>The Hopsworks Feature Store client libraries can also be installed in external environments, such as Databricks, AWS Sagemaker, or Azure Machine Learning. For more information, see Client Integrations.</p>"},{"location":"user_guides/fs/","title":"Feature Store Guides","text":"<p>This section serves to provide guides and examples for the common usage of abstractions and functionality of the Feature Store through the Hopsworks UI and APIs.</p> <ul> <li>Data Sources</li> <li>Feature Groups</li> <li>Feature Views</li> <li>Vector Similarity Search</li> <li>Compute Engines</li> <li>Integrations</li> <li>Transformations</li> </ul>"},{"location":"user_guides/fs/compute_engines/","title":"Compute Engines","text":""},{"location":"user_guides/fs/compute_engines/#compute-engines","title":"Compute Engines","text":"<p>In order to execute a feature pipeline to write to the Feature Store, as well as to retrieve data from the Feature Store, you need a compute engine. Hopsworks Feature Store APIs are built around dataframes, that means feature data is inserted into the Feature Store from a Dataframe and likewise when reading data from the Feature Store, it is returned as a Dataframe.</p> <p>As such, Hopsworks supports five computational engines:</p> <ol> <li>Apache Spark: Spark Dataframes and Spark Structured Streaming Dataframes are supported, both from Python environments (PySpark) and from Scala environments.</li> <li>Python: For pure Python environments without dependencies on Spark, Hopsworks supports Pandas Dataframes and Polars Dataframes.</li> <li>Apache Flink: Flink Data Streams are currently supported as an experimental feature from Java/Scala environments.</li> <li>Apache Beam experimental: Beam Data Streams are currently supported as an experimental feature from Java/Scala environments.</li> <li>Java: For pure Java environments without dependencies on Spark, Hopsworks supports writing using List of POJO Objects.</li> </ol> <p>Hopsworks supports running compute on the platform itself in the form of Jobs or in Jupyter Notebooks. Alternatively, you can also connect to Hopsworks using Python or Spark from external environments, given that there is network connectivity.</p>"},{"location":"user_guides/fs/compute_engines/#functionality-support","title":"Functionality Support","text":"<p>Hopsworks is aiming to provide functional parity between the computational engines, however, there are certain Hopsworks functionalities which are exclusive to the engines.</p> Functionality Method Spark Python Flink Beam Java Comment Feature Group Creation from dataframes <code>FeatureStore.create_feature_group</code> - - - Currently Flink/Beam/Java doesn't support registering feature group metadata. Thus it needs to be pre-registered before you can write real time features computed by Flink/Beam. Training Dataset Creation from dataframes <code>TrainingDataset.save</code> - - - - Functionality was deprecated in version 3.0 Data validation using Great Expectations for streaming dataframes [<code>FeatureGroup.validate</code>][hsfs.feature_group.FeatureGroup.validate]  <code>FeatureGroup.insert_stream</code> - - - - - <code>insert_stream</code> does not perform any data validation even when a expectation suite is attached. Stream ingestion <code>FeatureGroup.insert_stream</code> - Python/Pandas/Polars has currently no notion of streaming. Reading from Streaming Storage Connectors <code>KafkaConnector.read_stream</code> - - - - Python/Pandas/Polars has currently no notion of streaming. For Flink/Beam/Java only write operations are supported Reading training data from external storage other than S3 <code>FeatureView.get_training_data</code> - - - - Reading training data that was written to external storage using a Storage Connector other than S3 can currently not be read using HSFS APIs, instead you will have to use the storage's native client. Reading External Feature Groups into Dataframe <code>ExternalFeatureGroup.read</code> - - - - Reading an External Feature Group directly into a Pandas/Polars Dataframe is not supported, however, you can use the Query API to create Feature Views/Training Data containing External Feature Groups. Read Queries containing External Feature Groups into Dataframe <code>Query.read</code> - - - - Reading a Query containing an External Feature Group directly into a Pandas/Polars Dataframe is not supported, however, you can use the Query to create Feature Views/Training Data and write the data to a Storage Connector, from where you can read up the data into a Pandas/Polars Dataframe."},{"location":"user_guides/fs/compute_engines/#python","title":"Python","text":""},{"location":"user_guides/fs/compute_engines/#python-inside-hopsworks","title":"Python Inside Hopsworks","text":"<p>If you are using Spark or Python within Hopsworks, there is no further configuration required. Head over to the Getting Started Guide.</p>"},{"location":"user_guides/fs/compute_engines/#python-outside-hopsworks","title":"Python Outside Hopsworks","text":"<p>Connecting to the Feature Store from any Python environment, such as your local environment or Google Colab, requires setting up an API Key and installing the HSFS Python client library. The Python integration guide explains step by step how to connect to the Feature Store from any Python environment.</p>"},{"location":"user_guides/fs/compute_engines/#spark","title":"Spark","text":""},{"location":"user_guides/fs/compute_engines/#spark-inside-hopsworks","title":"Spark Inside Hopsworks","text":"<p>If you are using Spark or Python within Hopsworks, there is no further configuration required. Head over to the Getting Started Guide.</p>"},{"location":"user_guides/fs/compute_engines/#spark-outside-hopsworks","title":"Spark Outside Hopsworks","text":"<p>Connecting to the Feature Store from an external Spark cluster, such as Cloudera or Databricks, requires configuring it with the Hopsworks client jars, configuration and certificates. The Spark integration guide explains step by step how to connect to the Feature Store from an external Spark cluster.</p>"},{"location":"user_guides/fs/compute_engines/#flink","title":"Flink","text":""},{"location":"user_guides/fs/compute_engines/#flink-inside-hopsworks","title":"Flink Inside Hopsworks","text":"<p>If you are using Flink within Hopsworks, there is no further configuration required. For more details head over to the Getting Started Guide.</p>"},{"location":"user_guides/fs/compute_engines/#flink-outside-hopsworks","title":"Flink Outside Hopsworks","text":"<p>Connecting to the Feature Store from an external Flink cluster, such as GCP DataProc or AWS EMR, requires configuring the Hopsworks certificates. The Flink integration guide explains step by step how to connect to the Feature Store from an external Flink cluster.</p>"},{"location":"user_guides/fs/compute_engines/#beam","title":"Beam","text":""},{"location":"user_guides/fs/compute_engines/#beam-inside-hopsworks","title":"Beam Inside Hopsworks","text":"<p>Beam is only supported as an external client.</p>"},{"location":"user_guides/fs/compute_engines/#beam-outside-hopsworks","title":"Beam Outside Hopsworks","text":"<p>Connecting to the Feature Store from Beam DataFlowRunner, requires configuring the Hopsworks certificates. The Beam integration guide explains step by step how to connect to the Feature Store from Beam Dataflow Runner.</p> <p>Warning</p> <p>Apache Beam integration with Hopsworks feature store was only tested using Dataflow Runner.</p> <p>For more details head over to the Getting Started Guide.</p>"},{"location":"user_guides/fs/compute_engines/#java","title":"Java","text":"<p>It is also possible to interact to Hopsworks feature store using pure Java environments without dependencies on Spark, Flink or Beam.</p> <p>For more details head over to the Getting Started Guide.</p>"},{"location":"user_guides/fs/transformation_functions/","title":"Transformation Functions","text":"<p>In AI systems, transformation functions transform data to create features, the inputs to machine learning models (in both training and inference). The\u00a0taxonomy of data transformations\u00a0introduces three types of data transformation prevalent in all AI systems. Hopsworks offers simple Python APIs to define custom transformation functions. These can be used along with\u00a0feature groups and feature views to create on-demand transformations and model-dependent transformations, producing modular AI pipelines that are skew-free.</p>"},{"location":"user_guides/fs/transformation_functions/#custom-transformation-function-creation","title":"Custom Transformation Function Creation","text":"<p>User-defined transformation functions can be created in Hopsworks using the [<code>@udf</code>][hsfs.hopsworks_udf.udf] decorator. These functions can be either implemented as pure Python UDFs or Pandas UDFs (User-Defined Functions).</p> <p>Hopsworks offers three execution modes to control the execution of transformation functions during training dataset creation, batch inference, and online inference. By default, Hopsworks executes transformation functions as Python UDFs for feature vector retrieval in online inference pipelines and as Pandas UDFs for both batch data retrieval in batch inference pipelines and training dataset creation in training pipelines. Python UDFs are optimized for smaller data volumes, while Pandas UDFs provide better performance on larger datasets. This execution mode provides the optimal balance based on the data size across training dataset generations, batch inference, and online inference. Additionally, Hopsworks allows you to explicitly set the execution mode for a transformation function to <code>python</code> or <code>pandas</code>, forcing the transformation function to always run as either a Python or Pandas UDF as specified.</p> <p>A Pandas UDF in Hopsworks accepts one or more Pandas Series as input and can return either one or more Series or a Pandas DataFrame. When integrated with PySpark applications, Hopsworks automatically executes Pandas UDFs using PySpark\u2019s <code>pandas_udf</code>, enabling the transformation functions to efficiently scale for large datasets.</p> <p>Java/Scala support</p> <p>Hopsworks supports transformations functions in Python (Pandas UDFs, Python UDFs). Transformations functions can also be executed in Python-based DataFrame frameworks (PySpark, Pandas). There is currently no support for transformation functions in SQL or Java-based feature pipelines.</p> <p>Transformation functions created in Hopsworks can be directly attached to feature views or feature groups or stored in the feature store for later retrieval. These functions can be part of a library installed in Hopsworks or be defined in a Jupyter notebook running a Python kernel or added when starting a Jupyter notebook or Hopsworks job.</p> <p>PySpark Kernels</p> <p>Definition transformation function within a Jupyter notebook is only supported in Python Kernel. In a PySpark Kernel transformation function have to defined as modules or added when starting a Jupyter notebook.</p> <p>The <code>@udf</code> decorator in Hopsworks creates a metadata class called <code>HopsworksUdf</code>. This class manages the necessary operations to execute the transformation function.</p> <p>The decorator accepts three parameters:</p> <ul> <li> <p><code>return_type</code> (required): Specifies the data type(s) of the features returned by the transformation function.   It can be a single Python type if the function returns one transformed feature, or a list of Python types if it returns multiple transformed features.   The supported Python types that be used with the <code>return_type</code> argument are provided in the table below:</p> Supported Python Types str int float bool datetime.datetime datetime.date datetime.time </li> <li> <p><code>drop</code> (optional): Identifies input arguments to exclude from the output after transformations are applied.   By default, all inputs are retained in the output.   Further details on this argument can be found below.</p> </li> <li> <p><code>mode</code> (optional): Determines the execution mode of the transformation function.   The argument accepts three values: <code>default</code>, <code>python</code>, or <code>pandas</code>.   By default, the <code>mode</code> is set to <code>default</code>.  Further details on this argument can be found below.</p> </li> </ul> <p>Hopsworks supports four types of transformation functions across all execution modes:</p> <ol> <li>One-to-one: Transforms one feature into one transformed feature.</li> <li>One-to-many: Transforms one feature into multiple transformed features.</li> <li>Many-to-one: Transforms multiple features into one transformed feature.</li> <li>Many-to-many: Transforms multiple features into multiple transformed features.</li> </ol>"},{"location":"user_guides/fs/transformation_functions/#one-to-one-transformations","title":"One-to-one transformations","text":"<p>To create a one-to-one transformation function, the Hopsworks <code>@udf</code> decorator must be provided with the <code>return_type</code> as a single Python type. The transformation function should take one argument as input and return a Pandas Series.</p> Python <p>Creation of a one-to-one transformation function in Hopsworks.</p> <pre><code>from hopsworks import udf\n\n@udf(return_type=int)\ndef add_one(feature):\n    return feature + 1\n</code></pre>"},{"location":"user_guides/fs/transformation_functions/#many-to-one-transformations","title":"Many-to-one transformations","text":"<p>The creation of many-to-one transformation functions is similar to that of a one-to-one transformation function, the only difference being that the transformation function accepts multiple features as input.</p> Python <p>Creation of a many-to-one transformation function in Hopsworks.</p> <pre><code>from hopsworks import udf\n\n@udf(return_type=int)\ndef add_features(feature1, feature2, feature3):\n    return feature1 + feature2 + feature3\n</code></pre>"},{"location":"user_guides/fs/transformation_functions/#one-to-many-transformations","title":"One-to-many transformations","text":"<p>To create a one-to-many transformation function, the Hopsworks\u00a0<code>@udf</code>\u00a0decorator must be provided with the\u00a0<code>return_type</code>\u00a0as a list of Python types, and the transformation function should take one argument as input and return multiple features as a Pandas DataFrame. The return types provided to the decorator must match the types of each column in the returned Pandas DataFrame.</p> Python <p>Creation of a one-to-many transformation function in Hopsworks.</p> <pre><code>from hopsworks import udf\nimport pandas as pd\n\n@udf(return_type=[int, int])\ndef add_one_and_two(feature1):\n    return feature1 + 1, feature1 + 2\n</code></pre>"},{"location":"user_guides/fs/transformation_functions/#many-to-many-transformations","title":"Many-to-many transformations","text":"<p>The creation of a many-to-many transformation function is similar to that of a one-to-many transformation function, the only difference being that the transformation function accepts multiple features as input.</p> Python <p>Creation of a many-to-many transformation function in Hopsworks.</p> <pre><code>from hopsworks import udf\nimport pandas as pd\n\n@udf(return_type=[int, int, int])\ndef add_one_multiple(feature1, feature2, feature3):\n    return feature1 + 1, feature2 + 1, feature3 + 1\n</code></pre>"},{"location":"user_guides/fs/transformation_functions/#specifying-execution-modes","title":"Specifying execution modes","text":"<p>The <code>mode</code> parameter of the <code>@udf</code> decorator can be used to specify the execution mode of the transformation function. It accepts three possible values <code>default</code>, <code>python</code> and <code>pandas</code>.  Each mode is explained in more detail below:</p>"},{"location":"user_guides/fs/transformation_functions/#default","title":"Default","text":"<p>This execution mode assumes that the transformation function can be executed as either a Pandas UDF or a Python UDF. It serves as the default mode used when the <code>mode</code> parameter is not specified. In this mode, the transformation function is executed as a Pandas UDF during training and in the batch inference pipeline, while it operates as a Python UDF during online inference.</p> Python <p>Creating a many to many transformations function using the default execution mode</p> <pre><code>from hopsworks import udf\nimport pandas as pd\n\n# \"default\" mode is used if the parameter `mode` is not explicitly set.\n@udf(return_type=[int, int, int])\ndef add_one_multiple(feature1, feature2, feature3):\n    return feature1 + 1, feature2 + 1, feature3 + 1\n\n@udf(return_type=[int, int, int], mode=\"default\")\ndef add_two_multiple(feature1, feature2, feature3):\n    return feature1 + 2, feature2 + 2, feature3 + 2\n</code></pre>"},{"location":"user_guides/fs/transformation_functions/#python","title":"Python","text":"<p>The transformation function can be configured to always execute as a Python UDF by setting the <code>mode</code> parameter of the <code>@udf</code> decorator to <code>python</code>.</p> Python <p>Creating a many to many transformation function as a Python UDF</p> <pre><code>from hopsworks import udf\nimport pandas as pd\n\n@udf(return_type=[int, int, int], mode = \"python\")\ndef add_one_multiple(feature1, feature2, feature3):\n    return feature1 + 1, feature2 + 1, feature3 + 1\n</code></pre>"},{"location":"user_guides/fs/transformation_functions/#pandas","title":"Pandas","text":"<p>The transformation function can be configured to always execute as a Pandas UDF by setting the <code>mode</code> parameter of the <code>@udf</code> decorator to <code>pandas</code>.</p> Python <p>Creating a many to many transformations function as a Pandas UDF</p> <pre><code>from hopsworks import udf\nimport pandas as pd\n\n# A Pandas UDF returning a Pandas DataFrame\n@udf(return_type=[int, int, int], mode = \"pandas\")\ndef add_one_multiple(feature1, feature2, feature3):\n    return pd.DataFrame({\"add_one_feature1\":feature1 + 1, \"add_one_feature2\":feature2 + 1, \"add_one_feature3\":feature3 + 1})\n\n# A Pandas UDF returning multiple Pandas Series\n@udf(return_type=[int, int, int], mode=\"pandas\")\ndef add_two_multiple(feature1, feature2, feature3):\n    return feature1 + 2, feature2 + 2, feature3 + 2\n</code></pre>"},{"location":"user_guides/fs/transformation_functions/#dropping-input-features","title":"Dropping input features","text":"<p>The\u00a0<code>drop</code>\u00a0parameter of the\u00a0<code>@udf</code>\u00a0decorator is used to drop specific columns in the input DataFrame after transformation.  If any argument of the transformation function is passed to the <code>drop</code> parameter, then the column mapped to the argument is dropped after the transformation functions are applied. In the example below, the columns mapped to the arguments\u00a0<code>feature1</code>\u00a0and\u00a0<code>feature3</code>\u00a0are dropped after the application of all transformation functions.</p> Python <p>Specify arguments to drop after transformation</p> <pre><code>from hopsworks import udf\nimport pandas as pd\n\n@udf(return_type=[int, int, int], drop=[\"feature1\", \"feature3\"])\ndef add_one_multiple(feature1, feature2, feature3):\n    return feature1 + 1, feature2 + 1, feature3 + 1\n</code></pre>"},{"location":"user_guides/fs/transformation_functions/#specifying-output-features-names-for-transformation-functions","title":"Specifying output features names for transformation functions","text":"<p>The <code>TransformationFunction.alias</code> function of a transformation function allows the specification of names of transformed features generated by the transformation function. Each name must be uniques and should be at-most 63 characters long. If no name is provided via the <code>alias</code> function, Hopsworks generates default output feature names when on-demand or model-dependent transformation functions are created.</p> Python <p>Specifying output column names for transformation functions.</p> <pre><code>from hopsworks import udf\nimport pandas as pd\n\n@udf(return_type=[int, int, int], drop=[\"feature1\", \"feature3\"])\ndef add_one_multiple(feature1, feature2, feature3):\n    return feature1 + 1, feature2 + 1, feature3 + 1\n\n# Specifying output feature names of the transformation function.\nadd_one_multiple.alias(\"transformed_feature1\", \"transformed_feature2\", \"transformed_feature3\")\n</code></pre>"},{"location":"user_guides/fs/transformation_functions/#training-dataset-statistics","title":"Training dataset statistics","text":"<p>A keyword argument <code>statistics</code> can be defined in the transformation function if it requires training dataset statistics for any of its arguments. The <code>statistics</code> argument must be assigned an instance of the class\u00a0<code>TransformationStatistics</code> as the default value. The\u00a0<code>TransformationStatistics</code>\u00a0instance must be initialized using the names of the arguments requiring statistics.</p> <p>Transformation Statistics</p> <p>The statistics provided to the transformation function is the statistics computed using the train set. Training dataset statistics are not available for on-demand transformations.</p> <p>The\u00a0<code>TransformationStatistics</code>\u00a0instance contains separate objects with the same name as the arguments used to initialize it. These objects encapsulate statistics related to the argument as instances of the\u00a0class <code>FeatureTransformationStatistics</code>. Upon instantiation, instances of\u00a0<code>FeatureTransformationStatistics</code>\u00a0contain\u00a0<code>None</code>\u00a0values and are updated with the required statistics after the creation of a training dataset.</p> Python <p>Creation of a transformation function in Hopsworks that uses training dataset statistics</p> <pre><code>from hopsworks import udf\nfrom hopsworks.transformation_statistics import TransformationStatistics\n\nstats = TransformationStatistics(\"argument1\", \"argument2\", \"argument3\")\n\n@udf(int)\ndef add_features(argument1, argument2, argument3, statistics=stats):\n    return argument1 + argument2 + argument3 + statistics.argument1.mean + statistics.argument2.mean + statistics.argument3.mean\n</code></pre>"},{"location":"user_guides/fs/transformation_functions/#passing-context-variables-to-transformation-function","title":"Passing context variables to transformation function","text":"<p>The <code>context</code> keyword argument can be defined in a transformation function to access shared context variables. These variables contain common data used across transformation functions. By including the context argument, you can pass the necessary data as a dictionary into the into the <code>context</code> argument of the transformation function during training dataset creation or feature vector retrieval or batch data retrieval.</p> Python <p>Creation of a transformation function in Hopsworks that accepts context variables</p> <pre><code>from hopsworks import udf\n\n@udf(int)\ndef add_features(argument1, context):\n    return argument1 + context[\"value_to_add\"]\n</code></pre>"},{"location":"user_guides/fs/transformation_functions/#saving-to-the-feature-store","title":"Saving to the Feature Store","text":"<p>To save a transformation function to the feature store, use the function\u00a0<code>create_transformation_function</code>.\u00a0It creates a\u00a0<code>TransformationFunction</code>\u00a0object which can then be saved by calling the\u00a0save\u00a0function. The save function will throw an error if another transformation function with the same name and version is already saved in the feature store.</p> Python <p>Register transformation function <code>add_one</code> in the Hopsworks feature store</p> <pre><code>plus_one_meta = fs.create_transformation_function(\n            transformation_function=add_one,\n            version=1)\nplus_one_meta.save()\n</code></pre>"},{"location":"user_guides/fs/transformation_functions/#retrieval-from-the-feature-store","title":"Retrieval from the Feature Store","text":"<p>To retrieve all transformation functions from the feature store, use the function\u00a0<code>get_transformation_functions</code>,\u00a0which returns the list of <code>TransformationFunction</code>\u00a0objects.</p> <p>A specific transformation function can be retrieved using its <code>name</code> and <code>version</code> with the function <code>get_transformation_function</code>. If only the <code>name</code> is provided, then the version will default to 1.</p> Python <p>Retrieving transformation functions from the feature store</p> <pre><code># get all transformation functions\nfs.get_transformation_functions()\n\n# get transformation function by name. This will default to version 1\nplus_one_fn = fs.get_transformation_function(name=\"plus_one\")\n\n# get transformation function by name and version.\nplus_one_fn = fs.get_transformation_function(name=\"plus_one\", version=2)\n</code></pre>"},{"location":"user_guides/fs/transformation_functions/#using-transformation-functions","title":"Using transformation functions","text":"<p>Transformation functions can be used by attaching it to a feature view to create model-dependent transformations or attached to feature groups to  create on-demand transformations</p>"},{"location":"user_guides/fs/vector_similarity_search/","title":"Vector Similarity Search","text":""},{"location":"user_guides/fs/vector_similarity_search/#introduction","title":"Introduction","text":"<p>Vector similarity search (also called similarity search) is a technique enabling the retrieval of similar items based on their vector embeddings or representations. Its applications range across various domains, from recommendation systems to image similarity and beyond. In Hopsworks, vector similarity search is enabled by extending an online feature group with approximate nearest neighbor search capabilities through a vector database, such as Opensearch. This guide provides a detailed walkthrough on how to leverage Hopsworks for vector similarity search.</p>"},{"location":"user_guides/fs/vector_similarity_search/#extending-feature-groups-with-similarity-search","title":"Extending Feature Groups with Similarity Search","text":"<p>In Hopsworks, each vector embedding in a feature group is stored in an index within the backing vector database. By default, vector embeddings are stored in the default index for the project (created for every project in Hopsworks), but you have the option to create a new index for a feature group if needed. Creating a separate index per feature group is particularly useful for large volumes of data, ensuring that when a feature group is deleted, its associated index is also removed. For feature groups that use the default project index, the index will only be removed when the project is deleted - not when the feature group is deleted. The index will store all the vector embeddings defined in that feature group, if you have more than one vector embedding in the feature group.</p> <p>In the following example, we explicitly define an index for the feature group:</p> <pre><code>from hsfs import embedding\n\n# Specify optionally the index in the vector database\nemb = embedding.EmbeddingIndex(index_name=\"news_fg\")\n</code></pre> <p>Then, add one or more embedding features to the index. Name and dimension of the embedding features are required for identifying which features should be indexed for k-nearest neighbor (KNN) search. In this example, we get the dimension of the embedding by taking the length of the value of the <code>embedding_heading</code> column in the first row of the dataframe <code>df</code>. Optionally, you can specify the similarity function among <code>l2_norm</code>, <code>cosine</code>, and <code>dot_product</code>. Refer to <code>EmbeddingIndex.add_embedding</code> for the full list of arguments.</p> <pre><code># Add embedding feature to the index\nemb.add_embedding(\"embedding_heading\", len(df[\"embedding_heading\"][0]))\n</code></pre> <p>Next, you create a feature group with the <code>embedding_index</code> and ingest data to the feature group. When the <code>embedding_index</code> is provided, the vector database is used as online feature store. That is, all the features in the feature group are stored exclusively in the vector database. The advantage of storing all features in the vector database is that it enables similarity search, and push-down filtering for all feature values.</p> <pre><code># Create a feature group with the embedding index\nnews_fg = fs.get_or_create_feature_group(\n    name=f\"news_fg\",\n    embedding_index=emb, # Provide the embedding index created\n    primary_key=[\"news_id\"],\n    version=version,\n    online_enabled=True\n)\n\n# Write a DataFrame to the feature group, including the offline store and the ANN index (in the Vector Database)\nnews_fg.insert(df)\n</code></pre>"},{"location":"user_guides/fs/vector_similarity_search/#similarity-search-for-feature-groups-using-vector-embeddings","title":"Similarity Search for Feature Groups using Vector Embeddings","text":"<p>You provide a vector embedding as a parameter to the search query using <code>FeatureGroup.find_neighbors</code>, and it returns the rows in the online feature group that have vector embedding values most similar to the provided vector embedding.</p> <p>It is also possible to filter rows by specifying a filter on any of the features in the feature group. The filter is pushed down to the vector database to improve query performance.</p> <p>In the first code snippet below, <code>find_neighbor</code>s returns 3 rows in <code>news_fg</code> that have the closest <code>news_description</code> values to the provided <code>news_description</code>. In the second code snippet below, we only return news articles with a <code>newstype</code> of <code>sports</code>.</p> <pre><code># Search neighbor embedding with k=3\nnews_fg.find_neighbors(model.encode(news_description), k=3)\n\n# Filter and search\nnews_fg.find_neighbors(model.encode(news_description), k=3, filter=news_fg.newstype == \"sports\")\n</code></pre> <p>To analyze feature values at specific points in time, you can utilize time travel functionality:</p> <pre><code># Time travel and read from the offline feature store\nnews_fg.as_of(time_in_past).read()\n</code></pre>"},{"location":"user_guides/fs/vector_similarity_search/#querying-similar-embeddings-with-additional-features","title":"Querying Similar Embeddings with Additional features","text":"<p>You can also use similarity search for vector embedding features in feature views. In the code snippet below, we create a feature view by selecting features from the earlier <code>news_fg</code> and a new feature group <code>view_fg</code>. If you include a feature group with vector embedding features in a feature view, whether or not the vector embedding features are selected, you can call <code>find_neighbors</code> on the feature view, and it will return rows containing all the feature values in the feature view. In the example below, a list of <code>heading</code> and <code>view_cnt</code> will be returned for the news articles which are closet to provided <code>news_description</code>.</p> <pre><code>view_fg = fs.get_or_create_feature_group(\n    name=\"view_fg\",\n    primary_key=[\"news_id\"],\n    version=version,\n    online_enabled=True\n)\n\nfv = fs.get_or_create_feature_view(\n    \"news_view\", version=version,\n    query=news_fg.select([\"heading\"]).join(view_fg.select([\"view_cnt\"]))\n)\n\nfv.find_neighbors(model.encode(news_description), k=5)\n</code></pre> <p>Note that you can use similarity search from the feature view only if the feature group which you are querying with <code>find_neighbors</code> has all the primary keys of the other feature groups. In the example above, you are querying against the feature group <code>news_fg</code> which has the vector embedding features, and it has the feature \"news_id\" which is the primary key of the feature group <code>view_fg</code>. But if <code>page_fg</code> is used as illustrated below, <code>find_neighbors</code> will fail to return any features because primary key <code>page_id</code> does not exist in <code>news_fg</code>.</p> <p> Cases when find_neighbors not works </p> <p>It is also possible to get back feature vector by providing the primary keys, but it is not recommended as explained in the next section. The client fetches feature vector from the vector store and the online store for <code>news_fg</code> and <code>view_fg</code> respectively.</p> <pre><code>fv.get_feature_vector({\"news_id\": 1})\n</code></pre>"},{"location":"user_guides/fs/vector_similarity_search/#performance-considerations-for-feature-groups-with-embeddings","title":"Performance considerations for Feature Groups with Embeddings","text":""},{"location":"user_guides/fs/vector_similarity_search/#choose-features-for-vector-store","title":"Choose Features for Vector Store","text":"<p>While it is possible to update feature value in vector store, updating feature value in online store is more efficient. If you have features which are frequently being updated and do not require for filtering, consider storing them separately in a different feature group. As shown in the previous example, <code>view_cnt</code> is updated frequently and stored separately. You can then get all the required features by using feature view.</p>"},{"location":"user_guides/fs/vector_similarity_search/#choose-the-appropriate-online-feature-stores","title":"Choose the Appropriate Online Feature Stores","text":"<p>There are 2 types of online feature stores in Hopsworks: online store (RonDB) and vector store (Opensearch). Online store is designed for retrieving feature vectors efficiently with low latency. Vector store is designed for finding similar embedding efficiently. If similarity search is not required, using online store is recommended for low latency retrieval of feature values including embedding.</p>"},{"location":"user_guides/fs/vector_similarity_search/#use-new-index-per-feature-group","title":"Use New Index per Feature Group","text":"<p>Create a new index per feature group to optimize retrieval performance.</p>"},{"location":"user_guides/fs/vector_similarity_search/#next-steps","title":"Next steps","text":"<p>Explore the news search example, demonstrating how to use Hopsworks for implementing a news search application using natural language in the application. Additionally, you can see the application of querying similar embeddings with additional features in this news rank example.</p>"},{"location":"user_guides/fs/data_source/","title":"Data Source Guides","text":"<p>You can define data sources in Hopsworks for batch and streaming data sources. Data Sources securely store the authentication information about how to connect to an external data store. They can be used from programs within Hopsworks or externally.</p> <p>Warning</p> <p>In the previous versions of Hopsworks, this used to be called a storage connector.</p> <p>There are four main use cases for Data Sources:</p> <ul> <li>Simply use it to read data from the storage into a dataframe.</li> <li>External (on-demand) Feature Groups can be defined with data sources.   This way, Hopsworks stores only the metadata about the features, but does not keep a copy of the data itself.   This is also called the Connector API.</li> <li>Write training data to an external storage system to make it accessible by third parties.</li> <li>Managed feature group that stores offline data in an external storage system.   Currently S3 and GCS connectors are supported.</li> </ul> <p>Data Sources provide two main mechanisms for authentication: using credentials or an authentication role (IAM Role on AWS or Managed Identity on Azure). Hopsworks supports both a single IAM role (AWS) or Managed Identity (Azure) for the whole Hopsworks cluster or multiple IAM roles (AWS) or Managed Identities (Azure) that can only be assumed by users with a specific role in a specific project.</p> <p>By default, each project is created with three default Data Sources: A JDBC connector to the online feature store, a HopsFS connector to the Training Datasets directory of the project and a JDBC connector to the offline feature store.</p> <p> </p> The Data Source View in the User Interface"},{"location":"user_guides/fs/data_source/#cloud-agnostic","title":"Cloud Agnostic","text":"<p>Cloud agnostic storage systems:</p> <ol> <li>JDBC: Connect to JDBC compatible databases and query them using SQL.</li> <li>Snowflake: Query Snowflake databases and tables using SQL.</li> <li>Kafka: Read data from a Kafka cluster into a Spark Structured Streaming Dataframe.</li> <li>HopsFS: Easily connect and read from directories of Hopsworks' internal File System.</li> </ol>"},{"location":"user_guides/fs/data_source/#aws","title":"AWS","text":"<p>For AWS the following storage systems are supported:</p> <ol> <li>S3: Read data from a variety of file based storage in S3 such as parquet or CSV.</li> <li>Redshift: Query Redshift databases and tables using SQL.</li> <li>RDS: Query Amazon RDS (Relational Database Service) using SQL.</li> </ol>"},{"location":"user_guides/fs/data_source/#azure","title":"Azure","text":"<p>For AWS the following storage systems are supported:</p> <ol> <li>ADLS: Read data from a variety of file based storage in ADLS such as parquet or CSV.</li> </ol>"},{"location":"user_guides/fs/data_source/#gcp","title":"GCP","text":"<p>For GCP the following storage systems are supported:</p> <ol> <li>BigQuery: Query BigQuery databases and tables using SQL.</li> <li>GCS: Read data from a variety of file based storage in Google Cloud Storage such as parquet or CSV.</li> </ol>"},{"location":"user_guides/fs/data_source/#next-steps","title":"Next Steps","text":"<p>Move on to the Configuration and Creation Guides to learn how to set up a data source.</p>"},{"location":"user_guides/fs/data_source/usage/","title":"Data Source Usage","text":"<p>Here, we look at how to use a Data Source after it has been created. Data Sources provide an important first step for integrating with external data. The 4 fundamental functionalities where data sources are used are:</p> <ol> <li>Reading data into Spark Dataframes</li> <li>Creating external feature groups</li> <li>Writing training data</li> <li>Creating managed feature groups</li> </ol> <p>We will walk through each functionality in the sections below.</p>"},{"location":"user_guides/fs/data_source/usage/#retrieving-a-data-source","title":"Retrieving a Data Source","text":"<p>We retrieve a data source simply by its unique name.</p> PySparkScala <pre><code>import hopsworks\n# Connect to the Hopsworks feature store\nproject = hopsworks.login()\nfeature_store = project.get_feature_store()\n# Retrieve data source\nds = feature_store.get_data_source('data_source_name')\n</code></pre> <pre><code>import com.logicalclocks.hsfs._\nval connection = HopsworksConnection.builder().build();\nval featureStore = connection.getFeatureStore();\n// get directly via connector sub-type class, e.g., for GCS type\nval connector = featureStore.getGcsConnector(\"data_source_name\")\n</code></pre>"},{"location":"user_guides/fs/data_source/usage/#reading-a-spark-dataframe-from-a-data-source","title":"Reading a Spark Dataframe from a Data Source","text":"<p>One of the most common usages of a Data Source is to read data directly into a Spark Dataframe. It's achieved via the <code>read</code> API of the connector object, which hides all the complexity of authentication and integration with a data storage source. The <code>read</code> API primarily has two parameters for specifying the data source, <code>path</code> and <code>query</code>, depending on the data source type. The exact behaviour could change depending on the fdata source type, but broadly they could be classified as below</p>"},{"location":"user_guides/fs/data_source/usage/#data-lakeobject-based-connectors","title":"Data lake/object based connectors","text":"<p>For data sources based on object/file storage such as AWS S3, ADLS, GCS, we set the full object path in the <code>path</code> argument and users should pass a Spark data format (parquet, csv, orc, hudi, delta) to the <code>data_format</code> argument.</p> PySparkScala <pre><code># read data into dataframe using path\ndf = connector.read(data_format='data_format', path='fileScheme://bucket/path/')\n</code></pre> <pre><code>// read data into dataframe using path\nval df = connector.read(\"\", \"data_format\", new HashMap(), \"fileScheme://bucket/path/\")\n</code></pre>"},{"location":"user_guides/fs/data_source/usage/#prepare-spark-api","title":"Prepare Spark API","text":"<p>Additionally, for reading file based data sources, another way to read the data is using the <code>prepare_spark</code> method. This method can be used if you are reading the data directly through Spark.</p> <p>Firstly, it handles the setup of all Spark configurations or properties necessary for a particular type of connector and prepares the absolute path to read from, along with bucket name and the appropriate file scheme of the data source. A Spark session can handle only one configuration setup at a time, so HSFS cannot set the Spark configurations when retrieving the connector since it would lead to only always initialising the last connector being retrieved. Instead, user can do this setup explicitly with the <code>prepare_spark</code> method and therefore potentially use multiple connectors in one Spark session. <code>prepare_spark</code> handles only one bucket associated with that particular connector, however, it is possible to set up multiple connectors with different types as long as their Spark properties do not interfere with each other. So, for example a S3 connector and a Snowflake connector can be used in the same session, without calling <code>prepare_spark</code> multiple times, as the properties don\u2019t interfere with each other.</p> <p>If the data source is used in another API call, <code>prepare_spark</code> gets implicitly invoked, for example, when a user materialises a training dataset using a data source or uses the data source to set up an External Feature Group. So users do not need to call <code>prepare_spark</code> every time they do an operation with a connector, it is only necessary when reading directly using Spark. Using <code>prepare_spark</code> is also not necessary when using the <code>read</code> API.</p> <p>For example, to read directly from a S3 connector, we use the <code>prepare_spark</code> as follows:</p> PySpark <pre><code>connector.prepare_spark()\nspark.read.format(\"json\").load(\"s3a://[bucket]/path\")\n# or\nspark.read.format(\"json\").load(connector.prepare_spark(\"s3a://[bucket]/path\"))\n</code></pre>"},{"location":"user_guides/fs/data_source/usage/#data-warehousesql-based-connectors","title":"Data warehouse/SQL based connectors","text":"<p>For data sources accessed via SQL such as data warehouses and JDBC compliant databases, e.g., Redshift, Snowflake, BigQuery, JDBC, users pass the SQL query to read the data to the <code>query</code> argument. In most cases, this will be some form of a <code>SELECT</code> query. Depending on the connector type, users can also just set the table path and read the whole table without explicitly passing any SQL query to the <code>query</code> argument. This is mostly relevant for Google BigQuery.</p> PySparkScala <pre><code># read results from a SQL\ndf = connector.read(query=\"SELECT * FROM TABLE\")\n# or directly read a table if set on connector\ndf = connector.read()\n</code></pre> <pre><code>// read results from a SQL\nval df = connector.read(\"SELECT * FROM TABLE\", \"\" , new HashMap(),\"\")\n</code></pre>"},{"location":"user_guides/fs/data_source/usage/#streaming-based-connector","title":"Streaming based connector","text":"<p>For reading data streams, the Kafka Data Source supports reading a Kafka topic into Spark Structured Streaming Dataframes instead of a static Dataframe as in other connector types.</p> PySpark <pre><code>df = connector.read_stream(topic='kafka_topic_name')\n</code></pre>"},{"location":"user_guides/fs/data_source/usage/#creating-an-external-feature-group","title":"Creating an External Feature Group","text":"<p>Another important aspect of a data source is its ability to facilitate creation of external feature groups with the Connector API. External feature groups are basically offline feature groups and essentially stored as tables on external data sources. The <code>Connector API</code> relies on data sources behind the scenes to integrate with external datasource. This enables seamless integration with any data source as long as there is a data source defined.</p> <p>To create an external feature group, we use the <code>create_external_feature_group</code> API, also known as <code>Connector API</code>, and simply pass the data source created before to the <code>data_source</code> argument. Depending on the external source, we should set either the <code>query</code> argument for data warehouse based sources, or the <code>path</code> and <code>data_format</code> arguments for data lake based sources, similar to reading into dataframes as explained in above section.</p> <p>Example for any data warehouse/SQL based external sources, we set the desired SQL to <code>query</code> argument, and set the <code>data_source</code> argument to the data source object of desired data source.</p> PySpark <pre><code>ds.query=\"SELECT * FROM TABLE\"\n\nfg = feature_store.create_external_feature_group(name=\"sales\",\n    version=1,\n    description=\"Physical shop sales features\",\n    data_source = ds,\n    primary_key=['ss_store_sk'],\n    event_time='sale_date'\n)\n</code></pre> <p><code>Connector API</code> (external feature groups) only stores the metadata about the features within Hopsworks, while the actual data is still stored externally. This enables users to create feature groups within Hopsworks without the hassle of data migration. For more information on <code>Connector API</code>, read detailed guide about external feature groups.</p>"},{"location":"user_guides/fs/data_source/usage/#writing-training-data","title":"Writing Training Data","text":"<p>Data Sources are also used while writing training data to external sources. While calling the Feature View API <code>create_training_data</code>, we can pass the <code>data_source</code> argument which is necessary to materialise the data to external sources, as shown below.</p> PySpark <pre><code># materialise a training dataset\nversion, job = feature_view.create_training_data(\n    description = 'describe training data',\n    data_format = 'spark_data_format', # e.g., data_format = \"parquet\" or data_format = \"csv\"\n    write_options = {\"wait_for_job\": False},\n    data_source = ds\n)\n</code></pre> <p>For a detailed walkthrough on managing and utilizing training data, refer to the training data guide.</p>"},{"location":"user_guides/fs/data_source/usage/#next-steps","title":"Next Steps","text":"<p>We have gone through the basic use cases of a data source. For more details about the API functionality for any specific connector type, checkout the [API section][hsfs.storage_connector.StorageConnector].</p>"},{"location":"user_guides/fs/data_source/creation/adls/","title":"How-To set up a ADLS Data Source","text":""},{"location":"user_guides/fs/data_source/creation/adls/#introduction","title":"Introduction","text":"<p>Azure Data Lake Storage (ADLS) Gen2 is a HDFS-compatible filesystem on Azure for data analytics. The ADLS Gen2 filesystem stores its data in Azure Blob storage, ensuring low-cost storage, high availability, and disaster recovery. In Hopsworks, you can access ADLS Gen2 by defining a Data Source and creating and granting permissions to a service principal.</p> <p>In this guide, you will configure a Data Source in Hopsworks to save all the authentication information needed in order to set up a connection to your Azure ADLS filesystem. When you're finished, you'll be able to read files using Spark through HSFS APIs. You can also use the connector to write out training data from the Feature Store, in order to make it accessible by third parties.</p> <p>Note</p> <p>Currently, it is only possible to create data sources in the Hopsworks UI. You cannot create a data source programmatically.</p>"},{"location":"user_guides/fs/data_source/creation/adls/#prerequisites","title":"Prerequisites","text":"<p>Before you begin this guide you'll need to retrieve the following information from your Azure ADLS account:</p> <ul> <li>Data Lake Storage Gen2 Account: Create an Azure Data Lake Storage Gen2 account and initialize a filesystem, enabling the hierarchical namespace. Note that your storage account must belong to an Azure resource group.</li> <li>Azure AD application and service principal: Create an Azure AD application and service principal that can access your ADLS storage account and its resource group.</li> <li>Service Principal Registration: Register the service principal, granting it a role assignment such as Storage Blob Data Contributor, on the Azure Data Lake Storage Gen2 account.</li> </ul> <p>Info</p> <p>When you specify the 'container name' in the ADLS data source, you need to have previously created that container - the Hopsworks Feature Store will not create that storage container for you.</p>"},{"location":"user_guides/fs/data_source/creation/adls/#creation-in-the-ui","title":"Creation in the UI","text":""},{"location":"user_guides/fs/data_source/creation/adls/#step-1-set-up-new-data-source","title":"Step 1: Set up new Data Source","text":"<p>Head to the Data Source View on Hopsworks (1) and set up a new data source (2).</p> <p> </p> The Data Source View in the User Interface"},{"location":"user_guides/fs/data_source/creation/adls/#step-2-enter-adls-information","title":"Step 2: Enter ADLS Information","text":"<p>Enter the details for your ADLS connector. Start by giving it a name and an optional description.</p> <p> </p> ADLS Connector Creation Form <ol> <li>Select \"Azure Data Lake\" as the storage.</li> <li>Set directory ID.</li> <li>Enter the Application ID.</li> <li>Paste the Service Credentials.</li> <li>Specify account name.</li> <li>Provide the container name.</li> <li>Click on \"Save Credentials\".</li> </ol>"},{"location":"user_guides/fs/data_source/creation/adls/#step-3-azure-create-an-adls-resource","title":"Step 3: Azure Create an ADLS Resource","text":"<p>When programmatically signing in, you need to pass the tenant ID with your authentication request and the application ID. You also need a certificate or an authentication key (described in the following section). To get those values, use the following steps:</p> <ol> <li>Select Azure Active Directory.</li> <li>From App registrations in Azure AD, select your application.</li> <li> <p>Copy the Directory (tenant) ID and store it in your application code.      You need to copy the Directory (tenant) id and paste it to the Hopsworks ADLS Data Source \"Directory id\" text field. </p> </li> <li> <p>Copy the Application ID and store it in your application code.      &gt;You need to copy the Application id and paste it to the Hopsworks ADLS Data Source \"Application id\" text field. </p> </li> <li> <p>Create an Application Secret and copy it into the Service Credential field.      You need to copy the Application Secret and paste it to the Hopsworks ADLS Data Source \"Service Credential\" text field. </p> </li> </ol>"},{"location":"user_guides/fs/data_source/creation/adls/#common-problems","title":"Common Problems","text":"<p>If you get a permission denied error when writing or reading to/from a ADLS container, it is often because the storage principal (app) does not have the correct permissions. Have you added the \"Storage Blob Data Owner\" or \"Storage Blob Data Contributor\" role to the resource group for your storage account (or the subscription for your storage group, if you apply roles at the subscription level)? Go to your resource group, then in \"Access Control (IAM)\", click the \"Add\" button to add a \"role assignment\".</p> <p>If you get an error \"StatusCode=404 StatusDescription=The specified filesystem does not exist.\", then maybe you have not created the storage account or the storage container.</p>"},{"location":"user_guides/fs/data_source/creation/adls/#references","title":"References","text":"<ul> <li>How to create a service principal on Azure</li> </ul>"},{"location":"user_guides/fs/data_source/creation/adls/#next-steps","title":"Next Steps","text":"<p>Move on to the usage guide for data sources to see how you can use your newly created ADLS connector.</p>"},{"location":"user_guides/fs/data_source/creation/bigquery/","title":"How-To set up a BigQuery Data Source","text":""},{"location":"user_guides/fs/data_source/creation/bigquery/#introduction","title":"Introduction","text":"<p>A BigQuery data source provides integration to Google Cloud BigQuery. BigQuery is Google Cloud's managed data warehouse supporting that lets you run analytics and execute SQL queries over large scale data. Such data warehouses are often the source of raw data for feature engineering pipelines.</p> <p>In this guide, you will configure a Data Source in Hopsworks to connect to your BigQuery project by saving the necessary information. When you're finished, you'll be able to execute queries and read results of BigQuery using Spark through HSFS APIs.</p> <p>The data source uses the Google <code>spark-bigquery-connector</code> behind the scenes. To read more about the spark connector, like the spark options or usage, check Apache Spark SQL connector for Google BigQuery.</p> <p>Note</p> <p>Currently, it is only possible to create data sources in the Hopsworks UI. You cannot create a data source programmatically.</p>"},{"location":"user_guides/fs/data_source/creation/bigquery/#prerequisites","title":"Prerequisites","text":"<p>Before you begin this guide you'll need to retrieve the following information about your GCP account:</p> <ul> <li>BigQuery Project: You need a BigQuery project, dataset and table created and have read access to it.   Or, if you wish to query a public dataset you need its corresponding details.</li> <li>Authentication Method: Authentication to GCP account is handled by uploading the <code>JSON keyfile for service account</code> to the Hopsworks Project.   You will need to create this JSON keyfile from GCP.   For more information on service accounts and creating keyfile in GCP, read Google Cloud documentation.</li> </ul> <p>Note</p> <pre><code>To read data, the BigQuery service account user needs permission to `create read session` which is available in **BigQuery Admin role**.\n</code></pre>"},{"location":"user_guides/fs/data_source/creation/bigquery/#creation-in-the-ui","title":"Creation in the UI","text":""},{"location":"user_guides/fs/data_source/creation/bigquery/#step-1-set-up-new-data-source","title":"Step 1: Set up new Data Source","text":"<p>Head to the Data Source View on Hopsworks (1) and set up a new data source (2).</p> <p> </p> The Data Source View in the User Interface"},{"location":"user_guides/fs/data_source/creation/bigquery/#step-2-enter-source-details","title":"Step 2: Enter source details","text":"<p>Enter the details for your BigQuery storage. Start by giving it a unique name and an optional description.</p> <p> </p> BigQuery Creation Form <ol> <li>Select \"Google BigQuery\" as the storage.</li> <li>Next, set the name of the parent BigQuery project.    This is used for billing by GCP.</li> <li>Authentication: Here you should upload your <code>JSON keyfile for service account</code> used for authentication.    You can choose to either upload from your local using <code>Upload new file</code> or choose an existing file within project using <code>From Project</code>.</li> <li>Read Options:    In the UI set the below fields,</li> <li>BigQuery Project: The BigQuery project to read</li> <li>BigQuery Dataset: The dataset of the table (Optional)</li> <li>BigQuery Table: The table to read (Optional)</li> </ol> <p>!!! note        Materialization Dataset: Temporary dataset used by BigQuery for writing.        It must be set to a dataset where the GCP user has table creation permission.        The queried table must be in the same location as the <code>materializationDataset</code> (e.g 'EU' or 'US').        Also, if a table in the <code>SQL statement</code> is from project other than the <code>parentProject</code> then use the fully qualified table name i.e. <code>[project].[dataset].[table]</code>.        For details, read the Google documentation on usage of query for BigQuery Spark connector.</p> <ol> <li>Spark Options: Optionally, you can set additional spark options using the <code>Key - Value</code> pairs.</li> <li>Click on \"Save Credentials\".</li> </ol>"},{"location":"user_guides/fs/data_source/creation/bigquery/#next-steps","title":"Next Steps","text":"<p>Move on to the usage guide for data sources to see how you can use your newly created BigQuery connector.</p>"},{"location":"user_guides/fs/data_source/creation/gcs/","title":"How-To set up a GCS Data Source","text":""},{"location":"user_guides/fs/data_source/creation/gcs/#introduction","title":"Introduction","text":"<p>This particular type of Data Source provides integration to Google Cloud Storage (GCS). GCS is an object storage service offered by Google Cloud. An object could be simply any piece of immutable data consisting of a file of any format, for example a <code>CSV</code> or <code>PARQUET</code>. These objects are stored in containers called as <code>buckets</code>.</p> <p>These types of storages are often the source for raw data from which features can be engineered.</p> <p>In this guide, you will configure a Data Source in Hopsworks to connect to your GCS bucket by saving the necessary information. When you're finished, you'll be able to read files from the GCS bucket using Spark through HSFS APIs.</p> <p>The Data Source uses the Google <code>gcs-connector-hadoop</code> behind the scenes. For more information, check out Google Cloud Data Source for Spark and Hadoop.</p> <p>Note</p> <p>Currently, it is only possible to create data sources in the Hopsworks UI. You cannot create a data source programmatically.</p>"},{"location":"user_guides/fs/data_source/creation/gcs/#prerequisites","title":"Prerequisites","text":"<p>Before you begin this guide you'll need to retrieve the following information about your GCP account and bucket:</p> <ul> <li>Bucket: You need a GCS bucket created and have read access to it.   The bucket is identified by its name.</li> <li>Authentication Method: Authentication to GCP account is handled by uploading the <code>JSON keyfile for service account</code> to the Hopsworks Project.   You will need to create this JSON keyfile from GCP.   For more information on service accounts and creating keyfile in GCP, read Google Cloud documentation.</li> <li>Server-side Encryption GCS encrypts the data on server side by default.   The connector additionally supports the optional encryption method <code>Customer Supplied Encryption Key</code> by GCP.   You can choose the encryption option <code>AES-256</code> and provide AES-256 key and hash, encoded in standard Base64.   The encryption details are stored as Secrets in the Hopsworks for keeping it secure.   Read more about encryption on Google Documentation.</li> </ul>"},{"location":"user_guides/fs/data_source/creation/gcs/#creation-in-the-ui","title":"Creation in the UI","text":""},{"location":"user_guides/fs/data_source/creation/gcs/#step-1-set-up-new-data-source","title":"Step 1: Set up new Data Source","text":"<p>Head to the Data Source View on Hopsworks (1) and set up a new data source (2).</p> <p> </p> The Data Source View in the User Interface"},{"location":"user_guides/fs/data_source/creation/gcs/#step-2-enter-connector-details","title":"Step 2: Enter connector details","text":"<p>Enter the details for your GCS connector. Start by giving it a unique name and an optional description.</p> <p> </p> GCS Connector Creation Form <ol> <li>Select <code>Google Cloud Storage</code> as the storage.</li> <li>Next, set the name of the GCS Bucket you wish to connect with.</li> <li>Authentication: Here you should upload your <code>JSON keyfile for service account</code> used for authentication.    You can choose to either upload from your local using <code>Upload new file</code> or choose an existing file within project using <code>From Project</code>.</li> <li>GCS Server Side Encryption: You can leave this to <code>Default Encryption</code> if you do not wish to provide explicit encrypting keys.    Otherwise, optionally you can set the encryption setting for <code>AES-256</code> and provide the encryption key and hash when selected.</li> <li>Click on <code>Save Credentials</code>.</li> </ol>"},{"location":"user_guides/fs/data_source/creation/gcs/#next-steps","title":"Next Steps","text":"<p>Move on to the usage guide for data sources to see how you can use your newly created GCS connector.</p>"},{"location":"user_guides/fs/data_source/creation/hopsfs/","title":"How-To set up a HopsFS Data Source","text":""},{"location":"user_guides/fs/data_source/creation/hopsfs/#introduction","title":"Introduction","text":"<p>HopsFS is a HDFS-compatible filesystem on AWS/Azure/on-premises for data analytics. HopsFS stores its data on object storage in the cloud (S3 in AWs and Blob storage on Azure) and on commodity servers on-premises, ensuring low-cost storage, high availability, and disaster recovery. In Hopsworks, you can access HopsFS natively in programs (Spark, TensorFlow, etc) without the need to define a Data Source. By default, every Project has a Data Source for Training Datasets. When you create training datasets from features in the Feature Store the HopsFS connector is the default Data Source. However, if you want to output data to a different dataset, you can define a new Data Source for that dataset.</p> <p>In this guide, you will configure a HopsFS Data Source in Hopsworks which points at a different directory on the file system than the Training Datasets directory. When you're finished, you'll be able to write training data to different locations in your cluster through HSFS APIs.</p> <p>Note</p> <p>Currently, it is only possible to create data sources in the Hopsworks UI. You cannot create a data source programmatically.</p>"},{"location":"user_guides/fs/data_source/creation/hopsfs/#prerequisites","title":"Prerequisites","text":"<p>Before you begin this guide you'll need to identify a directory on the filesystem of Hopsworks, to which you want to point the Data Source that you are going to create.</p>"},{"location":"user_guides/fs/data_source/creation/hopsfs/#creation-in-the-ui","title":"Creation in the UI","text":""},{"location":"user_guides/fs/data_source/creation/hopsfs/#step-1-set-up-new-data-source","title":"Step 1: Set up new Data Source","text":"<p>Head to the Data Source View on Hopsworks (1) and set up a new data source (2).</p> <p> </p> The Data Source View in the User Interface"},{"location":"user_guides/fs/data_source/creation/hopsfs/#step-2-enter-hopsfs-settings","title":"Step 2: Enter HopsFS Settings","text":"<p>Enter the details for your HopsFS connector. Start by giving it a name and an optional description.</p> <ol> <li>Select \"HopsFS\" as the storage.</li> <li>Select the top-level dataset to point the connector to.</li> <li>Click on \"Save Credentials\".</li> </ol> <p> </p> HopsFS Connector Creation Form"},{"location":"user_guides/fs/data_source/creation/hopsfs/#next-steps","title":"Next Steps","text":"<p>Move on to the usage guide for data sources to see how you can use your newly created HopsFS connector.</p>"},{"location":"user_guides/fs/data_source/creation/jdbc/","title":"How-To set up a JDBC Data Source","text":""},{"location":"user_guides/fs/data_source/creation/jdbc/#introduction","title":"Introduction","text":"<p>JDBC is an API provided by many database systems. Using JDBC connections one can query and update data in a database, usually oriented towards relational databases. Examples of databases you can connect to using JDBC are MySQL, Postgres, Oracle, DB2, MongoDB or Microsoft SQLServer.</p> <p>In this guide, you will configure a Data Source in Hopsworks to save all the authentication information needed in order to set up a JDBC connection to your database of choice. When you're finished, you'll be able to query the database using Spark through HSFS APIs.</p> <p>Note</p> <p>Currently, it is only possible to create data sources in the Hopsworks UI. You cannot create a data source programmatically.</p>"},{"location":"user_guides/fs/data_source/creation/jdbc/#prerequisites","title":"Prerequisites","text":"<p>Before you begin this guide you'll need to retrieve the following information from your JDBC compatible database:</p> <ul> <li>JDBC Connection URL: Consult the documentation of your target database to determine the correct JDBC URL and parameters.   As an example, for MySQL the URL could be:</li> </ul> <pre><code>jdbc:mysql://10.0.2.15:3306/[databaseName]?useSSL=false&amp;allowPublicKeyRetrieval=true\n</code></pre> <ul> <li>Username and Password: Typically, you will need to add username and password in your JDBC URL or as key/value parameters.   So make sure you have retrieved a username and password with the suitable permissions for the database and table you want to query.</li> </ul>"},{"location":"user_guides/fs/data_source/creation/jdbc/#creation-in-the-ui","title":"Creation in the UI","text":""},{"location":"user_guides/fs/data_source/creation/jdbc/#step-1-set-up-new-data-source","title":"Step 1: Set up new Data Source","text":"<p>Head to the Data Source View on Hopsworks (1) and set up a new data source (2).</p> <p> </p> The Data Source View in the User Interface"},{"location":"user_guides/fs/data_source/creation/jdbc/#step-2-enter-jdbc-settings","title":"Step 2: Enter JDBC Settings","text":"<p>Enter the details for your JDBC enabled database.</p> <p> </p> JDBC Connector Creation Form <ol> <li>Select \"JDBC\" as the storage.</li> <li>Enter the JDBC connection url.    This can for example also contain the username and password.</li> <li>Add additional key/value arguments to be passed to the connection, such as username or password.    These might differ by database.</li> </ol> <p>!!! note        Driver class name is a mandatory argument even if using the default MySQL driver.        Add it by specifying a property with the name <code>driver</code> and class name as value.        The driver class name will differ based on the database.        For MySQL databases, the class name is <code>com.mysql.cj.jdbc.Driver</code>, as shown in the example image.</p> <ol> <li>Click on \"Save Credentials\".</li> </ol> <p>Note</p> <p>To be able to use the connector, you need to upload the driver JAR file to the Jupyter configuration or Job configuration in <code>Additional Jars</code>. For MySQL connections the default JDBC driver is already included in Hopsworks so this step can be skipped.</p>"},{"location":"user_guides/fs/data_source/creation/jdbc/#next-steps","title":"Next Steps","text":"<p>Move on to the usage guide for data sources to see how you can use your newly created JDBC connector.</p>"},{"location":"user_guides/fs/data_source/creation/kafka/","title":"How-To set up a Kafka Data Source","text":""},{"location":"user_guides/fs/data_source/creation/kafka/#introduction","title":"Introduction","text":"<p>Apache Kafka is a distributed event store and stream-processing platform. It's a very popular framework for handling realtime data streams and is often used as a message broker for events coming from production systems until they are being processed and either loaded into a data warehouse or aggregated into features for Machine Learning.</p> <p>In this guide, you will configure a Data Source in Hopsworks to save all the authentication information needed in order to set up a connection to your Kafka cluster. When you're finished, you'll be able to read from Kafka topics in your cluster using Spark through HSFS APIs.</p> <p>Note</p> <p>Currently, it is only possible to create data sources in the Hopsworks UI. You cannot create a data source programmatically.</p>"},{"location":"user_guides/fs/data_source/creation/kafka/#prerequisites","title":"Prerequisites","text":"<p>Before you begin this guide you'll need to retrieve the following information from Kafka cluster, the following options are mandatory:</p> <ul> <li>Kafka Bootstrap servers: It is the url of one of the Kafka brokers which you give to fetch the initial metadata about your Kafka cluster.   The metadata consists of the topics, their partitions, the leader brokers for those partitions etc.   Depending upon this metadata your producer or consumer produces or consumes the data.</li> <li>Security Protocol: The security protocol you want to use to authenticate with your Kafka cluster.   Make sure the chosen protocol is supported by your cluster.   For an overview of the available protocols, please see the Confluent Kafka Documentation.</li> <li>Certificates: Depending on the chosen security protocol, you might need TrustStore and KeyStore files along with the corresponding key password.   Contact your Kafka administrator, if you don't know how to retrieve these.   If you want to setup a data source to Hopsworks' internal Kafka cluster, you can download the needed certificates from the integration tab in your project settings.</li> </ul>"},{"location":"user_guides/fs/data_source/creation/kafka/#creation-in-the-ui","title":"Creation in the UI","text":""},{"location":"user_guides/fs/data_source/creation/kafka/#step-1-set-up-new-data-source","title":"Step 1: Set up new Data Source","text":"<p>Head to the Data Source View on Hopsworks (1) and set up a new data source (2).</p> <p> </p> The Data Source View in the User Interface"},{"location":"user_guides/fs/data_source/creation/kafka/#step-2-enter-kafka-settings","title":"Step 2: Enter Kafka Settings","text":"<p>Enter the details for your Kafka connector. Start by giving it a name and an optional description.</p> <ol> <li>Select \"Kafka\" as the storage.</li> <li>Add all the bootstrap server addresses and ports that you want the consumers/producers to connect to.     The client will make use of all servers irrespective of which servers are specified here for bootstrapping\u2014this list only impacts the initial hosts used to discover the full set of servers.</li> <li> <p>Choose the Security protocol.</p> <p>TSL/SSL</p> <p>By default, Apache Kafka communicates in <code>PLAINTEXT</code>, which means that all data is sent in the clear. To encrypt communication, you should configure all the Confluent Platform components in your deployment to use TLS/SSL encryption.</p> <p>TLS uses private-key/certificate pairs, which are used during the TLS handshake process.</p> <p>Each broker needs its own private-key/certificate pair, and the client uses the certificate to authenticate the broker. Each logical client needs a private-key/certificate pair if client authentication is enabled, and the broker uses the certificate to authenticate the client.</p> <p>These are provided in the form of TrustStore and KeyStore <code>JKS</code> files together with a key password. For more information, refer to the official Apacha Kafka Guide for TSL/SSL authentication.</p> <p>SASL SSL or SASL plaintext</p> <p>Apache Kafka brokers support client authentication using SASL. SASL authentication can be enabled concurrently with TLS/SSL encryption (TLS/SSL client authentication will be disabled).</p> <p>This authentication method often requires extra arguments depending on your setup. Make use of the optional additional key/value arguments (5) to provide these.</p> <p>SASL authentication can be enabled concurrently with TLS/SSL encryption (TLS/SSL client authentication will be disabled). For more information, please refer to the official Apache Kafka Guide for SASL authentication.</p> </li> <li> <p>The endpoint identification algorithm used by clients to validate server host name.     The default value is <code>https</code>.     Clients including client connections created by the broker for inter-broker communication verify that the broker host name matches the host name in the broker\u2019s certificate.</p> </li> <li>Optional additional key/value arguments.</li> <li>Click on \"Save Credentials\".</li> </ol> <p> </p> Kafka Connector Creation Form"},{"location":"user_guides/fs/data_source/creation/kafka/#next-steps","title":"Next Steps","text":"<p>Move on to the usage guide for Data Sources to see how you can use your newly created Kafka connector.</p>"},{"location":"user_guides/fs/data_source/creation/rds/","title":"How-To set up an Amazon RDS JDBC Data Source","text":""},{"location":"user_guides/fs/data_source/creation/rds/#introduction","title":"Introduction","text":"<p>Amazon RDS (Relational Database Service) is a managed relational database service that supports several popular database engines, such as MySQL, PostgreSQL, Oracle, and Microsoft SQL Server. Using JDBC connections, you can query and update data in your RDS database from Hopsworks.</p> <p>In this guide, you will configure a Data Source in Hopsworks to securely store the authentication information needed to set up a JDBC connection to your Amazon RDS instance. Once configured, you will be able to query your RDS database.</p> <p>Note</p> <p>Currently, it is only possible to create data sources in the Hopsworks UI. You cannot create a data source programmatically.</p>"},{"location":"user_guides/fs/data_source/creation/rds/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following information from your Amazon RDS instance:</p> <ul> <li> <p>Host: You can find the endpoint for your RDS instance in the AWS Console.</p> <ol> <li>Go to the AWS Console \u2192 <code>Aurora and RDS</code></li> <li>Click on your DB instance.</li> <li>Under <code>Connectivity &amp; security</code>, you'll find the endpoint</li> </ol> <p>Example:</p> <pre><code>mydb.abcdefg1234.us-west-2.rds.amazonaws.com\n</code></pre> </li> <li> <p>Database: You can specify which database to use</p> </li> <li> <p>Port: Provide the port to connect to</p> </li> <li> <p>Username and Password: Obtain the username and password for your RDS database with the necessary permissions to access the required tables.</p> </li> </ul>"},{"location":"user_guides/fs/data_source/creation/rds/#creation-in-the-ui","title":"Creation in the UI","text":""},{"location":"user_guides/fs/data_source/creation/rds/#step-1-set-up-a-new-data-source","title":"Step 1: Set up a new Data Source","text":"<p>Head to the Data Source View on Hopsworks (1) and set up a new data source (2).</p> <p> </p> The Data Source View in the User Interface"},{"location":"user_guides/fs/data_source/creation/rds/#step-2-enter-rds-settings","title":"Step 2: Enter RDS Settings","text":"<p>Enter the details for your Amazon RDS database.</p> <p> </p> RDS Connector Creation Form <ol> <li>Select \"RDS\" as the storage.</li> <li>Paste the Host details.</li> <li>Enter the database name.</li> <li>Specify which port to use.</li> <li>Provide the username and password.</li> <li>Click on \"Save Credentials\".</li> </ol>"},{"location":"user_guides/fs/data_source/creation/rds/#next-steps","title":"Next Steps","text":"<p>Proceed to the usage guide for data sources to learn how to use your newly created connector.</p>"},{"location":"user_guides/fs/data_source/creation/redshift/","title":"How-To set up a Redshift Data Source","text":""},{"location":"user_guides/fs/data_source/creation/redshift/#introduction","title":"Introduction","text":"<p>Amazon Redshift is a popular managed data warehouse on AWS, used as a data warehouse in many enterprises.</p> <p>Data warehouses are often the source of raw data for feature engineering pipelines and Redshift supports scalable feature computation with SQL. However, Redshift is not viable as an online feature store that serves features to models in production, with its columnar database layout its latency is too high compared to OLTP databases or key-value stores.</p> <p>In this guide, you will configure a Data Source in Hopsworks to save all the authentication information needed in order to set up a connection to your AWS Redshift cluster. When you're finished, you'll be able to query the database using Spark through HSFS APIs.</p> <p>Note</p> <p>Currently, it is only possible to create data sources in the Hopsworks UI. You cannot create a data source programmatically.</p>"},{"location":"user_guides/fs/data_source/creation/redshift/#prerequisites","title":"Prerequisites","text":"<p>Before you begin this guide you'll need to retrieve the following information from your AWS account and Redshift database, the following options are mandatory:</p> <ul> <li>Cluster identifier: The name of the cluster.</li> <li>Database endpoint: The endpoint for the database.   Should be in the format of <code>[UUID].eu-west-1.redshift.amazonaws.com</code>.</li> <li>Database name: The name of the database to query.</li> <li>Database port: The port of the cluster.   Defaults to 5349.</li> <li>Authentication method: There are three options available for authenticating with the Redshift cluster.   The first option is to configure a username and a password.   The second option is to configure an IAM role.   With IAM roles, Jobs or notebooks launched on Hopsworks do not need to explicitly authenticate with Redshift, as the HSFS library will transparently use the IAM role to acquire a temporary credential to authenticate the specified user.   Read more about IAM roles in our AWS credentials pass-through guide.   Lastly, option <code>Instance Role</code> will use the default ARN Role configured for the cluster instance.</li> </ul>"},{"location":"user_guides/fs/data_source/creation/redshift/#creation-in-the-ui","title":"Creation in the UI","text":""},{"location":"user_guides/fs/data_source/creation/redshift/#step-1-set-up-new-data-source","title":"Step 1: Set up new Data Source","text":"<p>Head to the Data Source View on Hopsworks (1) and set up a new data source (2).</p> <p> </p> The Data Source View in the User Interface"},{"location":"user_guides/fs/data_source/creation/redshift/#step-2-enter-the-connector-information","title":"Step 2: Enter The Connector Information","text":"<p>Enter the details for your Redshift connector. Start by giving it a name and an optional description.</p> <ol> <li>Select \"Redshift\" as the storage.</li> <li>The name of the cluster.</li> <li>The database endpoint.     Should be in the format <code>[UUID].eu-west-1.redshift.amazonaws.com</code>.     For example, if the endpoint info displayed in Redshift is <code>cluster-id.uuid.eu-north-1.redshift.amazonaws.com:5439/dev</code> the value to enter here is just <code>uuid.eu-north-1.redshift.amazonaws.com</code></li> <li>The database name.</li> <li>The database port.</li> <li>The database username, here you have the possibility to let Hopsworks auto-create the username for you.</li> <li>Database Driver (optional): You can use the default JDBC Redshift Driver <code>com.amazon.redshift.jdbc42.Driver</code> included in Hopsworks or set a different driver (More on this later).</li> <li>Optionally provide the database group and table for the connector.     A database group is the group created for the user if applicable.     More information, at redshift documentation</li> <li>Set the appropriate authentication method.</li> <li>Click on \"Save Credentials\".</li> </ol> <p> </p> Redshift Connector Creation Form <p>Session Duration</p> <p>By default, the session duration that the role will be assumed for is 1 hour or 3600 seconds. This means if you want to use the data source for example to read or create an external Feature Group from Redshift, the operation cannot take longer than one hour.</p> <p>Your administrator can change the default session duration for AWS data sources, by first increasing the max session duration of the IAM Role that you are assuming. And then changing the <code>fs_data_source_session_duration</code> configuration property to the appropriate value in seconds.</p>"},{"location":"user_guides/fs/data_source/creation/redshift/#step-3-upload-the-redshift-database-driver-optional","title":"Step 3: Upload the Redshift database driver (optional)","text":"<p>The <code>redshift-jdbc42</code> JDBC driver is included by default in the Hopsworks distribution. If you wish to use a different driver, you need to upload it on Hopsworks and add it as a dependency of Jobs and Jupyter Notebooks that need it. First, you need to download the library. Select the driver version without the AWS SDK.</p>"},{"location":"user_guides/fs/data_source/creation/redshift/#add-the-driver-to-jupyter-notebooks-and-spark-jobs","title":"Add the driver to Jupyter Notebooks and Spark jobs","text":"<p>You can now add the driver file to the default job and Jupyter configuration. This way, all jobs and Jupyter instances in the project will have the driver attached so that Spark can access it.</p> <ol> <li>Go into the Project's settings.</li> <li>Select \"Compute configuration\".</li> <li>Select \"Spark\".</li> <li>Under \"Additional Jars\" choose \"Upload new file\" to upload the driver jar file.</li> </ol> <p> </p> Attaching the Redshift Driver to all Jobs and Jupyter Instances of the Project <p>Alternatively, you can choose the \"From Project\" option. You will first have to upload the jar file to the Project using the File Browser. After you have uploaded the jar file, you can select it using the \"From Project\" option. To upload the jar file to the Project through the File Browser, see the example below:</p> <ol> <li>Open File Browser</li> <li>Navigate to \"Resources\" directory</li> <li>Upload the jar file</li> </ol> <p> </p> Redshift Driver Upload in the File Browser <p>Tip</p> <p>If you face network connectivity issues to your Redshift cluster, a common cause could be the cluster database port not being accessible from outside the Redshift cluster VPC network. A quick and dirty way to enable connectivity is to Enable Publicly Accessible. However, in a production setting, you should use VPC peering or some equivalent mechanism for connecting the clusters.</p>"},{"location":"user_guides/fs/data_source/creation/redshift/#next-steps","title":"Next Steps","text":"<p>Move on to the usage guide for data sources to see how you can use your newly created Redshift connector.</p>"},{"location":"user_guides/fs/data_source/creation/s3/","title":"How-To set up a S3 Data Source","text":""},{"location":"user_guides/fs/data_source/creation/s3/#introduction","title":"Introduction","text":"<p>Amazon S3 or Amazon Simple Storage Service is a service offered by AWS that provides object storage. That means you can store arbitrary objects associated with a key. These kinds of storage systems are often used as Data Lakes with large volumes of unstructured data or file based storage. Popular file formats are <code>CSV</code> or <code>PARQUET</code>.</p> <p>There are so called Data Lake House technologies such as Delta Lake or Apache Hudi, building an additional layer on top of object based storage with files, to provide database semantics like ACID transactions among others. This has the advantage that cheap storage can be turned into a cloud native data warehouse. These kinds of storages are often the source for raw data from which features can be engineered.</p> <p>In this guide, you will configure a Data Source in Hopsworks to save all the authentication information needed in order to set up a connection to your AWS S3 bucket. When you're finished, you'll be able to read files using Spark through HSFS APIs. You can also use the connector to write out training data from the Feature Store, in order to make it accessible by third parties.</p> <p>Note</p> <p>Currently, it is only possible to create data sources in the Hopsworks UI.</p> <p>You cannot create a data source programmatically.</p>"},{"location":"user_guides/fs/data_source/creation/s3/#prerequisites","title":"Prerequisites","text":"<p>Before you begin this guide you'll need to retrieve the following information from your AWS S3 account and bucket:</p> <ul> <li>Bucket: You will need a S3 bucket that you have access to.   The bucket is identified by its name.</li> <li>Path (Optional): If needed, a path can be defined to ensure that all operations are restricted to a specific location within the bucket.</li> <li>Region (Optional): You will need an S3 region to have complete control over data when managing the feature group that relies on this data source.   The region is identified by its code.</li> <li>Authentication Method: You can authenticate using Access Key/Secret, or use IAM roles.   If you want to use an IAM role it either needs to be attached to the entire Hopsworks cluster or Hopsworks needs to be able to assume the role.   See IAM role documentation for more information.</li> <li>Server Side Encryption details: If your bucket has server side encryption (SSE) enabled, make sure you know which algorithm it is using (AES256 or SSE-KMS).   If you are using SSE-KMS, you need the resource ARN of the managed key.</li> </ul>"},{"location":"user_guides/fs/data_source/creation/s3/#creation-in-the-ui","title":"Creation in the UI","text":""},{"location":"user_guides/fs/data_source/creation/s3/#step-1-set-up-new-data-source","title":"Step 1: Set up new Data Source","text":"<p>Head to the Data Source View on Hopsworks (1) and set up a new data source (2).</p> <p> </p> The Data Source View in the User Interface"},{"location":"user_guides/fs/data_source/creation/s3/#step-2-enter-bucket-information","title":"Step 2: Enter Bucket Information","text":"<p>Enter the details for your S3 connector. Start by giving it a name and an optional description. And set the name of the S3 Bucket you want to point the connector to. Optionally, specify the region if you wish to have a Hopsworks-managed feature group stored using this connector.</p> <p> </p> S3 Connector Creation Form"},{"location":"user_guides/fs/data_source/creation/s3/#step-3-configure-authentication","title":"Step 3: Configure Authentication","text":""},{"location":"user_guides/fs/data_source/creation/s3/#instance-role","title":"Instance Role","text":"<p>Choose instance role if you have an EC2 instance profile attached to your Hopsworks cluster nodes with a role which grants you access to the specified bucket.</p>"},{"location":"user_guides/fs/data_source/creation/s3/#temporary-credentials","title":"Temporary Credentials","text":"<p>Choose temporary credentials if you are using AWS Role chaining to control the access permission on a project and user role base. Once you have selected Temporary Credentials select the role that give access to the specified bucket. For this role to appear in the list it needs to have been configured by an administrator, see the AWS Role chaining documentation for more details.</p> <p>Session Duration</p> <p>By default, the session duration that the role will be assumed for is 1 hour or 3600 seconds. This means if you want to use the data source for example to write training data to S3, the training dataset creation cannot take longer than one hour.</p> <p>Your administrator can change the default session duration for AWS data sources, by first increasing the max session duration of the IAM Role that you are assuming. And then changing the <code>fs_data_source_session_duration</code> configuration variable to the appropriate value in seconds.</p>"},{"location":"user_guides/fs/data_source/creation/s3/#access-keysecret","title":"Access Key/Secret","text":"<p>The most simple authentication method are Access Key/Secret, choose this option to get started quickly, if you are able to retrieve the keys using the IAM user administration.</p>"},{"location":"user_guides/fs/data_source/creation/s3/#step-4-configure-server-side-encryption","title":"Step 4: Configure Server Side Encryption","text":"<p>Additionally, you can specify if your Bucket has SSE enabled.</p>"},{"location":"user_guides/fs/data_source/creation/s3/#aes256","title":"AES256","text":"<p>For AES256, there is nothing to do but enabling the encryption by toggling the <code>AES256</code> option. This is using S3-Managed Keys, also called SSE-S3.</p>"},{"location":"user_guides/fs/data_source/creation/s3/#sse-kms","title":"SSE-KMS","text":"<p>With this option the encryption key is managed by AWS KMS, with some additional benefits and charges for using this service. The difference is that you need to provide the resource ARN of the key.</p> <p>If you have SSE-KMS enabled for your bucket, you can find the key ARN in the \"Properties\" section of the bucket details on AWS.</p>"},{"location":"user_guides/fs/data_source/creation/s3/#step-5-add-spark-options-optional","title":"Step 5: Add Spark Options (optional)","text":"<p>Here you can specify any additional spark options that you wish to add to the spark context at runtime. Multiple options can be added as key - value pairs.</p> <p>To connect to a S3 compatible storage other than AWS S3, you can add the option with key as <code>fs.s3a.endpoint</code> and the endpoint you want to use as value. The data source will then be able to read from your specified S3 compatible storage.</p> <p>Spark Configuration</p> <p>When using the data source within a Spark application, the credentials are set at application level. This allows users to access multiple buckets with the same data source within the same application (assuming the credentials allow it). You can disable this behaviour by setting the option <code>fs.s3a.global-conf</code> to <code>False</code>. If the <code>global-conf</code> option is disabled, the credentials are set on a per-bucket basis and users will be able to use the credentials to access data only from the bucket specified in the data source configuration.</p>"},{"location":"user_guides/fs/data_source/creation/s3/#step-6-save-changes","title":"Step 6: Save changes","text":"<p>Click on \"Save Credentials\".</p>"},{"location":"user_guides/fs/data_source/creation/s3/#next-steps","title":"Next Steps","text":"<p>Move on to the usage guide for data sources to see how you can use your newly created S3 connector.</p>"},{"location":"user_guides/fs/data_source/creation/snowflake/","title":"How-To set up a Snowflake Data Source","text":""},{"location":"user_guides/fs/data_source/creation/snowflake/#introduction","title":"Introduction","text":"<p>Snowflake provides a cloud-based data storage and analytics service, used as a data warehouse in many enterprises.</p> <p>Data warehouses are often the source of raw data for feature engineering pipelines and Snowflake supports scalable feature computation with SQL. However, Snowflake is not viable as an online feature store that serves features to models in production, with its columnar database layout its latency is too high compared to OLTP databases or key-value stores.</p> <p>In this guide, you will configure a Data Source in Hopsworks to save all the authentication information needed in order to set up a connection to your Snowflake database. When you're finished, you'll be able to query the database using Spark through HSFS APIs.</p> <p>Note</p> <p>Currently, it is only possible to create data sources in the Hopsworks UI. You cannot create a data source programmatically.</p>"},{"location":"user_guides/fs/data_source/creation/snowflake/#prerequisites","title":"Prerequisites","text":"<p>Before you begin this guide you'll need to retrieve the following information from your Snowflake account and database, the following options are mandatory:</p> <ul> <li>Snowflake Connection URL: Consult the documentation of your target snowflake account to determine the correct connection URL.   This is usually some form of your Snowflake account identifier. For example:</li> </ul> <pre><code>&lt;account_identifier&gt;.snowflakecomputing.com\n</code></pre> <p>OR:</p> <pre><code>https://&lt;orgname&gt;-&lt;account_name&gt;.snowflakecomputing.com\n</code></pre> <p>The account and organization details can be viewed in the Snowsight UI under Admin &gt; Account or by querying it in SQL, as explained in Snowflake documentation. Below is an example of how to view the account and organization to get the account identifier from the Snowsight UI.</p> <p> </p> Viewing Snowflake account identifier <p>Authentication methods</p> <p>The Snowflake data source supports username and password, token-based and key-pair based authentication options. General information on snowflake key pair authentication and setup is at Snowflake key-pair authentication.</p> <ul> <li>Username and Password: Login name for the Snowflake user and password.   This is often also referred to as <code>sfUser</code> and <code>sfPassword</code>.</li> <li>Warehouse: The warehouse to use for the session after connecting</li> <li>Database: The database to use for the session after connecting.</li> <li>Schema: The schema to use for the session after connecting.</li> </ul> <p>These are a few additional optional arguments:</p> <ul> <li>Role: The role field can be used to specify which Snowflake security role to assume for the session after the connection is established.</li> <li>Application: The application field can also be specified to have better observability in Snowflake with regards to which application is running which query.   The application field can be a simple string like \u201cHopsworks\u201d or, for instance, the project name, to track usage and queries from each Hopsworks project.</li> </ul>"},{"location":"user_guides/fs/data_source/creation/snowflake/#creation-in-the-ui","title":"Creation in the UI","text":""},{"location":"user_guides/fs/data_source/creation/snowflake/#step-1-set-up-new-data-source","title":"Step 1: Set up new Data Source","text":"<p>Head to the Data Source View on Hopsworks (1) and set up a new data source (2).</p> <p> </p> The Data Source View in the User Interface"},{"location":"user_guides/fs/data_source/creation/snowflake/#step-2-enter-snowflake-settings","title":"Step 2: Enter Snowflake Settings","text":"<p>Enter the details for your Snowflake connector. Start by giving it a name and an optional description.</p> <ol> <li>Select \"Snowflake\" as storage.</li> <li>Specify the hostname for your account in the following format <code>&lt;account_identifier&gt;.snowflakecomputing.com</code> or <code>https://&lt;orgname&gt;-&lt;account_name&gt;.snowflakecomputing.com</code>.</li> <li>Login name for the Snowflake user.</li> <li>Authentication Choose between user account Password, Token or Private Key options.     In case of private key, upload your snowflake user Private Key file and set Passphrase if applicable.</li> <li>The warehouse to connect to.</li> <li>The database to use for the connection.</li> <li>Add any additional optional arguments.     For example, you can specify <code>Schema</code>, <code>Table</code>, <code>Role</code>, and <code>Application</code>.</li> <li>Optional additional key/value arguments.</li> <li>Click on \"Save Credentials\".</li> </ol> <p> </p> Snowflake Connector Creation Form"},{"location":"user_guides/fs/data_source/creation/snowflake/#next-steps","title":"Next Steps","text":"<p>Move on to the usage guide for data sources to see how you can use your newly created Snowflake connector.</p>"},{"location":"user_guides/fs/feature_group/","title":"Feature Group User Guides","text":"<p>This section serves to provide guides and examples for the common usage of abstractions and functionality of the Feature Group through the Hopsworks UI and APIs.</p> <ul> <li>Create a Feature Group</li> <li>Create an external Feature Group</li> <li>Deprecating Feature Group</li> <li>Data Types and Schema management</li> <li>Statistics</li> <li>Data Validation</li> <li>Feature Monitoring</li> <li>Time-To-Live (TTL)</li> </ul>"},{"location":"user_guides/fs/feature_group/create/","title":"How to create a Feature Group","text":""},{"location":"user_guides/fs/feature_group/create/#introduction","title":"Introduction","text":"<p>In this guide you will learn how to create and register a feature group with Hopsworks. This guide covers creating a feature group using the HSFS APIs as well as the user interface.</p>"},{"location":"user_guides/fs/feature_group/create/#prerequisites","title":"Prerequisites","text":"<p>Before you begin this guide we suggest you read the Feature Group concept page to understand what a feature group is and how it fits in the ML pipeline.</p>"},{"location":"user_guides/fs/feature_group/create/#create-using-the-hsfs-apis","title":"Create using the HSFS APIs","text":"<p>To create a feature group using the HSFS APIs, you need to provide a Pandas, Polars or Spark DataFrame. The DataFrame will contain all the features you want to register within the feature group, as well as the primary key, event time and partition key.</p>"},{"location":"user_guides/fs/feature_group/create/#create-a-feature-group","title":"Create a Feature Group","text":"<p>The first step to create a feature group is to create the API metadata object representing a feature group. Using the HSFS API you can execute:</p>"},{"location":"user_guides/fs/feature_group/create/#batch-write-api","title":"Batch Write API","text":"PySpark <pre><code>fg = feature_store.create_feature_group(name=\"weather\",\n    version=1,\n    description=\"Weather Features\",\n    online_enabled=True,\n    primary_key=['location_id'],\n    partition_key=['day'],\n    event_time='event_time',\n    time_travel_format='DELTA',\n)\n</code></pre> <p>You can read the full <code>FeatureStore.create_feature_group</code> documentation to get more details. If you need to create a feature group with vector similarity search supported, refer to the vector similarity guide. <code>name</code> is the only mandatory parameter of the <code>create_feature_group</code> and represents the name of the feature group.</p> <p>In the example above we created the first version of a feature group named weather, we provide a description to make it searchable to the other project members, as well as making the feature group available online.</p> <p>Additionally we specify which columns of the DataFrame will be used as primary key, partition key and event time. Composite primary key and multi level partitioning is also supported.</p> <p>The version number is optional, if you don't specify the version number the APIs will create a new version by default with a version number equals to the highest existing version number plus one.</p> <p>The last parameter used in the examples above is <code>stream</code>. The <code>stream</code> parameter controls whether to enable the streaming write APIs to the online and offline feature store. When using the APIs in a Python environment this behavior is the default and it requires the time travel format to be set to 'HUDI'.</p>"},{"location":"user_guides/fs/feature_group/create/#primary-key","title":"Primary key","text":"<p>A primary key is required when using the default table format (Hudi) to store offline feature data. When inserting data in a feature group on the offline feature store, the DataFrame you are writing is checked against the existing data in the feature group. If a row with the same primary key is found in the feature group, the row will be updated. If the primary key is not found, the row is appended to the feature group. When writing data on the online feature store, existing rows with the same primary key will be overwritten by new rows with the same primary key.</p>"},{"location":"user_guides/fs/feature_group/create/#event-time","title":"Event time","text":"<p>The event time column represents the time at which the event was generated. For example, with transaction data, the event time is the time at which a given transaction happened. In the context of feature pipelines, the event time is often also the end timestamp of the interval of events included in the feature computation. For example, computing the feature \"number of purchases by customer last week\", the event time should be the last day of this \"last week\" window.</p> <p>The event time is added to the primary key when writing to the offline feature store. This will make sure that the offline feature store has the entire history of feature values over time. As an example, if a user has made multiple purchases on a website, each of the purchases for a given user (identified by a user_id) will be saved in the feature group, with each purchase having a different event time (the combination of user_id and event_time makes up the primary key for the offline feature store).</p> <p>The event time is not part of the primary key when writing to the online feature store. This will ensure that the online feature store has the most recent version of the feature vector for each primary key.</p> <p>Event time data type restriction</p> <p>The supported data types for the event time column are: <code>timestamp</code>, <code>date</code> and <code>bigint</code>.</p>"},{"location":"user_guides/fs/feature_group/create/#partition-key","title":"Partition key","text":"<p>It is best practice to add a partition key. When you specify a partition key, the data in the feature group will be stored under multiple directories based on the value of the partition column(s). All the rows with a given value as partition key will be stored in the same directory.</p> <p>Choosing the correct partition key has significant impact on the query performance as the execution engine (Spark) will be able to skip listing and reading files belonging to partitions which are not included in the query. As an example, if you have partitioned your feature group by day and you are creating a training dataset that includes only the last year of data, Spark will read only 365 partitions and not the entire history of data. On the other hand, if the partition key is too fine grained (e.g., timestamp at millisecond resolution) - a large number of small partitions will be generated. This will slow down query execution as Spark will need to list and read a large amount of small directories/files.</p> <p>If you do not provide a partition key, all the feature data will be stored as files in a single directory. The system has a limit of 10240 direct children (files or other subdirectories) per directory. This means that, as you add new data to a non-partitioned feature group, new files will be created and you might reach the limit. If you do reach the limit, your feature engineering pipeline will fail with the following error:</p> <pre><code>MaxDirectoryItemsExceededException - The directory item limit is exceeded: limit=10240 items=10240\n</code></pre> <p>By using partitioning the system will write the feature data in different subdirectories, thus allowing you to write 10240 files per partition.</p>"},{"location":"user_guides/fs/feature_group/create/#table-format","title":"Table format","text":"<p>When you create a feature group, you can specify the table format you want to use to store the data in your feature group by setting the <code>time_travel_format</code> parameter. The currently supported values are \"HUDI\", \"DELTA\", \"NONE\" (which defaults to Parquet).</p>"},{"location":"user_guides/fs/feature_group/create/#data-source","title":"Data Source","text":"<p>During the creation of a feature group, it is possible to define the <code>data_source</code> parameter, this allows for management of offline data in the desired table format outside the Hopsworks cluster. Currently, S3 and GCS connectors with \"DELTA\" <code>time_travel_format</code> are supported.</p>"},{"location":"user_guides/fs/feature_group/create/#online-table-configuration","title":"Online Table Configuration","text":"<p>When defining online-enabled feature groups it is also possible to configure the online table. You can specify table options by providing comments. Additionally, it is also possible to define whether online data is stored in memory or on disk using table space.</p> <p>The code example shows the creation of an online-enabled feature group that stores online data on disk using <code>ts_1</code> table space and sets several table properties in the comment section.</p> <pre><code>fg = fs.create_feature_group(\n    name='air_quality',\n    description='Air Quality characteristics of each day',\n    version=1,\n    primary_key=['city','date'],\n    online_enabled=True,\n    online_config={'table_space': 'ts_1', 'online_comments': ['NDB_TABLE=READ_BACKUP=1', 'NDB_TABLE=PARTITION_BALANCE=FOR_RP_BY_LDM_X_2']}\n)\n</code></pre> <p>Note</p> <p>The table space needs to be provisioned at system level before it can be used. You can do so by adding the following parameters to the values.yaml file used for your deployment with the Helm Charts:</p> <pre><code>rondb:\n  resources:\n    requests:\n      storage:\n        diskColumnGiB: 2\n</code></pre>"},{"location":"user_guides/fs/feature_group/create/#streaming-write-api","title":"Streaming Write API","text":"<p>As explained above, the stream parameter controls whether to enable the streaming write APIs to the online and offline feature store. For Python environments, only the stream API is supported (stream=True).</p> PythonPySpark <pre><code>fg = feature_store.create_feature_group(name=\"weather\",\n    version=1,\n    description=\"Weather Features\",\n    online_enabled=True,\n    primary_key=['location_id'],\n    partition_key=['day'],\n    event_time='event_time',\n    time_travel_format='HUDI',\n)\n</code></pre> <pre><code>fg = feature_store.create_feature_group(name=\"weather\",\n    version=1,\n    description=\"Weather Features\",\n    online_enabled=True,\n    primary_key=['location_id'],\n    partition_key=['day'],\n    event_time='event_time',\n    time_travel_format='HUDI',\n    stream=True\n)\n</code></pre> <p>When using the streaming API, the data will be written directly to the online storage (if <code>online_enabled=True</code>). However, you can control when the sync to the offline storage is going to happen. You can do it synchronously after every call to <code>fg.insert()</code>, which is the default. Often, you defer writes to a later point in order to batch together multiple writes to the offline storage (useful to reduce the overhead of many small writes):</p> <pre><code># run multiple inserts without starting the offline materialization job\njob, _ = fg.insert(df1, write_options={\"start_offline_materialization\": False})\njob, _ = fg.insert(df2, write_options={\"start_offline_materialization\": False})\njob, _ = fg.insert(df3, write_options={\"start_offline_materialization\": False})\n\n# start the materialization job for all three inserts\n# note the job object is always the same, you don't need to call it three times\njob.run()\n</code></pre> <p>It is also possible to define the topics used for data ingestion, this can be done by setting the <code>topic_name</code> parameter with your preferred value. By default, feature groups in Hopsworks will share a project-wide topic.</p>"},{"location":"user_guides/fs/feature_group/create/#best-practices-for-writing","title":"Best Practices for Writing","text":"<p>When designing a feature group, it is worth taking a look at how this feature group will be queried in the future, in order to optimize it for those query patterns. At the same time, Spark and Hudi tend to overpartition writes, creating too many small parquet files, which is inefficient and slows down writes. But they also slow down queries, because file listings take more time and reading many small files is slower than fewer larger files. The best practices described in this section hold both for the Streaming API and the Batch API.</p> <p>Four main considerations influence the write and the query performance:</p> <ol> <li>Partitioning on a feature group level</li> <li>Parquet file size within a feature group partition</li> <li>Backfilling of feature group partitions</li> <li>The choice of topic for data ingestion</li> </ol>"},{"location":"user_guides/fs/feature_group/create/#partitioning-on-a-feature-group-level","title":"Partitioning on a feature group level","text":"<p>Partitioning on the feature group level allows Hopsworks and the table format (Hudi or Delta) to push down filters to the filesystem when reading from feature groups. In practice that means fewer directories need to be listed and fewer files need to be read, speeding up queries.</p> <p>For example, most commonly, filtering is done on the event time column of a feature group when generating training data or batches of data:</p> <pre><code>query = fg.select_all()\n\n# create a simple feature view\nfv = fs.create_feature_view(\n    name='transactions_view',\n    query=query\n)\n\n# set up dates\nstart_time = \"2022-01-01\"\nend_time = \"2022-06-30\"\n\n# create a training dataset\nversion, job = fv.create_training_data(\n    start_time=start_time,\n    end_time=end_time,\n    description='Description of a dataset',\n)\n</code></pre> <p>Assuming the feature group was partitioned by a daily event time column, for example, the features are updated with a daily batch job, the feature store will only have to list and read the files in the directories of those six months that are being queried.</p> <p>Too granular event time columns</p> <p>An event time column which is too granular, such as a timestamp, shouldn't be used as partition key. For example, a streaming pipeline generating features where the event time includes seconds, and therefore almost all event timestamps are unique can lead to many partition directories and small files, each of which contains only a few number of rows, which are inefficient to query even with pushed down filters.</p> <p>A good practice are partition keys with at most daily granularity, if they are based on time. Additionally, one can look at the size of a partition directory, which should be in the 100s of MB.</p> <p>Additionally, if you are commonly training models for different categories of your data, you can add another level of partitioning for this. That is, if the query contains an additional filter:</p> <pre><code>query = fg.select_all().filter(fg.country_code == \"US\")\n</code></pre> <p>The feature group can be created with the following partition key in order to push down filters also for the <code>country_code</code> category:</p> <pre><code>fg = feature_store.create_feature_group(...\n    partition_key=['day', 'country_code'],\n    event_time='day',\n)\n</code></pre>"},{"location":"user_guides/fs/feature_group/create/#parquet-file-size-within-a-feature-group-partition","title":"Parquet file size within a feature group partition","text":"<p>Once you have decided on the feature group level partitioning and you start inserting data to the feature group, there are multiple ways in order to influence how the table format (Hudi or Delta) will split the data between parquet files within the feature group partitions. The two things that influence the number of parquet files per partition are</p> <ol> <li>The number of feature group partitions written in a single insert</li> <li>The shuffle parallelism used by the table format</li> </ol> <p>For example, the inserted dataframe (unique combination of partition key values) will be parallelized according to the following Hudi settings:</p> <p>Default Hudi partitioning</p> <pre><code>write_options = {\n    'hoodie.bulkinsert.shuffle.parallelism': 5,\n    'hoodie.insert.shuffle.parallelism': 5,\n    'hoodie.upsert.shuffle.parallelism': 5\n}\n</code></pre> <p>That means, using Spark, Hudi shuffles the data into five in-memory partitions, which each fill map to a task and finally a parquet file (see figure below). If the inserted Dataframe contains only a single feature group partition, this feature group partition will be written with five parquet files. If the inserted Dataframe contains multiple feature group partitions, the parquet files will be split among those partition, potentially more parquet files will be added.</p> <p> </p> Mapping in-memory partitions to tasks, workers, executors and feature group partition files for a feature group insert <p>Setting shuffle parallelism</p> <p>In practice that means the shuffle parallelism should be set equal to the number of feature group partitions in the inserted dataframe. This will create one parquet file per feature group partition, which in many cases is optimal.</p> <p>Theoretically, this rule holds up to a partition size of 2GB, which is the limit of Spark. However, one should bump this up accordingly already for smaller inputs. We recommend having shuffle parallelism <code>hoodie.[insert|upsert|bulkinsert].shuffle.parallelism</code> such that it's at least input_data_size/500MB.</p> <p>You can change the write options on every insert, depending also on the size of the data you are writing: <pre><code>write_options = {\n    'hoodie.bulkinsert.shuffle.parallelism': 5,\n    'hoodie.insert.shuffle.parallelism': 5,\n    'hoodie.upsert.shuffle.parallelism': 5\n}\nfg.insert(df, write_options=write_options)\n</code></pre></p>"},{"location":"user_guides/fs/feature_group/create/#backfilling-of-feature-group-partitions","title":"Backfilling of feature group partitions","text":"<p>Hudi scales well with the number of partitions to write, when performing backfilling of old feature partitions, meaning moving backwards in time with the event-time, it makes sense to batch those feature group partitions together into a single <code>fg.insert()</code> call. As shown in the figure above, the number of utilised executors you choose for the insert depends highly on the number of partitions and shuffle parallelism you are writing. So by writing multiple feature group partitions in a single insert, you can scale up your Spark application and fully utilise the workers. In that case you can increase the Hudi shuffle parallelism accordingly.</p> <p>Concurrent feature group inserts</p> <p>Hopsworks 3.1 and earlier, currently does not support concurrent inserts to feature groups. This means that if your feature pipeline writes to one feature group partition at a time, you cannot run it multiple times in parallel for backfilling.</p> <p>The recommended approach is to unionise the dataframes and insert them with a single <code>fg.insert()</code> instead. For clients that write with the Stream API, it is enough to defer starting the backfill job until after multiple inserts, as described above.</p>"},{"location":"user_guides/fs/feature_group/create/#the-choice-of-topic-for-data-ingestion","title":"The choice of topic for data ingestion","text":"<p>When creating a feature group that uses streaming write APIs for data ingestion it is possible to define the Kafka topics that should be utilized. The default approach of using a project-wide topic functions great for use cases involving little to no overlap when producing data. However, concurrently inserting into multiple feature groups could cause read amplification for the offline materialization job (e.g., Hudi Delta Streamer). Therefore, it is advised to utilize separate topics when ingestions overlap or there is a large frequently running insertion into a specific feature group.</p>"},{"location":"user_guides/fs/feature_group/create/#register-the-metadata-and-save-the-feature-data","title":"Register the metadata and save the feature data","text":"<p>The snippet above only created the metadata object on the Python interpreter running the code. To register the feature group metadata and to save the feature data with Hopsworks, you should invoke the <code>insert</code> method:</p> <pre><code>fg.insert(df)\n</code></pre> <p>The save method takes in input a Pandas, Polars or Spark DataFrame. HSFS will use the DataFrame columns and types to determine the name and types of features, primary key, partition key and event time.</p> <p>The DataFrame must contain the columns specified as primary keys, partition key and event time in the <code>create_feature_group</code> call.</p> <p>If a feature group is online enabled, the <code>insert</code> method will store the feature data to both the online and offline storage.</p>"},{"location":"user_guides/fs/feature_group/create/#api-reference","title":"API Reference","text":"<p><code>FeatureGroup</code></p>"},{"location":"user_guides/fs/feature_group/create/#create-using-the-ui","title":"Create using the UI","text":"<p>You can also create a new feature group through the UI. For this, navigate to the <code>Feature Groups</code> section and press the <code>Create</code> button at the top-right corner.</p> <p> </p> <p>Subsequently, you will be able to define its properties (such as name, mode, features, and more). Refer to the documentation above for an explanation of the parameters available, they are the same as when you create a feature group using the SDK. Finally, complete the creation by clicking <code>Create New Feature Group</code> at the bottom of the page.</p> <p> </p>"},{"location":"user_guides/fs/feature_group/create_external/","title":"How to create an External Feature Group","text":""},{"location":"user_guides/fs/feature_group/create_external/#introduction","title":"Introduction","text":"<p>In this guide you will learn how to create and register an external feature group with Hopsworks. This guide covers creating an external feature group using the HSFS APIs as well as the user interface.</p>"},{"location":"user_guides/fs/feature_group/create_external/#prerequisites","title":"Prerequisites","text":"<p>Before you begin this guide we suggest you read the External Feature Group concept page to understand what a feature group is and how it fits in the ML pipeline.</p>"},{"location":"user_guides/fs/feature_group/create_external/#create-using-the-hsfs-apis","title":"Create using the HSFS APIs","text":""},{"location":"user_guides/fs/feature_group/create_external/#retrieve-the-data-source","title":"Retrieve the Data Source","text":"<p>To create an external feature group using the HSFS APIs you need to provide an existing data source.</p> Python <pre><code>ds = feature_store.get_data_source(\"data_source_name\")\n</code></pre>"},{"location":"user_guides/fs/feature_group/create_external/#create-an-external-feature-group","title":"Create an External Feature Group","text":"<p>The first step is to instantiate the metadata through the <code>create_external_feature_group</code> method. Once you have defined the metadata, you can persist the metadata and create the feature group in Hopsworks by calling <code>fg.save()</code>.</p>"},{"location":"user_guides/fs/feature_group/create_external/#sql-based-external-feature-group","title":"SQL based external feature group","text":"Python <pre><code>query = \"\"\"\n    SELECT TO_NUMERIC(ss_store_sk) AS ss_store_sk\n        , AVG(ss_net_profit) AS avg_ss_net_profit\n        , SUM(ss_net_profit) AS total_ss_net_profit\n        , AVG(ss_list_price) AS avg_ss_list_price\n        , AVG(ss_coupon_amt) AS avg_ss_coupon_amt\n        , sale_date\n        , ss_store_sk\n    FROM STORE_SALES\n    GROUP BY ss_store_sk, sales_date\n\"\"\"\n\nfg = feature_store.create_external_feature_group(name=\"sales\",\n    version=1,\n    description=\"Physical shop sales features\",\n    query=query,\n    data_source=ds,\n    primary_key=['ss_store_sk'],\n    event_time='sale_date'\n)\n\nfg.save()\n</code></pre>"},{"location":"user_guides/fs/feature_group/create_external/#data-lake-based-external-feature-group","title":"Data Lake based external feature group","text":"Python <pre><code>fg = feature_store.create_external_feature_group(name=\"sales\",\n    version=1,\n    description=\"Physical shop sales features\",\n    data_format=\"parquet\",\n    data_source=ds,\n    primary_key=['ss_store_sk'],\n    event_time='sale_date'\n)\n\nfg.save()\n</code></pre> <p>You can read the full <code>FeatureStore.create_external_feature_group</code> documentation for more details. <code>name</code> is a mandatory parameter of the <code>create_external_feature_group</code> and represents the name of the feature group.</p> <p>The version number is optional, if you don't specify the version number the APIs will create a new version by default with a version number equals to the highest existing version number plus one.</p> <p>If the data source is defined for a data warehouse (e.g., JDBC, Snowflake, Redshift) you need to provide a SQL statement that will be executed to compute the features. If the data source is defined for a data lake, the location of the data as well as the format need to be provided.</p> <p>Additionally we specify which columns of the DataFrame will be used as primary key, and event time. Composite primary keys are also supported.</p>"},{"location":"user_guides/fs/feature_group/create_external/#register-the-metadata","title":"Register the metadata","text":"<p>In the snippet above it's important that the created metadata object gets registered in Hopsworks. To do so, you should invoke the <code>save</code> method:</p> Python <pre><code>fg.save()\n</code></pre>"},{"location":"user_guides/fs/feature_group/create_external/#enable-online-storage","title":"Enable online storage","text":"<p>You can enable online storage for external feature groups, however, the sync from the external storage to Hopsworks online storage is not automatic and needs to be setup manually. For an external feature group to be available online, during the creation of the feature group, the <code>online_enabled</code> option needs to be set to <code>True</code>.</p> Python <pre><code>external_fg = fs.create_external_feature_group(\n            name=\"sales\",\n            version=1,\n            description=\"Physical shop sales features\",\n            query=query,\n            data_source=ds,\n            primary_key=['ss_store_sk'],\n            event_time='sale_date',\n            online_enabled=True)\nexternal_fg.save()\n\n# read from external storage and filter data to sync to online\ndf = external_fg.read().filter(external_fg.customer_status == \"active\")\n\n# insert to online storage\nexternal_fg.insert(df)\n</code></pre> <p>The <code>insert()</code> method takes a DataFrame as parameter and writes it only to the online feature store. Users can select which subset of the feature group data they want to make available on the online feature store by using the query APIs.</p>"},{"location":"user_guides/fs/feature_group/create_external/#limitations","title":"Limitations","text":"<p>Hopsworks Feature Store does not support time-travel queries on external feature groups.</p> <p>Additionally, support for <code>.read()</code> and <code>.show()</code> methods when using by the Python engine is limited to external feature groups defined on BigQuery and Snowflake and only when using the Feature Query Service. Nevertheless, external feature groups defined top of any data source can be used to create a training dataset from a Python environment invoking one of the following methods: <code>FeatureView.create_training_data</code>, <code>FeatureView.create_train_test_split</code> or <code>FeatureView.create_train_validation_test_split</code>.</p>"},{"location":"user_guides/fs/feature_group/create_external/#api-reference","title":"API Reference","text":"<p><code>ExternalFeatureGroup</code>.</p>"},{"location":"user_guides/fs/feature_group/create_external/#create-using-the-ui","title":"Create using the UI","text":"<p>You can also create a new feature group through the UI. For this, navigate to the <code>Data Source</code> section and make sure you have you have available Data Source for the desired platform or create new.</p> <p> </p> <p>To create a feature group, proceed by clicking <code>Next: Select Tables</code> once all of the necessary details have been provided.</p> <p> </p> <p>The database navigation structure depends on your specific data source. You'll navigate through the appropriate hierarchy for your platform\u2014such as Database \u2192 Schema \u2192 Table for Snowflake, or Project \u2192 Dataset \u2192 Table for BigQuery.</p> <p>In the UI you can select one or more tables, for each selected table, you must designate one or more columns as primary keys before proceeding. You can also optionally select a single column as a timestamp for the row (supported types are timestamp, date and bigint), edit names and data types of individual columns you want to include.</p> <p> </p> <p>Complete the creation by clicking <code>Next: Review Configuration</code> at the bottom of the page. As the last step, you will be able to rename the feature groups and confirm their creation.</p> <p> </p>"},{"location":"user_guides/fs/feature_group/create_spine/","title":"How to create Spine Group","text":""},{"location":"user_guides/fs/feature_group/create_spine/#introduction","title":"Introduction","text":"<p>In this guide you will learn how to create and register a Spine Group with Hopsworks.</p>"},{"location":"user_guides/fs/feature_group/create_spine/#prerequisites","title":"Prerequisites","text":"<p>Before you begin this guide we suggest you read the Spine Group concept page to understand what a Spine Group is and how it fits in the ML pipeline.</p>"},{"location":"user_guides/fs/feature_group/create_spine/#create-using-the-hsfs-apis","title":"Create using the HSFS APIs","text":""},{"location":"user_guides/fs/feature_group/create_spine/#create-a-spine-group","title":"Create a Spine Group","text":"<p>Instead of using a feature group to save the label, you can also use a spine to use a Dataframe containing the labels on the fly. A spine is essentially a metadata object similar to a Feature Group, which tells the feature store the relevant event time column and primary key columns to perform point-in-time correct joins.</p> <p>Additionally, apart from primary key and event time information, a Spark dataframe is required in order to infer the schema of the group from.</p> Python <pre><code>trans_spine = fs.get_or_create_spine_group(\n    name=\"spine_transactions\",\n    version=1,\n    description=\"Transaction data\",\n    primary_key=['cc_num'],\n    event_time='datetime',\n    dataframe=trans_df\n)\n</code></pre> <p>Once created, note that you can inspect the dataframe in the Spine Group:</p> Python <pre><code>trans_spine.dataframe.show()\n</code></pre> <p>And you can always also replace the dataframe contained within the Spine Group. You just need to make sure it has the same schema.</p> Python <pre><code>trans_spine.dataframe = new_df\n</code></pre>"},{"location":"user_guides/fs/feature_group/create_spine/#limitations","title":"Limitations","text":"<p>Python support</p> <p>Currently the HSFS library does not support usage of Spine Groups for training data creation or batch data retrieval in the Python engine. However, it is supported to create Spine Groups from the Python engine.</p>"},{"location":"user_guides/fs/feature_group/create_spine/#api-reference","title":"API Reference","text":"<p><code>SpineGroup</code>.</p>"},{"location":"user_guides/fs/feature_group/data_types/","title":"How to manage schema and feature data types","text":""},{"location":"user_guides/fs/feature_group/data_types/#introduction","title":"Introduction","text":"<p>In this guide, you will learn how to manage the feature group schema and control the data type of the features in a feature group.</p>"},{"location":"user_guides/fs/feature_group/data_types/#prerequisites","title":"Prerequisites","text":"<p>Before you begin this guide we suggest you read the Feature Group concept page to understand what a feature group is and how it fits in the ML pipeline. We also suggest you familiarize yourself with the APIs to create a feature group.</p>"},{"location":"user_guides/fs/feature_group/data_types/#feature-group-schema","title":"Feature group schema","text":"<p>When a feature is stored in both the online and offline feature stores, it will be stored in a data type native to each store.</p> <ul> <li>Offline data type: The data type of the feature when stored on the offline feature store.   The offline feature store is based on Apache Hudi and Hive Metastore, as such, Hive Data Types can be leveraged.</li> <li>Online data type: The data type of the feature when stored on the online feature store.   The online storage is based on RonDB and hence, MySQL Data Types can be leveraged.</li> </ul> <p>The offline data type is always required, even if the feature group is stored only online. On the other hand, if the feature group is not online_enabled, its features will not have an online data type.</p> <p>The offline and online types for each feature are automatically inferred from the Spark or Pandas types of the input DataFrame as outlined in the following two sections. The default mapping, however, can be overwritten by using an explicit schema definition.</p>"},{"location":"user_guides/fs/feature_group/data_types/#offline-data-types","title":"Offline data types","text":"<p>When registering a Spark DataFrame in a PySpark environment (S), or a Pandas DataFrame, or a Polars DataFrame in a Python-only environment (P) the following default mapping to offline feature types applies:</p> Spark Type (S) Pandas Type (P) Polars Type (P) Offline Feature Type Remarks BooleanType bool, object(bool) Boolean BOOLEAN ByteType int8, Int8 Int8 TINYINT or INT INT when time_travel_type=\"HUDI\" ShortType uint8, int16, Int16 UInt8, Int16 SMALLINT or INT INT when time_travel_type=\"HUDI\" IntegerType uint16, int32, Int32 UInt16, Int32 INT LongType int, uint32, int64, Int64 UInt32, Int64 BIGINT FloatType float, float16, float32 Float32 FLOAT DoubleType float64 Float64 DOUBLE DecimalType decimal.decimal Decimal DECIMAL(PREC, SCALE) Not supported in PO env. when time_travel_type=\"HUDI\" TimestampType datetime64[ns], datetime64[ns, tz] Datetime TIMESTAMP s. Timestamps and Timezones DateType object (datetime.date) Date DATE StringType object (str), object(np.unicode) String, Utf8 STRING ArrayType object (list), object (np.ndarray) List ARRAY&lt;TYPE&gt; StructType object (dict) Struct STRUCT&lt;NAME: TYPE, ...&gt; BinaryType object (binary) Binary BINARY MapType - - MAP&lt;String,TYPE&gt; Only when time_travel_type!=\"HUDI\"; Only string keys permitted <p>When registering a Pandas DataFrame in a PySpark environment (S) the Pandas DataFrame is first converted to a Spark DataFrame, using Spark's default conversion. It results in a less fine-grained mapping between Python and Spark types:</p> Pandas Type (S) Spark Type Remarks bool BooleanType int8, uint8, int16, uint16, int32, int, uint32, int64 LongType float, float16, float32, float64 DoubleType object (decimal.decimal) DecimalType datetime64[ns], datetime64[ns, tz] TimestampType s. Timestamps and Timezones object (datetime.date) DateType object (str), object(np.unicode) StringType object (list), object (np.ndarray) - Not supported object (dict) StructType object (binary) BinaryType"},{"location":"user_guides/fs/feature_group/data_types/#online-data-types","title":"Online data types","text":"<p>The online data type is determined based on the offline type according to the following mapping, regardless of which environment the data originated from. Only a subset of the data types can be used as primary key, as indicated in the table as well:</p> Offline Feature Type Online Feature Type Primary Key Remarks BOOLEAN TINYINT x TINYINT TINYINT x SMALLINT SMALLINT x INT INT x Also supports: TINYINT, SMALLINT BIGINT BIGINT x FLOAT FLOAT DOUBLE DOUBLE DECIMAL(PREC, SCALE) DECIMAL(PREC, SCALE) e.g. DECIMAL(38, 18) TIMESTAMP TIMESTAMP s. Timestamps and Timezones DATE DATE x STRING VARCHAR(100) x Also supports: TEXT ARRAY&lt;TYPE&gt; VARBINARY(100) x Also supports: BLOB STRUCT&lt;NAME: TYPE, ...&gt; VARBINARY(100) x Also supports: BLOB BINARY VARBINARY(100) x Also supports: BLOB MAP&lt;String,TYPE&gt; VARBINARY(100) x Also supports: BLOB <p>More on how Hopsworks handles string types,  complex data types and the online restrictions for primary keys and row size in the following sections.</p>"},{"location":"user_guides/fs/feature_group/data_types/#string-online-data-types","title":"String online data types","text":"<p>String types are stored as VARCHAR(100) by default. This type is fixed-size, meaning it can only hold as many characters as specified in the argument (e.g., VARCHAR(100) can hold up to 100 unicode characters). The size should thus be within the maximum string length of the input data. Furthermore, the VARCHAR size has to be in line with the online restrictions for row size.</p> <p>If the string size exceeds 100 characters, a larger type (e.g., VARCHAR(500)) can be specified via an explicit schema definition. If the string size is unknown or if it exceeds the maximum row size, then the TEXT type can be used instead.</p> <p>String data that exceeds the specified VARCHAR size will lead to an error when data gets written to the online feature store. When in doubt, use the TEXT type instead, but note that it comes with a potential performance overhead.</p>"},{"location":"user_guides/fs/feature_group/data_types/#complex-online-data-types","title":"Complex online data types","text":"<p>Hopsworks allows users to store complex types (e.g. ARRAY) in the online feature store. Hopsworks serializes the complex features transparently and stores them as VARBINARY in the online feature store. The serialization happens when calling the <code>FeatureGroup.save</code>, <code>FeatureGroup.insert</code> or <code>FeatureGroup.insert_stream</code> methods. The deserialization will be executed when calling the <code>TrainingDataset.get_serving_vector</code> method to retrieve data from the online feature store. If users query directly the online feature store, for instance using the <code>fs.sql(\"SELECT ...\", online=True)</code> statement, it will return a binary blob. <p>On the feature store UI, the online feature type for complex features will be reported as VARBINARY.</p> <p>If the binary size exceeds 100 bytes, a larger type (e.g., VARBINARY(500)) can be specified via an explicit schema definition. If the binary size is unknown of if it exceeds the maximum row size, then the BLOB type can be used instead.</p> <p>Binary data that exceeds the specified VARBINARY size will lead to an error when data gets written to the online feature store. When in doubt, use the BLOB type instead, but note that it comes with a potential performance overhead.</p>"},{"location":"user_guides/fs/feature_group/data_types/#online-restrictions-for-primary-key-data-types","title":"Online restrictions for primary key data types","text":"<p>When a feature is being used as a primary key, certain types are not allowed. Examples of such types are FLOAT, DOUBLE, TEXT and BLOB. Additionally, the size of the sum of the primary key online data types storage requirements should not exceed 4KB.</p>"},{"location":"user_guides/fs/feature_group/data_types/#online-restrictions-for-row-size","title":"Online restrictions for row size","text":"<p>The online feature store supports up to 500 columns and all column types combined should not exceed 30000 Bytes. The byte size of each column is determined by its data type and calculated as follows:</p> Online Data Type Byte Size TINYINT 1 SMALLINT 2 INT 4 BIGINT 8 FLOAT 4 DOUBLE 8 DECIMAL(PREC, SCALE) 16 TIMESTAMP 8 DATE 8 VARCHAR(LENGTH) LENGTH * 4 VARCHAR(LENGTH) charset latin1; LENGTH * 1 TEXT 256 VARBINARY(LENGTH) LENGTH BLOB 256 other 8 <p>VARCHAR / VARBINARY overhead</p> <p>For VARCHAR and VARBINARY data types, an additional 1 byte is required if the size is less than 256 bytes. If the size is 256 bytes or greater, 2 additional bytes are required.</p> <p>Memory allocation is performed in groups of 4 bytes. For example, a VARBINARY(100) requires 104 bytes of memory:</p> <ul> <li>100 bytes for the data itself</li> <li>1 byte of overhead</li> <li>Total = 101 bytes</li> </ul> <p>Since memory is allocated in 4-byte groups, storing 101 bytes requires 26 groups (26 \u00d7 4 = 104 bytes) of allocated memory.</p>"},{"location":"user_guides/fs/feature_group/data_types/#pre-insert-schema-validation-for-online-feature-groups","title":"Pre-insert schema validation for online feature groups","text":"<p>For online enabled feature groups, the dataframe to be ingested needs to adhere to the online schema definitions. The input dataframe is validated for schema checks accordingly. The validation is enabled by default and can be disabled by setting below key word argument when calling <code>insert()</code></p> Python <pre><code>feature_group.insert(df, validation_options={'online_schema_validation':False})\n</code></pre> <p>The most important validation checks or error messages are mentioned below along with possible corrective actions.</p> <ol> <li> <p>Primary key contains null values</p> <ul> <li>Rule Primary key column should not contain any null values.</li> <li>Example correction Drop the rows containing null primary keys.   Alternatively, find the null values and assign them an unique value as per preferred strategy for data imputation.</li> </ul> <p>=== \"Pandas\"       <pre><code># Drop rows: assuming 'id' is the primary key column\ndf = df.dropna(subset=['id'])\n# For composite keys\ndf = df.dropna(subset=['id1', 'id2'])\n\n# Data imputation: replace null values with incrementing last integer id\n# existing max id\nmax_id = df['id'].max()\n# counter to generate new id\nnext_id = max_id + 1\n# for each null id, assign the next id incrementally\nfor idx in df[df['id'].isna()].index:\n    df.loc[idx, 'id'] = next_id\n    next_id += 1\n</code></pre></p> </li> <li> <p>Primary key column missing</p> <ul> <li>Rule The dataframe to be inserted must contain all the columns defined as primary key(s) in the feature group.</li> <li>Example correction Add all the primary key columns in the dataframe.</li> </ul> <p>=== \"Pandas\"       <pre><code># incrementing primary key upto the length of dataframe\ndf['id'] = range(1, len(df) + 1)\n</code></pre></p> </li> <li> <p>String length exceeded</p> <ul> <li>Rule The character length of a string should be within the maximum length capacity in the online schema type of a feature.   If the feature group is not created and explicit feature schema was not provided, the limit will be auto-increased to the maximum length found in a string column in the dataframe.</li> <li> <p>Example correction</p> </li> <li> <p>Trim the string values to fit within maximum limit set during feature group creation.</p> </li> </ul> <p>=== \"Pandas\"       <pre><code>max_length = 100\ndf['text_column'] = df['text_column'].str.slice(0, max_length)\n</code></pre></p> <ul> <li>Another option is to simply create new version of the feature group and insert the dataframe.</li> </ul> <p>!!!note       The total row size limit should be less than 30kb as per row size restrictions.       In such cases it is possible to define the feature as TEXT or BLOB.       Below is an example of explicitly defining the string column as TEXT as online type.</p> <p>=== \"Pandas\"       <pre><code>import pandas as pd\n# example dummy dataframe with the string column\ndf = pd.DataFrame(columns=['id', 'string_col'])\nfrom hsfs.feature import Feature\nfeatures = [\nFeature(name=\"id\",type=\"bigint\",online_type=\"bigint\"),\nFeature(name=\"string_col\",type=\"string\",online_type=\"text\")\n]\n\nfg = fs.get_or_create_feature_group(name=\"fg_manual_text_schema\",\n                            version=1,\n                            features=features,\n                            online_enabled=True,\n                            primary_key=['id'])\nfg.insert(df)\n</code></pre></p> </li> </ol>"},{"location":"user_guides/fs/feature_group/data_types/#timestamps-and-timezones","title":"Timestamps and Timezones","text":"<p>All timestamp features are stored in Hopsworks in UTC time. Also, all timestamp-based functions (such as point-in-time joins) use UTC time. This ensures consistency of timestamp features across different client timezones and simplifies working with timestamp-based functions in general. When ingesting timestamp features, the <code>FeatureGroup.insert</code> will automatically handle the conversion to UTC, if necessary. The following table summarizes how different timestamp types are handled:</p> Data Frame (Data Type) Environment Handling Pandas DataFrame (datetime64[ns]) Python-only and PySpark interpreted as UTC, independent of the client's timezone Pandas DataFrame (datetime64[ns, tz]) Python-only and PySpark timezone-sensitive conversion from 'tz' to UTC Spark (TimestampType) PySpark and Spark interpreted as UTC, independent of the client's timezone <p>Timestamp features retrieved from the Feature Store, e.g., using the Feature Store Read API, use a timezone-unaware format:</p> Data Frame (Data Type) Environment Timezone Pandas DataFrame (datetime64[ns]) Python-only timezone-unaware (UTC) Spark (TimestampType) PySpark and Spark timezone-unaware (UTC) <p>Note that our PySpark/Spark client automatically sets the Spark SQL session's timezone to UTC. This ensures that Spark SQL will correctly interpret all timestamps as UTC. The setting will only apply to the client's session, and you don't have to worry about setting/unsetting the configuration yourself.</p>"},{"location":"user_guides/fs/feature_group/data_types/#explicit-schema-definition","title":"Explicit schema definition","text":"<p>When creating a feature group it is possible for the user to control both the offline and online data type of each column. If users explicitly define the schema for the feature group, Hopsworks is going to use that schema to create the feature group, without performing any type mapping. You can explicitly define the feature group schema as follows:</p> Python <pre><code>from hsfs.feature import Feature\n\nfeatures = [\n    Feature(name=\"id\",type=\"int\",online_type=\"int\"),\n    Feature(name=\"name\",type=\"string\",online_type=\"varchar(20)\")\n]\n\nfg = fs.create_feature_group(name=\"fg_manual_schema\",\n                             features=features,\n                             online_enabled=True)\nfg.save(features)\n</code></pre>"},{"location":"user_guides/fs/feature_group/data_types/#append-features-to-existing-feature-groups","title":"Append features to existing feature groups","text":"<p>Hopsworks supports appending additional features to an existing feature group. Adding additional features to an existing feature group is not considered a breaking change.</p> Python <pre><code>from hsfs.feature import Feature\n\nfeatures = [\n    Feature(name=\"id\",type=\"int\",online_type=\"int\"),\n    Feature(name=\"name\",type=\"string\",online_type=\"varchar(20)\")\n]\n\nfg = fs.get_feature_group(name=\"example\", version=1)\nfg.append_features(features)\n</code></pre> <p>When adding additional features to a feature group, you can provide a default values for existing entries in the feature group. You can also backfill the new features for existing entries by running an <code>insert()</code> operation and update all existing combinations of primary key - event time.</p>"},{"location":"user_guides/fs/feature_group/data_validation/","title":"Data Validation","text":"Validation on Insertion with Hopsworks and Great Expectations."},{"location":"user_guides/fs/feature_group/data_validation/#introduction","title":"Introduction","text":"<p>Clean, high quality feature data is of paramount importance to being able to train and serve high quality models. Hopsworks offers integration with Great Expectations to enable a smooth data validation workflow. This guide is designed to help you integrate a data validation step when inserting new DataFrames into a Feature Group. Note that validation is performed inline as part of your feature pipeline (on the client machine) - it is not executed by Hopsworks after writing features.</p>"},{"location":"user_guides/fs/feature_group/data_validation/#ui","title":"UI","text":""},{"location":"user_guides/fs/feature_group/data_validation/#create-a-feature-group-pre-requisite","title":"Create a Feature Group (Pre-requisite)","text":"<p>In the UI, you must create a Feature Group first before attaching an Expectation Suite. You can find out more information about creating a Feature Group. You can attach at most one expectation suite to a Feature Group. Data validation is an optional step and is not required to write to a Feature Group.</p>"},{"location":"user_guides/fs/feature_group/data_validation/#step-1-find-and-edit-feature-group","title":"Step 1: Find and Edit Feature Group","text":"<p>Click on the Feature Group section in the navigation menu. Find your Feature Group in the list and click on its name to access the Feature Group page. Select <code>edit</code> in the top right corner or scroll to the Expectations section and click on <code>Edit Expectation Suite</code>.</p>"},{"location":"user_guides/fs/feature_group/data_validation/#step-2-edit-general-expectation-suite-settings","title":"Step 2: Edit General Expectation Suite Settings","text":"<p>Scroll to the Expectation Suite section. Click add Expectation Suite and edit its metadata:</p> <ul> <li>Choose a name for your expectation suite.</li> <li>Checkbox enabled.   This controls whether the Expectation Suite will be used to validate a Dataframe automatically upon insertion into a Feature Group.   Note that validation is executed by the client.   Disabling validation allows you to skip the validation step without deleting the Expectation Suite.</li> <li>'ALWAYS' vs. 'STRICT' mode.   This option controls what happens after validation.   Hopsworks defaults to 'ALWAYS', where data is written to the Feature Group regardless of the validation result.   This means that even if expectations are failing or throw an exception, Hopsworks will attempt to insert the data into the Feature Group.   In 'STRICT' mode, Hopsworks will only write data to the Feature Group if each individual expectation has been successful.</li> </ul>"},{"location":"user_guides/fs/feature_group/data_validation/#step-3-add-new-expectations","title":"Step 3: Add new expectations","text":"<p>By clicking on <code>Add expectation</code> one can choose an expectation type from a searchable dropdown menu. Currently, only the built-in expectations from the Great Expectations framework are supported. For user-defined expectations, please use the Rest API or python client.</p> <p>All default kwargs associated to the selected expectation type are populated as a json below the dropdown menu. Edit the arguments in the json to configure the Expectation. In particular, arguments such as <code>column</code>, <code>columnA</code>, <code>columnB</code>, <code>column_set</code> and <code>column_list</code> require valid feature name(s). Click the tick button to save the expectation configuration and append it to the Expectation Suite locally.</p> <p>Info</p> <p>Click the <code>Save feature group</code> button to persist your changes!</p> <p>You can use the button <code>Clear Expectation Suite</code> to clean up before saving changes if you changed your mind. If the Expectation Suite is already registered, it will instead show a button to delete the Expectation Suite.</p>"},{"location":"user_guides/fs/feature_group/data_validation/#step-4-save-new-data-to-a-feature-group","title":"Step 4: Save new data to a Feature Group","text":"<p>Use the python client to write a DataFrame to the Feature Group. Note that if an expectation suite is enabled for a Feature Group, calling the <code>insert</code> method will run validation and default to uploading the corresponding validation report to Hopsworks. The report is uploaded even if validation fails and 'STRICT' mode is selected.</p>"},{"location":"user_guides/fs/feature_group/data_validation/#step-5-check-validation-results-summary","title":"Step 5: Check Validation Results Summary","text":"<p>Hopsworks shows a visual summary of validation reports. To check it out, go to your Feature Group overview and scroll to the expectation section. Click on the <code>Validation Results</code> tab and check that all went according to plan. Each row corresponds to an expectation in the suite. Features can have several corresponding expectations and the same type of expectation can be applied to different features.</p> <p>You can navigate to older reports using the dropdown menu. Should you need more than the information displayed in the UI for e.g., debugging, the full report can be downloaded by clicking on the corresponding button.</p>"},{"location":"user_guides/fs/feature_group/data_validation/#step-6-check-validation-history","title":"Step 6: Check Validation History","text":"<p>The <code>Validation Reports</code> tab in the Expectations section displays a brief history of recent validations. Each row corresponds to a validation report, with some summary information about the success of the validation step. You can download the full report by clicking the download icon button that appears at the end of the row.</p>"},{"location":"user_guides/fs/feature_group/data_validation/#code","title":"Code","text":"<p>Hopsworks python client interfaces with the Great Expectations library to enable you to add data validation to your feature engineering pipeline. In this section, we show you how in a single line you enable automatic validation on each insertion of new data into your Feature Group. Whether you have an existing Feature Group you want to add validation to or Follow the guide or get your hands dirty by running our tutorial data validation notebook in google colab.</p> <p>First checkout the pre-requisite and Hopsworks setup to follow the guide below. Create a project, install the hopsworks client and connect via the generated API key. You are ready to load your data in a DataFrame. The second step is a short introduction to the relevant Great Expectations API to build data validation suited to your data. Third and final step shows how to attach your Expectation Suite to the Feature Group to benefit from automatic validation on insertion capabilities.</p>"},{"location":"user_guides/fs/feature_group/data_validation/#step-1-pre-requisite","title":"Step 1: Pre-requisite","text":"<p>In order to define and validate an expectation when writing to a Feature Group, you will need:</p> <ul> <li>A Hopsworks project.   If you don't have a project yet you can go to app.hopsworks.ai, signup with your email and create your first project.</li> <li>An API key, you can get one by going to \"Account Settings\" on app.hopsworks.ai.</li> <li>The Hopsworks Python library installed in your client.   See the installation guide.</li> </ul>"},{"location":"user_guides/fs/feature_group/data_validation/#connect-your-notebook-to-hopsworks","title":"Connect your notebook to Hopsworks","text":"<p>Connect the client running your notebooks to Hopsworks.</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nfs = project.get_feature_store()\n</code></pre> <p>You will be prompt to paste your API key to connect the notebook to your project. The <code>fs</code> Feature Store entity is now ready to be used to insert or read data from Hopsworks.</p>"},{"location":"user_guides/fs/feature_group/data_validation/#import-your-data","title":"Import your data","text":"<p>Load your data in a DataFrame using the usual pandas API.</p> <pre><code>import pandas as pd\n\ndf = pd.read_csv(\"https://repo.hops.works/master/hopsworks-tutorials/data/card_fraud_data/transactions.csv\", parse_dates=[\"datetime\"])\n\ndf.head(3)\n</code></pre>"},{"location":"user_guides/fs/feature_group/data_validation/#step-2-great-expectation-introduction","title":"Step 2: Great Expectation Introduction","text":"<p>To validate the data, we will use the Great Expectations library. Below is a short introduction on how to build an Expectation Suite to validate your data. Everything is done using the Great Expectations API so you can re-use any prior knowledge you may have of the library.</p>"},{"location":"user_guides/fs/feature_group/data_validation/#create-an-expectation-suite","title":"Create an Expectation Suite","text":"<p>Create (or import an existing) expectation suite using the Great Expectations library. This suite will hold all the validation tests we want to perform on our data before inserting them into Hopsworks.</p> <pre><code>import great_expectations as ge\n\nexpectation_suite = ge.core.ExpectationSuite(\n    expectation_suite_name=\"validate_on_insert_suite\"\n)\n</code></pre>"},{"location":"user_guides/fs/feature_group/data_validation/#add-expectations-in-the-source-code","title":"Add Expectations in the Source Code","text":"<p>Add some expectation to your suite. Each expectation configuration corresponds to a validation test to be run against your data.</p> <pre><code>expectation_suite.add_expectation(\n    ge.core.ExpectationConfiguration(\n        expectation_type=\"expect_column_min_to_be_between\",\n        kwargs={\n            \"column\": \"foo_id\",\n            \"min_value\": 0,\n            \"max_value\": 1\n        }\n    )\n)\n\nexpectation_suite.add_expectation(\n    ge.core.ExpectationConfiguration(\n        expectation_type=\"expect_column_value_lengths_to_be_between\",\n        kwargs={\n            \"column\": \"bar_name\",\n            \"min_value\": 3,\n            \"max_value\": 10\n        }\n    )\n)\n</code></pre>"},{"location":"user_guides/fs/feature_group/data_validation/#using-great-expectations-profiler","title":"Using Great Expectations Profiler","text":"<p>Building Expectation Suite by hand can be a major time commitment when you have dozens of Features. Great Expectations offers <code>Profiler</code> classes to inspect a sample of your data and infers a suitable Expectation Suite that you will be able to register with Hopsworks.</p> <pre><code>ge_profiler = ge.profile.BasicSuiteBuilderProfiler()\nexpectation_suite_profiler, _ = ge_profiler.profile(ge.from_pandas(df))\n</code></pre> <p>Once you have built an Expectation Suite you are satisfied with, it is time to create your first validation enabled Feature Group.</p>"},{"location":"user_guides/fs/feature_group/data_validation/#step-3-attach-an-expectation-suite-to-your-feature-group-to-enable-automatic-validation-on-insertion","title":"Step 3: Attach an Expectation Suite to your Feature Group to enable Automatic Validation on Insertion","text":"<p>Writing data in Hopsworks is done using Feature Groups. Once a Feature Group is registered in the Feature Store, you can use it to insert your pandas DataFrames. For more information see create Feature Group. To benefit from automatic validation on insertion, attach your newly created Expectation Suite when creating the Feature Group:</p> <pre><code>fg = fs.create_feature_group(\n  \"fg_with_data_validation\",\n  version=1,\n  description=\"Validated data\",\n  primary_key=['foo_id'],\n  online_enabled=False,\n  expectation_suite=expectation_suite\n)\n</code></pre> <p>or, if the Feature Group already exist, you can simply run:</p> <pre><code>fg.save_expectation_suite(expectation_suite)\n</code></pre> <p>That is all there is to it. Hopsworks will now automatically use your suite to validate the DataFrames you want to write to the Feature Group. Try it out!</p> <pre><code>job, validation_report = fg.insert(df.head(5))\n</code></pre> <p>As you can see, Hopsworks runs the validation in the client before attempting to insert the data. By default, Hopsworks will try to insert the data even if validation fails to prevent data loss. However it can be configured for production setup to be more restrictive, checkout the data validation advanced guide.</p> <p>Info</p> <p>Note that once the Expectation Suite is attached to the Feature Group, any subsequent attempt to insert to this Feature Group will apply the Data Validation step even from a different client or in a scheduled job.</p>"},{"location":"user_guides/fs/feature_group/data_validation/#step-4-data-quality-monitoring","title":"Step 4: Data Quality Monitoring","text":"<p>Upon running validation, Great Expectations generates a report to help you assess the quality of your data. Nothing to do here, Hopsworks client automatically uploads the validation report to the backend when ingesting new data. It enables you to monitor the quality of the inserted data in the Feature Group over time.</p> <p>You can checkout a summary of the reports in the UI on your Feature Group page. As you can see, your Feature Group conveniently gather all in one place: your data, the Expectation Suite and the reports generated each time you inserted data!</p> <p>Hopsworks client API allows you to retrieve validation reports for further analysis.</p> <pre><code># load multiple reports\nvalidation_reports = fg.get_validation_reports()\n\n# convenience method for rapid development\nge_latest_report = fg.get_latest_validation_report()\n</code></pre> <p>Similarly you can retrieve the historic of validation results for a particular expectation, e.g to plot a time-series of a given expectation observed value over time.</p> <pre><code>validation_history = fg.get_validation_history(\n    expectationId=1\n)\n</code></pre> <p>You can find the expectationIds in the UI or using <code>fg.get_expectation_suite</code> and looking it up in the expectation's meta field.</p> <p>Info</p> <p>If Validation Reports or Results are too long, they can be truncated to fit in the database. A full version of the reports can be downloaded from the UI.</p>"},{"location":"user_guides/fs/feature_group/data_validation/#conclusion","title":"Conclusion","text":"<p>The integration between Hopsworks and Great Expectations makes it simple to add a data validation step to your feature engineering pipeline. Build your Expectation Suite and attach it to your Feature Group with a single line of code. No need to add any code to your pipeline or job scripts, calling <code>fg.insert</code> will now automatically validate the data before inserting them in the Feature Group. The validation reports are stored along your data in Hopsworks allowing us to provide basic monitoring capabilities to quickly spot a data quality issue in the UI.</p>"},{"location":"user_guides/fs/feature_group/data_validation/#going-further","title":"Going Further","text":"<p>If you wish to find out more about how to use the data validation API or best practices for development or production pipelines in Hopsworks, checkout the advanced guide and best practices guide.</p>"},{"location":"user_guides/fs/feature_group/data_validation_advanced/","title":"Advanced Data Validation Options and Best Practices","text":"<p>The introduction to the data validation guide can be found in the Data Validation Guide. The notebook example to get started with Data Validation in Hopsworks can be found in the Fraud Batch Data Validation Tutorial.</p>"},{"location":"user_guides/fs/feature_group/data_validation_advanced/#data-validation-configuration-options-in-hopsworks","title":"Data Validation Configuration Options in Hopsworks","text":""},{"location":"user_guides/fs/feature_group/data_validation_advanced/#validation-ingestion-policy","title":"Validation Ingestion Policy","text":"<p>Depending on your use case you can setup data validation as a monitoring or gatekeeping tool when trying to insert new data in your Feature Group. Switch behaviour by using the <code>validation_ingestion_policy</code> kwarg:</p> <ul> <li><code>\"ALWAYS\"</code> is the default option and will attempt to insert the data regardless of the validation result.   Hassle free, it is ideal to monitor data ingestion in a development setup.</li> <li><code>\"STRICT\"</code> is the best option for production ready projects.   This will prevent insertion of DataFrames which do not pass all data quality requirements.   Ideal to avoid \"garbage-in, garbage-out\" scenarios, at the price of a potential loss of data.   Check out the best practice section for more on that.</li> </ul>"},{"location":"user_guides/fs/feature_group/data_validation_advanced/#validation-ingestion-policy-in-ui","title":"Validation Ingestion Policy in UI","text":"<p>Go to the Feature Group edit page, in the Expectation section you can choose between the options above.</p>"},{"location":"user_guides/fs/feature_group/data_validation_advanced/#validation-ingestion-policy-in-python","title":"Validation Ingestion Policy in Python","text":"<pre><code>fg.expectation_suite.validation_ingestion_policy = \"ALWAYS\" # \"STRICT\"\n</code></pre> <p>If your suite is registered with Hopsworks, it will persist the change to the server.</p>"},{"location":"user_guides/fs/feature_group/data_validation_advanced/#disable-data-validation","title":"Disable Data Validation","text":"<p>Should you wish to do so, you can disable data validation on a punctual basis or until further notice.</p>"},{"location":"user_guides/fs/feature_group/data_validation_advanced/#disable-data-validation-in-ui","title":"Disable Data Validation in UI","text":"<p>You can do it in the UI in the Expectation section of the Feature Group edit page. Simply tick or untick the enabled checkbox. This will be used as the default option but can be overridden via the API.</p>"},{"location":"user_guides/fs/feature_group/data_validation_advanced/#disable-data-validation-in-python","title":"Disable Data Validation in Python","text":"<p>To disable data validation until further notice in the API, you can update the <code>run_validation</code> field of the expectation suite. If your suite is registered with Hopsworks, this will persist the change to the server.</p> <pre><code>fg.expectation_suite.run_validation = False\n</code></pre> <p>If you wish to override the default behaviour of the suite when inserting data in the Feature Group, you can do so via the <code>validate_options</code> kwarg. The example below will enable validation for this insertion only.</p> <pre><code>fg.insert(df_to_validate, validation_options={\"run_validation\" : True})\n</code></pre> <p>We recommend to avoid using this option in scheduled job as it silently changes the expected behaviour that is displayed in the UI and prevents changes to the default behaviour to change the behaviour of the job.</p>"},{"location":"user_guides/fs/feature_group/data_validation_advanced/#edit-expectations","title":"Edit Expectations","text":"<p>The one constant in life is change. If you need to add, remove or edit an expectation you can do it both in the UI or via the python client. Note that changing the expectation type or its corresponding feature will throw an error in order to preserve a meaningful validation history.</p>"},{"location":"user_guides/fs/feature_group/data_validation_advanced/#edit-expectations-in-ui","title":"Edit Expectations in UI","text":"<p>Go to the Feature Group edit page, in the expectation section. You can click on the expectation you want to edit and edit the json configuration. Check out Great Expectations documentation if you need more information on a particular expectation.</p>"},{"location":"user_guides/fs/feature_group/data_validation_advanced/#edit-expectations-in-python","title":"Edit Expectations in Python","text":"<p>There are several way to edit an Expectation in the python client. You can use Great Expectations API or directly go through Hopsworks. In the latter case, if you want to edit or remove an expectation, you will need the Hopsworks expectation ID. It can be found in the UI or in the meta field of an expectation. Note that you must have inserted data in the FG and attached the expectation suite to enable the Expectation API.</p> <p>Get an expectation with a given id:</p> <pre><code>my_expectation = fg.expectation_suite.get_expectation(\n    expectation_id = my_expectation_id\n)\n</code></pre> <p>Add a new expectation:</p> <pre><code>new_expectation = ge.core.ExpectationConfiguration(\n    expectation_type=\"expect_column_values_not_to_be_null\",\n    kwargs={\n        \"mostly\": 1\n    }\n)\n\nfg.expectation_suite.add_expectation(new_expectation)\n</code></pre> <p>Edit expectation kwargs of an existing expectation :</p> <pre><code>existing_expectation = fg.expectation_suite.get_expectation(\n    expectation_id=existing_expectation_id\n)\n\nexisting_expectation.kwargs[\"mostly\"] = 0.95\n\nfg.expectation_suite.replace_expectation(existing_expectation)\n</code></pre> <p>Remove an expectation:</p> <pre><code>fg.expectation_suite.remove_expectation(\n    expectation_id=id_of_expectation_to_delete\n)\n</code></pre> <p>If you want to deal only with the Great Expectation API:</p> <pre><code>my_suite = fg.get_expectation_suite()\n\nmy_suite.add_expectation(new_expectation)\n\nfg.save_expectation_suite(my_suite)\n</code></pre>"},{"location":"user_guides/fs/feature_group/data_validation_advanced/#save-validation-reports","title":"Save Validation Reports","text":"<p>When running validation using Great Expectations, a validation report is generated containing all validation results for the different expectations. Each result provides information about whether the provided DataFrame conforms to the corresponding expectation. These reports can be stored in Hopsworks to save a validation history for the data written to a particular Feature Group.</p> <p>The boilerplate of uploading report on insertion is taken care of by hopsworks, however for custom pipelines we provide an alternative method in the python client. The UI does not currently support upload of a validation report.</p>"},{"location":"user_guides/fs/feature_group/data_validation_advanced/#save-validation-reports-in-python","title":"Save Validation Reports in Python","text":"<pre><code>fg.save_validation_report(ge_report)\n</code></pre>"},{"location":"user_guides/fs/feature_group/data_validation_advanced/#monitor-and-fetch-validation-reports","title":"Monitor and Fetch Validation Reports","text":"<p>A summary of uploaded reports will then be available via an API call or in the Hopsworks UI enabling easy monitoring. For in-depth analysis, it is possible to download the complete report from the UI.</p>"},{"location":"user_guides/fs/feature_group/data_validation_advanced/#monitor-and-fetch-validation-reports-in-ui","title":"Monitor and Fetch Validation Reports in UI","text":"<p>Open the Feature Group overview page and go to the Expectations section. One tab allows you to check the report history with general information, while the other tab allows you to explore a summary of the result for individual expectations.</p>"},{"location":"user_guides/fs/feature_group/data_validation_advanced/#monitor-and-fetch-validation-reports-in-python","title":"Monitor and Fetch Validation Reports in Python","text":"<pre><code># convenience method for rapid development\nge_latest_report = fg.get_latest_validation_report()\n# fetching the latest summary prints a link to the UI\n# where you can download full report if summary is insufficient\n\n# or load multiple reports\nvalidation_history = fg.get_validation_reports()\n</code></pre>"},{"location":"user_guides/fs/feature_group/data_validation_advanced/#validate-your-data-manually","title":"Validate Your Data Manually","text":"<p>While Hopsworks provides automatic validation on insertion logic, we recognise that some use cases may require a more fine-grained control over the validation process. Therefore, Feature Group objects offers a convenience wrapper around Great Expectations to manually trigger validation using the registered Expectation Suite.</p>"},{"location":"user_guides/fs/feature_group/data_validation_advanced/#validate-your-data-manually-in-ui","title":"Validate Your Data Manually in UI","text":"<p>You can validate data already ingested in the Feature Group by going to the Feature Group overview page. In the top right corner is a button to trigger a validation. The button will launch a job which will read the Feature Group data, run validation and persist the associated report.</p>"},{"location":"user_guides/fs/feature_group/data_validation_advanced/#validate-your-data-manually-in-python","title":"Validate Your Data Manually in Python","text":"<pre><code>ge_report = fg.validate(df, ingestion_result=\"EXPERIMENT\")\n\n# set the save_report parameter to False to skip uploading the report to Hopsworks\n# ge_report = fg.validate(df, save_report=False)\n</code></pre> <p>If you want to apply validation to the data already in the Feature Group you can call the <code>.validate</code> without providing data. It will read the data in the Feature Group.</p> <pre><code>report = fg.validate()\n</code></pre> <p>As validation objects returned by Hopsworks are native Great Expectation objects you can run validation using the usual Great Expectations syntax:</p> <pre><code>ge_df = ge.from_pandas(df, expectation_suite=fg.get_expectation_suite())\nge_report = ge_df.validate()\n</code></pre> <p>Note that you should always use an expectation suite that has been saved to Hopsworks if you intend to upload the associated validation report.</p>"},{"location":"user_guides/fs/feature_group/data_validation_best_practices/","title":"Best practices","text":"<p>Below is a set of recommendations and code snippets to help our users follow best practices when it comes to integrating a data validation step in your feature engineering pipelines. Rather than being prescriptive, we want to showcase how the API and configuration options can help adapt validation to your use-case.</p>"},{"location":"user_guides/fs/feature_group/data_validation_best_practices/#development","title":"Development","text":"<p>Data validation is generally considered to be a production-only feature and as such is often only setup once a project has reached the end of the development phase. At Hopsworks, we think there is a lot of value in setting up validation during early development. That's why we made it quick to get started and ensured that by default data validation is never an obstacle to inserting data.</p>"},{"location":"user_guides/fs/feature_group/data_validation_best_practices/#validate-early","title":"Validate Early","text":"<p>As often with data validation, the best piece of advice is to set it up early in your development process. Use this phase to build a history you can then use when it becomes time to set quality requirements for a project in production. We made a code snippet to help you get started quickly:</p> <pre><code># Load sample data.\n# Replace it with your own!\nmy_data_df = pd.read_csv(\"https://repo.hops.works/master/hopsworks-tutorials/data/card_fraud_data/credit_cards.csv\")\n\n# Use Great Expectation profiler (ignore deprecation warning)\nexpectation_suite_profiled, validation_report = ge.from_pandas(my_data_df).profile(profiler=ge.profile.BasicSuiteBuilderProfiler)\n\n# Create a Feature Group on hopsworks with an expectation suite attached.\n# Don't forget to change the primary key!\nmy_validated_data_fg = fs.get_or_create_feature_group(\n    name=\"my_validated_data_fg\",\n    version=1,\n    description=\"My data\",\n    primary_key=['cc_num'],\n    expectation_suite=expectation_suite_profiled)\n</code></pre> <p>Any data you insert in the Feature Group from now will be validated and a report will be uploaded to Hopsworks.</p> <pre><code># Insert and validate your data\ninsert_job, validation_report = my_validated_data_fg.insert(my_data_df)\n</code></pre> <p>Great Expectations profiler can inspect your data to build a standard Expectation Suite. You can attach this Expectation Suite directly when creating your Feature Group to make sure every piece of data finding its way in Hopsworks gets validated. Hopsworks will default to its <code>\"ALWAYS\"</code> ingestion policy, meaning data are ingested whether validation succeeds or not. This way data validation is not a barrier, just a monitoring tool.</p>"},{"location":"user_guides/fs/feature_group/data_validation_best_practices/#identify-unreliable-features","title":"Identify Unreliable Features","text":"<p>Once you setup data validation, every insertion will upload a validation report to Hopsworks. Identifying Features which often have null values or wild statistical variations can help detecting unreliable Features that need refinements or should be avoided. Here are a few expectations you might find useful:</p> <ul> <li><code>expect_column_values_to_not_be_null</code></li> <li><code>expect_column_(min/max/mean/stdev)_to_be_between</code></li> <li><code>expect_column_values_to_be_unique</code></li> </ul>"},{"location":"user_guides/fs/feature_group/data_validation_best_practices/#get-the-stakeholders-involved","title":"Get the stakeholders involved","text":"<p>Hopsworks UI helps involve every project stakeholder by enabling both setting and monitoring of data quality requirements. No coding skills needed! You can monitor data quality requirements by checking out the validation reports and results on the Feature Group page.</p> <p>If you need to set or edit the existing requirements, you can go on the Feature Group edit page. The Expectation suite section allows you to edit individual expectations and set success parameters that match ever changing business requirements.</p>"},{"location":"user_guides/fs/feature_group/data_validation_best_practices/#production","title":"Production","text":"<p>Models in production require high-quality data to make accurate predictions for your customers. Hopsworks can use your Expectation Suite as a gatekeeper to make it simple to prevent low-quality data to make its way into production. Below are some simple tips and snippets to make the most of your data validation when your project is ready to enter its production phase.</p>"},{"location":"user_guides/fs/feature_group/data_validation_best_practices/#be-strict-in-production","title":"Be Strict in Production","text":"<p>Whether you use an existing or create a new (recommended) Feature Group for production, we recommend you set the validation ingestion policy of your Expectation Suite to <code>\"STRICT\"</code>.</p> <pre><code>fg_prod.save_expectation_suite(\n    my_suite,\n    validation_ingestion_policy=\"STRICT\")\n</code></pre> <p>In this setup, Hopsworks will abort inserting a DataFrame that does not successfully fulfill all expectations in the attached Expectation Suite. This ensures data quality standards are upheld for every insertion and provide downstream users with strong guarantees.</p>"},{"location":"user_guides/fs/feature_group/data_validation_best_practices/#avoid-data-loss-on-materialization-jobs","title":"Avoid Data Loss on materialization jobs","text":"<p>Aborting insertions of DataFrames which do not satisfy the data quality standards can lead to data loss in your materialization job. To avoid such loss we recommend creating a duplicate Feature Group with the same Expectation Suite in <code>\"ALWAYS\"</code> mode which will hold the rejected data.</p> <pre><code>job, report = fg_prod.insert(df)\n\nif report[\"success\"] is False:\n    job, report = fg_rejected.insert(df)\n</code></pre>"},{"location":"user_guides/fs/feature_group/data_validation_best_practices/#take-advantage-of-the-validation-history","title":"Take Advantage of the Validation History","text":"<p>You can easily retrieve the validation history of a specific expectation to export it to your favourite visualisation tool. You can filter on time and on whether insertion was successful or not.</p> <pre><code>validation_history = fg.get_validation_history(\n    expectation_id=my_id,\n    filters=[\"REJECTED\", \"UNKNOWN\"],\n    ge_type=False\n)\n\ntimeseries = pd.DataFrame(\n    {\n        \"observed_value\": [res.result[\"observed_value\"] for res in validation_history],\n        \"validation_time\": [res.validation_time for res in validation_history]\n    }\n)\n\n# export to your preferred Dashboard\n</code></pre>"},{"location":"user_guides/fs/feature_group/data_validation_best_practices/#setup-alerts","title":"Setup Alerts","text":"<p>While checking your feature engineering pipeline executed properly in the morning can be good enough in the development phase, it won't make the cut for demanding production use-cases. In Hopsworks, you can setup alerts if ingestion fails or succeeds.</p> <p>First you will need to configure your preferred communication endpoint: slack, email or pagerduty. Check out this page for more information on how to set it up. A typical use-case would be to add an alert on ingestion success to a Feature Group you created to hold data that failed validation. Here is a quick walkthrough:</p> <ol> <li>Go the Feature Group page in the UI</li> <li>Scroll down and click on the <code>Add an alert</code> button.</li> <li>Choose the trigger, receiver and severity and click save.</li> </ol>"},{"location":"user_guides/fs/feature_group/data_validation_best_practices/#conclusion","title":"Conclusion","text":"<p>Hopsworks extends Great Expectations by automatically running the validation, persisting the reports along your data and allowing you to monitor data quality in its UI. How you decide to make use of these tools depends on your application and requirements. Whether in development or in production, real-time or batch, we think there is configuration that will work for your team. Check out our quick hands-on tutorial to start applying what you learned so far.</p>"},{"location":"user_guides/fs/feature_group/deprecation/","title":"How to deprecate a Feature Group","text":""},{"location":"user_guides/fs/feature_group/deprecation/#introduction","title":"Introduction","text":"<p>To discourage the usage of specific feature groups it is possible to deprecate them. When a feature group is deprecated, user will be warned when they try to use it or use a feature view that depends on it.</p> <p>In this guide you will learn how to deprecate a feature group within Hopsworks, showing examples in HSFS APIs as well as the user interface.</p>"},{"location":"user_guides/fs/feature_group/deprecation/#prerequisites","title":"Prerequisites","text":"<p>Before you begin this guide it is expected that there is an existing feature group in your project. You can familiarize yourself with the creation of a feature group in the user guide.</p>"},{"location":"user_guides/fs/feature_group/deprecation/#deprecate-using-the-hsfs-apis","title":"Deprecate using the HSFS APIs","text":""},{"location":"user_guides/fs/feature_group/deprecation/#retrieve-the-feature-group","title":"Retrieve the feature group","text":"<p>To deprecate a feature group using the HSFS APIs you need to provide a Feature Group.</p> Python <pre><code>fg = fs.get_feature_group(name=\"feature_group_name\", version=feature_group_version)\n</code></pre>"},{"location":"user_guides/fs/feature_group/deprecation/#deprecate-feature-group","title":"Deprecate Feature Group","text":"<p>Feature group deprecation occurs by calling the <code>update_deprecated</code> method on the feature group.</p> Python <pre><code>fg.update_deprecated()\n</code></pre> <p>Users can also un-deprecate the feature group if need be, by setting the <code>deprecate</code> parameter to False.</p> Python <pre><code>fg.update_deprecated(deprecate=False)\n</code></pre>"},{"location":"user_guides/fs/feature_group/deprecation/#deprecate-using-the-ui","title":"Deprecate using the UI","text":"<p>You can deprecate/de-deprecate feature groups through the UI. For this, navigate to the <code>Feature Groups</code> section and select a feature group.</p> <p> </p> <p>Subsequently, make sure that the necessary feature group version is picked.</p> <p> </p> <p>Finally, click on the button with three vertical dots in the right corner and select <code>Deprecate</code>.</p> <p> </p> <p>The Feature group can be de-deprecated by selecting the <code>Undeprecate</code> option on a deprecated feature group.</p>"},{"location":"user_guides/fs/feature_group/feature_monitoring/","title":"Feature Monitoring for Feature Groups","text":"<p>Feature Monitoring complements the Hopsworks data validation capabilities for Feature Groups by allowing you to monitor your data once they have been ingested into the Feature Store. Hopsworks feature monitoring is centered around two functionalities: scheduled statistics and statistics comparison.</p> <p>Before continuing with this guide, see the Feature monitoring guide to learn more about how feature monitoring works, and get familiar with the different use cases of feature monitoring for Feature Groups described in the Use cases sections of the Scheduled statistics guide and Statistics comparison guide.</p> <p>Limited UI support</p> <p>Currently, feature monitoring can only be configured using the Hopsworks Python library. However, you can enable/disable a feature monitoring configuration or trigger the statistics comparison manually from the UI, as shown in the Advanced guide.</p>"},{"location":"user_guides/fs/feature_group/feature_monitoring/#code","title":"Code","text":"<p>In this section, we show you how to setup feature monitoring in a Feature Group using the Hopsworks Python library. Alternatively, you can get started quickly by running our tutorial for feature monitoring.</p> <p>First, checkout the pre-requisite and Hopsworks setup to follow the guide below. Create a project, install the Hopsworks Python library in your environment, connect via the generated API key. The second step is to start a new configuration for feature monitoring.</p> <p>After that, you can optionally define a detection window of data to compute statistics on, or use the default detection window (i.e., whole feature data). If you want to setup scheduled statistics alone, you can jump to the last step to save your configuration. Otherwise, the third and fourth steps are also optional and show you how to setup the comparison of statistics on a schedule by defining a reference window and specifying the statistics metric to monitor.</p>"},{"location":"user_guides/fs/feature_group/feature_monitoring/#step-1-pre-requisite","title":"Step 1: Pre-requisite","text":"<p>In order to setup feature monitoring for a Feature Group, you will need:</p> <ul> <li>A Hopsworks project.   If you don't have a project yet you can go to app.hopsworks.ai, signup with your email and create your first project.</li> <li>An API key, you can get one by going to \"Account Settings\" on app.hopsworks.ai.</li> <li>The Hopsworks Python library installed in your client.   See the installation guide.</li> <li>A Feature Group</li> </ul>"},{"location":"user_guides/fs/feature_group/feature_monitoring/#connect-your-notebook-to-hopsworks","title":"Connect your notebook to Hopsworks","text":"<p>Connect the client running your notebooks to Hopsworks.</p> Python <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nfs = project.get_feature_store()\n</code></pre> <p>You will be prompted to paste your API key to connect the notebook to your project. The <code>fs</code> Feature Store entity is now ready to be used to insert or read data from Hopsworks.</p>"},{"location":"user_guides/fs/feature_group/feature_monitoring/#get-or-create-a-feature-group","title":"Get or create a Feature Group","text":"<p>Feature monitoring can be enabled on already created Feature Groups. We suggest you read the Feature Group concept page to understand what a feature group is and how it fits in the ML pipeline. We also suggest you familiarize with the APIs to create a feature group.</p> <p>The following is a code example for getting or creating a Feature Group with name <code>trans_fg</code> for transaction data.</p> Python <pre><code># Retrieve an existing feature group\ntrans_fg = fs.get_feature_group(\"trans_fg\", version=1)\n\n# Or, create a new feature group with transactions\ntrans_fg = fs.get_or_create_feature_group(\n    name=\"trans_fg\",\n    version=1,\n    description=\"Transaction data\",\n    primary_key=[\"cc_num\"],\n    event_time=\"datetime\",\n)\ntrans_fg.insert(transactions_df)\n</code></pre>"},{"location":"user_guides/fs/feature_group/feature_monitoring/#step-2-initialize-configuration","title":"Step 2: Initialize configuration","text":""},{"location":"user_guides/fs/feature_group/feature_monitoring/#scheduled-statistics","title":"Scheduled statistics","text":"<p>You can setup statistics monitoring on a single feature or multiple features of your Feature Group.</p> Python <pre><code># compute statistics for all the features\nfg_monitoring_config = trans_fg.create_statistics_monitoring(\n    name=\"trans_fg_all_features_monitoring\",\n    description=\"Compute statistics on all data of all features of the Feature Group on a daily basis\",\n)\n\n# or for a single feature\nfg_monitoring_config = trans_fg.create_statistics_monitoring(\n    name=\"trans_fg_amount_monitoring\",\n    description=\"Compute statistics on all data of a single feature of the Feature Group on a daily basis\",\n    feature_name=\"amount\",\n)\n</code></pre>"},{"location":"user_guides/fs/feature_group/feature_monitoring/#statistics-comparison","title":"Statistics comparison","text":"<p>When enabling the comparison of statistics in a feature monitoring configuration, you need to specify a single feature of your Feature Group. You can create multiple feature monitoring configurations for the same Feature Group, but each of them should point to a single feature in the Feature Group.</p> Python <pre><code>fg_monitoring_config = trans_fg.create_feature_monitoring(\n    name=\"trans_fg_amount_monitoring\",\n    feature_name=\"amount\",\n    description=\"Compute descriptive statistics on the amount Feature of the Feature Group on a daily basis\",\n)\n</code></pre>"},{"location":"user_guides/fs/feature_group/feature_monitoring/#custom-schedule-or-percentage-of-window-data","title":"Custom schedule or percentage of window data","text":"<p>By default, the computation of statistics is scheduled to run endlessly, every day at 12PM. You can modify the default schedule by adjusting the <code>cron_expression</code>, <code>start_date_time</code> and <code>end_date_time</code> parameters.</p> Python <pre><code>fg_monitoring_config = trans_fg.create_statistics_monitoring(\n    name=\"trans_fg_all_features_monitoring\",\n    description=\"Compute statistics on all data of all features of the Feature Group on a weekly basis\",\n    cron_expression=\"0 0 12 ? * MON *\",  # weekly\n    row_percentage=0.8,                  # use 80% of the data\n)\n\n# or\nfg_monitoring_config = trans_fg.create_feature_monitoring(\n    name=\"trans_fg_amount_monitoring\",\n    feature_name=\"amount\",\n    description=\"Compute descriptive statistics on the amount Feature of the Feature Group on a weekly basis\",\n    cron_expression=\"0 0 12 ? * MON *\",  # weekly\n    row_percentage=0.8,                  # use 80% of the data\n)\n</code></pre>"},{"location":"user_guides/fs/feature_group/feature_monitoring/#step-3-optional-define-a-detection-window","title":"Step 3: (Optional) Define a detection window","text":"<p>By default, the detection window is an expanding window covering the whole Feature Group data. You can define a different detection window using the <code>window_length</code> and <code>time_offset</code> parameters provided in the <code>with_detection_window</code> method. Additionally, you can specify the percentage of feature data on which statistics will be computed using the <code>row_percentage</code> parameter.</p> Python <pre><code>fm_monitoring_config.with_detection_window(\n    window_length=\"1w\",  # data ingested during one week\n    time_offset=\"1w\",    # starting from last week\n    row_percentage=0.8,  # use 80% of the data\n)\n</code></pre>"},{"location":"user_guides/fs/feature_group/feature_monitoring/#step-4-optional-define-a-reference-window","title":"Step 4: (Optional) Define a reference window","text":"<p>When setting up feature monitoring for a Feature Group, reference windows can be either a regular window or a specific value (i.e., window of size 1).</p> Python <pre><code># compare statistics against a reference window\nfm_monitoring_config.with_reference_window(\n    window_length=\"1w\",  # data ingested during one week\n    time_offset=\"2w\",    # starting from two weeks ago\n    row_percentage=0.8,  # use 80% of the data\n)\n\n# or a specific value\nfm_monitoring_config.with_reference_value(\n    value=100,\n)\n</code></pre>"},{"location":"user_guides/fs/feature_group/feature_monitoring/#step-5-optional-define-the-statistics-comparison-criteria","title":"Step 5: (Optional) Define the statistics comparison criteria","text":"<p>In order to compare detection and reference statistics, you need to provide the criteria for such comparison. First, you select the metric to consider in the comparison using the <code>metric</code> parameter. Then, you can define a relative or absolute threshold using the <code>threshold</code> and <code>relative</code> parameters.</p> Python <pre><code>fm_monitoring_config.compare_on(\n    metric=\"mean\",\n    threshold=0.2,  # a relative change over 20% is considered anomalous\n    relative=True,  # relative or absolute change\n    strict=False,   # strict or relaxed comparison\n)\n</code></pre> <p>Difference values and thresholds</p> <p>For more information about the computation of difference values and the comparison against threshold bounds see the Comparison criteria section in the Statistics comparison guide.</p>"},{"location":"user_guides/fs/feature_group/feature_monitoring/#step-6-save-configuration","title":"Step 6: Save configuration","text":"<p>Finally, you can save your feature monitoring configuration by calling the <code>save</code> method. Once the configuration is saved, the schedule for the statistics computation and comparison will be activated automatically.</p> Python <pre><code>fm_monitoring_config.save()\n</code></pre> <p>Next steps</p> <p>See the Advanced guide to learn how to delete, disable or trigger feature monitoring manually.</p>"},{"location":"user_guides/fs/feature_group/notification/","title":"Change Data Capture for feature groups","text":""},{"location":"user_guides/fs/feature_group/notification/#introduction","title":"Introduction","text":"<p>Changes to online-enabled feature groups can be captured by listening to events on specified topics. This optimizes the user experience by allowing users to proactively make predictions as soon as there is an update on the features.</p> <p>In this guide you will learn how to enable Change Data Capture (CDC) for online feature groups within Hopsworks, showing examples in HSFS APIs as well as the user interface.</p>"},{"location":"user_guides/fs/feature_group/notification/#prerequisites","title":"Prerequisites","text":"<p>Before you begin this guide we suggest you read the Feature Group concept page to understand what a feature group is and how it fits in the ML pipeline. Subsequently create a Kafka topic, this topic will be used for storing Change Data Capture events.</p>"},{"location":"user_guides/fs/feature_group/notification/#using-hsfs-apis","title":"Using HSFS APIs","text":""},{"location":"user_guides/fs/feature_group/notification/#create-a-feature-group-with-change-data-capture-using-python","title":"Create a Feature Group with Change Data Capture using Python","text":"<p>To enable Change Data Capture for an online-enabled feature group using the HSFS APIs you need to create a feature group and set the <code>notification_topic_name</code> properties value to the previously created topic.</p> Python <pre><code>fg = fs.create_feature_group(\n  name=\"feature_group_name\",\n  version=feature_group_version,\n  primary_key=feature_group_primary_keys,\n  online_enabled=True,\n  notification_topic_name=\"notification_topic_name\")\n</code></pre>"},{"location":"user_guides/fs/feature_group/notification/#update-feature-group-with-change-data-capture-topic-using-python","title":"Update Feature Group with Change Data Capture topic using Python","text":"<p>The notification topic name can be changed after the creation of the feature group. By setting the <code>notification_topic_name</code> value to <code>None</code> or empty string notification will be disabled. With the default configuration, it can take up to 30 minutes for these changes to take place since the onlinefs service internally caches feature groups.</p> Python <pre><code>fg.update_notification_topic_name(\n  notification_topic_name=\"new_notification_topic_name\")\n</code></pre>"},{"location":"user_guides/fs/feature_group/notification/#using-ui","title":"Using UI","text":""},{"location":"user_guides/fs/feature_group/notification/#create-a-feature-group-with-change-data-capture-using-ui","title":"Create a Feature Group with Change Data Capture using UI","text":"<p>During the creation of the feature group enable online feature serving. When enabled you will be able to set the <code>CDC topic name</code> property.</p> <p> </p>"},{"location":"user_guides/fs/feature_group/notification/#update-feature-group-with-change-data-capture-topic-using-ui","title":"Update Feature Group with Change Data Capture topic using UI","text":"<p>The notification topic name can be changed after creation by editing the feature group. By setting the <code>CDC topic name</code> value to empty the notifications will be disabled. With the default configuration, it can take up to 30 minutes for these changes to take place since the onlinefs service internally caches feature groups.</p> <p> </p>"},{"location":"user_guides/fs/feature_group/notification/#example-of-change-data-capture-event","title":"Example of Change Data Capture event","text":"<p>Once properly set up the online feature store service will produce events to the provided topic when data ingestion is completed for records.</p> <p>Here is an example output:</p> <pre><code>{\n  \"projectName\":\"project_name\",  // name of the project the feature group belongs to\n  \"projectId\":119,  // id of the project the feature group belongs to\n  \"featureStoreId\":67,  // feature store where changes took place\n  \"featureGroupId\":14,  // id of the feature group\n  \"featureGroupName\":\"fg_name\",  // name of the feature group\n  \"featureGroupVersion\":1,  // version of the feature group\n  \"entry\":{ // values of the affected feature group entry\n    \"id\":\"15\",\n    \"text\":\"test\"\n  },\n  \"featureViews\":[  // list of feature views affected\n    {\n      \"projectName\":\"project_name\", // name of the project the feature view belongs to\n      \"id\":9,  // id of the feature view\n      \"name\":\"test\",  // name of the feature view\n      \"version\":1,  // version of the feature view\n      \"featurestoreId\":67  // feature store where feature view resides\n    }\n  ]\n}\n</code></pre> <p>The list of <code>featureViews</code> in the event could be outdated for up to 10 minutes, due to internal logging in onlinefs service.</p>"},{"location":"user_guides/fs/feature_group/on_demand_transformations/","title":"On-Demand Transformation Functions","text":"<p>On-demand transformations produce on-demand features, which usually require parameters accessible during inference for their calculation. Hopsworks facilitates the creation of on-demand transformations without introducing online-offline skew, ensuring consistency while allowing their dynamic computation during online inference.</p>"},{"location":"user_guides/fs/feature_group/on_demand_transformations/#on-demand-transformation-function-creation","title":"On Demand Transformation Function Creation","text":"<p>An on-demand transformation function may be created by associating a transformation function with a feature group. Each on-demand transformation function can generate one or multiple on-demand features. If the on-demand transformation function returns a single feature, it is automatically assigned the same name as the transformation function. However, if it returns multiple features, they are by default named using the format <code>functionName_outputColumnNumber</code>. For instance, in the example below, the on-demand transformation function <code>transaction_age</code> produces an on-demand feature named <code>transaction_age</code> and the on-demand transformation function <code>stripped_strings</code> produces the on-demand features names <code>stripped_strings_0</code> and <code>stripped_strings_1</code>. Alternatively, the name of the resulting on-demand feature can be explicitly defined using the <code>alias</code> function.</p> <p>On-demand transformation</p> <p>All on-demand transformation functions attached to a feature group must have unique names and, in contrast to model-dependent transformations, they do not have access to training dataset statistics.</p> <p>Each on-demand transformation function can map specific features to its arguments by explicitly providing their names as arguments to the transformation function. If no feature names are provided, the transformation function will default to using features that match the name of the transformation function's argument.</p> Python <p>Creating on-demand transformation functions.</p> <pre><code># Define transformation function\n@hopsworks.udf(return_type=int, drop=[\"current_date\"])\ndef transaction_age(transaction_date, current_date):\n    return (current_date - transaction_date).dt.days\n\n@hopsworks.udf(return_type=[str, str], drop=[\"current_date\"])\ndef stripped_strings(country, city):\n    return country.strip(), city.strip()\n\n# Attach transformation function to feature group to create on-demand transformation function.\nfg = feature_store.create_feature_group(name=\"fg_transactions\",\n            version=1,\n            description=\"Transaction Features\",\n            online_enabled=True,\n            primary_key=['id'],\n            event_time='event_time',\n            transformation_functions=[transaction_age, stripped_strings]\n            )\n</code></pre>"},{"location":"user_guides/fs/feature_group/on_demand_transformations/#specifying-input-features","title":"Specifying input features","text":"<p>The features to be used by the on-demand transformation function can be specified by providing the feature names as input to the transformation functions.</p> Python <p>Creating on-demand transformations by specifying features to be passed to transformation function.</p> <pre><code>fg = feature_store.create_feature_group(name=\"fg_transactions\",\n            version=1,\n            description=\"Transaction Features\",\n            online_enabled=True,\n            primary_key=['id'],\n            event_time='event_time',\n            transformation_functions=[age_transaction('transaction_time', 'current_time')]\n            )\n</code></pre>"},{"location":"user_guides/fs/feature_group/on_demand_transformations/#usage","title":"Usage","text":"<p>On-demand transformation functions attached to a feature group are automatically executed in the feature pipeline when you\u00a0insert data\u00a0into a feature group and by the Python client while retrieving feature vectors for online inference using feature views that contain on-demand features.</p> <p>The on-demand features computed by on-demand transformation functions are positioned after all other features in a feature group and are ordered alphabetically by their names.</p>"},{"location":"user_guides/fs/feature_group/on_demand_transformations/#inserting-data","title":"Inserting data","text":"<p>All on-demand transformation functions attached to a feature group are executed whenever new data is inserted. This process computes on-demand features from historical data. The DataFrame used for insertion must include all features required for executing all on-demand transformation functions in the feature group.</p> <p>Inserting on-demand features as historical features saves time and computational resources by removing the need to compute all on-demand features while generating training or batch data.</p>"},{"location":"user_guides/fs/feature_group/on_demand_transformations/#accessing-on-demand-features-in-feature-views","title":"Accessing on-demand features in feature views","text":"<p>A feature view can include on-demand features from feature groups by selecting them in the query used to create the feature view. These on-demand features are equivalent to regular features, and model-dependent transformations can be applied to them if required.</p> Python <p>Creating feature view with on-demand features</p> <pre><code># Selecting on-demand features in query\nquery = fg.select([\"id\", \"feature1\", \"feature2\", \"on_demand_feature3\", \"on_demand_feature4\"])\n\n# Creating a feature view using a query that contains on-demand transformations and model-dependent transformations\nfeature_view = fs.create_feature_view(\n        name='transactions_view',\n        query=query,\n        transformation_functions=[\n            min_max_scaler(\"feature1\"),\n            min_max_scaler(\"on_demand_feature3\"),\n        ]\n    )\n</code></pre>"},{"location":"user_guides/fs/feature_group/on_demand_transformations/#computing-on-demand-features","title":"Computing on-demand features","text":"<p>On-demand features in the feature view are computed in real-time during online inference using the same on-demand transformation functions used to create them. Hopsworks, by default, automatically computes all on-demand features when retrieving feature view input features (feature vectors) with the functions <code>get_feature_vector</code> and <code>get_feature_vectors</code>. Additionally, on-demand features can be computed using the <code>compute_on_demand_features</code> function or by manually executing the same on-demand transformation function.</p> <p>The values for the input parameters required to compute on-demand features can be provided using the <code>request_parameters</code> argument. If values are not provided through the <code>request_parameters</code> argument, the transformation function will verify if the feature vector contains the necessary input parameters and will use those values instead. However, if the required input parameters are also not present in the feature vector, an error will be thrown.</p> <p>Note</p> <p>By default the functions <code>get_feature_vector</code> and <code>get_feature_vectors</code> will apply model-dependent transformation present in the feature view after computing on-demand features.</p>"},{"location":"user_guides/fs/feature_group/on_demand_transformations/#retrieving-a-feature-vector","title":"Retrieving a feature vector","text":"<p>The <code>get_feature_vector</code> function retrieves a single feature vector based on the feature view's serving key(s). The on-demand features in the feature vector can be computed using real-time data by passing a dictionary that associates the name of each input parameter needed for the on-demand transformation function with its respective new value to the <code>request_parameter</code> argument.</p> Python <p>Computing on-demand features while retrieving a feature vector</p> <pre><code>feature_vector = feature_view.get_feature_vector(\n    entry={\"id\": 1},\n    request_parameter={\n        \"transaction_time\": datetime(2022, 12, 28, 23, 55, 59),\n        \"current_time\": datetime.now(),\n    },\n)\n</code></pre>"},{"location":"user_guides/fs/feature_group/on_demand_transformations/#retrieving-feature-vectors","title":"Retrieving feature vectors","text":"<p>The <code>get_feature_vectors</code> function retrieves multiple feature vectors using a list of feature view serving keys. The <code>request_parameter</code> in this case, can be a list of dictionaries that specifies the input parameters for the computation of on-demand features for each serving key or can be a dictionary if the on-demand transformations require the same parameters for all serving keys.</p> Python <p>Computing on-demand features while retrieving a feature vectors</p> <pre><code># Specify unique request parameters for each serving key.\nfeature_vector = feature_view.get_feature_vectors(\n    entry=[{\"id\": 1}, {\"id\": 2}],\n    request_parameter=[\n        {\n            \"transaction_time\": datetime(2022, 12, 28, 23, 55, 59),\n            \"current_time\": datetime.now(),\n        },\n        {\n            \"transaction_time\": datetime(2022, 11, 20, 12, 50, 00),\n            \"current_time\": datetime.now(),\n        },\n    ],\n)\n\n# Specify common request parameters for all serving key.\nfeature_vector = feature_view.get_feature_vectors(\n    entry=[{\"id\": 1}, {\"id\": 2}],\n    request_parameter={\n        \"transaction_time\": datetime(2022, 12, 28, 23, 55, 59),\n        \"current_time\": datetime.now(),\n    },\n)\n</code></pre>"},{"location":"user_guides/fs/feature_group/on_demand_transformations/#retrieving-feature-vector-without-on-demand-features","title":"Retrieving feature vector without on-demand features","text":"<p>The <code>get_feature_vector</code> and <code>get_feature_vectors</code> methods can return untransformed feature vectors without on-demand features by disabling model-dependent transformations and excluding on-demand features. To achieve this, set the  parameters <code>transform</code> and <code>on_demand_features</code> to <code>False</code>.</p> Python <p>Returning untransformed feature vectors</p> <pre><code>untransformed_feature_vector = feature_view.get_feature_vector(\n    entry={\"id\": 1}, transform=False, on_demand_features=False\n)\nuntransformed_feature_vectors = feature_view.get_feature_vectors(\n    entry=[{\"id\": 1}, {\"id\": 2}], transform=False, on_demand_features=False\n)\n</code></pre>"},{"location":"user_guides/fs/feature_group/on_demand_transformations/#compute-all-on-demand-features","title":"Compute all on-demand features","text":"<p>The <code>compute_on_demand_features</code> function computes all on-demand features attached to a feature view and adds them to the feature vectors provided as input to the function. This function does not apply model-dependent transformations to any of the features. The <code>transform</code> function can be used to apply model-dependent transformations to the returned values if required.</p> <p>The <code>request_parameter</code> in this case, can be a list of dictionaries that specifies the input parameters for the computation of on-demand features for each feature vector given as input to the function or can be a dictionary if the on-demand transformations require the same parameters for all input feature vectors.</p> Python <p>Computing all on-demand features and manually applying model dependent transformations.</p> <pre><code># Specify request parameters for each serving key.\nuntransformed_feature_vector = feature_view.get_feature_vector(\n    entry={\"id\": 1}, transform=False, on_demand_features=False\n)\n\n# re-compute and add on-demand features to the feature vector\nfeature_vector_with_on_demand_features = fv.compute_on_demand_features(\n    untransformed_feature_vector,\n    request_parameter={\n        \"transaction_time\": datetime(2022, 12, 28, 23, 55, 59),\n        \"current_time\": datetime.now(),\n    },\n)\n\n# Applying model dependent transformations\nencoded_feature_vector = fv.transform(feature_vector_with_on_demand_features)\n\n# Specify request parameters for each serving key.\nuntransformed_feature_vectors = feature_view.get_feature_vectors(\n    entry=[{\"id\": 1}, {\"id\": 2}], transform=False, on_demand_features=False\n)\n\n# re-compute and add on-demand features to the feature vectors - Specify unique request parameter for each feature vector\nfeature_vectors_with_on_demand_features = fv.compute_on_demand_features(\n    untransformed_feature_vectors,\n    request_parameter=[\n        {\n            \"transaction_time\": datetime(2022, 12, 28, 23, 55, 59),\n            \"current_time\": datetime.now(),\n        },\n        {\n            \"transaction_time\": datetime(2022, 11, 20, 12, 50, 00),\n            \"current_time\": datetime.now(),\n        },\n    ],\n)\n\n# re-compute and add on-demand feature to the feature vectors - Specify common request parameter for all feature vectors\nfeature_vectors_with_on_demand_features = fv.compute_on_demand_features(\n    untransformed_feature_vectors,\n    request_parameter={\n        \"transaction_time\": datetime(2022, 12, 28, 23, 55, 59),\n        \"current_time\": datetime.now(),\n    },\n)\n\n# Applying model dependent transformations\nencoded_feature_vector = fv.transform(feature_vectors_with_on_demand_features)\n</code></pre>"},{"location":"user_guides/fs/feature_group/on_demand_transformations/#compute-one-on-demand-feature","title":"Compute one on-demand feature","text":"<p>On-demand transformation functions can also be accessed and executed as normal functions by using the dictionary <code>on_demand_transformations</code> that maps the on-demand features to their corresponding on-demand transformation function.</p> Python <p>Executing each on-demand transformation function</p> <pre><code># Specify request parameters for each serving key.\nfeature_vector = feature_view.get_feature_vector(\n    entry={\"id\": 1}, transform=False, on_demand_features=False, return_type=\"pandas\"\n)\n\n# Applying model dependent transformations\nfeature_vector[\"on_demand_feature1\"] = fv.on_demand_transformations[\n    \"on_demand_feature1\"\n](feature_vector[\"transaction_time\"], datetime.now())\n</code></pre>"},{"location":"user_guides/fs/feature_group/online_ingestion_observability/","title":"Online ingestion observability","text":""},{"location":"user_guides/fs/feature_group/online_ingestion_observability/#introduction","title":"Introduction","text":"<p>Knowing when ingested data becomes available for online serving\u2014and understanding the cause of any ingestion failures\u2014is crucial for users. To address this, the Hopsworks API provides observability features for online ingestion, allowing you to monitor ingestion status and troubleshoot issues.</p> <p>This guide explains how to use these observability features for online feature groups in Hopsworks, with examples using both the HSFS APIs and the user interface.</p>"},{"location":"user_guides/fs/feature_group/online_ingestion_observability/#prerequisites","title":"Prerequisites","text":"<p>Before you begin this guide we suggest you read the Feature Group concept page to understand what a feature group is and how it fits in the ML pipeline.</p>"},{"location":"user_guides/fs/feature_group/online_ingestion_observability/#using-the-hopsworks-api","title":"Using the Hopsworks API","text":""},{"location":"user_guides/fs/feature_group/online_ingestion_observability/#create-a-feature-group-and-ingest-data","title":"Create a Feature Group and Ingest Data","text":"<p>First, create an online-enabled feature group and insert data into it:</p> Python <pre><code>fg = fs.create_feature_group(\n    name=\"feature_group_name\",\n    version=feature_group_version,\n    primary_key=feature_group_primary_keys,\n    online_enabled=True\n)\n\nfg.insert(fg_df)\n</code></pre>"},{"location":"user_guides/fs/feature_group/online_ingestion_observability/#retrieve-online-ingestion-status","title":"Retrieve Online Ingestion Status","text":"<p>After inserting data, you can monitor the ingestion progress:</p>"},{"location":"user_guides/fs/feature_group/online_ingestion_observability/#get-the-latest-ingestion-instance","title":"Get the latest ingestion instance","text":"Python <pre><code>oi = fg.get_latest_online_ingestion()\n</code></pre>"},{"location":"user_guides/fs/feature_group/online_ingestion_observability/#get-a-specific-ingestion-by-its-id","title":"Get a specific ingestion by its ID","text":"Python <pre><code>oi = fg.get_online_ingestion(ingestion_id)\n</code></pre>"},{"location":"user_guides/fs/feature_group/online_ingestion_observability/#use-the-online-ingestion-object","title":"Use the Online Ingestion Object","text":"<p>The online ingestion object provides methods to track and debug the ingestion process:</p>"},{"location":"user_guides/fs/feature_group/online_ingestion_observability/#wait-for-completion","title":"Wait for completion","text":"<p>Wait for the online ingestion to finish (equivalent to <code>fg.insert(fg_df, wait=True)</code>):</p> Python <pre><code>oi.wait_for_completion()\n</code></pre>"},{"location":"user_guides/fs/feature_group/online_ingestion_observability/#print-mini-batch-results","title":"Print mini-batch results","text":"<p>Check the results of the ingestion. If the status is <code>UPSERTED</code> and the number of rows matches your data, the ingestion was successful:</p> Python <pre><code>print([result.to_dict() for result in oi.results])\n# Example output: [{'onlineIngestionId': 1, 'status': 'UPSERTED', 'rows': 10}]\n</code></pre>"},{"location":"user_guides/fs/feature_group/online_ingestion_observability/#print-ingestion-service-logs","title":"Print ingestion service logs","text":"<p>Retrieve logs from the online ingestion service to diagnose any issues:</p> Python <pre><code>oi.print_logs(priority=\"error\", size=5)\n</code></pre>"},{"location":"user_guides/fs/feature_group/online_ingestion_observability/#using-the-ui","title":"Using the UI","text":""},{"location":"user_guides/fs/feature_group/online_ingestion_observability/#viewing-online-ingestion-status","title":"Viewing Online Ingestion Status","text":"<p>After inserting data into an online-enabled feature group, you can track the ingestion progress in the <code>Recent activities</code> section of the feature group in the Hopsworks UI.</p> <p> </p>"},{"location":"user_guides/fs/feature_group/statistics/","title":"How to compute statistics on feature data","text":""},{"location":"user_guides/fs/feature_group/statistics/#introduction","title":"Introduction","text":"<p>In this guide you will learn how to configure, compute and visualize statistics for the features registered with Hopsworks.</p> <p>Hopsworks groups statistics in four categories:</p> <ul> <li> <p>Descriptive: These are the basic statistics Hopsworks computes.   They include an approximate count of the distinctive values and the completeness (i.e., the percentage of non null values).   For numerical features Hopsworks also computes the minimum, maximum, mean, standard deviation and the sum of each feature.   Enabled by default.</p> </li> <li> <p>Histograms: Hopsworks computes the distribution of the values of a feature.   Exact histograms are computed as long as the number of distinct values is less than 20. If a feature has a numerical data type (e.g., integer, float, double, ...) and has more than 20 unique values, then the values are bucketed in 20 buckets and the histogram represents the distribution of values in those buckets.   By default histograms are disabled.</p> </li> <li> <p>Correlation: If enabled, Hopsworks computes the Pearson correlation between features of numerical data type within a feature group.   By default correlation is disabled.</p> </li> <li> <p>Exact Statistics: Exact statistics are an enhancement of the descriptive statistics that provide an exact count of distinctive values, entropy, uniqueness and distinctiveness of the value of a feature.   These statistics are more expensive to compute as they take into consideration all the values and they don't use approximations.   By default they are disabled.</p> </li> </ul> <p>When statistics are enabled, they are computed every time new data is written into the offline storage of a feature group. Statistics are then displayed on the Hopsworks UI and users can track how data has changed over time.</p>"},{"location":"user_guides/fs/feature_group/statistics/#prerequisites","title":"Prerequisites","text":"<p>Before you begin this guide we suggest you read the Feature Group concept page to understand what a feature group is and how it fits in the ML pipeline. We also suggest you familiarize with the APIs to create a feature group.</p>"},{"location":"user_guides/fs/feature_group/statistics/#enable-statistics-when-creating-a-feature-group","title":"Enable statistics when creating a feature group","text":"<p>As mentioned above, by default only descriptive statistics are enabled when creating a feature group. To enable histograms, correlations or exact statistics the <code>statistics_config</code> configuration parameter can be provided in the create statement.</p> <p>The <code>statistics_config</code> parameter takes a dictionary with the keys: <code>enabled</code>, <code>correlations</code>, <code>histograms</code> and <code>exact_uniqueness</code> and, as values, a boolean to describe whether or not to compute the specific class of statistics.</p> <p>Additionally it is possible to restrict the statistics computation to only a subset of columns. This is configurable by adding a <code>columns</code> key to the <code>statistics_config</code> parameter. The key should contain the list of columns for which to compute statistics. By default the value is empty list <code>[]</code> and the statistics are computed for all columns in the feature group.</p> Python <pre><code>fg = feature_store.create_feature_group(name=\"weather\",\n    version=1,\n    description=\"Weather Features\",\n    online_enabled=True,\n    primary_key=['location_id'],\n    partition_key=['day'],\n    event_time='event_time',\n    statistics_config={\n        \"enabled\": True,\n        \"histograms\": True,\n        \"correlations\": True,\n        \"exact_uniqueness\": False,\n        \"columns\": []\n    }\n)\n</code></pre>"},{"location":"user_guides/fs/feature_group/statistics/#enable-statistics-after-creating-a-feature-group","title":"Enable statistics after creating a feature group","text":"<p>It is possible users to change the statistics configuration after a feature group was created. Either to add or remove a class of statistics, or to change the set of features for which to compute statistics.</p> Python <pre><code>fg.statistics_config = {\n    \"enabled\": True,\n    \"histograms\": False,\n    \"correlations\": False,\n    \"exact_uniqueness\": False,\n    \"columns\": ['location_id', 'min_temp', 'max_temp']\n}\n\nfg.update_statistics_config()\n</code></pre>"},{"location":"user_guides/fs/feature_group/statistics/#explicitly-compute-statistics","title":"Explicitly compute statistics","text":"<p>As mentioned above, the statistics are computed every time new data is written into the offline storage of a feature group. By invoking the <code>compute_statistics</code> method, users can trigger explicitly the statistics computation for the data available in a feature group.</p> <p>This is useful when a feature group is receiving frequent updates. Users can schedule periodic statistics computation that take into consideration several data commits.</p> <p>By default, the <code>compute_statistics</code> method computes statistics on the most recent version of the data available in a feature group. Users can provide a specific time using the <code>wallclock_time</code> parameter, to compute the statistics for a previous version of the data.</p> <p>Hopsworks can compute statistics of external feature groups. As external feature groups are read only from an Hopsworks perspective, statistics computation can be triggered using the <code>compute_statistics</code> method.</p> Python <pre><code>fg.compute_statistics(wallclock_time='20220611 20:00')\n</code></pre>"},{"location":"user_guides/fs/feature_group/statistics/#inspect-statistics","title":"Inspect statistics","text":"<p>You can also create a new feature group through the UI.</p>"},{"location":"user_guides/fs/feature_group/ttl/","title":"Time-To-Live (TTL)","text":""},{"location":"user_guides/fs/feature_group/ttl/#feature-group-ttl-usage-guide","title":"Feature Group TTL Usage Guide","text":"<p>Time To Live (TTL) is a feature that automatically expires data in feature groups after a specified time period. This guide explains when and how to use TTL in your feature groups.</p>"},{"location":"user_guides/fs/feature_group/ttl/#use-case-when-to-use-ttl","title":"Use Case: When to Use TTL","text":"<p>TTL is particularly useful for feature groups that contain time-sensitive data that becomes stale or irrelevant after a certain period. Common use cases include:</p> <ul> <li>Regulatory compliance: Data that must be automatically purged after a retention period for privacy or compliance reasons (e.g., GDPR, HIPAA)</li> <li>Cost optimization: Reducing storage costs by automatically removing outdated data that is no longer needed for model inference</li> <li>Data freshness: Ensuring that only recent, relevant data is available for online serving, preventing models from using stale features</li> </ul> <p>For example, if you're building a recommendation system, you might want user interaction features (like \"items viewed in the last hour\") to automatically expire after 1 hour, ensuring your model only uses current, relevant data.</p>"},{"location":"user_guides/fs/feature_group/ttl/#getting-started","title":"Getting Started","text":""},{"location":"user_guides/fs/feature_group/ttl/#creating-a-feature-group-with-ttl","title":"Creating a Feature Group with TTL","text":"<p>When creating a new feature group, you can enable TTL by specifying the <code>ttl</code> parameter. The TTL value determines how long data will remain in the feature group before being automatically expired. The TTL is calculated based on the <code>event_time</code> column. Data rows where <code>event_time</code> is older than the TTL period will be automatically removed.</p> <pre><code>from datetime import datetime, timezone\nimport pandas as pd\n\n# Assume you already have a feature store handle\n# fs = ...\n\nnow = datetime.now(timezone.utc)\ndf = pd.DataFrame(\n    {\n        \"id\": [0, 1, 2],\n        \"timestamp\": [now, now, now],\n        \"feature1\": [10, 20, 30],\n        \"feature2\": [\"a\", \"b\", \"c\"],\n    }\n)\n\n# Create a feature group with TTL enabled (60 seconds)\nfg = fs.create_feature_group(\n    name=\"fg_ttl_example\",\n    version=1,\n    primary_key=[\"id\"],\n    event_time=\"timestamp\",\n    online_enabled=True,\n    ttl=60,  # TTL in seconds - data will expire after 60 seconds\n)\n\nfg.insert(\n    df,\n    write_options={\n        \"start_offline_materialization\": False,\n        \"wait_for_online_ingestion\": True,\n    },\n)\n\n# After 60 seconds, reading online will return empty data\nfg.read(online=True)  # Returns empty DataFrame after TTL expires\n</code></pre> <p>For detailed API reference on all possible types of TTL values, see the FeatureStore.create_feature_group API documentation.</p>"},{"location":"user_guides/fs/feature_group/ttl/#managing-ttl-on-existing-feature-groups","title":"Managing TTL on Existing Feature Groups","text":""},{"location":"user_guides/fs/feature_group/ttl/#updating-the-ttl-value","title":"Updating the TTL Value","text":"<p>You can change the TTL value for an existing feature group at any time. This is useful when you need to adjust the retention period based on changing requirements.</p> <pre><code># Get your existing feature group\nfg = fs.get_feature_group(\n    name=\"fg_ttl_example\",\n    version=1,\n)\n\n# Update TTL to a new value (120 seconds = 2 minutes)\nfg.enable_ttl(ttl=120)\n</code></pre> <p>After updating the TTL, the new retention period will apply to all future data insertions and will affect when existing data expires.</p>"},{"location":"user_guides/fs/feature_group/ttl/#disabling-and-re-enabling-ttl","title":"Disabling and Re-enabling TTL","text":"<p>You can temporarily disable TTL on a feature group if you need to retain data indefinitely, and then re-enable it later.</p>"},{"location":"user_guides/fs/feature_group/ttl/#disabling-ttl","title":"Disabling TTL","text":"<pre><code># Disable TTL - data will no longer expire automatically\nfg.disable_ttl()\n</code></pre>"},{"location":"user_guides/fs/feature_group/ttl/#re-enabling-ttl","title":"Re-enabling TTL","text":"<p>When re-enabling TTL, you have two options:</p> <ol> <li> <p>Re-enable with the previous TTL value: If you don't specify a TTL value, the feature group will use the last TTL value that was set.</p> <pre><code># Re-enable TTL using the previous TTL value\nfg.enable_ttl()\n</code></pre> </li> <li> <p>Re-enable with a new TTL value: Specify a new TTL value when re-enabling.</p> <pre><code># Re-enable TTL with a new value (90 seconds)\nfg.enable_ttl(ttl=90)\n</code></pre> </li> </ol> <p>Important: If TTL was never set on the feature group before, you must provide a TTL value when enabling it. Otherwise, TTL cannot be enabled.</p>"},{"location":"user_guides/fs/feature_group/ttl/#enabling-ttl-on-an-existing-feature-group","title":"Enabling TTL on an Existing Feature Group","text":"<p>If you created a feature group without TTL initially, you can enable it later:</p> <pre><code># Get an existing feature group that was created without TTL\nfg = fs.get_feature_group(\n    name=\"fg_existing_no_ttl\",\n    version=1,\n)\n\n# Enable TTL for the first time (60 seconds)\nfg.enable_ttl(ttl=60)\n</code></pre> <p>Once enabled, TTL will apply to all data in the feature group based on the <code>event_time</code> column. For detailed API reference on all possible types of TTL values and additional options, see the [FeatureGroup.enable_ttl API documentation][hsfs.feature_group.FeatureGroup.enable_ttl].</p>"},{"location":"user_guides/fs/feature_monitoring/","title":"Feature Monitoring","text":""},{"location":"user_guides/fs/feature_monitoring/#introduction","title":"Introduction","text":"<p>Feature Monitoring complements the Hopsworks data validation capabilities by allowing you to monitor your data once they have been ingested into the Feature Store. Hopsworks feature monitoring user interface is centered around two functionalities:</p> <ul> <li> <p>Scheduled Statistics: The user defines a detection window over its data for which Hopsworks will compute the statistics on a regular basis.   The results are stored in Hopsworks and enable the user to visualise the temporal evolution of statistical metrics on its data.   This can be enabled for a whole Feature Group or Feature View, or for a particular Feature.   For more details, see the Scheduled statistics guide.</p> </li> <li> <p>Statistics Comparison: Enabled only for individual features, this variant allows the user to schedule the statistics computation on both a detection and a reference window.   By providing information about how to compare those statistics, you can setup alerts to quickly detect critical change in the data.   For more details, see the Statistics comparison guide.</p> </li> </ul> <p>Important</p> <p>To enable feature monitoring in Hopsworks, you need to set the <code>enable_feature_monitoring</code> configuration option to <code>true</code>. This can also be achieved in the cluster definition by setting the following attribute:</p> <pre><code>hopsworks:\n  enable_feature_monitoring: \"true\"\n</code></pre>"},{"location":"user_guides/fs/feature_monitoring/#statistics-computation-on-windows-of-feature-data","title":"Statistics computation on windows of feature data","text":"<p>Statistics are computed on feature data defined by windows. There are different types of windows depending on how they evolve over time. A window can have either a fixed length (e.g., static window) or variable length (e.g., expanding window). Moreover, windows can stick to a specific point in time (e.g., static window) or move over time (e.g., sliding or rolling window).</p> <p></p> <p>Specific values</p> <p>A specific value can be seen as a window of length 1 where the start and end of the window have the same value.</p> <p>These types of windows apply to both detection and reference windows. Different types of windows allows for different use cases depending on whether you enable feature monitoring on your Feature Groups or Feature Views.</p> <p>See more details about detection and reference windows in the Detection windows and Reference windows guides.</p>"},{"location":"user_guides/fs/feature_monitoring/#visualize-statistics-on-a-time-series","title":"Visualize statistics on a time series","text":"<p>Hopsworks provides an interactive graph to make the exploration of statistics and statistics comparison results more efficient and help you find unexpected trends or anomalous values faster. See the Interactive graph guide for more information.</p> <p></p>"},{"location":"user_guides/fs/feature_monitoring/#alerting","title":"Alerting","text":"<p>Moreover, feature monitoring integrates with the Hopsworks built-in system for alerts, enabling you to setup alerts that will notify you as soon as shift is detected in your feature values. You can setup alerts for feature monitoring at a Feature Group, Feature View, and project level.</p> <p>Select the correct trigger</p> <p>When configuring alerts for feature monitoring, make sure you select the <code>feature monitoring-shift detected</code> or <code>feature monitoring-shift undetected</code> trigger.</p> <p></p>"},{"location":"user_guides/fs/feature_monitoring/feature_monitoring_advanced/","title":"Advanced guide","text":"<p>An introduction to Feature Monitoring can be found in the guides for Feature Groups and Feature Views. In addition, you can get started quickly by running our tutorial for feature monitoring.</p>"},{"location":"user_guides/fs/feature_monitoring/feature_monitoring_advanced/#retrieve-feature-monitoring-configurations","title":"Retrieve feature monitoring configurations","text":""},{"location":"user_guides/fs/feature_monitoring/feature_monitoring_advanced/#retrieve-feature-monitoring-configurations-from-ui","title":"Retrieve feature monitoring configurations from UI","text":"<p>An overview of all feature monitoring configurations is listed in the Feature Monitoring section in the Feature Group and Feature View overview page.</p>"},{"location":"user_guides/fs/feature_monitoring/feature_monitoring_advanced/#retrieve-feature-monitoring-configurations-from-python","title":"Retrieve feature monitoring configurations from Python","text":"<p>You can retrieve one or more feature monitoring configurations from the Feature Group and Feature View Python objects and filter them by name, configuration id or feature name.</p> Python <pre><code># retrieve all configurations\nfm_configs = trans_fg.get_feature_monitoring_configs()  # from a feature group\nfm_configs = trans_fv.get_feature_monitoring_configs()  # or a feature view\n\n# retrieve a configuration by name\nfm_config = trans_fg.get_feature_monitoring_configs(\n    name=\"trans_fg_all_features_monitoring\",\n)\n\n# or by config id\nfm_config = trans_fg.get_feature_monitoring_configs(\n    config_id=1,\n)\n\n# or for a specific feature\nfm_configs = trans_fg.get_feature_monitoring_configs(\n    feature_name=\"amount\",\n)\n</code></pre>"},{"location":"user_guides/fs/feature_monitoring/feature_monitoring_advanced/#disable-feature-monitoring","title":"Disable feature monitoring","text":"<p>You can enable or disable feature monitoring while keeping the historical statistics and comparison results.</p>"},{"location":"user_guides/fs/feature_monitoring/feature_monitoring_advanced/#disable-feature-monitoring-from-ui","title":"Disable feature monitoring from UI","text":"<p>In the overview page for feature monitoring, you can enable or disable a specific configuration by clicking on the Disable button.</p> <p></p>"},{"location":"user_guides/fs/feature_monitoring/feature_monitoring_advanced/#disable-feature-monitoring-from-python","title":"Disable feature monitoring from Python","text":"<p>You can easily enable or disable a specific feature monitoring configuration using the Python object.</p> Python <pre><code># disable a specific feature monitoring configuration\nfm_config.disable()\n\n# disable a specific feature monitoring configuration\nfm_config.enable()\n</code></pre>"},{"location":"user_guides/fs/feature_monitoring/feature_monitoring_advanced/#run-the-statistics-comparison-manually","title":"Run the statistics comparison manually","text":"<p>You can trigger the feature monitoring job on demand, to compute and compare statistics on the detection and reference windows according to the feature monitoring configuration.</p>"},{"location":"user_guides/fs/feature_monitoring/feature_monitoring_advanced/#run-the-statistics-comparison-manually-from-ui","title":"Run the statistics comparison manually from UI","text":"<p>In the overview page for feature monitoring, you can trigger the computation and comparison of statistics for a specific configuration by clicking on the Run once button.</p> <p>Note</p> <p>Triggering the feature monitoring job manually does not affect the underlying schedule.</p> <p></p>"},{"location":"user_guides/fs/feature_monitoring/feature_monitoring_advanced/#run-the-statistics-comparison-manually-from-python","title":"Run the statistics comparison manually from Python","text":"<p>To trigger the feature monitoring job once from the Python API, use the feature monitoring Python object as shown in the example below.</p> Python <pre><code># run the feature monitoring job once\nfm_config.run_job()\n</code></pre>"},{"location":"user_guides/fs/feature_monitoring/feature_monitoring_advanced/#get-feature-monitoring-results","title":"Get feature monitoring results","text":""},{"location":"user_guides/fs/feature_monitoring/feature_monitoring_advanced/#get-feature-monitoring-results-from-ui","title":"Get feature monitoring results from UI","text":"<p>The easiest way to explore the statistics and comparison results is using the Hopsworks interactive graph for Feature Monitoring. See more information on the Interactive graph guide.</p> <p></p>"},{"location":"user_guides/fs/feature_monitoring/feature_monitoring_advanced/#get-feature-monitoring-results-from-python","title":"Get feature monitoring results from Python","text":"<p>Alternatively, you can retrieve all the statistics and comparison results using the feature monitoring configuration Python object as shown in the example below.</p> Python <pre><code># retrieve all feature monitoring results from a specific config\nfm_results = fm_config.get_history()\n\n# or filter results by date\nfm_results = fm_config.get_history(\n    start_time=\"2023-01-01\",\n    end_time=\"2023-01-31\",\n)\n</code></pre>"},{"location":"user_guides/fs/feature_monitoring/feature_monitoring_advanced/#delete-a-feature-monitoring-configuration","title":"Delete a feature monitoring configuration","text":"<p>Deleting a feature monitoring configuration also deletes the historical statistics and comparison results attached to this configuration.</p>"},{"location":"user_guides/fs/feature_monitoring/feature_monitoring_advanced/#delete-a-feature-monitoring-configuration-from-python","title":"Delete a feature monitoring configuration from Python","text":"<p>You can delete feature monitoring configurations using the Python API only, as shown in the example below.</p> Python <pre><code>fm_config.delete()\n</code></pre>"},{"location":"user_guides/fs/feature_monitoring/interactive_graph/","title":"Interactive Graph","text":"<p>Hopsworks provides an interactive graph to help you explore the statistics computed on your feature data more efficiently and help you identify anomalies faster.</p>"},{"location":"user_guides/fs/feature_monitoring/interactive_graph/#select-a-feature-monitoring-configuration","title":"Select a feature monitoring configuration","text":"<p>First, you need to select a feature monitoring configuration to visualize. You can achieve that by clicking on the dropdown menu under Feature Selection on the controls menu.</p> <p></p>"},{"location":"user_guides/fs/feature_monitoring/interactive_graph/#select-a-statistics-metric-to-visualize","title":"Select a statistics metric to visualize","text":"<p>When you select a feature monitoring configuration, the mean values computed over time are visualized by default on the time series graph. You can choose a different statistics metric on the dropdown menu under Statistics Selection on the controls menu.</p> <p></p>"},{"location":"user_guides/fs/feature_monitoring/interactive_graph/#visualize-multiple-configurations-simultaneously","title":"Visualize multiple configurations simultaneously","text":"<p>Multiple feature monitoring configurations can be visualized at the same time on the graph. You can add a feature monitoring configuration by clicking on the + button on the controls menu.</p> <p>Note</p> <p>The same statistics metric will be visualized for every feature monitoring configuration selected.</p> <p></p>"},{"location":"user_guides/fs/feature_monitoring/interactive_graph/#show-reference-statistics","title":"Show reference statistics","text":"<p>In feature monitoring configurations with reference windows, you can also visualize the reference values by enabling the Reference values checkbox on the controls menu. Reference values can be either statistics computed over time or a specific value shown as an horizontal line.</p> <p>Note</p> <p>The same statistics metric will be visualized for both detection and reference values.</p> <p></p> <p>Info</p> <p>More details about reference windows can be found in Reference windows.</p>"},{"location":"user_guides/fs/feature_monitoring/interactive_graph/#show-threshold-bounds","title":"Show threshold bounds","text":"<p>In addition to reference windows, you can define thresholds to automate the identification of data points as anomalous values. A threshold can be absolute, or relative to the statistics values under comparison. You can visualize the threshold bounds by enabling the Threshold bounds checkbox on the controls menu.</p> <p></p> <p>Info</p> <p>More details about statistics comparison options can be found in Comparison criteria.</p>"},{"location":"user_guides/fs/feature_monitoring/interactive_graph/#highlight-shifted-data-points","title":"Highlight shifted data points","text":"<p>If a reference window and threshold are provided, data points that fall out of the threshold bounds are considered anomalous values. You can highlight these data points by enabling the Shift detected checkbox on the controls menu.</p> <p></p>"},{"location":"user_guides/fs/feature_monitoring/interactive_graph/#visualize-the-computed-differences-between-statistics","title":"Visualize the computed differences between statistics","text":"<p>Alternatively, you can change the time series to show the differences computed between detection and reference statistics rather than the statistics values themselves. You can achieve that by enabling the Difference checkbox on the controls menu.</p> <p></p>"},{"location":"user_guides/fs/feature_monitoring/interactive_graph/#list-of-configurations","title":"List of configurations","text":"<p>Following the time series graph, you can find an overview of all feature monitoring configurations defined for the corresponding Feature Group or Feature View. This overview includes a summary of the detection and reference windows, statistics comparison criteria and job schedule.</p> <p>In addition, you can trigger the statistics comparison manually, or disable the schedule of the feature monitoring job by clicking on Run once or Disable buttons, respectively.</p> <p>Note</p> <p>Triggering the statistics comparison manually does not affect the schedule of the feature monitoring.</p> <p></p>"},{"location":"user_guides/fs/feature_monitoring/scheduled_statistics/","title":"Scheduled Statistics","text":"<p>Hopsworks scheduled statistics allows you to monitor your feature data once they have been ingested into the Feature Store. You can define a detection window over your data for which Hopsworks will compute the statistics on a regular basis. Statistics can be computed on all or a subset of feature values, and on one or more features simultaneously.</p> <p>Hopsworks stores the computed statistics and enable you to visualise the temporal evolution of statistical metrics on your data.</p> <p></p> <p>Interactive graph</p> <p>See the Interactive graph guide to learn how to explore statistics more efficiently.</p>"},{"location":"user_guides/fs/feature_monitoring/scheduled_statistics/#use-cases","title":"Use cases","text":"<p>Scheduled statistics monitoring is a powerful tool that allows you to monitor your data over time and detect anomalies in your feature data at a glance by visualizing the evolution of the statistics properties of your data in a time series. It can be enabled in both Feature Groups and Feature Views, but for different purposes.</p> <p>For Feature Groups, scheduled statistics enables you to analyze how your Feature Group data evolve over time, and leverage your intuition to identify trends or detect noisy values in the inserted feature data.</p> <p>For Feature Views, scheduled statistics enables you to analyze the statistical properties of potentially new training dataset versions without having to actually create new training datasets and, thus, helping you decide when your training data show sufficient significant changes to create a new version.</p>"},{"location":"user_guides/fs/feature_monitoring/scheduled_statistics/#detection-windows","title":"Detection windows","text":"<p>Statistics are computed in a scheduled basis on a pre-defined detection window of feature data. Detection windows can be defined on the whole feature data or a subset of feature data depending on the <code>time_offset</code> and <code>window_length</code> parameters of the <code>with_detection_window</code> method.</p> <p></p> <p>In a previous section we described different types of windows available. Taking a Feature Group as an example, the figure above describes how these windows are applied to Feature Group data, resulting in three different applications:</p> <ul> <li>A expanding window covering the whole Feature Group data from its creation until the time when statistics are computing.   It can be seen as an snapshot of the latest version of your feature data.</li> <li>A rolling window covering a variable subset of feature data (e.g., feature data written last week).   It helps you analyze the properties of newly inserted feature data.</li> </ul> <p>See more details on how to define a detection window for your Feature Groups and Feature Views in the Feature Monitoring Guides for Feature Groups and Feature Views.</p> <p>Next steps</p> <p>You can also define a reference window to be used as a baseline to compare against the detection window. See more details in the Statistics comparison guide.</p>"},{"location":"user_guides/fs/feature_monitoring/statistics_comparison/","title":"Statistics Comparison","text":"<p>Hopsworks feature monitoring allows you to monitor your feature data once they have been ingested into the Feature Store. You can define detection and reference windows over your data for which Hopsworks will compute the statistics on a regular basis, compare them, and optionally trigger alerts when significant differences are detected. Statistics can be computed on all or a subset of feature values, and on one or more features simultaneously. Also, you can specify the criteria under which statistics will be compared and set thresholds used to classify feature values as anomalous.</p> <p>Hopsworks stores both detection and reference statistics and enable you to visualise the temporal evolution of statistical metrics.</p> <p></p> <p>Interactive graph</p> <p>See the Interactive graph guide to learn how to explore statistics and comparison results more efficiently.</p>"},{"location":"user_guides/fs/feature_monitoring/statistics_comparison/#use-cases","title":"Use cases","text":"<p>Feature monitoring is a powerful tool that allows you to monitor your data over time and quickly detect anomalies in your feature data by comparing statistics computed on different windows of your feature data, notifying you about anomalies, and/or visualizing the evolution of these statistics and comparison results in a time series. It can be enabled in both Feature Groups and Feature Views, but for different purposes.</p> <p>For Feature Groups, feature monitoring helps you rapidly identify unexpected trends or anomalous values in your Feature Group data, facilitating the debugging of possible root causes such as newly introduced changes in your feature pipelines.</p> <p>For Feature Views, feature monitoring helps you quickly detect when newly inserted Feature Group data differs statistically from your existing training datasets, and decide whether to retrain your ML models using a new training dataset version or analyze possible issues in your feature pipelines or inference pipelines.</p>"},{"location":"user_guides/fs/feature_monitoring/statistics_comparison/#reference-windows","title":"Reference windows","text":"<p>To compare statistics computed on a detection window against a baseline, you need to define a reference window of feature data. Reference windows can be defined in different ways depending on whether you are configuring feature monitoring on a Feature Group or Feature View.</p> <p></p> <p>In a previous section we described different types of windows available. Taking a Feature View as an example, the figure above describes how these windows are applied to Feature Group data read by a Feature View query and Training data, resulting in the following applications:</p> <ul> <li>A expanding window covering the whole Feature Group data from its creation until the time when statistics are computing.   It can be seen as an snapshot of the latest version of your feature data.   This reference window is useful when you want to compare the statistics of newly inserted feature data against all the Feature Group data.</li> <li>A rolling window covering a variable subset of feature data (e.g., feature data written last week).   It helps you compare the properties of feature data inserted at different cadences (e.g., feature data inserted last month and two months ago).</li> <li>A static window representing a snapshot of Feature Group data read using the Feature View query at a specific point in time (i.e., Training Dataset).   It helps you compare newly inserted feature data into your Feature Groups against a Training Dataset version.</li> <li>A specific value.   It helps you target the analysis of feature data to a specific feature and statistics metric.</li> </ul> <p>See more details on how to define a reference window for your Feature Groups and Training Datasets in the Feature Monitoring guides for Feature Groups and Feature Views.</p>"},{"location":"user_guides/fs/feature_monitoring/statistics_comparison/#comparison-criteria","title":"Comparison criteria","text":"<p>After defining the detection and reference windows, you can specify the criteria under which computed statistics will be compared.</p> Statistics metric <p>Although all descriptive statistics are computed on the pre-defined windows of feature data, the comparison of statistics is performed only on one of the statistics metrics. In other words, difference values are only computed for a single statistics metric. You can select the targeted statistics metric using the <code>metric</code> parameter when calling the <code>compare_on</code> method.</p> Threshold bounds <p>Threshold bounds are used to classify feature values as anomalous, by comparing them against the difference values computed on a specific statistics metric. You can defined a threshold value using the <code>threshold</code> parameter when calling the <code>compare_on</code> method.</p> Relative or absolute <p>Difference values represent the amount of change in the detection statistics with regards to the reference values. They can be computed in absolute or relative terms, as specified in the <code>relative</code> boolean parameter when calling the <code>compare_on</code> method.</p> <ul> <li>Absolute: detection value - reference value</li> <li>Relative: (detection value - reference value) / reference value</li> </ul> Strict or relaxed <p>Threshold bounds set the limits under which the amount of change between detection and reference values is normal. These bounds can be strict (<code>&lt;</code> or <code>&gt;</code>) or relaxed (<code>&lt;=</code> and <code>=&gt;</code>), as defined in the <code>strict</code> parameter when calling the <code>compare_on</code> method.</p> <p>Hopsworks stores the results of each statistics comparison and enables you to visualise them together with the detection and reference values in a time series graph.</p> <p></p> <p>Next steps</p> <p>You can setup alerts that will notify you whenever anomalies are detected on your feature data. See more details in the alerting section of the feature monitoring guide.</p>"},{"location":"user_guides/fs/feature_view/","title":"Feature View User Guides","text":"<p>This section serves to provide guides and examples for the common usage of abstractions and functionality of the Feature View through the Hopsworks UI and APIs.</p> <ul> <li>Create a Feature View</li> <li>Create a Training Data with different splits</li> <li>Batch Data</li> <li>Feature Vectors</li> <li>Feature Server</li> <li>Query</li> <li>Helper columns</li> <li>Model-Dependent Transformation Functions</li> <li>Spines</li> <li>Feature Monitoring</li> </ul>"},{"location":"user_guides/fs/feature_view/batch-data/","title":"Batch data (analytical ML systems)","text":""},{"location":"user_guides/fs/feature_view/batch-data/#creation","title":"Creation","text":"<p>It is very common that ML models are deployed in a \"batch\" setting where ML pipelines score incoming new data at a regular interval, for example, daily or weekly. Feature views support batch prediction by returning batch data as a DataFrame over a time range, by <code>start_time</code> and <code>end_time</code>. The resultant DataFrame (or batch-scoring DataFrame) can then be fed to models to make predictions.</p> PythonJava <pre><code># get batch data\ndf = feature_view.get_batch_data(\n    start_time = \"20220620\",\n    end_time = \"20220627\"\n) # return a dataframe\n</code></pre> <pre><code>Dataset&lt;Row&gt; ds = featureView.getBatchData(\"20220620\", \"20220627\")\n</code></pre>"},{"location":"user_guides/fs/feature_view/batch-data/#retrieve-batch-data-with-primary-keys-and-event-time","title":"Retrieve batch data with primary keys and event time","text":"<p>For certain use cases, e.g., time series models, the input data needs to be sorted according to the primary key(s) and event time combination. Or one might want to merge predictions back with the original input data for postmortem analysis. Primary key(s) and event time are not usually included in the feature view query as they are not features used for training. To retrieve the primary key(s) and/or event time when retrieving batch data for inference, you need to set the parameters <code>primary_key=True</code> and/or <code>event_time=True</code>.</p> Python <pre><code># get batch data\ndf = feature_view.get_batch_data(\nstart_time = \"20220620\",\nend_time = \"20220627\",\nprimary_key=True,\nevent_time=True\n) # return a dataframe with primary keys and event time\n</code></pre> <p>Note</p> <p>All primary and event time columns of all the feature groups included in the feature view will be returned. If they have the same names across feature groups and the join prefix was not provided then reading operation will fail with ambiguous column exception. Make sure to define the join prefix if primary key and event time columns have the same names across feature groups.</p> <p>For Python-clients, handling small or moderately-sized data, we recommend enabling the ArrowFlight Server with DuckDB, which will provide significant speedups over Spark/Hive for reading batch data. If the service is enabled, and you want to read this particular batch data with Hive instead, you can set the read_options to <code>{\"use_hive\": True}</code>.</p> <pre><code># get batch data with Hive\ndf = feature_view.get_batch_data(\n    start_time = \"20220620\",\n    end_time = \"20220627\",\n    read_options={\"use_hive\": True}\n)\n</code></pre>"},{"location":"user_guides/fs/feature_view/batch-data/#creation-with-transformation","title":"Creation with transformation","text":"<p>If you have specified transformation functions when creating a feature view, you will get back transformed batch data as well. If your transformation functions require statistics of training dataset, you must also provide the training data version. <code>init_batch_scoring</code> will then fetch the statistics and initialize the functions with required statistics. Then you can follow the above examples and create the batch data. Please note that transformed batch data can only be returned in the python client but not in the java client.</p> <pre><code>feature_view.init_batch_scoring(training_dataset_version=1)\n</code></pre> <p>It is important to note that in addition to the filters defined in feature view, extra filters will be applied if they are defined in the given training dataset version.</p>"},{"location":"user_guides/fs/feature_view/batch-data/#retrieving-untransformed-batch-data","title":"Retrieving untransformed batch data","text":"<p>By default, the <code>get_batch_data</code> function returns batch data with model-dependent transformations applied. However, you can retrieve untransformed batch data\u2014while still including on-demand features\u2014by setting the <code>transform</code> parameter to <code>False</code>.</p> Python <p>Returning untransformed batch data</p> <pre><code># Fetching untransformed batch data.\nuntransformed_batch_data = feature_view.get_batch_data(transform=False)\n</code></pre>"},{"location":"user_guides/fs/feature_view/batch-data/#passing-context-variables-to-transformation-functions","title":"Passing Context Variables to Transformation Functions","text":"<p>After defining a transformation function using a context variable, you can pass the necessary context variables through the <code>transformation_context</code> parameter when fetching batch data.</p> Python <p>Passing context variables while fetching batch data.</p> <pre><code># Passing context variable to IN-MEMORY Training Dataset.\nbatch_data = feature_view.get_batch_data(transformation_context={\"context_parameter\":10})\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature-server/","title":"Feature Store REST API Server","text":"<p>This API server allows users to retrieve single/batch feature vectors from a feature view.</p>"},{"location":"user_guides/fs/feature_view/feature-server/#how-to-use","title":"How to use","text":"<p>From Hopsworks 3.3, you can connect to the Feature Vector Server via any REST client which supports POST requests. Set the <code>X-API-KEY</code> to your Hopsworks API Key and send the request with a JSON body, single or batch. By default, the server listens on the <code>0.0.0.0:4406</code> and the api version is set to <code>0.1.0</code>. Please refer to <code>/srv/hops/mysql-cluster/rdrs_config.json</code> config file located on machines running the REST Server for additional configuration parameters.</p> <p>In Hopsworks 3.7, we introduced a python client for the Online Store REST API Server. The python client is available in the <code>hsfs</code> module and can be installed using <code>pip install hsfs</code>. This client can be used instead of the Online Store SQL client in the <code>FeatureView.get_feature_vector(s)</code> methods. Check the corresponding documentation for these methods.</p>"},{"location":"user_guides/fs/feature_view/feature-server/#single-feature-vector","title":"Single Feature Vector","text":""},{"location":"user_guides/fs/feature_view/feature-server/#single-feature-vector-request","title":"Single Feature Vector Request","text":"<p><code>POST /{api-version}/feature_store</code></p>"},{"location":"user_guides/fs/feature_view/feature-server/#single-feature-vector-request-body","title":"Single Feature Vector Request Body","text":"<pre><code>{\n        \"featureStoreName\": \"fsdb002\",\n        \"featureViewName\": \"sample_2\",\n        \"featureViewVersion\": 1,\n        \"passedFeatures\": {},\n        \"entries\": {\n                \"id1\": 36\n        },\n        \"metadataOptions\": {\n                \"featureName\": true,\n                \"featureType\": true\n        },\n        \"options\": {\n                \"validatePassedFeatures\": true,\n                \"includeDetailedStatus\": true\n        }\n}\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature-server/#single-feature-vector-request-parameters","title":"Single Feature Vector Request Parameters","text":"parameter type note featureStoreName string featureViewName string featureViewVersion number(int) entries objects Map of serving key of feature view as key and value of serving key as value. Serving key are a set of the primary key of feature groups which are included in the feature view query. If feature groups are joint with prefix, the primary key needs to be attached with prefix. passedFeatures objects Optional. Map of feature name as key and feature value as value. This overwrites feature values in the response. metadataOptions objects Optional. Map of metadataoption as key and boolean as value. Default metadata option is false. Metadata is returned on request. Metadata options available: 1. featureName 2. featureType options objects Optional. Map of option as key and boolean as value. Default option is false. Options available: 1. validatePassedFeatures 2. includeDetailedStatus"},{"location":"user_guides/fs/feature_view/feature-server/#single-feature-vector-response","title":"Single Feature Vector Response","text":"<pre><code>{\n        \"features\": [\n                36,\n                \"2022-01-24\",\n                \"int24\",\n                \"str14\"\n        ],\n        \"metadata\": [\n                {\n                        \"featureName\": \"id1\",\n                        \"featureType\": \"bigint\"\n                },\n                {\n                        \"featureName\": \"ts\",\n                        \"featureType\": \"date\"\n                },\n                {\n                        \"featureName\": \"data1\",\n                        \"featureType\": \"string\"\n                },\n                {\n                        \"featureName\": \"data2\",\n                        \"featureType\": \"string\"\n                }\n        ],\n        \"status\": \"COMPLETE\",\n        \"detailedStatus\": [\n                {\n                        \"featureGroupId\": 1,\n                        \"httpStatus\": 200,\n                },\n                {\n                        \"featureGroupId\": 2,\n                        \"httpStatus\": 200,\n                },\n        ]\n}\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature-server/#single-feature-vector-errors","title":"Single Feature Vector Errors","text":"Code reason response 200 400 Requested metadata does not exist 400 Error in pk or passed feature value 401 Access denied Access unshared feature store failed 500 Failed to read feature store metadata"},{"location":"user_guides/fs/feature_view/feature-server/#response-with-pkpass-feature-error","title":"Response with PK/pass feature error","text":"<pre><code>{\n        \"code\": 12,\n        \"message\": \"Wrong primay-key column. Column: ts\",\n        \"reason\": \"Incorrect primary key.\"\n}\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature-server/#response-with-metadata-error","title":"Response with metadata error","text":"<pre><code>{\n        \"code\": 2,\n        \"message\": \"\",\n        \"reason\": \"Feature store does not exist.\"\n}\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature-server/#pk-value-no-match","title":"PK value no match","text":"<pre><code>{\n        \"features\": [\n                9876543,\n                null,\n                null,\n                null\n        ],\n        \"metadata\": null,\n        \"status\": \"MISSING\"\n}\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature-server/#detailed-status","title":"Detailed Status","text":"<p>If <code>includeDetailedStatus</code> option is set to true, detailed status is returned in the response. Detailed status is a list of feature group id and http status code, corresponding to each read operations perform internally by RonDB. Meaning is as follows:</p> <ul> <li><code>featureGroupId</code>: Id of the feature group, used to identify which table the operation correspond from.</li> <li><code>httpStatus</code>: Http status code of the operation.</li> <li>200 means success</li> <li>400 means bad request, likely pk name is wrong or pk is incomplete.     In particular, if pk for this table/feature group is not provided in the request, this http status is returned.</li> <li>404 means no row corresponding to PK</li> <li>500 means internal error.</li> </ul> <p>Both <code>404</code> and <code>400</code> set the status to <code>MISSING</code> in the response. Examples below corresponds respectively to missing row and bad request.</p> <p>Missing Row: The PK name-value pair was correctly passed, but the corresponding row was not found in the feature group.</p> <pre><code>{\n        \"features\": [\n                36,\n                \"2022-01-24\",\n                null,\n                null\n        ],\n        \"status\": \"MISSING\",\n        \"detailedStatus\": [\n                {\n                        \"featureGroupId\": 1,\n                        \"httpStatus\": 200,\n                },\n                {\n                        \"featureGroupId\": 2,\n                        \"httpStatus\": 404,\n                },\n        ]\n}\n</code></pre> <p>Bad Request, e.g., when PK name-value pair for FG2 not provided or the corresponding column names was incorrect:</p> <pre><code>{\n        \"features\": [\n                36,\n                \"2022-01-24\",\n                null,\n                null\n        ],\n        \"status\": \"MISSING\",\n        \"detailedStatus\": [\n                {\n                        \"featureGroupId\": 1,\n                        \"httpStatus\": 200,\n                },\n                {\n                        \"featureGroupId\": 2,\n                        \"httpStatus\": 400,\n                },\n        ]\n}\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature-server/#batch-feature-vectors","title":"Batch Feature Vectors","text":""},{"location":"user_guides/fs/feature_view/feature-server/#batch-feature-vectors-request","title":"Batch Feature Vectors Request","text":"<p><code>POST /{api-version}/batch_feature_store</code></p>"},{"location":"user_guides/fs/feature_view/feature-server/#batch-feature-vectors-request-body","title":"Batch Feature Vectors Request Body","text":"<pre><code>{\n        \"featureStoreName\": \"fsdb002\",\n        \"featureViewName\": \"sample_2\",\n        \"featureViewVersion\": 1,\n        \"passedFeatures\": [],\n        \"entries\": [\n                {\n                        \"id1\": 16\n                },\n                {\n                        \"id1\": 36\n                },\n                {\n                        \"id1\": 71\n                },\n                {\n                        \"id1\": 48\n                },\n                {\n                        \"id1\": 29\n                }\n        ],\n        \"requestId\": null,\n        \"metadataOptions\": {\n                \"featureName\": true,\n                \"featureType\": true\n        },\n        \"options\": {\n                \"validatePassedFeatures\": true,\n                \"includeDetailedStatus\": true\n        }\n}\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature-server/#batch-feature-vectors-request-parameters","title":"Batch Feature Vectors Request Parameters","text":"parameter type note featureStoreName string featureViewName string featureViewVersion number(int) entries <code>array&lt;objects&gt;</code> Each items is a map of serving key as key and value of serving key as value. Serving key of feature view. passedFeatures <code>array&lt;objects&gt;</code> Optional. Each items is a map of feature name as key and feature value as value. This overwrites feature values in the response. If provided, its size and order has to be equal to the size of entries. Item can be null. metadataOptions objects Optional. Map of metadataoption as key and boolean as value. Default metadata option is false. Metadata is returned on request. Metadata options available: 1. featureName 2. featureType options objects Optional. Map of option as key and boolean as value. Default option is false. Options available: 1. validatePassedFeatures 2. includeDetailedStatus"},{"location":"user_guides/fs/feature_view/feature-server/#batch-feature-vectors-response","title":"Batch Feature Vectors Response","text":"<pre><code>{\n        \"features\": [\n                [\n                        16,\n                        \"2022-01-27\",\n                        \"int31\",\n                        \"str24\"\n                ],\n                [\n                        36,\n                        \"2022-01-24\",\n                        \"int24\",\n                        \"str14\"\n                ],\n                [\n                        71,\n                        null,\n                        null,\n                        null\n                ],\n                [\n                        48,\n                        \"2022-01-26\",\n                        \"int92\",\n                        \"str31\"\n                ],\n                [\n                        29,\n                        \"2022-01-03\",\n                        \"int53\",\n                        \"str91\"\n                ]\n        ],\n        \"metadata\": [\n                {\n                        \"featureName\": \"id1\",\n                        \"featureType\": \"bigint\"\n                },\n                {\n                        \"featureName\": \"ts\",\n                        \"featureType\": \"date\"\n                },\n                {\n                        \"featureName\": \"data1\",\n                        \"featureType\": \"string\"\n                },\n                {\n                        \"featureName\": \"data2\",\n                        \"featureType\": \"string\"\n                }\n        ],\n        \"status\": [\n                \"COMPLETE\",\n                \"COMPLETE\",\n                \"MISSING\",\n                \"COMPLETE\",\n                \"COMPLETE\"\n        ],\n        \"detailedStatus\": [\n                [{\n                        \"featureGroupId\": 1,\n                        \"httpStatus\": 200,\n                }],\n                [{\n                        \"featureGroupId\": 1,\n                        \"httpStatus\": 200,\n                }],\n                [{\n                        \"featureGroupId\": 1,\n                        \"httpStatus\": 404,\n                }],\n                [{\n                        \"featureGroupId\": 1,\n                        \"httpStatus\": 200,\n                }],\n                [{\n                        \"featureGroupId\": 1,\n                        \"httpStatus\": 200,\n                }]\n        ]\n}\n</code></pre> <p>note: Order of the returned features are the same as the order of entries in the request.</p>"},{"location":"user_guides/fs/feature_view/feature-server/#batch-feature-vectors-errors","title":"Batch Feature Vectors Errors","text":"Code reason response 200 400 Requested metadata does not exist 404 Missing row corresponding to pk value 401 Access denied Access unshared feature store failed 500 Failed to read feature store metadata"},{"location":"user_guides/fs/feature_view/feature-server/#response-with-partial-failure","title":"Response with partial failure","text":"<pre><code>{\n        \"features\": [\n                [\n                        81,\n                        \"id81\",\n                        \"2022-01-29 00:00:00\",\n                        6\n                ],\n                null,\n                [\n                        51,\n                        null,\n                        null,\n                        null,\n                ]\n        ],\n        \"metadata\": null,\n        \"status\": [\n                \"COMPLETE\",\n                \"ERROR\",\n                \"MISSING\"\n        ],\n        \"detailedStatus\": [\n                [{\n                        \"featureGroupId\": 1,\n                        \"httpStatus\": 200,\n                }],\n                [{\n                        \"featureGroupId\": 1,\n                        \"httpStatus\": 400,\n                }],\n                [{\n                        \"featureGroupId\": 1,\n                        \"httpStatus\": 404,\n                }]\n        ]\n}\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature-server/#access-control-to-feature-store","title":"Access control to feature store","text":"<p>Currently, the REST API server only supports Hopsworks API Keys for authentication and authorization. Add the API key to the HTTP requests using the <code>X-API-KEY</code> header.</p>"},{"location":"user_guides/fs/feature_view/feature-vectors/","title":"Feature Vectors","text":"<p>The Hopsworks Platform integrates real-time capabilities with its Online Store. Based on RonDB, your feature vectors are served at scale at in-memory latency (~1-10ms). Checkout the benchmarks results and the benchmark code. The same Feature View which was used to create training datasets can be used to retrieve feature vectors for real-time predictions. This allows you to serve the same features to your model in training and serving, ensuring consistency and reducing boilerplate. Whether you are either inside the Hopsworks platform, a model serving platform, or in an external environment, such as your application server.</p> <p>Below is a practical guide on how to use the Online Store Python and Java Client. The aim is to get you started quickly by providing code snippets which illustrate various use cases and functionalities of the clients. If you need to get more familiar with the concept of feature vectors, you can read this short introduction first.</p>"},{"location":"user_guides/fs/feature_view/feature-vectors/#retrieval","title":"Retrieval","text":"<p>You can get back feature vectors from either python or java client by providing the primary key value(s) for the feature view. Note that filters defined in feature view and training data will not be applied when feature vectors are returned. If you need to retrieve a complete value of feature vectors without missing values, the required <code>entry</code> are FeatureView.primary_keys. Alternative, you can provide the primary key of the feature groups as the key of the entry. It is also possible to provide a subset of the entry, which will be discussed below.</p> PythonJava <pre><code># get a single vector\nfeature_view.get_feature_vector(\n    entry = {\"pk1\": 1, \"pk2\": 2}\n)\n\n# get multiple vectors\nfeature_view.get_feature_vectors(\n    entry = [\n        {\"pk1\": 1, \"pk2\": 2},\n        {\"pk1\": 3, \"pk2\": 4},\n        {\"pk1\": 5, \"pk2\": 6}\n    ]\n)\n</code></pre> <pre><code>// get a single vector\nMap&lt;String, Object&gt; entry1 = Maps.newHashMap();\nentry1.put(\"pk1\", 1);\nentry1.put(\"pk2\", 2);\nfeatureView.getFeatureVector(entry1);\n\n// get multiple vectors\nMap&lt;String, Object&gt; entry2 = Maps.newHashMap();\nentry2.put(\"pk1\", 3);\nentry2.put(\"pk2\", 4);\nfeatureView.getFeatureVectors(Lists.newArrayList(entry1, entry2));\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature-vectors/#required-entry","title":"Required entry","text":"<p>Starting from python client v3.4, you can specify different values for the primary key of the same name which exists in multiple feature groups but are not joint by the same name. The table below summarises the value of <code>primary_keys</code> in different settings. Considering that you are joining 2 feature groups, namely, <code>left_fg</code> and <code>right_fg</code>, the feature groups have different primary keys, and features (<code>feature_*</code>) in each setting. Also, the 2 feature groups are joint on different join conditions and prefix as <code>left_fg.join(right_fg, &lt;join conditions&gt;, prefix=&lt;prefix&gt;)</code>.</p> <p>For java client, and python client before v3.4, the <code>primary_keys</code> are the set of primary key of all the feature groups in the query. Python client is backward compatible. It means that the <code>primary_keys</code> used before v3.4 can be applied to python client of later versions as well.</p> Setting primary key of <code>left_fg</code> primary key of <code>right_fg</code> join conditions prefix primary_keys note 1 id id <code>on=[\"id\"]</code> id Same feature name is used in the join. 2 id1 id2 <code>left_on=[\"id1\"], right_on=[\"id2\"]</code> id1 Different feature names are used in the join. 3 id1, id2 id1 <code>on=[\"id1\"]</code> id1, id2 <code>id2</code> is not part of the join conditions 4 id, user_id id <code>left_on=[\"user_id\"], right_on=[\"id\"]</code> id, user_id Value of <code>user_id</code> is used for retrieving features from <code>right_fg</code> 5 id1 id1, id2 <code>on=[\"id1\"]</code> id1, id2 <code>id2</code> is not part of the join conditions 6 id id, user_id <code>left_on=[\"id\"], right_on=[\"user_id\"]</code> \u201cright_\u201c id, \u201cright_id\u201c Value of \u201cright_id\u201c and \"id\" are used for retrieving features from <code>right_fg</code> 7 id id, user_id <code>left_on=[\"id\"], right_on=[\"user_id\"]</code> id, \u201cfgId_&lt;rightFgId&gt;_&lt;joinIndex&gt;_id\u201d Value of \u201cfgId_&lt;rightFgId&gt;_&lt;joinIndex&gt;_id\u201c and \"id\" are used for retrieving features from <code>right_fg</code>. See note below. 8 id id <code>left_on=[\"id\"], right_on=[\"feature_1\"]</code> \u201cright_\u201c id, \u201cright_id\u201c No primary key from <code>right_fg</code> is used in the join. Value of <code>right_id</code> is used for retrieving features from <code>right_fg</code> 9 id id <code>left_on=[\"id\"], right_on=[\"feature_1\"]</code> id1, \u201cfgId_&lt;rightFgId&gt;_&lt;joinIndex&gt;_id\u201d No primary key from <code>right_fg</code> is used in the join. Value of \"fgId_&lt;rightFgId&gt;_&lt;joinIndex&gt;_id\" is used for retrieving features from \"right_fg`. See note below. 10 id id <code>left_on=[\"feature_1\"], right_on=[\"id\"]</code> \u201cright_\u201c id, \u201cright_id\u201c No primary key from <code>left_fg</code> is used in the join. Value of <code>right_id</code> is used for retrieving features from <code>right_fg</code> 11 id id <code>left_on=[\"feature_1\"], right_on=[\"id\"]</code> id1, \u201cfgId_&lt;rightFgId&gt;_&lt;joinIndex&gt;_id\u201d No primary key from <code>left_fg</code> is used in the join. Value of \u201cfgId_&lt;rightFgId&gt;_&lt;joinIndex&gt;_id\u201d is used for retrieving features from <code>right_fg</code>. See note below. 12 user, year user, year <code>left_on=[\"user\"], right_on=[\"user\"]</code> \u201cright_\u201c user, year, \u201cright_year\u201c Value of \"user\" and \"right_year\" are used for retrieving features from <code>right_fg</code>. <code>right_fg</code> can be the same as feature group as <code>left_fg</code>. 13 user, year user, year <code>left_on=[\"user\"], right_on=[\"user\"]</code> user, year, \u201cfgId_&lt;rightFgId&gt;_&lt;joinIndex&gt;_year\u201d Value of \"user\" and \"fgId_&lt;rightFgId&gt;_&lt;joinIndex&gt;_year\" are used for retrieving features from <code>right_fg</code>. <code>right_fg</code> can be the same as feature group as <code>left_fg</code>. See note below. <p>Note:</p> <p>\"&lt;rightFgId&gt;\" can be found by <code>right_fg.id</code>. \"&lt;joinIndex&gt;\" is the order or the feature group in the join. In the example, it is 1 because <code>right_fg</code> is in the first join in the query <code>left_fg.join(right_fg, &lt;join conditions&gt;)</code>.</p>"},{"location":"user_guides/fs/feature_view/feature-vectors/#missing-primary-key-entries","title":"Missing Primary Key Entries","text":"<p>It can happen that some of the primary key entries are not available in some or all of the feature groups used by a feature view.</p> <p>Take the above example assuming the feature view consists of two joined feature groups, first one with primary key column <code>pk1</code>, the second feature group with primary key column <code>pk2</code>.</p> PythonJava <pre><code># get a single vector\nfeature_view.get_feature_vector(\n    entry = {\"pk1\": 1, \"pk2\": 2}\n)\n</code></pre> <pre><code>// get a single vector\nMap&lt;String, Object&gt; entry1 = Maps.newHashMap();\nentry1.put(\"pk1\", 1);\nentry1.put(\"pk2\", 2);\nfeatureView.getFeatureVector(entry1);\n</code></pre> <p>This call will raise an exception if <code>pk1 = 1</code> OR <code>pk2 = 2</code> can't be found but also if <code>pk1 = 1</code> AND <code>pk2 = 2</code> can't be found, meaning, it will not return a partial or empty feature vector.</p> <p>When retrieving a batch of vectors, the behaviour is slightly different.</p> PythonJava <pre><code># get multiple vectors\nfeature_view.get_feature_vectors(\n    entry = [\n        {\"pk1\": 1, \"pk2\": 2},\n        {\"pk1\": 3, \"pk2\": 4},\n        {\"pk1\": 5, \"pk2\": 6}\n    ]\n)\n</code></pre> <pre><code>// get multiple vectors\nMap&lt;String, Object&gt; entry2 = Maps.newHashMap();\nentry2.put(\"pk1\", 3);\nentry2.put(\"pk2\", 4);\nMap&lt;String, Object&gt; entry3 = Maps.newHashMap();\nentry3.put(\"pk1\", 5);\nentry3.put(\"pk2\", 6);\nfeatureView.getFeatureVectors(Lists.newArrayList(entry1, entry2, entry3));\n</code></pre> <p>This call will raise an exception if for example for the third entry <code>pk1 = 5</code> OR <code>pk2 = 6</code> can't be found, however, it will simply not return a vector for this entry if <code>pk1 = 5</code> AND <code>pk2 = 6</code> can't be found. That means, <code>get_feature_vectors</code> will never return partial feature vector, but will omit empty feature vectors.</p> <p>If you are aware of missing features, you can use the passed features or Partial feature retrieval functionality, described down below.</p>"},{"location":"user_guides/fs/feature_view/feature-vectors/#partial-feature-retrieval","title":"Partial feature retrieval","text":"<p>If your model can handle missing value or if you want to impute the missing value, you can get back feature vectors with partial values using python client starting from version 3.4 (Note that this does not apply to java client.). In the example below, let's say you join 2 feature groups by <code>fg1.join(fg2, left_on=[\"pk1\"], right_on=[\"pk2\"])</code>, required keys of the <code>entry</code> are <code>pk1</code> and <code>pk2</code>. If <code>pk2</code> is not provided, this returns feature values from the first feature group and null values from the second feature group when using the option <code>allow_missing=True</code>, otherwise it raises exception.</p> Python <pre><code># get a single vector with\nfeature_view.get_feature_vector(\n    entry = {\"pk1\": 1},\n    allow_missing=True\n)\n\n# get multiple vectors\nfeature_view.get_feature_vectors(\n    entry = [\n        {\"pk1\": 1},\n        {\"pk1\": 3},\n    ],\n    allow_missing=True\n)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature-vectors/#retrieval-with-transformation","title":"Retrieval with transformation","text":"<p>If you have specified transformation functions when creating a feature view, you receive transformed feature vectors. If your transformation functions require statistics of training dataset, you must also provide the training data version. <code>init_serving</code> will then fetch the statistics and initialize the functions with the required statistics. Then you can follow the above examples and retrieve the feature vectors. Please note that transformed feature vectors can only be returned in the python client but not in the java client.</p> Python <pre><code>feature_view.init_serving(training_dataset_version=1)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature-vectors/#passed-features","title":"Passed features","text":"<p>If some of the features values are only known at prediction time and cannot be computed and cached in the online feature store, you can provide those values as <code>passed_features</code> option. The <code>get_feature_vector</code> method is going to use the passed values to construct the final feature vector to submit to the model.</p> <p>You can use the <code>passed_features</code> parameter to overwrite individual features being retrieved from the online feature store. The feature view will apply the necessary transformations to the passed features as it does for the feature data retrieved from the online feature store.</p> <p>Please note that passed features is only available in the python client but not in the java client.</p> Python <pre><code># get a single vector\nfeature_view.get_feature_vector(\n    entry = {\"pk1\": 1, \"pk2\": 2},\n    passed_features = {\"feature_a\": \"value_a\"}\n)\n\n# get multiple vectors\nfeature_view.get_feature_vectors(\n    entry = [\n        {\"pk1\": 1, \"pk2\": 2},\n        {\"pk1\": 3, \"pk2\": 4},\n        {\"pk1\": 5, \"pk2\": 6}\n    ],\n    passed_features = [\n        {\"feature_a\": \"value_a1\"},\n        {\"feature_a\": \"value_a2\"},\n        {\"feature_a\": \"value_a3\"},\n    ]\n)\n</code></pre> <p>You can also use the parameter to provide values for all the features which are part of a specific feature group and used in the feature view. In this second case, you do not have to provide the primary key value for that feature group as no data needs to be retrieved from the online feature store.</p> Python <pre><code># get a single vector, replace values from an entire feature group\n# note how in this example you don't have to provide the value of\n# pk2, but you need to provide the features coming from that feature group\n# in this case feature_b and feature_c\n\nfeature_view.get_feature_vector(\n    entry = { \"pk1\": 1 },\n    passed_features = {\n        \"feature_a\": \"value_a\",\n        \"feature_b\": \"value_b\",\n        \"feature_c\": \"value_c\"\n    }\n)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature-vectors/#retrieving-untransformed-feature-vectors","title":"Retrieving untransformed feature vectors","text":"<p>By default, the <code>get_feature_vector</code> and <code>get_feature_vectors</code> functions return transformed feature vectors, which has model-dependent transformations applied and includes on-demand features.</p> <p>However, you can retrieve the untransformed feature vectors without applying model-dependent transformations while still including on-demand features by setting the <code>transform</code> parameter to False.</p> Python <p>Returning untransformed feature vectors</p> <pre><code># Fetching untransformed feature vector.\nuntransformed_feature_vector = feature_view.get_feature_vector(\n    entry={\"id\": 1}, transform=False\n)\n\n# Fetching untransformed feature vectors.\nuntransformed_feature_vectors = feature_view.get_feature_vectors(\n    entry=[{\"id\": 1}, {\"id\": 2}], transform=False\n)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature-vectors/#retrieving-feature-vector-without-on-demand-features","title":"Retrieving feature vector without on-demand features","text":"<p>The <code>get_feature_vector</code> and <code>get_feature_vectors</code> methods can also return untransformed feature vectors without on-demand features by disabling model-dependent transformations and excluding on-demand features. To achieve this, set the  parameters <code>transform</code> and <code>on_demand_features</code> to <code>False</code>.</p> Python <p>Returning untransformed feature vectors</p> <pre><code>untransformed_feature_vector = feature_view.get_feature_vector(\n    entry={\"id\": 1}, transform=False, on_demand_features=False\n)\nuntransformed_feature_vectors = feature_view.get_feature_vectors(\n    entry=[{\"id\": 1}, {\"id\": 2}], transform=False, on_demand_features=False\n)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature-vectors/#passing-context-variables-to-transformation-functions","title":"Passing Context Variables to Transformation Functions","text":"<p>After defining a transformation function using a context variable, you can pass the required context variables using the <code>transformation_context</code> parameter when fetching the feature vectors.</p> Python <p>Passing context variables while fetching batch data.</p> <pre><code># Passing context variable to IN-MEMORY Training Dataset.\nbatch_data = feature_view.get_feature_vectors(\n    entry = [{ \"pk1\": 1 }],\n    transformation_context={\"context_parameter\":10}\n)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature-vectors/#choose-the-right-client","title":"Choose the right Client","text":"<p>The Online Store can be accessed via the Python or Java client allowing you to use your language of choice to connect to the Online Store. Additionally, the Python client provides two different implementations to fetch data: SQL or REST. The SQL client is the default implementation. It requires a direct SQL connection to your RonDB cluster and uses python asyncio to offer high performance even when your Feature View rows involve querying multiple different tables. The REST client is an alternative implementation connecting to RonDB Feature Vector Server. Perfect if you want to avoid exposing ports of your database cluster directly to clients. This implementation is available as of Hopsworks 3.7.</p> <p>Initialise the client by calling the <code>init_serving</code> method on the Feature View object before starting to fetch feature vectors. This will initialise the chosen client, test the connection, and initialise the transformation functions registered with the Feature View. Note to use the REST client in the Hopsworks Cluster python environment you will need to provide an API key explicitly as JWT authentication is not yet supported. More configuration options can be found in the API documentation.</p> Python <pre><code># initialize the SQL client to fetch feature vectors from the Online Store\nmy_feature_view.init_serving()\n\n# or use the REST client\nmy_feature_view.init_serving(\n    init_rest_client=True,\n    config_rest_client={\n        \"api_key\": \"your_api_key\",\n    }\n)\n</code></pre> <p>Once the client is initialised, you can start fetching feature vector(s) via the Feature View methods: <code>get_feature_vector(s)</code>. You can initialise both clients for a given Feature View and switch between them by using the force flags in the get_feature_vector(s) methods.</p> Python <pre><code># initialize both clients and set the default to REST\nmy_feature_view.init_serving(\n    init_rest_client=True,\n    init_sql_client=True,\n    config_rest_client={\n        \"api_key\": \"your_api_key\",\n    },\n    default_client=\"rest\"\n)\n\n# this will fetch a feature vector via REST\ntry:\n    my_feature_view.get_feature_vector(\n        entry = {\"pk1\": 1, \"pk2\": 2},\n    )\nexcept TimeoutException:\n    # if the REST client times out, the SQL client will be used\n    my_feature_view.get_feature_vector(\n        entry = {\"pk1\": 1, \"pk2\": 2},\n        force_sql=True\n    )\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature-vectors/#feature-server","title":"Feature Server","text":"<p>In addition to Python/Java clients, from Hopsworks 3.3, a new feature server implemented in Go is introduced. With this new API, single or batch feature vectors can be retrieved in any programming language. Note that you can connect to the Feature Vector Server via any REST client. However registered transformation function will not be applied to values in the JSON response and values stored in Feature Groups which contain embeddings will be missing.</p>"},{"location":"user_guides/fs/feature_view/feature_logging/","title":"User Guide: Feature and Prediction Logging with a Feature View","text":"<p>Feature logging is essential for debugging, monitoring, and auditing the data your models use. This guide explains how to log features and predictions, and retrieve and manage these logs with feature view in Hopsworks.</p>"},{"location":"user_guides/fs/feature_view/feature_logging/#feature-and-prediction-logging","title":"Feature and Prediction Logging","text":"<p>After you have trained a model, you can log the features it uses and the predictions with the feature view used to create the training data for the model. You can log either transformed or/and untransformed features values.</p>"},{"location":"user_guides/fs/feature_view/feature_logging/#enabling-feature-logging","title":"Enabling Feature Logging","text":"<p>To enable logging, set <code>logging_enabled=True</code> when creating the feature view. Two feature groups will be created for storing transformed and untransformed features, but they are not visible in the UI. The logged features will be written to the offline feature store every hour by scheduled materialization jobs which are created automatically.</p> <pre><code>feature_view = fs.create_feature_view(\"name\", query, logging_enabled=True)\n</code></pre> <p>Alternatively, you can enable logging on an existing feature view by calling <code>feature_view.enable_logging()</code>. Also, calling <code>feature_view.log()</code> will implicitly enable logging if it has not already been enabled.</p>"},{"location":"user_guides/fs/feature_view/feature_logging/#logging-features-and-predictions","title":"Logging Features and Predictions","text":"<p>You can log features and predictions by calling <code>feature_view.log</code>. The logged features are written periodically to the offline store. If you need it to be available immediately, call <code>feature_view.materialize_log</code>.</p> <p>You can log either transformed or/and untransformed features. To get untransformed features, you can specify <code>transform=False</code> in <code>feature_view.get_batch_data</code> or <code>feature_view.get_feature_vector(s)</code>. Inference helper columns are returned along with the untransformed features. If you have On-Demand features as well, call <code>feature_view.compute_on_demand_features</code> to get the on demand features before calling <code>feature_view.log</code>.To get the transformed features, you can call <code>feature_view.transform</code> and pass the untransformed feature with the on-demand feature.</p> <p>Predictions can be optionally provided as one or more columns in the DataFrame containing the features or separately in the <code>predictions</code> argument. There must be the same number of prediction columns as there are labels in the feature view. It is required to provide predictions in the <code>predictions</code> argument if you provide the features as <code>list</code> instead of pandas <code>dataframe</code>. The training dataset version will also be logged if you have called either <code>feature_view.init_serving(...)</code> or <code>feature_view.init_batch_scoring(...)</code> or if the provided model has a training dataset version.</p> <p>The wallclock time of calling <code>feature_view.log</code> is automatically logged, enabling filtering by logging time when retrieving logs.</p>"},{"location":"user_guides/fs/feature_view/feature_logging/#example-1-log-features-only","title":"Example 1: Log Features Only","text":"<p>You have a DataFrame of features you want to log.</p> <pre><code>import pandas as pd\n\nfeatures = pd.DataFrame({\n    \"feature1\": [1.1, 2.2, 3.3],\n    \"feature2\": [4.4, 5.5, 6.6]\n})\n\n# Log features\nfeature_view.log(features)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature_logging/#example-2-log-features-predictions-and-model","title":"Example 2: Log Features, Predictions, and Model","text":"<p>You can also log predictions, and optionally the training dataset and the model used for prediction.</p> <pre><code>predictions = pd.DataFrame({\n    \"prediction\": [0, 1, 0]\n})\n\n# Log features and predictions\nfeature_view.log(features,\n                 predictions=predictions,\n                 training_dataset_version=1,\n                 model=Model(1, \"model\", version=1)\n)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature_logging/#example-3-log-both-transformed-and-untransformed-features","title":"Example 3: Log Both Transformed and Untransformed Features","text":""},{"location":"user_guides/fs/feature_view/feature_logging/#batch-features","title":"Batch Features","text":"<pre><code>untransformed_df = fv.get_batch_data(transformed=False)\n# then apply the transformations after:\ntransformed_df = fv.transform(untransformed_df)\n# Log untransformed features\nfeature_view.log(untransformed_df)\n# Log transformed features\nfeature_view.log(transformed_features=transformed_df)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature_logging/#real-time-features","title":"Real-time Features","text":"<pre><code>untransformed_vector = fv.get_feature_vector({\"id\": 1}, transform=False)\n# then apply the transformations after:\ntransformed_vector = fv.transform(untransformed_vector)\n# Log untransformed features\nfeature_view.log(untransformed_vector)\n# Log transformed features\nfeature_view.log(transformed_features=transformed_vector)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature_logging/#retrieving-the-log-timeline","title":"Retrieving the Log Timeline","text":"<p>To audit and review the feature/prediction logs, you might want to retrieve the timeline of log entries. This helps understand when data was logged and monitor the logs.</p>"},{"location":"user_guides/fs/feature_view/feature_logging/#retrieve-log-timeline","title":"Retrieve Log Timeline","text":"<p>A log timeline is the hudi commit timeline of the logging feature group.</p> <pre><code># Retrieve the latest 10 log entries\nlog_timeline = feature_view.get_log_timeline(limit=10)\nprint(log_timeline)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature_logging/#reading-log-entries","title":"Reading Log Entries","text":"<p>You may need to read specific log entries for analysis, such as entries within a particular time range or for a specific model version and training dataset version.</p>"},{"location":"user_guides/fs/feature_view/feature_logging/#read-all-log-entries","title":"Read all Log Entries","text":"<p>Read all log entries for comprehensive analysis. The output will return all values of the same primary keys instead of just the latest value.</p> <pre><code># Read all log entries\nlog_entries = feature_view.read_log()\nprint(log_entries)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature_logging/#read-log-entries-within-a-time-range","title":"Read Log Entries within a Time Range","text":"<p>Focus on logs within a specific time range. You can specify <code>start_time</code> and <code>end_time</code> for filtering, but the time columns will not be returned in the DataFrame. You can provide the <code>start/end_time</code> as <code>datetime</code>, <code>date</code>, <code>int</code>, or <code>str</code> type. Accepted date format are: <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code></p> <pre><code># Read log entries from January 2022\nlog_entries = feature_view.read_log(start_time=\"2022-01-01\", end_time=\"2022-01-31\")\nprint(log_entries)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature_logging/#read-log-entries-by-training-dataset-version","title":"Read Log Entries by Training Dataset Version","text":"<p>Analyze logs from a particular version of the training dataset. The training dataset version column will be returned in the DataFrame.</p> <pre><code># Read log entries of training dataset version 1\nlog_entries = feature_view.read_log(training_dataset_version=1)\nprint(log_entries)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature_logging/#read-log-entries-by-model-in-hopsworks","title":"Read Log Entries by Model in Hopsworks","text":"<p>Analyze logs from a particular name and version of the HSML model. The HSML model column will be returned in the DataFrame.</p> <pre><code># Read log entries of a specific HSML model\nlog_entries = feature_view.read_log(model=Model(1, \"model\", version=1))\nprint(log_entries)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature_logging/#read-log-entries-using-a-custom-filter","title":"Read Log Entries using a Custom Filter","text":"<p>Provide filters which work similarly to the filter method in the <code>Query</code> class. The filter should be part of the query in the feature view.</p> <pre><code># Read log entries where feature1 is greater than 0\nlog_entries = feature_view.read_log(filter=fg.feature1 &gt; 0)\nprint(log_entries)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature_logging/#pausing-and-resuming-logging","title":"Pausing and Resuming Logging","text":"<p>During maintenance or updates, you might need to pause logging to save computation resources.</p>"},{"location":"user_guides/fs/feature_view/feature_logging/#pause-logging","title":"Pause Logging","text":"<p>Pause the schedule of the materialization job for writing logs to the offline store.</p> <pre><code># Pause logging\nfeature_view.pause_logging()\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature_logging/#resume-logging","title":"Resume Logging","text":"<p>Resume the schedule of the materialization job for writing logs to the offline store.</p> <pre><code># Resume logging\nfeature_view.resume_logging()\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature_logging/#materializing-logs","title":"Materializing Logs","text":"<p>Besides the scheduled materialization job, you can materialize logs from Kafka to the offline store on demand. This does not pause the scheduled job. By default, it materializes both transformed and untransformed logs, optionally specifying whether to materialize transformed (transformed=True) or untransformed (transformed=False) logs.</p>"},{"location":"user_guides/fs/feature_view/feature_logging/#materialize-logs","title":"Materialize Logs","text":"<p>Materialize logs and optionally wait for the process to complete.</p> <pre><code># Materialize logs and wait for completion\nmaterialization_result = feature_view.materialize_log(wait=True)\n# Materialize only transformed log entries\nfeature_view.materialize_log(wait=True, transformed=True)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature_logging/#deleting-logs","title":"Deleting Logs","text":"<p>When log data is no longer needed, you might want to delete it to free up space and maintain data hygiene. This operation deletes the feature groups and recreates new ones. Scheduled materialization job and log timeline are reset as well.</p>"},{"location":"user_guides/fs/feature_view/feature_logging/#delete-logs","title":"Delete Logs","text":"<p>Remove all log entries (both transformed and untransformed logs), optionally specifying whether to delete transformed (transformed=True) or untransformed (transformed=False) logs.</p> <pre><code># Delete all log entries\nfeature_view.delete_log()\n\n# Delete only transformed log entries\nfeature_view.delete_log(transformed=True)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature_logging/#summary","title":"Summary","text":"<p>Feature logging is a crucial part of maintaining and monitoring your machine learning workflows. By following these examples, you can effectively log, retrieve, and delete logs, as well as manage the lifecycle of log materialization jobs, adding observability for your AI system and making it auditable.</p>"},{"location":"user_guides/fs/feature_view/feature_monitoring/","title":"Feature Monitoring for Feature Views","text":"<p>Feature Monitoring complements the Hopsworks data validation capabilities for Feature Group data by allowing you to monitor your data once they have been ingested into the Feature Store. Hopsworks feature monitoring is centered around two functionalities: scheduled statistics and statistics comparison.</p> <p>Before continuing with this guide, see the Feature monitoring guide to learn more about how feature monitoring works, and get familiar with the different use cases of feature monitoring for Feature Views described in the Use cases sections of the Scheduled statistics guide and Statistics comparison guide.</p> <p>Limited UI support</p> <p>Currently, feature monitoring can only be configured using the Hopsworks Python library. However, you can enable/disable a feature monitoring configuration or trigger the statistics comparison manually from the UI, as shown in the Advanced guide.</p>"},{"location":"user_guides/fs/feature_view/feature_monitoring/#code","title":"Code","text":"<p>In this section, we show you how to setup feature monitoring in a Feature View using the Hopsworks Python library. Alternatively, you can get started quickly by running our tutorial for feature monitoring.</p> <p>First, checkout the pre-requisite and Hopsworks setup to follow the guide below. Create a project, install the Hopsworks Python library in your environment and connect via the generated API key. The second step is to start a new configuration for feature monitoring.</p> <p>After that, you can optionally define a detection window of data to compute statistics on, or use the default detection window (i.e., whole feature data). If you want to setup scheduled statistics alone, you can jump to the last step to save your configuration. Otherwise, the third and fourth steps are also optional and show you how to setup the comparison of statistics on a schedule by defining a reference window and specifying the statistics metric to be compared.</p>"},{"location":"user_guides/fs/feature_view/feature_monitoring/#step-1-pre-requisite","title":"Step 1: Pre-requisite","text":"<p>In order to setup feature monitoring for a Feature View, you will need:</p> <ul> <li>A Hopsworks project.   If you don't have a project yet you can go to app.hopsworks.ai, signup with your email and create your first project.</li> <li>An API key, you can get one by going to \"Account Settings\" on app.hopsworks.ai.</li> <li>The Hopsworks Python library installed in your client.   See the installation guide.</li> <li>A Feature View</li> <li>A Training Dataset</li> </ul>"},{"location":"user_guides/fs/feature_view/feature_monitoring/#connect-your-notebook-to-hopsworks","title":"Connect your notebook to Hopsworks","text":"<p>Connect the client running your notebooks to Hopsworks.</p> Python <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nfs = project.get_feature_store()\n</code></pre> <p>You will be prompted to paste your API key to connect the notebook to your project. The <code>fs</code> Feature Store entity is now ready to be used to insert or read data from Hopsworks.</p>"},{"location":"user_guides/fs/feature_view/feature_monitoring/#get-or-create-a-feature-view","title":"Get or create a Feature View","text":"<p>Feature monitoring can be enabled on already created Feature Views. We suggest you read the Feature View concept page to understand what a feature view is. We also suggest you familiarize with the APIs to create a feature view and how to create them using the query abstraction.</p> <p>The following is a code example for getting or creating a Feature View with name <code>trans_fv</code> for transaction data.</p> Python <pre><code># Retrieve an existing feature view\ntrans_fv = fs.get_feature_view(\"trans_fv\", version=1)\n\n# Or, create a new feature view\nquery = trans_fg.select([\"fraud_label\", \"amount\", \"cc_num\"])\ntrans_fv = fs.create_feature_view(\n    name=\"trans_fv\",\n    version=1,\n    query=query,\n    labels=[\"fraud_label\"],\n)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature_monitoring/#get-or-create-a-training-dataset","title":"Get or create a Training Dataset","text":"<p>The following is a code example for creating a training dataset with two splits using a previously created feature view.</p> Python <pre><code># Create a training dataset with train and test splits\n_, _ = trans_fv.create_train_validation_test_split(\n    description = 'transactions fraud batch training dataset',\n    data_format = 'csv',\n    validation_size = 0.2,\n    test_size = 0.1,\n)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature_monitoring/#step-2-initialize-configuration","title":"Step 2: Initialize configuration","text":""},{"location":"user_guides/fs/feature_view/feature_monitoring/#scheduled-statistics","title":"Scheduled statistics","text":"<p>You can setup statistics monitoring on a single feature or multiple features of your Feature Group data, included in your Feature View query.</p> Python <pre><code># compute statistics for all the features\nfg_monitoring_config = trans_fv.create_statistics_monitoring(\n    name=\"trans_fv_all_features_monitoring\",\n    description=\"Compute statistics on all data of all features of the Feature Group data on a daily basis\",\n)\n\n# or for a single feature\nfg_monitoring_config = trans_fv.create_statistics_monitoring(\n    name=\"trans_fv_amount_monitoring\",\n    description=\"Compute statistics on all data of a single feature of the Feature Group data on a daily basis\",\n    feature_name=\"amount\",\n)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature_monitoring/#statistics-comparison","title":"Statistics comparison","text":"<p>When enabling the comparison of statistics in a feature monitoring configuration, you need to specify a single feature of your Feature Group data, included in your Feature View query. You can create multiple feature monitoring configurations on the same Feature View, but each of them should point to a single feature in the Feature View query.</p> Python <pre><code>fg_monitoring_config = trans_fv.create_feature_monitoring(\n    name=\"trans_fv_amount_monitoring\",\n    feature_name=\"amount\",\n    description=\"Compute descriptive statistics on the amount Feature of the Feature Group data on a daily basis\",\n)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature_monitoring/#custom-schedule-or-percentage-of-window-data","title":"Custom schedule or percentage of window data","text":"<p>By default, the computation of statistics is scheduled to run endlessly, every day at 12PM. You can modify the default schedule by adjusting the <code>cron_expression</code>, <code>start_date_time</code> and <code>end_date_time</code> parameters.</p> Python <pre><code>fg_monitoring_config = trans_fv.create_statistics_monitoring(\n    name=\"trans_fv_all_features_monitoring\",\n    description=\"Compute statistics on all data of all features of the Feature Group data on a weekly basis\",\n    cron_expression=\"0 0 12 ? *MON*\",  # weekly\n    row_percentage=0.8,                # use 80% of the data\n)\n\n# or\nfg_monitoring_config = trans_fv.create_feature_monitoring(\n    name=\"trans_fv_amount_monitoring\",\n    feature_name=\"amount\",\n    description=\"Compute descriptive statistics on the amount Feature of the Feature Group data on a weekly basis\",\n    cron_expression=\"0 0 12 ? * MON *\",  # weekly\n    row_percentage=0.8,                  # use 80% of the data\n)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature_monitoring/#step-3-optional-define-a-detection-window","title":"Step 3: (Optional) Define a detection window","text":"<p>By default, the detection window is an expanding window covering the whole Feature Group data. You can define a different detection window using the <code>window_length</code> and <code>time_offset</code> parameters provided in the <code>with_detection_window</code> method. Additionally, you can specify the percentage of feature data on which statistics will be computed using the <code>row_percentage</code> parameter.</p> Python <pre><code>fm_monitoring_config.with_detection_window(\n    window_length=\"1w\",  # data ingested during one week\n    time_offset=\"1w\",    # starting from last week\n    row_percentage=0.8,  # use 80% of the data\n)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature_monitoring/#step-4-optional-define-a-reference-window","title":"Step 4: (Optional) Define a reference window","text":"<p>When setting up feature monitoring for a Feature View, reference windows can be either a regular window, a specific value (i.e., window of size 1) or a training dataset.</p> Python <pre><code># compare statistics against a reference window\nfm_monitoring_config.with_reference_window(\n    window_length=\"1w\",  # data ingested during one week\n    time_offset=\"2w\",    # starting from two weeks ago\n    row_percentage=0.8,  # use 80% of the data\n)\n\n# or a specific value\nfm_monitoring_config.with_reference_value(\n    value=100,\n)\n\n# or a training dataset\nfm_monitoring_config.with_reference_training_dataset(\n    training_dataset_version=1, # use the training dataset used to train your production model\n)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature_monitoring/#step-5-optional-define-the-statistics-comparison-criteria","title":"Step 5: (Optional) Define the statistics comparison criteria","text":"<p>In order to compare detection and reference statistics, you need to provide the criteria for such comparison. First, you select the metric to consider in the comparison using the <code>metric</code> parameter. Then, you can define a relative or absolute threshold using the <code>threshold</code> and <code>relative</code> parameters.</p> Python <pre><code>fm_monitoring_config.compare_on(\n    metric=\"mean\",\n    threshold=0.2,  # a relative change over 20% is considered anomalous\n    relative=True,  # relative or absolute change\n    strict=False,   # strict or relaxed comparison\n)\n</code></pre> <p>Difference values and thresholds</p> <p>For more information about the computation of difference values and the comparison against threshold bounds see the Comparison criteria section in the Statistics comparison guide.</p>"},{"location":"user_guides/fs/feature_view/feature_monitoring/#step-6-save-configuration","title":"Step 6: Save configuration","text":"<p>Finally, you can save your feature monitoring configuration by calling the <code>save</code> method. Once the configuration is saved, the schedule for the statistics computation and comparison will be activated automatically.</p> Python <pre><code>fm_monitoring_config.save()\n</code></pre> <p>Next steps</p> <p>See the Advanced guide to learn how to delete, disable or trigger feature monitoring manually.</p>"},{"location":"user_guides/fs/feature_view/helper-columns/","title":"Helper columns","text":"<p>Hopsworks Feature Store provides a functionality to define two types of helper columns <code>inference_helper_columns</code> and <code>training_helper_columns</code> for feature views.</p> <p>Note</p> <p>Both inference and training helper column name(s) must be part of the <code>Query</code> object. If helper column name(s) belong to feature group that is part of a <code>Join</code> with <code>prefix</code> defined, then this prefix needs to prepended to the original column name when defining helper column list.</p>"},{"location":"user_guides/fs/feature_view/helper-columns/#inference-helper-columns","title":"Inference Helper columns","text":"<p><code>inference_helper_columns</code> are a list of feature names that are not used for training the model itself but are used for extra information during online or batch inference. For example, computing an on-demand feature such as <code>days_valid</code> (days left that a credit card is valid at the time of the transaction) in a credit card fraud detection system. The feature <code>days_valid</code> will be computed using the credit card expiry date that needs to be fetched from the feature store and compared to the transaction date that the transaction is performed on (<code>days_valid</code> = <code>expiry_date</code> - <code>current_date</code>). In this use case <code>expiry_date</code> is an inference helper column. It is not used for training but is necessary for computing the on-demand feature<code>days_valid</code> feature.</p> Python <p>Define inference columns for feature views.</p> <pre><code># define query object\nquery = label_fg.select(\"fraud_label\")\\\n                .join(trans_fg.select([\"amount\", \"days_valid\", \"expiry_date\", \"category\"]))\n\n# define feature view with helper columns\nfeature_view = fs.get_or_create_feature_view(\n    name='fv_with_helper_col',\n    version=1,\n    query=query,\n    labels=[\"fraud_label\"],\n    transformation_functions=transformation_functions,\n    inference_helper_columns=[\"expiry_date\"],\n)\n</code></pre>"},{"location":"user_guides/fs/feature_view/helper-columns/#inference-data-retrieval","title":"Inference Data Retrieval","text":"<p>When retrieving data for model inference, helper columns will be omitted. However, they can be optionally fetched with inference or training data.</p>"},{"location":"user_guides/fs/feature_view/helper-columns/#batch-inference","title":"Batch inference","text":"Python <p>Fetch inference helper column values and compute on-demand features during batch inference.</p> <pre><code># import feature functions\nfrom feature_functions import time_delta\n\n# Fetch feature view object\nfeature_view = fs.get_feature_view(\n    name='fv_with_helper_col',\n    version=1,\n)\n\n# Fetch feature data for batch inference with helper columns\ndf = feature_view.get_batch_data(start_time=start_time, end_time=end_time, inference_helpers=True, event_time=True)\n\n# compute location delta\ndf['days_valid'] = df.apply(lambda row: time_delta(row['expiry_date'], row['transaction_date']), axis=1)\n\n# prepare datatame for prediction\ndf = df[[f.name for f in feature_view.features if not (f.label or f.inference_helper_column or f.training_helper_column)]]\n</code></pre>"},{"location":"user_guides/fs/feature_view/helper-columns/#online-inference","title":"Online inference","text":"Python <p>Fetch inference helper column values and compute on-demand features during online inference.</p> <pre><code>from feature_functions import time_delta\n\n# Fetch feature view object\nfeature_view = fs.get_feature_view(\n    name='fv_with_helper_col',\n    version=1,\n)\n\n# Fetch feature data for batch inference without helper columns\ndf_without_inference_helpers = feature_view.get_batch_data()\n\n# Fetch feature data for batch inference with helper columns\ndf_with_inference_helpers = feature_view.get_batch_data(inference_helpers=True)\n\n# here cc_num, longitude and latitude are provided as parameters to the application\ncc_num = ...\ntransaction_date = ...\n\n# get previous transaction location of this credit card\ninference_helper = feature_view.get_inference_helper({\"cc_num\": cc_num}, return_type=\"dict\")\n\n# compute location delta\ndays_valid = time_delta(transaction_date, inference_helper['expiry_date'])\n\n# Now get assembled feature vector for prediction\nfeature_vector = feature_view.get_feature_vector({\"cc_num\": cc_num},\n                                                  passed_features={\"days_valid\": days_valid}\n                                                 )\n</code></pre>"},{"location":"user_guides/fs/feature_view/helper-columns/#training-helper-columns","title":"Training Helper columns","text":"<p><code>training_helper_columns</code> are a list of feature names that are not the part of the model schema itself but are used during training for the extra information. For example one might want to use feature like <code>category</code> of the purchased product to assign different weights.</p> Python <p>Define training helper columns for feature views.</p> <pre><code># define query object\nquery = label_fg.select(\"fraud_label\")\\\n                .join(trans_fg.select([\"amount\", \"days_valid\", \"expiry_date\", \"category\"]))\n\n# define feature view with helper columns\nfeature_view = fs.get_or_create_feature_view(\n    name='fv_with_helper_col',\n    version=1,\n    query=query,\n    labels=[\"fraud_label\"],\n    transformation_functions=transformation_functions,\n    training_helper_columns=[\"category\"]\n)\n</code></pre>"},{"location":"user_guides/fs/feature_view/helper-columns/#training-data-retrieval","title":"Training Data Retrieval","text":"<p>When retrieving training data helper columns will be omitted. However, they can be optionally fetched.</p> Python <p>Fetch training data with or without inference helper column values.</p> <pre><code># import feature functions\nfrom feature_functions import location_delta, time_delta\n\n# Fetch feature view object\nfeature_view = fs.get_feature_view(\n    name='fv_with_helper_col',\n    version=1,\n)\n\n# Create and training data with training helper columns\nTEST_SIZE = 0.2\nX_train, X_test, y_train, y_test = feature_view.train_test_split(\n    description='transactions fraud training dataset',\n    test_size=TEST_SIZE,\n     training_helper_columns=True\n)\n\n# Get existing training data with training helper columns\nX_train, X_test, y_train, y_test = feature_view.get_train_test_split(\n     training_dataset_version=1,\n     training_helper_columns=True\n)\n</code></pre> <p>Note</p> <p>To use helper columns with materialized training dataset it needs to be created with <code>training_helper_columns=True</code>.</p>"},{"location":"user_guides/fs/feature_view/model-dependent-transformations/","title":"Model Dependent Transformation Functions","text":"<p>Model-dependent transformations transform feature data for a specific model. Feature encoding is one example of such a transformations. Feature encoding is parameterized by statistics from the training dataset, and, as such, many model-dependent transformations require the training dataset statistics as a parameter. Hopsworks enhances the robustness of AI pipelines by preventing training-inference skew by ensuring that the same model-dependent transformations and statistical parameters are used during both training dataset generation and online inference.</p> <p>Additionally, Hopsworks offers built-in model-dependent transformation functions, such as <code>min_max_scaler</code>, <code>standard_scaler</code>, <code>robust_scaler</code>, <code>label_encoder</code>, and <code>one_hot_encoder</code>, which can be easily imported and declaratively applied to features in a feature view.</p>"},{"location":"user_guides/fs/feature_view/model-dependent-transformations/#model-dependent-transformation-function-creation","title":"Model Dependent Transformation Function Creation","text":"<p>Hopsworks allows you to create a model-dependent transformation function by attaching a\u00a0transformation function\u00a0to a feature view. The attached transformation function can be a simple function that takes one feature as input and outputs the transformed feature data. For example, in the case of min-max scaling a numerical feature, you will have a number as input parameter to the transformation function and a number as output. However, in the case of one-hot encoding a categorical variable, you will have a string as input and an array of 1s and 0s and output. You can also have transformation functions that take multiple features as input and produce one or more values as output. That is, transformation functions can be one-to-one, one-to-many, many-to-one, or many-to-many.</p> <p>Each model-dependent transformation function can map specific features to its arguments by explicitly providing their names as arguments to the transformation function. If no feature names are provided, the transformation function will default to using features from the feature view that match the name of the transformation function's argument.</p> <p>Hopsworks by default generates default names of transformed features output by a model-dependent transformation function. The generated names follows a naming convention structured as\u00a0<code>functionName_features_outputColumnNumber</code> if the transformation function outputs multiple columns and <code>functionName_features</code> if the transformation function outputs one column. For instance, for the function named\u00a0<code>add_one_multiple</code> that outputs multiple columns\u00a0in the example given below, produces output columns that would be labeled as\u00a0 <code>add_one_multiple_feature1_feature2_feature3_0</code>,\u00a0 <code>add_one_multiple_feature1_feature2_feature3_1</code>  and  \u00a0<code>add_one_multiple_feature1_feature2_feature3_2</code>. The function named\u00a0<code>add_two</code> that outputs a single column in the example given below, produces a single output column names as <code>add_two_feature</code>. Additionally, Hopsworks also allows users to specify custom names for transformed feature using the <code>alias</code> function.</p> Python <p>Creating model-dependent transformation functions</p> <pre><code># Defining a many to many transformation function.\n@udf(return_type=[int, int, int], drop=[\"feature1\", \"feature3\"])\ndef add_one_multiple(feature1, feature2, feature3):\n    return pd.DataFrame({\"add_one_feature1\":feature1 + 1, \"add_one_feature2\":feature2 + 1, \"add_one_feature3\":feature3 + 1})\n\n# Defining a one to one transformation function.\n@udf(return_type=int)\ndef add_two(feature):\n    return feature + 2\n\n# Creating model-dependent transformations by attaching transformation functions to feature views.\nfeature_view = fs.create_feature_view(\n    name='transactions_view',\n    query=query,\n    labels=[\"fraud_label\"],\n    transformation_functions=[\n        add_two,\n        add_one_multiple\n    ]\n)\n</code></pre>"},{"location":"user_guides/fs/feature_view/model-dependent-transformations/#specifying-input-features","title":"Specifying input features","text":"<p>The features to be used by a model-dependent transformation function can be specified by providing the feature names (from the feature view / feature group) as input to the transformation functions.</p> Python <p>Specifying input features to be passed to a model-dependent transformation function</p> <pre><code>feature_view = fs.create_feature_view(\n    name='transactions_view',\n    query=query,\n    labels=[\"fraud_label\"],\n    transformation_functions=[\n        add_two(\"feature_1\"),\n        add_two(\"feature_2\"),\n        add_one_multiple(\"feature_5\", \"feature_6\", \"feature_7\")\n    ]\n)\n</code></pre>"},{"location":"user_guides/fs/feature_view/model-dependent-transformations/#using-built-in-transformations","title":"Using built-in transformations","text":"<p>Built-in transformation functions are attached in the same way. The only difference is that they can either be retrieved from the Hopsworks or imported from the <code>hopsworks</code> module.</p> Python <p>Creating model-dependent transformation using built-in transformation functions retrieved from Hopsworks</p> <pre><code>min_max_scaler = fs.get_transformation_function(name=\"min_max_scaler\")\nstandard_scaler = fs.get_transformation_function(name=\"standard_scaler\")\nrobust_scaler = fs.get_transformation_function(name=\"robust_scaler\")\nlabel_encoder = fs.get_transformation_function(name=\"label_encoder\")\n\nfeature_view = fs.create_feature_view(\n    name='transactions_view',\n    query=query,\n    labels=[\"fraud_label\"],\n    transformation_functions = [\n        label_encoder(\"category\"),\n        robust_scaler(\"amount\"),\n        min_max_scaler(\"loc_delta\"),\n        standard_scaler(\"age_at_transaction\")\n    ]\n)\n</code></pre> <p>To attach built-in transformation functions from the <code>hopsworks</code> module they can be directly imported into the code from <code>hopsworks.builtin_transformations</code>.</p> Python <p>Creating model-dependent transformation using built-in transformation functions imported from hopsworks</p> <pre><code>from hopsworks.hsfs.builtin_transformations import min_max_scaler, label_encoder, robust_scaler, standard_scaler\n\nfeature_view = fs.create_feature_view(\n    name='transactions_view',\n    query=query,\n    labels=[\"fraud_label\"],\n    transformation_functions = [\n        label_encoder(\"category\"),\n        robust_scaler(\"amount\"),\n        min_max_scaler(\"loc_delta\"),\n        standard_scaler(\"age_at_transaction\")\n    ]\n)\n</code></pre>"},{"location":"user_guides/fs/feature_view/model-dependent-transformations/#using-model-dependent-transformations","title":"Using Model Dependent Transformations","text":"<p>Model-dependent transformations attached to a feature view are automatically applied when you create training data, read training data, read batch inference data, or get feature vectors. The generated data includes untransformed features, on-demand features, if any, and the transformed features. The transformed features are organized by their output column names in alphabetical order and are positioned after the untransformed and on-demand features.</p> <p>Model-dependent transformation functions can also be manually applied to a feature vector using the <code>transform</code> function.</p> Python <p>Manually applying model-dependent transformations during online inference</p> <pre><code># Initialize the feature view with the correct training dataset version used for model-dependent transformations\nfv.init_serving(training_dataset_version)\n\n# Get untransformed feature Vector\nfeature_vector = fv.get_feature_vector(entry={\"index\":10}, transform=False, return_type=\"pandas\")\n\n# Apply Model Dependent transformations\nencoded_feature_vector = fv.transform(feature_vector)\n</code></pre>"},{"location":"user_guides/fs/feature_view/model-dependent-transformations/#retrieving-untransformed-feature-vector-and-batch-inference-data","title":"Retrieving untransformed feature vector and batch inference data","text":"<p>The <code>get_feature_vector</code>, <code>get_feature_vectors</code>, and <code>get_batch_data</code> methods can return untransformed feature vectors and batch data without applying model-dependent transformations while still including on-demand features. To achieve this, set the <code>transform</code> parameter to False.</p> Python <p>Returning untransformed feature vectors and batch data.</p> <pre><code># Fetching untransformed feature vector.\nuntransformed_feature_vector = feature_view.get_feature_vector(\n    entry={\"id\": 1}, transform=False\n)\n\n# Fetching untransformed feature vectors.\nuntransformed_feature_vectors = feature_view.get_feature_vectors(\n    entry=[{\"id\": 1}, {\"id\": 2}], transform=False\n)\n\n# Fetching untransformed batch data.\nuntransformed_batch_data = feature_view.get_batch_data(\n    transform=False\n)\n</code></pre>"},{"location":"user_guides/fs/feature_view/overview/","title":"Feature View","text":"<p>A feature view is a set of features that come from one or more feature groups. It is a logical view over the feature groups, as the feature data is only stored in feature groups. Feature views are used to read feature data for both training and serving (online and batch). You can create training datasets, create batch data and get feature vectors.</p> <p>If you want to understand more about the concept of feature view, you can refer to the Feature View Overview.</p>"},{"location":"user_guides/fs/feature_view/overview/#feature-view-creation","title":"Feature View Creation","text":"<p>Query and transformation function are the building blocks of a feature view. You can define your set of features by building a <code>query</code>. You can also define which columns in your feature view are the <code>labels</code>, which is useful for supervised machine learning tasks. Furthermore, in python client, each feature can be attached to its own transformation function. This way, when a feature is read (for training or scoring), the transformation is executed on-demand - just before the feature data is returned. For example, when a client reads a numerical feature, the feature value could be normalized by a StandardScalar transformation function before it is returned to the client.</p> PythonJava <pre><code># create a simple feature view\nfeature_view = fs.create_feature_view(\n    name='transactions_view',\n    query=query\n)\n\n# create a feature view with transformation and label\nfeature_view = fs.create_feature_view(\n    name='transactions_view',\n    query=query,\n    labels=[\"fraud_label\"],\n    transformation_functions={\n        \"amount\": fs.get_transformation_function(name=\"standard_scaler\", version=1)\n    }\n)\n</code></pre> <pre><code>// create a simple feature view\nFeatureView featureView = featureStore.createFeatureView()\n                                        .name(\"transactions_view\")\n                                        .query(query)\n                                        .build();\n\n// create a feature view with label\nFeatureView featureView = featureStore.createFeatureView()\n                                        .name(\"transactions_view\")\n                                        .query(query)\n                                        .labels(Lists.newArrayList(\"fraud_label\"))\n                                        .build();\n</code></pre> <p>You can refer to query and transformation function for creating <code>query</code> and <code>transformation_function</code>. To see a full example of how to create a feature view, you can read this notebook.</p>"},{"location":"user_guides/fs/feature_view/overview/#retrieval","title":"Retrieval","text":"<p>Once you have created a feature view, you can retrieve it by its name and version.</p> PythonJava <pre><code>feature_view = fs.get_feature_view(name=\"transactions_view\", version=1)\n</code></pre> <pre><code>FeatureView featureView = featureStore.getFeatureView(\"transactions_view\", 1)\n</code></pre>"},{"location":"user_guides/fs/feature_view/overview/#deletion","title":"Deletion","text":"<p>If there are some feature view instances which you do not use anymore, you can delete a feature view. It is important to mention that all training datasets (include all materialised hopsfs training data) will be deleted along with the feature view.</p> PythonJava <pre><code>feature_view.delete()\n</code></pre> <pre><code>featureView.delete()\n</code></pre>"},{"location":"user_guides/fs/feature_view/overview/#tags","title":"Tags","text":"<p>Feature views also support tags. You can attach, get, and remove tags. You can learn more in Tags Guide.</p> PythonJava <pre><code># attach\nfeature_view.add_tag(name=\"tag_schema\", value={\"key\": \"value\"})\n\n# get\nfeature_view.get_tag(name=\"tag_schema\")\n\n#remove\nfeature_view.delete_tag(name=\"tag_schema\")\n</code></pre> <pre><code>// attach\nMap&lt;String, String&gt; tag = Maps.newHashMap();\ntag.put(\"key\", \"value\");\nfeatureView.addTag(\"tag_schema\", tag)\n\n// get\nfeatureView.getTag(\"tag_schema\")\n\n// remove\nfeatureView.deleteTag(\"tag_schema\")\n</code></pre>"},{"location":"user_guides/fs/feature_view/overview/#next","title":"Next","text":"<p>Once you have created a feature view, you can now create training data</p>"},{"location":"user_guides/fs/feature_view/query/","title":"Query vs DataFrame","text":"<p>HSFS provides a DataFrame API to ingest data into the Hopsworks Feature Store. You can also retrieve feature data in a DataFrame, that can either be used directly to train models or materialized to file(s) for later use to train models.</p> <p>The idea of the Feature Store is to have pre-computed features available for both training and serving models. The key functionality required to generate training datasets from reusable features are: feature selection, joins, filters, and point in time queries. The Query object enables you to select features from different feature groups to join together to be used in a feature view.</p> <p>The joining functionality is heavily inspired by the APIs used by Pandas to merge DataFrames. The APIs allow you to specify which features to select from which feature group, how to join them and which features to use in join conditions.</p> PythonScala <pre><code>fs = ...\ncredit_card_transactions_fg = fs.get_feature_group(name=\"credit_card_transactions\", version=1)\naccount_details_fg = fs.get_feature_group(name=\"account_details\", version=1)\nmerchant_details_fg = fs.get_feature_group(name=\"merchant_details\", version=1)\n\n# create a query\nselected_features = credit_card_transactions_fg.select_all() \\\n    .join(account_details_fg.select_all(), on=[\"cc_num\"]) \\\n    .join(merchant_details_fg.select_all())\n\n# save the query to feature view\nfeature_view = fs.create_feature_view(\n    version=1,\n    name='credit_card_fraud',\n    labels=[\"is_fraud\"],\n    query=selected_features\n)\n\n# retrieve the query back from the feature view\nfeature_view = fs.get_feature_view(\u201ccredit_card_fraud\u201d, version=1)\nquery = feature_view.query\n</code></pre> <pre><code>val fs = ...\nval creditCardTransactionsFg = fs.getFeatureGroup(\"credit_card_transactions\", 1)\nval accountDetailsFg = fs.getFeatureGroup(name=\"account_details\", version=1)\nval merchantDetailsFg = fs.getFeatureGroup(\"merchant_details\", 1)\n\n// create a query\nval selectedFeatures = (creditCardTransactionsFg.selectAll()\n    .join(accountDetailsFg.selectAll(), on=Seq(\"cc_num\"))\n    .join(merchantDetailsFg.selectAll()))\n\nval featureView = featureStore.createFeatureView()\n    .name(\"credit_card_fraud\")\n    .query(selectedFeatures)\n    .build();\n\n// retrieve the query back from the feature view\nval featureView = fs.getFeatureView(\u201ccredit_card_fraud\u201d, 1)\nval query = featureView.getQuery()\n</code></pre> <p>If a data scientist wants to modify a new feature that is not available in the feature store, she can write code to compute the new feature (using existing features or external data) and ingest the new feature values into the feature store. If the new feature is based solely on existing feature values in the Feature Store, we call it a derived feature. The same HSFS APIs can be used to compute derived features as well as features using external data sources.</p>"},{"location":"user_guides/fs/feature_view/query/#the-query-abstraction","title":"The Query Abstraction","text":"<p>Most operations performed on <code>FeatureGroup</code> metadata objects will return a <code>Query</code> with the applied operation.</p>"},{"location":"user_guides/fs/feature_view/query/#examples","title":"Examples","text":"<p>Selecting features from a feature group is a lazy operation, returning a query with the selected features only:</p> PythonScala <pre><code>credit_card_transactions_fg = fs.get_feature_group(\"credit_card_transactions\")\n\n# Returns Query\nselected_features = credit_card_transactions_fg.select([\"amount\", \"latitude\", \"longitude\"])\n</code></pre> <pre><code>val creditCardTransactionsFg = fs.getFeatureGroup(\"credit_card_transactions\")\n\n# Returns Query\nval selectedFeatures = creditCardTransactionsFg.select(Seq(\"amount\", \"latitude\", \"longitude\"))\n</code></pre>"},{"location":"user_guides/fs/feature_view/query/#join","title":"Join","text":"<p>Similarly, joins return query objects. The simplest join in one where we join all of the features together from two different feature groups without specifying a join key - <code>HSFS</code> will infer the join key as a common primary key between the two feature groups. By default, Hopsworks will use the maximal matching subset of the primary keys of the two feature groups as joining key(s), if not specified otherwise.</p> PythonScala <pre><code># Returns Query\nselected_features = credit_card_transactions_fg.join(account_details_fg)\n</code></pre> <pre><code>// Returns Query\nval selectedFeatures = creditCardTransactionsFg.join(accountDetailsFg)\n</code></pre> <p>More complex joins are possible by selecting subsets of features from the joined feature groups and by specifying a join key and type. Possible join types are \"inner\", \"left\" or \"right\". By default<code>join_type</code> is `\"left\". Furthermore, it is possible to specify different features for the join key of the left and right feature group. The join key lists should contain the names of the features to join on.</p> PythonScala <pre><code>selected_features = credit_card_transactions_fg.select_all() \\\n    .join(account_details_fg.select_all(), on=[\"cc_num\"]) \\\n    .join(merchant_details_fg.select_all(), left_on=[\"merchant_id\"], right_on=[\"id\"], join_type=\"inner\")\n</code></pre> <pre><code>val selectedFeatures = (creditCardTransactionsFg.selectAll()\n    .join(accountDetailsFg.selectAll(), Seq(\"cc_num\"))\n    .join(merchantDetailsFg.selectAll(), Seq(\"merchant_id\"), Seq(\"id\"), \"inner\"))\n</code></pre> <p>Warning</p> <p>If there is feature name clash in the query then prefixes will be automatically generated and applied. Generated prefix is feature group alias in the query (e.g., fg1, fg2). Prefix is applied to the right feature group of the query.</p>"},{"location":"user_guides/fs/feature_view/query/#data-modeling-in-hopsworks","title":"Data modeling in Hopsworks","text":"<p>Since v4.0 Hopsworks Feature selection API supports both Star and Snowflake Schema data models.</p>"},{"location":"user_guides/fs/feature_view/query/#star-schema-data-model","title":"Star schema data model","text":"<p>When choosing Star Schema data model all tables are children of the parent (the left most) feature group, which has all foreign keys for its child feature groups.</p> <p> Star schema data model </p> Python <pre><code>   selected_features = credit_card_transactions.select_all()\n    .join(aggregated_cc_transactions.select_all())\n    .join(account_details.select_all())\n    .join(merchant_details.select_all())\n    .join(cc_issuer_details.select_all())\n</code></pre> <p>In online inference, when you want to retrieve features in your online model, you have to provide all foreign key values, known as the serving_keys, from the parent feature group to retrieve your precomputed feature values using the feature view.</p> Python <pre><code>  feature vector = feature_view.get_feature_vector({\n    \u2018cc_num\u2019: \u201c1234 5555 3333 8888\u201d,\n    \u2018issuer_id\u2019: 20440455,\n    \u2018merchant_id\u2019: 44208484,\n    \u2018account_id\u2019: 84403331\n    })\n</code></pre>"},{"location":"user_guides/fs/feature_view/query/#snowflake-schema","title":"Snowflake schema","text":"<p>Hopsworks also provides the possibility to define a feature view that consists of a nested tree of children (to up to a depth of 20) from the root (left most) feature group. This is called  Snowflake Schema data model where you need to build nested tables (subtrees) using joins, and then join the subtrees to their parents iteratively until you reach the root node (the leftmost feature group in the feature selection):</p> <p> Snowflake schema data model </p> Python <pre><code>    nested_selection = aggregated_cc_transactions.select_all()\n    .join(account_details.select_all())\n    .join(cc_issuer_details.select_all())\n\n    selected_features = credit_card_transactions.select_all()\n            .join(nested_selection)\n    .join(merchant_details.select_all())\n</code></pre> <p>Now, you have the benefit that in online inference you only need to pass two serving key values (the foreign keys of the leftmost feature group) to retrieve the precomputed features:</p> Python <pre><code>    feature vector = feature_view.get_feature_vector({\n      \u2018cc_num\u2019: \u201c1234 5555 3333 8888\u201d,\n      \u2018merchant_id\u2019: 44208484,\n    })\n</code></pre>"},{"location":"user_guides/fs/feature_view/query/#filter","title":"Filter","text":"<p>In the same way as joins, applying filters to feature groups creates a query with the applied filter.</p> <p>Filters are constructed with Python Operators <code>==</code>, <code>&gt;=</code>, <code>&lt;=</code>, <code>!=</code>, <code>&gt;</code>, <code>&lt;</code> and additionally with the methods <code>isin</code> and <code>like</code>. Bitwise Operators <code>&amp;</code> and <code>|</code> are used to construct conjunctions. For the Scala part of the API, equivalent methods are available in the <code>Feature</code> and <code>Filter</code> classes.</p> PythonScala <pre><code>filtered_credit_card_transactions = credit_card_transactions_fg.filter(credit_card_transactions_fg.category == \"Grocery\")\n</code></pre> <pre><code>val filteredCreditCardTransactions = creditCardTransactionsFg.filter(creditCardTransactionsFg.getFeature(\"category\").eq(\"Grocery\"))\n</code></pre> <p>Filters are fully compatible with joins:</p> PythonScala <pre><code>selected_features = credit_card_transactions_fg.select_all() \\\n    .join(account_details_fg.select_all(), on=[\"cc_num\"]) \\\n    .join(merchant_details_fg.select_all(), left_on=[\"merchant_id\"], right_on=[\"id\"]) \\\n    .filter((credit_card_transactions_fg.category == \"Grocery\") | (credit_card_transactions_fg.category == \"Restaurant/Cafeteria\"))\n</code></pre> <pre><code>val selectedFeatures = (creditCardTransactionsFg.selectAll()\n    .join(accountDetailsFg.selectAll(), Seq(\"cc_num\"))\n    .join(merchantDetailsFg.selectAll(), Seq(\"merchant_id\"), Seq(\"id\"), \"left\")\n    .filter(creditCardTransactionsFg.getFeature(\"category\").eq(\"Grocery\").or(creditCardTransactionsFg.getFeature(\"category\").eq(\"Restaurant/Cafeteria\"))))\n</code></pre> <p>The filters can be applied at any point of the query:</p> PythonScala <pre><code>selected_features = credit_card_transactions_fg.select_all() \\\n    .join(accountDetails_fg.select_all().filter(accountDetails_fg.avg_temp &gt;= 22), on=[\"cc_num\"]) \\\n    .join(merchant_details_fg.select_all(), left_on=[\"merchant_id\"], right_on=[\"id\"]) \\\n    .filter(credit_card_transactions_fg.category == \"Grocery\")\n</code></pre> <pre><code>val selectedFeatures = (creditCardTransactionsFg.selectAll()\n    .join(accountDetailsFg.selectAll().filter(accountDetailsFg.getFeature(\"avg_temp\").ge(22)), Seq(\"cc_num\"))\n    .join(merchantDetailsFg.selectAll(), Seq(\"merchant_id\"), Seq(\"id\"), \"left\")\n    .filter(creditCardTransactionsFg.getFeature(\"category\").eq(\"Grocery\")))\n</code></pre>"},{"location":"user_guides/fs/feature_view/query/#joins-andor-filters-on-feature-view-query","title":"Joins and/or Filters on feature view query","text":"<p>The query retrieved from a feature view can be extended with new joins and/or new filters. However, this operation will not update the metadata and persist the updated query of the feature view itself. This query can then be used to create a new feature view.</p> PythonScala <pre><code>fs = ...\nmerchant_details_fg = fs.get_feature_group(name=\"merchant_details\", version=1)\ncredit_card_transactions_fg = fs.get_feature_group(name=\"credit_card_transactions\", version=1)\nfeature_view = fs.get_feature_view(\u201ccredit_card_fraud\u201d, version=1)\nfeature_view.query \\\n    .join(merchant_details_fg.select_all()) \\\n    .filter(credit_card_transactions_fg.category == \"Cash Withdrawal\")\n</code></pre> <pre><code>val fs = ...\nval merchantDetailsFg = fs.getFeatureGroup(\"merchant_details\", 1)\nval creditCardTransactionsFg = fs.getFeatureGroup(\"credit_card_transactions\", 1)\nval featureView = fs.getFeatureView(\u201ccredit_card_fraud\u201d, 1)\nfeatureView.getQuery()\n    .join(merchantDetailsFg.selectAll())\n    .filter(creditCardTransactionsFg.getFeature(\"category\").eq(\"Cash Withdrawal\"))\n</code></pre> <p>Warning</p> <p>Every join/filter operation applied to an existing feature view query instance will update its state and accumulate. To successfully apply new join/filter logic it is recommended to refresh the query instance by re-fetching the feature view:</p> PythonScala <pre><code>fs = ...\n\nmerchant_details_fg = fs.get_feature_group(name=\"merchant_details\", version=1)\naccount_details_fg = fs.get_feature_group(name=\"account_details\", version=1)\ncredit_card_transactions_fg = fs.get_feature_group(name=\"credit_card_transactions\", version=1)\n\n# fetch new feature view and its query instance\nfeature_view = fs.get_feature_view(\u201ccredit_card_fraud\u201d, version=1)\n\n# apply join/filter logic based on purchase type\nfeature_view.query.join(merchant_details_fg.select_all()) \\\n    .filter(credit_card_transactions_fg.category == \"Cash Withdrawal\")\n\n# to apply new logic independent of purchase type from above\n# re-fetch new feature view and its query instance\nfeature_view = fs.get_feature_view(\u201ccredit_card_fraud\u201d, version=1)\n\n# apply new join/filter logic based on account details\nfeature_view.query.join(merchant_details_fg.select_all()) \\\n    .filter(account_details_fg.gender == \"F\")\n</code></pre> <pre><code>fs = ...\nmerchantDetailsFg = fs.getFeatureGroup(\"merchant_details\", 1)\naccountDetailsFg = fs.getFeatureGroup(\"account_details\", 1)\ncreditCardTransactionsFg = fs.getFeatureGroup(\"credit_card_transactions\", 1)\n\n// fetch new feature view and its query instance\nval featureView = fs.getFeatureView(\u201ccredit_card_fraud\u201d, version=1)\n\n// apply join/filter logic based on purchase type\nfeatureView.getQuery.join(merchantDetailsFg.selectAll())\n    .filter(creditCardTransactionsFg.getFeature(\"category\").eq(\"Cash Withdrawal\"))\n\n// to apply new logic independent of purchase type from above\n// re-fetch new feature view and its query instance\nval featureView = fs.getFeatureView(\u201ccredit_card_fraud\u201d, 1)\n\n// apply new join/filter logic based on account details\nfeatureView.getQuery.join(merchantDetailsFg.selectAll())\n    .filter(accountDetailsFg.getFeature(\"gender\").eq(\"F\"))\n</code></pre>"},{"location":"user_guides/fs/feature_view/spine-query/","title":"Using Spines","text":"<p>In this section we will illustrate how to use a Spine Group instead of a regular Feature Group for performing point-in-time joins when reading batch data for inference or when creating training datasets.</p>"},{"location":"user_guides/fs/feature_view/spine-query/#prerequisites","title":"Prerequisites","text":"<ol> <li>Make sure you have read the concept section about spines in feature and inference pipelines.</li> <li>Make sure you have gone through the Spine Group creation guide.</li> <li>Make sure you understand the concept of feature views and how to create them using the query abstraction</li> </ol>"},{"location":"user_guides/fs/feature_view/spine-query/#feature-view-with-a-spine-group","title":"Feature View with a Spine Group","text":""},{"location":"user_guides/fs/feature_view/spine-query/#step-1-query-definition","title":"Step 1: Query Definition","text":"<p>The first step before creating a Feature View, is to construct the query by selecting the label and features which are needed:</p> <pre><code># Select features for training data.\nds_query = trans_fg.select([\"fraud_label\"])\\\n    .join(window_aggs_fg.select_except([\"cc_num\"]), on=\"cc_num\")\n\nds_query.show(5)\n</code></pre> <p>Similarly you can construct the query using a previously created spine equivalent.</p> <p>However, there are two thing to note:</p> <ol> <li>If you want to use the query for a feature view to be used for online serving, you can only select the \"label\" or target feature from the spine.</li> <li>Spine groups can only be used on the left side of the join. Think of the left side of the join as the base set of entities that should be included in you batch of data or training dataset, which we enrich with the relevant and point-in-time correct feature values.</li> </ol> <pre><code>trans_spine = fs.get_or_create_spine_group(\n    name=\"spine_transactions\",\n    version=1,\n    description=\"Transaction data\",\n    primary_key=['cc_num'],\n    event_time='datetime',\n    dataframe=trans_df\n)\n\n# Select features for training data.\nds_query_spine = trans_spine.select([\"fraud_label\"])\\\n    .join(window_aggs_fg.select_except([\"cc_num\"]), on=\"cc_num\")\n</code></pre> <p>Calling the <code>show()</code> or <code>read()</code> method of this query object will use the spine dataframe included in the Spine Group object to perform the join.</p> <pre><code>ds_query_spine.show(10)\n</code></pre>"},{"location":"user_guides/fs/feature_view/spine-query/#step-2-feature-view-creation","title":"Step 2: Feature View Creation","text":"<p>With the above defined query, we can continue to create the Feature View in the same way we would do it also without a spine:</p> <pre><code>feature_view_spine = fs.get_or_create_feature_view(\n    name='transactions_view_spine',\n    query=ds_query_spine,\n    version=1,\n    labels=[\"fraud_label\"],\n)\n</code></pre>"},{"location":"user_guides/fs/feature_view/spine-query/#step-3-training-dataset-creation","title":"Step 3: Training Dataset Creation","text":"<p>With the regular feature view, the labels are fetched from the feature store, but with the feature view created with a spine, you need to provide the dataframe. Here you have the chance to pass a different set of entities to generate the training dataset.</p> <pre><code>X_train, X_test, y_train, y_test = feature_view_spine.train_test_split(0.2, spine=new_entities_df)\n\nX_train.show()\n</code></pre>"},{"location":"user_guides/fs/feature_view/spine-query/#step-4-retrieving-new-batches-inference-data","title":"Step 4: Retrieving New Batches Inference Data","text":"<p>You can now use the offline and online API of the feature stores to read features for inference. Similarly to training dataset creation, every time you read up a new batch of data, you can pass a different spine dataframe.</p> <pre><code>feature_view_spine.get_batch_data(spine=scoring_spine_df).show()\n</code></pre>"},{"location":"user_guides/fs/feature_view/spine-query/#step-5-online-feature-lookup","title":"Step 5: Online Feature Lookup","text":"<p>For the online lookup, the label is not required, therefore it was important to only select label from the left spine group, so that we don't need to provide a spine for online serving:</p> <pre><code># Note: no spine needs to be passed\nfeature_view.get_feature_vector({\"cc_num\": 4473593503484549})\n</code></pre>"},{"location":"user_guides/fs/feature_view/spine-query/#replacing-a-regular-feature-group-with-a-spine-at-serving-time","title":"Replacing a Regular Feature Group with a Spine at Serving Time","text":"<p>In the case where you create a feature view with a regular feature group, but you would like to retrieve batch inference data using IDs (primary key values), you can use a spine to replace the left feature group. To do this, you can pass the Spine Group instead of a dataframe.</p> <pre><code># Note: here feature_view was created with regular feature groups only\n# and trans_spine is of type SpineGroup instead of a dataframe\nfeature_view.get_batch_data(spine=trans_spine).show()\n</code></pre>"},{"location":"user_guides/fs/feature_view/training-data/","title":"Training data","text":"<p>Training data can be created from the feature view and used by different ML libraries for training different models.</p> <p>You can read training data concepts for more details. To see a full example of how to create training data, you can read this notebook.</p> <p>For Python-clients, handling small or moderately-sized data, we recommend enabling the ArrowFlight Server with DuckDB service, which will provide significant speedups over Spark/Hive for reading and creating in-memory training datasets.</p>"},{"location":"user_guides/fs/feature_view/training-data/#creation","title":"Creation","text":"<p>It can be created as in-memory DataFrames or materialised as <code>tfrecords</code>, <code>parquet</code>, <code>csv</code>, or <code>tsv</code> files to HopsFS or in all other locations, for example, S3, GCS. If you materialise a training dataset, a <code>PySparkJob</code> will be launched. By default, <code>create_training_data</code> waits for the job to finish. However, you can run the job asynchronously by passing <code>write_options={\"wait_for_job\": False}</code>. You can monitor the job status in the jobs overview UI.</p> <pre><code># create a training dataset as dataframe\nfeature_df, label_df = feature_view.training_data(\n    description = 'transactions fraud batch training dataset',\n)\n\n# materialise a training dataset\nversion, job = feature_view.create_training_data(\n    description = 'transactions fraud batch training dataset',\n    data_format = 'csv',\n    write_options = {\"wait_for_job\": False}\n) # By default, it is materialised to HopsFS\nprint(job.id) # get the job's id and view the job status in the UI\n</code></pre>"},{"location":"user_guides/fs/feature_view/training-data/#extra-filters","title":"Extra filters","text":"<p>Sometimes data scientists need to train different models using subsets of a dataset. For example, there can be different models for different countries, seasons, and different groups. One way is to create different feature views for training different models. Another way is to add extra filters on top of the feature view when creating training data.</p> <p>In the transaction fraud example, there are different transaction categories, for example: \"Health/Beauty\", \"Restaurant/Cafeteria\", \"Holliday/Travel\" etc. Examples below show how to create training data for different transaction categories.</p> <pre><code># Create a training dataset for Health/Beauty\ndf_health = feature_view.training_data(\n    description = 'transactions fraud batch training dataset for Health/Beauty',\n    extra_filter = trans_fg.category == \"Health/Beauty\"\n)\n# Create a training dataset for Restaurant/Cafeteria and Holliday/Travel\ndf_restaurant_travel = feature_view.training_data(\n    description = 'transactions fraud batch training dataset for Restaurant/Cafeteria and Holliday/Travel',\n    extra_filter = trans_fg.category == \"Restaurant/Cafeteria\" and trans_fg.category == \"Holliday/Travel\"\n)\n</code></pre>"},{"location":"user_guides/fs/feature_view/training-data/#trainvalidationtest-splits","title":"Train/Validation/Test Splits","text":"<p>In most cases, ML practitioners want to slice a dataset into multiple splits, most commonly train-test splits or train-validation-test splits, so that they can train and test their models. Feature view provides a sklearn-like API for this purpose, so it is very easy to create a training dataset with different splits.</p> <p>Create a training dataset (as in-memory DataFrames) or materialise a training dataset with train and test splits.</p> <pre><code># create a training dataset\nX_train, X_test, y_train, y_test = feature_view.train_test_split(test_size=0.2)\n\n# materialise a training dataset\nversion, job = feature_view.create_train_test_split(\n    test_size = 0.2,\n    description = 'transactions fraud batch training dataset',\n    data_format = 'csv'\n)\n</code></pre> <p>Create a training dataset (as in-memory DataFrames) or materialise a training dataset with train, validation, and test splits.</p> <pre><code># create a training dataset as DataFrame\nX_train, X_val, X_test, y_train, y_val, y_test = feature_view.train_validation_test_split(validation_size=0.3, test_size=0.2)\n\n# materialise a training dataset\nversion, job = feature_view.create_train_validation_test_split(\n    validation_size = 0.3,\n    test_size = 0.2,\n    description = 'transactions fraud batch training dataset',\n    data_format = 'csv'\n)\n</code></pre> <p>If the ArrowFlight Server with DuckDB service is enabled, and you want to create a particular in-memory training dataset with Hive instead, you can set <code>read_options={\"use_hive\": True}</code>.</p> <pre><code># create a training dataset as DataFrame with Hive\nX_train, X_test, y_train, y_test = feature_view.train_test_split(test_size=0.2, read_options={\"use_hive\": True})\n</code></pre>"},{"location":"user_guides/fs/feature_view/training-data/#read-training-data","title":"Read Training Data","text":"<p>Once you have created a training dataset, all its metadata are saved in Hopsworks. This enables you to reproduce exactly the same dataset at a later point in time. This holds for training data as both DataFrames or files. That is, you can delete the training data files (for example, to reduce storage costs), but still reproduce the training data files later on if you need to.</p> <pre><code># get a training dataset\nfeature_df, label_df = feature_view.get_training_data(training_dataset_version=1)\n\n# get a training dataset with train and test splits\nX_train, X_test, y_train, y_test = feature_view.get_train_test_split(training_dataset_version=1)\n\n# get a training dataset with train, validation and test splits\nX_train, X_val, X_test, y_train, y_val, y_test = feature_view.get_train_validation_test_split(training_dataset_version=1)\n</code></pre>"},{"location":"user_guides/fs/feature_view/training-data/#passing-context-variables-to-transformation-functions","title":"Passing Context Variables to Transformation Functions","text":"<p>Once you have defined a transformation function using a context variable, you can pass the required context variables using the <code>transformation_context</code> parameter when generating IN-MEMORY training data or materializing a training dataset.</p> <p>Note</p> <p>Passing context variables for materializing a training dataset is only supported in the PySpark Kernel.</p> Python <p>Passing context variables while creating training data.</p> <pre><code># Passing context variable to IN-MEMORY Training Dataset.\nX_train, X_test, y_train, y_test = feature_view.get_train_test_split(training_dataset_version=1,\n                                                                 primary_key=True,\n                                                                 event_time=True,\n                                                                 transformation_context={\"context_parameter\":10})\n\n# Passing context variable to Materialized Training Dataset.\nversion, job = feature_view.get_train_test_split(training_dataset_version=1,\n                                                                 primary_key=True,\n                                                                 event_time=True,\n                                                                 transformation_context={\"context_parameter\":10})\n</code></pre>"},{"location":"user_guides/fs/feature_view/training-data/#read-training-data-with-primary-keys-and-event-time","title":"Read training data with primary key(s) and event time","text":"<p>For certain use cases, e.g., time series models, the input data needs to be sorted according to the primary key(s) and event time combination. Primary key(s) and event time are not usually included in the feature view query as they are not features used for training. To retrieve the primary key(s) and/or event time when retrieving training data, you need to set the parameters <code>primary_key=True</code> and/or <code>event_time=True</code>.</p> <pre><code># get a training dataset\nX_train, X_test, y_train, y_test = feature_view.get_train_test_split(training_dataset_version=1,\n                                                                     primary_key=True,\n                                                                     event_time=True)\n</code></pre> <p>Note</p> <p>All primary and event time columns of all the feature groups included in the feature view will be returned. If they have the same names across feature groups and the join prefix was not provided then reading operation will fail with ambiguous column exception. Make sure to define the join prefix if primary key and event time columns have the same names across feature groups.</p> <p>To use primary key(s) and event time column with materialized training datasets it needs to be created with <code>primary_key=True</code> and/or <code>with_event_time=True</code>.</p>"},{"location":"user_guides/fs/feature_view/training-data/#deletion","title":"Deletion","text":"<p>To clean up unused training data, you can delete all training data or for a particular version. Note that all metadata of training data and materialised files stored in HopsFS will be deleted and cannot be recreated anymore.</p> <pre><code># delete a training data version\nfeature_view.delete_training_dataset(training_dataset_version=1)\n\n# delete all training datasets\nfeature_view.delete_all_training_datasets()\n</code></pre> <p>It is also possible to keep the metadata and delete only the materialised files. Then you can recreate the deleted files by just specifying a version, and you get back the exact same dataset again. This is useful when you are running out of storage.</p> <pre><code># delete files of a training data version\nfeature_view.purge_training_data(training_dataset_version=1)\n\n# delete files of all training datasets\nfeature_view.purge_all_training_data()\n</code></pre> <p>To recreate a training dataset:</p> <pre><code>feature_view.recreate_training_dataset(training_dataset_version =1)\n</code></pre>"},{"location":"user_guides/fs/feature_view/training-data/#tags","title":"Tags","text":"<p>Similar to feature view, You can attach, get, and remove tags. You can learn more in Tags Guide.</p> <pre><code># attach\nfeature_view.add_training_dataset_tag(\n    training_dataset_version=1,\n    name=\"tag_schema\",\n    value={\"key\": \"value\"}\n)\n\n# get\nfeature_view.get_training_dataset_tag(training_dataset_version=1, name=\"tag_schema\")\n\n#remove\nfeature_view.delete_training_dataset_tag(training_dataset_version=1, name=\"tag_schema\")\n</code></pre>"},{"location":"user_guides/fs/feature_view/training-data/#next","title":"Next","text":"<p>Once you have created a training dataset and trained your model, you can deploy your model in a \"batch\"  or \"online\" setting. Next, you can learn how to create batch data and get feature vectors.</p>"},{"location":"user_guides/fs/provenance/provenance/","title":"Provenance","text":""},{"location":"user_guides/fs/provenance/provenance/#introduction","title":"Introduction","text":"<p>Hopsworks allows users to track provenance (lineage) between:</p> <ul> <li>data sources</li> <li>feature groups</li> <li>feature views</li> <li>training datasets</li> <li>models</li> </ul> <p>In the provenance pages we will call a provenance artifact or shortly artifact, any of the five entities above.</p> <p>With the following provenance graph:</p> <pre><code>data source -&gt; feature group -&gt; feature group -&gt; feature view -&gt; training dataset -&gt; model\n</code></pre> <p>we will call the parent, the artifact to the left, and the child, the artifact to the right. So a feature view has a number of feature groups as parents and can have a number of training datasets as children.</p> <p>Tracking provenance allows users to determine where and if an artifact is being used. You can track, for example, if feature groups are being used to create additional (derived) feature groups or feature views, or if their data is eventually used to train models.</p> <p>You can interact with the provenance graph using the UI or the APIs.</p>"},{"location":"user_guides/fs/provenance/provenance/#step-1-data-source-lineage","title":"Step 1: Data Source lineage","text":"<p>The relationship between data sources and feature groups is captured automatically when you create an external feature group. You can inspect the relationship between data sources and feature groups using the APIs.</p> Python <pre><code># Retrieve the data source\nds = fs.get_data_source(\"snowflake_sc\")\nds.query = \"SELECT * FROM USER_PROFILES\"\n\n# Create the user profiles feature group\nuser_profiles_fg = fs.create_external_feature_group(\n    name=\"user_profiles\",\n    version=1,\n    data_source=ds\n)\nuser_profiles_fg.save()\n</code></pre>"},{"location":"user_guides/fs/provenance/provenance/#step-1-using-python","title":"Step 1, Using Python","text":"<p>Starting from a feature group metadata object, you can traverse upstream the provenance graph to retrieve the metadata objects of the data sources that are part of the feature group. To do so, you can use the [<code>FeatureGroup.get_data_source_provenance</code>][hsfs.feature_group.FeatureGroup.get_data_source_provenance] method.</p> PythonPython <pre><code># Returns all data sources linked to the provided feature group\nlineage = user_profiles_fg.get_data_source_provenance()\n\n# List all accessible parent data sources\nlineage.accessible\n\n# List all deleted parent data sources\nlineage.deleted\n\n# List all the inaccessible parent data sources\nlineage.inaccessible\n</code></pre> <pre><code># Returns an accessible data source linked to the feature group (if it exists)\nuser_profiles_fg.get_data_source()\n</code></pre> <p>To traverse the provenance graph in the opposite direction (i.e., from the data source to the feature group), you can use the [<code>StorageConnector.get_feature_groups_provenance</code>][hsfs.storage_connector.StorageConnector.get_feature_groups_provenance] method. When navigating the provenance graph downstream, the <code>deleted</code> feature groups are not tracked by provenance, as such, the <code>deleted</code> property will always return an empty list.</p> PythonPython <pre><code># Returns all feature groups linked to the provided data source\nlineage = snowflake_sc.get_feature_groups_provenance()\n\n# List all accessible downstream feature groups\nlineage.accessible\n\n# List all the inaccessible downstream feature groups\nlineage.inaccessible\n</code></pre> <pre><code># Returns all accessible feature groups linked to the data source (if any exists)\nsnowflake_sc.get_feature_groups()\n</code></pre>"},{"location":"user_guides/fs/provenance/provenance/#step-2-feature-group-lineage","title":"Step 2: Feature group lineage","text":""},{"location":"user_guides/fs/provenance/provenance/#assign-parents-to-a-feature-group","title":"Assign parents to a feature group","text":"<p>When creating a feature group, it is possible to specify a list of feature groups used to create the derived features. For example, you could have an external feature group defined over a Snowflake or Redshift table, which you use to compute the features and save them in a feature group. You can mark the external feature group as parent of the feature group you are creating by using the <code>parents</code> parameter in the <code>FeatureStore.get_or_create_feature_group</code> or <code>FeatureStore.create_feature_group</code> methods:</p> Python <pre><code># Retrieve the feature group\nprofiles_fg = fs.get_external_feature_group(\"user_profiles\", version=1)\n\n# Do feature engineering\nage_df = transaction_df.merge(profiles_fg.read(), on=\"cc_num\", how=\"left\")\ntransaction_df[\"age_at_transaction\"] = (age_df[\"datetime\"] - age_df[\"birthdate\"]) / np.timedelta64(1, \"Y\")\n\n# Create the transaction feature group\ntransaction_fg = fs.get_or_create_feature_group(\n    name=\"transaction_fraud_batch\",\n    version=1,\n    description=\"Transaction features\",\n    primary_key=[\"cc_num\"],\n    event_time=\"datetime\",\n    parents=[profiles_fg]\n)\ntransaction_fg.insert(transaction_df)\n</code></pre> <p>Another example use case for derived feature group is if you have a feature group containing features with daily resolution and you are using the content of that feature group to populate a second feature group with monthly resolution:</p> Python <pre><code># Retrieve the feature group\ndaily_transaction_fg = fs.get_feature_group(\"daily_transaction\", version=1)\ndaily_transaction_df = daily_transaction_fg.read()\n\n# Do feature engineering\ncc_group = daily_transaction_df[[\"cc_num\", \"amount\", \"datetime\"]] \\\n                .groupby(\"cc_num\") \\\n                .rolling(\"1M\", on=\"datetime\")\nmonthly_transaction_df  = pd.DataFrame(cc_group.mean())\n\n# Create the transaction feature group\nmonthly_transaction_fg = fs.get_or_create_feature_group(\n    name=\"monthly_transaction_fraud_batch\",\n    version=1,\n    description=\"Transaction features - monthly aggregates\",\n    primary_key=[\"cc_num\"],\n    event_time=\"datetime\",\n    parents=[daily_transaction_fg]\n)\nmonthly_transaction_fg.insert(monthly_transaction_df)\n</code></pre>"},{"location":"user_guides/fs/provenance/provenance/#list-feature-group-parents","title":"List feature group parents","text":"<p>You can query the provenance graph of a feature group using the UI and the APIs. From the APIs you can list the parent feature groups by calling the method [<code>FeatureGroup.get_parent_feature_groups</code>][hsfs.feature_group.FeatureGroup.get_parent_feature_groups]</p> Python <pre><code>lineage = transaction_fg.get_parent_feature_groups()\n\n# List all accessible parent feature groups\nlineage.accessible\n\n# List all deleted parent feature groups\nlineage.deleted\n\n# List all the inaccessible parent feature groups\nlineage.inaccessible\n</code></pre> <p>A parent is marked as <code>deleted</code> (and added to the deleted list) if the parent feature group was deleted. <code>inaccessible</code> if you no longer have access to the parent feature group (e.g., the parent feature group belongs to a project you no longer have access to).</p> <p>To traverse the provenance graph in the opposite direction (i.e., from the parent feature group to the child), you can use the [<code>FeatureGroup.get_generated_feature_groups</code>][hsfs.feature_group.FeatureGroup.get_generated_feature_groups] method. When navigating the provenance graph downstream, the <code>deleted</code> feature groups are not tracked by provenance, as such, the <code>deleted</code> property will always return an empty list.</p> Python <pre><code>lineage = transaction_fg.get_generated_feature_groups()\n\n# List all accessible child feature groups\nlineage.accessible\n\n# List all the inaccessible child feature groups\nlineage.inaccessible\n</code></pre> <p>You can also visualize the relationship between the parent and child feature groups in the UI. In each feature group overview page you can find a provenance section with the graph of parent data source/feature groups and child feature groups/feature views.</p> <p> Provenance graph of derived feature groups </p>"},{"location":"user_guides/fs/provenance/provenance/#step-3-feature-view-lineage","title":"Step 3: Feature view lineage","text":"<p>The relationship between feature groups and feature views is captured automatically when you create a feature view. You can inspect the relationship between feature groups and feature views using the APIs or the UI.</p>"},{"location":"user_guides/fs/provenance/provenance/#step-3-using-python","title":"Step 3, Using Python","text":"<p>Starting from a feature view metadata object, you can traverse upstream the provenance graph to retrieve the metadata objects of the feature groups that are part of the feature view. To do so, you can use the <code>FeatureView.get_parent_feature_groups</code> method.</p> Python <pre><code>lineage = fraud_fv.get_parent_feature_groups()\n\n# List all accessible parent feature groups\nlineage.accessible\n\n# List all deleted parent feature groups\nlineage.deleted\n\n# List all the inaccessible parent feature groups\nlineage.inaccessible\n</code></pre> <p>You can also traverse the provenance graph in the opposite direction. Starting from a feature group you can navigate downstream and list all the feature views the feature group is used in. As for the derived feature group example above, when navigating the provenance graph downstream <code>deleted</code> feature views are not tracked. As such, the <code>deleted</code> property will always be empty.</p> Python <pre><code>lineage = transaction_fg.get_generated_feature_views()\n\n# List all accessible downstream feature views\nlineage.accessible\n\n# List all the inaccessible downstream feature views\nlineage.inaccessible\n</code></pre> <p>Users can call the <code>FeatureView.get_models_provenance</code> method which will return a provenance Link object.</p> <p>You can also retrieve directly the accessible models, without the need to extract them from the provenance links object:</p> Python <pre><code>#List all accessible models\nmodels = fraud_fv.get_models()\n\n#List accessible models trained from a specific training dataset version\nmodels = fraud_fv.get_models(training_dataset_version: 1)\n</code></pre> <p>Also we added a utility method to retrieve from the user's accessible models, the last trained one. Last is determined based on timestamp when it was saved into the model registry.</p> Python <pre><code>#Retrieve newest model from all user's accessible models based on this feature view\nmodel = fraud_fv.get_newest_model()\n#Retrieve newest model from all user's accessible models based on this training dataset version\nmodel = fraud_fv.get_newest_model(training_dataset_version: 1)\n</code></pre>"},{"location":"user_guides/fs/provenance/provenance/#step-3-using-ui","title":"Step 3, Using UI","text":"<p>In the feature view overview UI you can explore the provenance graph of the feature view:</p> <p> Feature view provenance graph </p>"},{"location":"user_guides/fs/provenance/provenance/#provenance-links","title":"Provenance Links","text":"<p>All the <code>_provenance</code> methods return a <code>Link</code> dictionary object that contains <code>accessible</code>, <code>inaccessible</code>, <code>deleted</code> lists.</p> <ul> <li><code>accessible</code> - contains any artifact from the result, that the user has access to.</li> <li><code>inaccessible</code> - contains any artifacts that might have been shared at some point in the past, but where this sharing was retracted. Since the relation between artifacts is still maintained in the provenance, the user will only have access to limited metadata and the artifacts will be included in this <code>inaccessible</code> list.</li> <li><code>deleted</code> - contains artifacts that are deleted with children still present in the system. There is minimum amount of metadata for the deleted allowing for some limited human readable identification.</li> </ul>"},{"location":"user_guides/fs/sharing/sharing/","title":"Sharing","text":""},{"location":"user_guides/fs/sharing/sharing/#introduction","title":"Introduction","text":"<p>Hopsworks allows artifacts (such as feature groups and feature views) to be shared between projects. There are two main use cases for sharing features:</p> <ul> <li> <p>Cross-team collaboration: When multiple teams work on the same Hopsworks deployment, each team typically has its own set of projects. If team A wants to leverage features built by team B, team B can share their feature groups with team A's project.</p> </li> <li> <p>Environment isolation: By creating separate projects for different stages of the development lifecycle (development, testing, and production), you can ensure that changes in the development project don't impact production features. At the same time, you can share production features to use them when developing new models or additional features.</p> </li> </ul>"},{"location":"user_guides/fs/sharing/sharing/#sharing-the-entire-feature-store","title":"Sharing the entire feature store","text":"<p>You can share your project's entire feature store with another project, granting read-only access to all feature groups.</p>"},{"location":"user_guides/fs/sharing/sharing/#step-1-navigate-to-sharing-settings","title":"Step 1: Navigate to sharing settings","text":"<p>Open the project containing the feature store you want to share. In <code>Project Settings</code>, navigate to the <code>Shared with other projects</code> section.</p> <p> Shared with other projects section in Project Settings </p>"},{"location":"user_guides/fs/sharing/sharing/#step-2-share-the-feature-store","title":"Step 2: Share the feature store","text":"<p>Click <code>Share feature store</code> to open the sharing dialog, then select the target project.</p> <p> Share feature store dialog </p> <p>Read-only access</p> <p>Shared feature stores are always read-only. Members of the target project cannot modify any data in the shared feature store.</p> <p>After clicking <code>Share</code>, the feature store appears under the <code>Shared with other projects</code> section.</p> <p> List of projects the feature store is shared with </p>"},{"location":"user_guides/fs/sharing/sharing/#sharing-a-feature-group-with-selected-features","title":"Sharing a feature group with selected features","text":"<p>For more granular control, you can share individual feature groups and select which features to expose. This allows you to share specific data without granting access to your entire feature store.</p>"},{"location":"user_guides/fs/sharing/sharing/#step-1-navigate-to-the-feature-group","title":"Step 1: Navigate to the feature group","text":"<p>In the <code>Feature Groups</code> section, select the feature group you want to share and click the <code>Sharing</code> tab.</p> <p> Feature group sharing tab </p>"},{"location":"user_guides/fs/sharing/sharing/#step-2-share-the-feature-group","title":"Step 2: Share the feature group","text":"<p>Click <code>Share</code> to open the sharing dialog. You can share with either a project or an individual user with <code>Feature store restricted</code> role.</p> Share with a projectShare with a user <p>Select <code>Share with Project</code>, choose the target project, and select which features to share using <code>Select the features to share</code>.</p> <p><p> Share feature group with project dialog </p></p> <p>After clicking <code>Share</code>, the project appears under the <code>This feature group is shared with X projects</code> section.</p> <p><p> List of projects the feature group is shared with </p></p> <p>Select <code>Share with User</code>, choose a user with the Feature store restricted role, and select which features to share.</p> <p><p> Share feature group with user dialog </p></p> <p>After clicking <code>Share</code>, the user appears under the <code>This feature group is shared with X users</code> section.</p> <p><p> List of users the feature group is shared with </p></p>"},{"location":"user_guides/fs/sharing/sharing/#using-shared-features","title":"Using shared features","text":"<p>Once features have been shared with your project, you can access them through the UI or the API.</p>"},{"location":"user_guides/fs/sharing/sharing/#using-the-ui","title":"Using the UI","text":"<p>Navigate to the project that has access to shared features. In the <code>Feature Groups</code> section, use the dropdown in the upper right corner to select which feature store to view.</p> <p> Selecting a shared feature store in the UI </p>"},{"location":"user_guides/fs/sharing/sharing/#using-the-api","title":"Using the API","text":"<p>To access features from a shared feature store programmatically, retrieve the handle for the shared feature store using the Hopsworks API.</p>"},{"location":"user_guides/fs/sharing/sharing/#step-1-get-feature-store-handles","title":"Step 1: Get feature store handles","text":"<p>Use the <code>get_feature_store()</code> method with the name of the shared feature store:</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# Get your project's feature store\nproject_feature_store = project.get_feature_store()\n\n# Get the shared feature store by name\nshared_feature_store = project.get_feature_store(name=\"name_of_shared_feature_store\")\n</code></pre>"},{"location":"user_guides/fs/sharing/sharing/#step-2-fetch-feature-groups","title":"Step 2: Fetch feature groups","text":"<pre><code># Fetch a feature group from the shared feature store\nshared_fg = shared_feature_store.get_feature_group(\n    name=\"shared_fg_name\",\n    version=1\n)\n\n# Fetch a feature group from your project's feature store\nfg = project_feature_store.get_or_create_feature_group(\n    name=\"feature_group_name\",\n    version=1\n)\n</code></pre>"},{"location":"user_guides/fs/tags/tags/","title":"Tags","text":""},{"location":"user_guides/fs/tags/tags/#introduction","title":"Introduction","text":"<p>Hopsworks feature store enables users to attach tags to artifacts, such as feature groups, feature views or training datasets.</p> <p>A tag is a <code>{key: value}</code> pair which provides additional information about the data managed by Hopsworks. Tags allow you to design custom metadata for your artifacts. For example, you could design a tag schema that encodes governance rules for your feature store, such as classifying data as personally identifiable, defining a data retention period for the data, and defining who signed off on the creation of some feature.</p>"},{"location":"user_guides/fs/tags/tags/#prerequisites","title":"Prerequisites","text":"<p>Tags have a schema. Before you can attach a tag to an artifact and fill in the tag values, you first need to select an existing tag schema or create a new tag schema.</p> <p>Tag schemas can be defined by Hopsworks administrator in the <code>Cluster settings</code> section of the platform. Schemas are defined globally across all projects. When users attach tags to an artifact, the tag will be validated against a specific schema. This allows tags to be consistent no matter the project or the team generating them.</p> <p>Immutable</p> <p>Tag schemas are immutable. Once defined, a tag schema cannot be edited nor deleted.</p>"},{"location":"user_guides/fs/tags/tags/#step-1-define-a-tag-schema","title":"Step 1: Define a tag schema","text":"<p>Tag schemas can be defined using the UI wizard in the <code>Cluster settings</code> &gt; <code>Tag schemas</code> section. Tag schemas have a name, the name is used to uniquely identify the schema. You can also provide an optional description.</p> <p>You can define a schema by using the UI tool or by providing the schema in JSON format. If you use the UI tool, you should provide the name of the property in the schema, the type of the property, whether or not the property is required and an optional description.</p> <p> UI tag schema definition </p> <p>The UI tool allows you to define simple not-nested schemas. For more advanced use cases, more complex schemas (e.g., nested schemas) might be required to fully express the content of a given artifact. In such cases it is possible to provide the schema directly as JSON string. The JSON should follow the standard https://json-schema.org. An example of complex schema is the following:</p> <pre><code>{\n  \"type\" : \"object\",\n  \"properties\" :\n  {\n    \"first_name\" : { \"type\" : \"string\" },\n    \"last_name\" : { \"type\" : \"string\" },\n    \"age\" : { \"type\" : \"integer\" },\n    \"hobbies\" : {\n        \"type\" : \"array\",\n        \"items\" : { \"type\" : \"string\" }\n    }\n  },\n  \"required\" : [\"first_name\", \"last_name\", \"age\"],\n  \"additionalProperties\": false\n}\n</code></pre> <p>Additionally it is also possible to define a single property as tag. You can achieve this by defining a JSON schema like the following:</p> <pre><code>{ \"type\" : \"string\" }\n</code></pre> <p>Where the type is a valid primitive type: <code>string</code>, <code>boolean</code>, <code>integer</code>, <code>number</code>.</p>"},{"location":"user_guides/fs/tags/tags/#step-2-attach-a-tag-to-an-artifact","title":"Step 2: Attach a tag to an artifact","text":"<p>Once the tag schema has been created, you can attach a tag with that schema to a feature group, feature view or training datasets either using the feature store APIs, or by using the UI.</p>"},{"location":"user_guides/fs/tags/tags/#using-the-api","title":"Using the API","text":"<p>You can attach tags to feature groups and feature views by using the <code>add_tag()</code> method of the feature store APIs:</p> Python <pre><code># Retrieve the feature group\nfg = fs.get_feature_group(\"transactions_4h_aggs_fraud_batch_fg\", version=1)\n\n# Define the tag\ntag = {\n    'business_unit': 'Fraud',\n    'data_owner': 'email@hopsworks.ai',\n    'pii': True\n}\n\n# Attach the tag\nfg.add_tag(\"data_privacy\", tag)\n</code></pre> <p>You can see the list of tags attached to a given artifact by using the <code>get_tags()</code> method:</p> Python <pre><code># Retrieve the feature group\nfg = fs.get_feature_group(\"transactions_4h_aggs_fraud_batch_fg\", version=1)\n\n# Retrieve the tags for this feature group\nfg.get_tags()\n</code></pre> <p>Finally you can remove a tag from a given artifact by calling the <code>delete_tag()</code> method:</p> Python <pre><code># Retrieve the feature group\nfg = fs.get_feature_group(\"transactions_4h_aggs_fraud_batch_fg\", version=1)\n\n# Retrieve the tags for this feature group\nfg.delete_tag(\"data_privacy\")\n</code></pre> <p>The same APIs work for feature views and training dataset alike.</p>"},{"location":"user_guides/fs/tags/tags/#using-the-ui","title":"Using the UI","text":"<p>You can attach tags to feature groups and feature views directly from the UI. You can navigate on the artifact page and click on the <code>Add tags</code> button. From there you can select the tag schema of the tag you want to attach and populate the values as shown in the gif below.</p> <p> Attach tag to a feature group </p>"},{"location":"user_guides/fs/tags/tags/#step-3-search","title":"Step 3: Search","text":"<p>Hopsworks indexes the tags attached to feature groups, feature views and training datasets. The tags will then be searchable using the free text search box located at the top of the UI.</p> <p> Search for tags in the feature store </p>"},{"location":"user_guides/integrations/","title":"Client Integrations","text":"<p>Hopsworks is an open platform aiming to be accessible from a variety of tools. Learn in this section how to connect to Hopsworks from</p> <ul> <li>Python, AWS SageMaker, Google Colab, Kubeflow</li> <li>Java</li> <li>Databricks</li> <li>AWS EMR</li> <li>Azure HDInsight</li> <li>Azure Machine Learning</li> <li>Apache Spark</li> <li>Apache Flink</li> <li>Apache Beam</li> </ul>"},{"location":"user_guides/integrations/beam/","title":"Apache Beam Dataflow Runner","text":"<p>Connecting to the Feature Store from an Apache Beam Dataflow Runner, requires configuring the Hopsworks certificates. For this in your Beam Java application <code>pom.xml</code> file include following snippet:</p> <pre><code>    &lt;resources&gt;\n      &lt;resource&gt;\n        &lt;directory&gt;java.io.tmpdir&lt;/directory&gt;\n        &lt;includes&gt;\n          &lt;include&gt;**/*.jks&lt;/include&gt;\n        &lt;/includes&gt;\n      &lt;/resource&gt;\n    &lt;/resources&gt;\n</code></pre>"},{"location":"user_guides/integrations/beam/#generating-an-api-key","title":"Generating an API Key","text":"<p>For instructions on how to generate an API key follow this user guide. For the Beam integration to work correctly make sure you add the following scopes to your API key:</p> <ol> <li>featurestore</li> <li>project</li> <li>job</li> <li>kafka</li> </ol>"},{"location":"user_guides/integrations/beam/#connecting-to-the-feature-store","title":"Connecting to the Feature Store","text":"<p>You are now ready to connect to the Hopsworks Feature Store from Beam:</p> <pre><code>//Establish connection with Hopsworks.\nHopsworksConnection hopsworksConnection = HopsworksConnection.builder()\n  .host(\"my_instance\")                      // DNS of your Feature Store instance\n  .port(443)                                // Port to reach your Hopsworks instance, defaults to 443\n  .project(\"my_project\")                    // Name of your Hopsworks Feature Store project\n  .apiKeyValue(\"api_key\")                   // The API key to authenticate with the feature store\n  .hostnameVerification(false)              // Disable for self-signed certificates\n  .build();\n\n//get feature store handle\nFeatureStore fs = hopsworksConnection.getFeatureStore();\n</code></pre>"},{"location":"user_guides/integrations/beam/#next-steps","title":"Next Steps","text":"<p>For more information and how to integrate Beam feature pipeline  to the Hopsworks Feature store follow the tutorial.</p>"},{"location":"user_guides/integrations/flink/","title":"Flink Integration","text":"<p>Connecting to the Feature Store from an external Flink cluster, such as AWS EMR and GCP DataProc requires configuring it with the Hopsworks certificates, done automatically when using Hopsworks API. This guide explains how to connect to the Feature Store from an external Flink cluster.</p>"},{"location":"user_guides/integrations/flink/#generating-an-api-key","title":"Generating an API Key","text":"<p>For instructions on how to generate an API key follow this user guide. For the Flink integration to work correctly make sure you add the following scopes to your API key:</p> <ol> <li>featurestore</li> <li>project</li> <li>job</li> <li>kafka</li> </ol>"},{"location":"user_guides/integrations/flink/#connecting-to-the-feature-store","title":"Connecting to the Feature Store","text":"<p>You are now ready to connect to the Hopsworks Feature Store from Flink:</p> <pre><code>//Establish connection with Hopsworks.\nHopsworksConnection hopsworksConnection = HopsworksConnection.builder()\n  .host(\"my_instance\")                      // DNS of your Feature Store instance\n  .port(443)                                // Port to reach your Hopsworks instance, defaults to 443\n  .project(\"my_project\")                    // Name of your Hopsworks Feature Store project\n  .apiKeyValue(\"api_key\")                   // The API key to authenticate with the feature store\n  .hostnameVerification(false)              // Disable for self-signed certificates\n  .build();\n\n//get feature store handle\nFeatureStore fs = hopsworksConnection.getFeatureStore();\n</code></pre>"},{"location":"user_guides/integrations/flink/#next-steps","title":"Next Steps","text":"<p>For more information and how to integrate Flink streaming feature pipeline to the Hopsworks Feature store follow the tutorial.</p>"},{"location":"user_guides/integrations/hdinsight/","title":"Configure HDInsight for the Hopsworks Feature Store","text":"<p>To enable HDInsight to access the Hopsworks Feature Store, you need to set up a Hopsworks API key, add a script action and configurations to your HDInsight cluster.</p> <p>Prerequisites</p> <p>A HDInsight cluster with cluster type Spark is required to connect to the Feature Store. You can either use an existing cluster or create a new one.</p> <p>Network Connectivity</p> <p>To be able to connect to the Feature Store, please ensure that your HDInsight cluster and the Hopsworks Feature Store are either in the same Virtual Network or Virtual Network Peering is set up between the different networks. In addition, ensure that the Network Security Group of your Hopsworks instance is configured to allow incoming traffic from your HDInsight cluster on ports 443, 3306, 8020, 30010, 9083 and 9085 (443,3306,8020,30010,9083,9085). See Network security groups for more information.</p>"},{"location":"user_guides/integrations/hdinsight/#step-1-set-up-a-hopsworks-api-key","title":"Step 1: Set up a Hopsworks API key","text":"<p>For instructions on how to generate an API key follow this user guide. For the HDInsight integration to work correctly make sure you add the following scopes to your API key:</p> <ol> <li>featurestore</li> <li>project</li> <li>job</li> <li>kafka</li> </ol>"},{"location":"user_guides/integrations/hdinsight/#step-2-use-a-script-action-to-install-the-feature-store-connector","title":"Step 2:  Use a script action to install the Feature Store connector","text":"<p>HDInsight requires Hopsworks connectors to be able to communicate with the Hopsworks Feature Store. These connectors can be installed with the script action shown below. Copy the content into a file, name the file <code>hopsworks.sh</code> and replace MY_INSTANCE, MY_PROJECT, MY_VERSION, MY_API_KEY and MY_CONDA_ENV with your values. Copy the <code>hopsworks.sh</code> file into any storage that is readable by your HDInsight clusters and take note of the URI of that file e.g., <code>https://account.blob.core.windows.net/scripts/hopsworks.sh</code>.</p> <p>The script action needs to be applied head and worker nodes and can be applied during cluster creation or to an existing cluster. Ensure to persist the script action so that it is run on newly created nodes. For more information about how to use script actions, see Customize Azure HDInsight clusters by using script actions.</p> <p>Matching Hopsworks version</p> <p>We recommend that the major and minor version of the Python library match the major and minor version of the Hopsworks deployment.</p> <p><p> You find the Hopsworks version inside any of your Project's settings tab on Hopsworks </p></p> <p>Feature Store script action:</p> <pre><code>set -e\n\nHOST=\"MY_INSTANCE.cloud.hopsworks.ai\" # DNS of your Feature Store instance\nPROJECT=\"MY_PROJECT\"                  # Port to reach your Hopsworks instance, defaults to 443\nHOPSWORKS_VERSION=\"MY_VERSION\"        # The major version of Hopsworks library needs to match the major version of Hopsworks\nAPI_KEY=\"MY_API_KEY\"                  # The API key to authenticate with Hopsworks\nCONDA_ENV=\"MY_CONDA_ENV\"              # py35 is the default for HDI 3.6\n\napt-get --assume-yes install python3-dev\napt-get --assume-yes install jq\n\n/usr/bin/anaconda/envs/$CONDA_ENV/bin/pip install hopsworks==$HOPSWORKS_VERSION\n\nPROJECT_ID=$(curl -H \"Authorization: ApiKey ${API_KEY}\" https://$HOST/hopsworks-api/api/project/getProjectInfo/$PROJECT | jq -r .projectId)\n\nmkdir -p /usr/lib/hopsworks\nchown root:hadoop /usr/lib/hopsworks\ncd /usr/lib/hopsworks\n\ncurl -o client.tar.gz -H \"Authorization: ApiKey ${API_KEY}\" https://$HOST/hopsworks-api/api/project/$PROJECT_ID/client\n\ntar -xvf client.tar.gz\ntar -xzf client/apache-hive-*-bin.tar.gz\nmv apache-hive-*-bin apache-hive-bin\nrm client.tar.gz\nrm client/apache-hive-*-bin.tar.gz\n\ncurl -H \"Authorization: ApiKey ${API_KEY}\" https://$HOST/hopsworks-api/api/project/$PROJECT_ID/credentials | jq -r .kStore | base64 -d &gt; keyStore.jks\n\ncurl -H \"Authorization: ApiKey ${API_KEY}\" https://$HOST/hopsworks-api/api/project/$PROJECT_ID/credentials | jq -r .tStore | base64 -d &gt; trustStore.jks\n\necho -n $(curl -H \"Authorization: ApiKey ${API_KEY}\" https://$HOST/hopsworks-api/api/project/$PROJECT_ID/credentials | jq -r .password) &gt; material_passwd\n\nchown -R root:hadoop /usr/lib/hopsworks\n</code></pre>"},{"location":"user_guides/integrations/hdinsight/#step-3-configure-hdinsight-for-feature-store-access","title":"Step 3: Configure HDInsight for Feature Store access","text":"<p>The Hadoop and Spark installations of the HDInsight cluster need to be configured in order to access the Feature Store. This can be achieved either by using a bootstrap script when creating clusters or using Ambari on existing clusters. Apply the following configurations to your HDInsight cluster.</p> <p>Using Hive and the Feature Store</p> <p>HDInsight clusters cannot use their local Hive when being configured for the Feature Store as the Feature Store relies on custom Hive binaries and its own Metastore which will overwrite the local one. If you rely on Hive for feature engineering then it is advised to write your data to an external data storage such as ADLS from your main HDInsight cluster and in the Feature Store, create an on-demand Feature Group on the storage container in ADLS.</p> <p>Hadoop hadoop-env.sh:</p> <pre><code>export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/usr/lib/hopsworks/client/*\n</code></pre> <p>Hadoop core-site.xml:</p> <pre><code>hops.ipc.server.ssl.enabled=true\nfs.hopsfs.impl=io.hops.hopsfs.client.HopsFileSystem\nclient.rpc.ssl.enabled.protocol=TLSv1.2\nhops.ssl.keystore.name=/usr/lib/hopsworks/keyStore.jks\nhops.rpc.socket.factory.class.default=io.hops.hadoop.shaded.org.apache.hadoop.net.HopsSSLSocketFactory\nhops.ssl.keystores.passwd.name=/usr/lib/hopsworks/material_passwd\nhops.ssl.hostname.verifier=ALLOW_ALL\nhops.ssl.trustore.name=/usr/lib/hopsworks/trustStore.jks\n</code></pre> <p>Spark spark-defaults.conf:</p> <pre><code>spark.executor.extraClassPath=/usr/lib/hopsworks/client/*\nspark.driver.extraClassPath=/usr/lib/hopsworks/client/*\nspark.sql.hive.metastore.jars=path\nspark.sql.hive.metastore.jars.path=/usr/lib/hopsworks/apache-hive-bin/lib/*\n</code></pre> <p>Spark hive-site.xml:</p> <pre><code>hive.metastore.uris=thrift://MY_HOPSWORKS_INSTANCE_PRIVATE_IP:9083\n</code></pre> <p>Info</p> <p>Replace MY_HOPSWORKS_INSTANCE_PRIVATE_IP with the private IP address of you Hopsworks Feature Store.</p>"},{"location":"user_guides/integrations/hdinsight/#step-5-connect-to-the-feature-store","title":"Step 5: Connect to the Feature Store","text":"<p>You are now ready to connect to the Hopsworks Feature Store, for instance using a Jupyter notebook in HDInsight with a PySpark3 kernel:</p> <pre><code>import hopsworks\n\n# Put the API key into Key Vault for any production setup:\n# See, https://azure.microsoft.com/en-us/services/key-vault/\nsecret_value = 'MY_API_KEY'\n\n# Create a connection\nproject = hopsworks.login(\n    host='MY_INSTANCE.cloud.hopsworks.ai', # DNS of your Feature Store instance\n    port=443,                              # Port to reach your Hopsworks instance, defaults to 443\n    project='MY_PROJECT',                  # Name of your Hopsworks project\n    api_key_value=secret_value,            # The API key to authenticate with Hopsworks\n    hostname_verification=True             # Disable for self-signed certificates\n)\n\n# Get the feature store handle for the project's feature store\nfs = project.get_feature_store()\n</code></pre>"},{"location":"user_guides/integrations/hdinsight/#next-steps","title":"Next Steps","text":"<p>For more information on how to use the Hopsworks API check out the other guides or the Login API.</p>"},{"location":"user_guides/integrations/java/","title":"Java client","text":"<p>This guide explains step by step how to connect to Hopsworks from a Java client.</p>"},{"location":"user_guides/integrations/java/#generate-an-api-key","title":"Generate an API key","text":"<p>For instructions on how to generate an API key follow this user guide. For the Java client to work correctly make sure you add the following scopes to your API key:</p> <ol> <li>featurestore</li> <li>project</li> <li>job</li> <li>kafka</li> </ol>"},{"location":"user_guides/integrations/java/#connecting-to-the-feature-store","title":"Connecting to the Feature Store","text":"<p>You are now ready to connect to the Hopsworks Feature Store from a Java client:</p> <pre><code>//Import necessary classes\nimport com.logicalclocks.hsfs.FeatureStore;\nimport com.logicalclocks.hsfs.FeatureView;\nimport com.logicalclocks.hsfs.HopsworksConnection;\n\n//Establish connection with Hopsworks.\nHopsworksConnection hopsworksConnection = HopsworksConnection.builder()\n  .host(\"my_instance\")                      // DNS of your Feature Store instance\n  .port(443)                                // Port to reach your Hopsworks instance, defaults to 443\n  .project(\"my_project\")                    // Name of your Hopsworks Feature Store project\n  .apiKeyValue(\"api_key\")                   // The API key to authenticate with the feature store\n  .hostnameVerification(false)               // Disable for self-signed certificates\n  .build();\n\n//get feature store handle\nFeatureStore fs = hopsworksConnection.getFeatureStore();\n\n//get feature view handle\nFeatureView fv = fs.getFeatureView(fvName, fvVersion);\n\n// get feature vector\nList&lt;Object&gt; singleVector = fv.getFeatureVector(new HashMap&lt;String, Object&gt;() {{\n        put(\"id\", 100);\n        }});\n</code></pre>"},{"location":"user_guides/integrations/java/#next-steps","title":"Next Steps","text":"<p>For more information how to interact from Java client with the Hopsworks Feature store follow this tutorial.</p>"},{"location":"user_guides/integrations/mlstudio_designer/","title":"Azure Machine Learning Designer Integration","text":"<p>Connecting to Hopsworks from the Azure Machine Learning Designer requires setting up a Hopsworks API key for the Designer and installing the Hopsworks Python library on the Designer. This guide explains step by step how to connect to the Feature Store from Azure Machine Learning Designer.</p> <p>Network Connectivity</p> <p>To be able to connect to the Feature Store, please ensure that the Network Security Group of your Hopsworks instance on Azure is configured to allow incoming traffic from your compute target on ports 443, 9083 and 9085 (443,9083,9085). See Network security groups for more information. If your compute target is not in the same VNet as your Hopsworks instance and the Hopsworks instance is not accessible from the internet then you will need to configure Virtual Network Peering.</p>"},{"location":"user_guides/integrations/mlstudio_designer/#generate-an-api-key","title":"Generate an API key","text":"<p>For instructions on how to generate an API key follow this user guide. For the Azure ML Designer integration to work correctly make sure you add the following scopes to your API key:</p> <ol> <li>featurestore</li> <li>project</li> <li>job</li> <li>kafka</li> </ol>"},{"location":"user_guides/integrations/mlstudio_designer/#connect-to-hopsworks","title":"Connect to Hopsworks","text":"<p>To connect to Hopsworks from the Azure Machine Learning Designer, create a new pipeline or open an existing one:</p> <p> Add an Execute Python Script step </p> <p>In the pipeline, add a new <code>Execute Python Script</code> step and replace the Python script from the next step:</p> <p> Add the code to access the Hopsworks </p> <p>Updating the script</p> <p>Replace MY_VERSION, MY_API_KEY, MY_INSTANCE, MY_PROJECT and MY_FEATURE_GROUP with the respective values. The major version set for MY_VERSION needs to match the major version of Hopsworks. Check PyPI for available releases.</p> <p><p> You find the Hopsworks version inside any of your Project's settings tab on Hopsworks </p></p> <pre><code>import os\nimport importlib.util\n\n\npackage_name = 'hopsworks'\nversion = 'MY_VERSION'\nspec = importlib.util.find_spec(package_name)\nif spec is None:\n    import os\n    os.system(f\"pip install %s[python]==%s\" % (package_name, version))\n\n# Put the API key into Key Vault for any production setup:\n# See, https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-secrets-in-runs\n#from azureml.core import Experiment, Run\n#run = Run.get_context()\n#secret_value = run.get_secret(name=\"fs-api-key\")\nsecret_value = 'MY_API_KEY'\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n\n    import hopsworks\n    project = hopsworks.login(\n        host='MY_INSTANCE.cloud.hopsworks.ai', # DNS of your Hopsworks instance\n        port=443,                              # Port to reach your Hopsworks instance, defaults to 443\n        project='MY_PROJECT',                  # Name of your Hopsworks project\n        api_key_value=secret_value,            # The API key to authenticate with Hopsworks\n        hostname_verification=True,            # Disable for self-signed certificates\n        engine='python'                        # Choose python as engine\n    )\n    fs = project.get_feature_store()              # Get the project's default feature store\n\n    return fs.get_feature_group('MY_FEATURE_GROUP', version=1).read(),\n</code></pre> <p>Select a compute target and save the step. The step is now ready to use:</p> <p> Select a compute target </p> <p>As a next step, you have to connect the previously created <code>Execute Python Script</code> step with the next step in the pipeline. For instance, to export the features to a CSV file, create a <code>Export Data</code> step:</p> <p> Add an Export Data step </p> <p>Configure the <code>Export Data</code> step to write to you data store of choice:</p> <p> Configure the Export Data step </p> <p>Connect the to steps by drawing a line between them:</p> <p> Connect the steps </p> <p>Finally, submit the pipeline and wait for it to finish:</p> <p>Performance on the first execution</p> <p>The <code>Execute Python Script</code> step can be slow when being executed for the first time as the Hopsworks library needs to be installed on the compute target. Subsequent executions on the same compute target should use the already installed library.</p> <p> Execute the pipeline </p>"},{"location":"user_guides/integrations/mlstudio_designer/#next-steps","title":"Next Steps","text":"<p>For more information on how to use the Hopsworks API check out the other guides or the Login API.</p>"},{"location":"user_guides/integrations/mlstudio_notebooks/","title":"Azure Machine Learning Notebooks Integration","text":"<p>Connecting to the Hopsworks from Azure Machine Learning Notebooks requires setting up a Hopsworks API key for Azure Machine Learning Notebooks and installing the Hopsworks Python library on the notebook. This guide explains step by step how to connect to the Hopsworks from Azure Machine Learning Notebooks.</p> <p>Network Connectivity</p> <p>To be able to connect to the Feature Store, please ensure that the Network Security Group of your Hopsworks instance on Azure is configured to allow incoming traffic from your compute target on ports 443, 9083 and 9085 (443,9083,9085). See Network security groups for more information. If your compute target is not in the same VNet as your Hopsworks instance and the Hopsworks instance is not accessible from the internet then you will need to configure Virtual Network Peering.</p>"},{"location":"user_guides/integrations/mlstudio_notebooks/#install-hopsworks-python-library","title":"Install Hopsworks Python Library","text":"<p>To be able to interact with Hopsworks from a Python environment you need to install the <code>Hopsworks</code> Python library. The library is available on PyPi and can be installed using <code>pip</code>:</p> <pre><code>pip install hopsworks[python]~=[HOPSWORKS_VERSION]\n</code></pre> <p>Python Profile</p> <p>By default, <code>pip install hopsworks</code> does not install all the necessary dependencies required to use the Hopsworks library from a local Python environment. To ensure that all the dependencies are installed, you should install the library using with the Python profile <code>pip install hopsworks[python]</code>.</p> <p>Matching Hopsworks version</p> <p>We recommend that the major and minor version of the Python library match the major and minor version of the Hopsworks deployment.</p> <p><p> You find the Hopsworks version inside any of your Project's settings tab on Hopsworks </p></p>"},{"location":"user_guides/integrations/mlstudio_notebooks/#generate-an-api-key","title":"Generate an API key","text":"<p>For instructions on how to generate an API key follow this user guide. For the Azure ML Notebooks integration to work correctly make sure you add the following scopes to your API key:</p> <ol> <li>featurestore</li> <li>project</li> <li>job</li> <li>kafka</li> </ol>"},{"location":"user_guides/integrations/mlstudio_notebooks/#connect-from-an-azure-machine-learning-notebook","title":"Connect from an Azure Machine Learning Notebook","text":"<p>To access Hopsworks from Azure Machine Learning, open a Python notebook and proceed with the following steps to install Hopsworks and connect to the Feature Store:</p> <p> Connecting from an Azure Machine Learning Notebook </p>"},{"location":"user_guides/integrations/mlstudio_notebooks/#connect-to-hopsworks","title":"Connect to Hopsworks","text":"<p>You are now ready to connect to Hopsworks Feature Store from the notebook:</p> <pre><code>import hopsworks\n\n# Put the API key into Key Vault for any production setup:\n# See, https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-secrets-in-runs\n#from azureml.core import Experiment, Run\n#run = Run.get_context()\n#secret_value = run.get_secret(name=\"fs-api-key\")\nsecret_value = 'MY_API_KEY'\n\n# Create a connection\nproject = hopsworks.login(\n    host='MY_INSTANCE.cloud.hopsworks.ai', # DNS of your Hopsworks instance\n    port=443,                              # Port to reach your Hopsworks instance, defaults to 443\n    project='MY_PROJECT',                  # Name of your Hopsworks project\n    api_key_value=secret_value,            # The API key to authenticate with Hopsworks\n    hostname_verification=True,            # Disable for self-signed certificates\n    engine='python'                        # Choose Python as engine\n)\n\n# Get the feature store handle for the project's feature store\nfs = project.get_feature_store()\n</code></pre>"},{"location":"user_guides/integrations/mlstudio_notebooks/#next-steps","title":"Next Steps","text":"<p>For more information on how to use the Hopsworks API check out the other guides or the Login API.</p>"},{"location":"user_guides/integrations/python/","title":"Python Environments (Local, AWS SageMaker, Google Colab or Kubeflow)","text":"<p>This guide explains step by step how to connect to Hopsworks from any Python environment such as your local environment, AWS SageMaker, Google Colab or Kubeflow.</p>"},{"location":"user_guides/integrations/python/#install-python-library","title":"Install Python Library","text":"<p>To be able to interact with Hopsworks from a Python environment you need to install the <code>Hopsworks</code> Python library. The library is available on PyPi and can be installed using <code>pip</code>:</p> <pre><code>pip install hopsworks[python]~=[HOPSWORKS_VERSION]\n</code></pre> <p>Python Profile</p> <p>By default, <code>pip install hopsworks</code>, does not install all the necessary dependencies required to use the Hopsworks library from a pure Python environment. To ensure that all the dependencies are installed, you should install the library using with the Python profile <code>pip install hopsworks[python]</code>.</p> <p>Matching Hopsworks version</p> <p>We recommend that the major and minor version of the Python library match the major and minor version of the Hopsworks deployment.</p> <p><p> You find the Hopsworks version inside any of your Project's settings tab on Hopsworks </p></p>"},{"location":"user_guides/integrations/python/#generate-an-api-key","title":"Generate an API key","text":"<p>For instructions on how to generate an API key follow this user guide. For the Python client to work correctly make sure you add the following scopes to your API key:</p> <ol> <li>featurestore</li> <li>project</li> <li>job</li> <li>kafka</li> </ol>"},{"location":"user_guides/integrations/python/#connect-to-the-feature-store","title":"Connect to the Feature Store","text":"<p>You are now ready to connect to Hopsworks from your Python environment:</p> <pre><code>import hopsworks\nproject = hopsworks.login(\n    host='my_instance',                 # DNS of your Hopsworks instance\n    port=443,                           # Port to reach your Hopsworks instance, defaults to 443\n    project='my_project',               # Name of your Hopsworks project\n    api_key_value='apikey',             # The API key to authenticate with Hopsworks\n    engine='python',                    # Use the Python engine\n)\nfs = project.get_feature_store()        # Get the project's default feature store\n</code></pre> <p>Engine</p> <p><code>Hopsworks</code> leverages several engines depending on whether you are running using Apache Spark or Pandas/Polars. The default behaviour of the library is to use the <code>spark</code> engine if you do not specify any <code>engine</code> option in the <code>login</code> method and if the <code>PySpark</code> library is available in the environment.</p> <p>Please refer to the Spark integration guide to configure your PySpark cluster to interact with Hopsworks.</p>"},{"location":"user_guides/integrations/python/#next-steps","title":"Next Steps","text":"<p>For more information on how to use the Hopsworks API check out the other guides or the Login API.</p>"},{"location":"user_guides/integrations/spark/","title":"Spark Integration","text":"<p>Connecting to the Feature Store from an external Spark cluster, such as Cloudera, requires configuring it with the Hopsworks client jars and configuration. This guide explains step by step how to connect to the Feature Store from an external Spark cluster.</p>"},{"location":"user_guides/integrations/spark/#download-the-hopsworks-client-jars","title":"Download the Hopsworks Client Jars","text":"<p>In the Project Settings, select the integration tab and scroll to the Configure Spark Integration section. Click on Download client Jars. This will start the download of the client.tar.gz archive. The archive contains two jar files for HopsFS, the Apache Hudi jar and the Java version of the HSFS library. You should upload these libraries to your Spark cluster and attach them as local resources to your Job. If you are using <code>spark-submit</code>, you should specify the <code>--jar</code> option. For more details see: Spark Dependency Management.</p> <p> The Spark Integration gives access to Jars and configuration for an external Spark cluster </p>"},{"location":"user_guides/integrations/spark/#download-the-certificates","title":"Download the certificates","text":"<p>Download the certificates from the same section as above. Hopsworks uses X.509 certificates for authentication and authorization. If you are interested in the Hopsworks security model, you can read more about it in this blog post. The certificates are composed of three different components: the <code>keyStore.jks</code> containing the private key and the certificate for your project user, the <code>trustStore.jks</code> containing the certificates for the Hopsworks certificates authority, and a password to unlock the private key in the <code>keyStore.jks</code>. The password is displayed in a pop-up when downloading the certificate and should be saved in a file named <code>material_passwd</code>.</p> <p>Warning</p> <p>When you copy-paste the password to the <code>material_passwd</code> file, pay attention to not introduce additional empty spaces or new lines.</p> <p>The three files (<code>keyStore.jks</code>, <code>trustStore.jks</code> and <code>material_passwd</code>) should be attached as resources to your Spark application as well.</p>"},{"location":"user_guides/integrations/spark/#configure-your-spark-cluster","title":"Configure your Spark cluster","text":"<p>Spark version limitation</p> <p>Currently Spark version 3.3.x is suggested to be able to use the full suite of Hopsworks Feature Store capabilities.</p> <p>Add the following configuration to the Spark application:</p> <pre><code>spark.hadoop.fs.hopsfs.impl                         io.hops.hopsfs.client.HopsFileSystem\nspark.hadoop.hops.ipc.server.ssl.enabled            true\nspark.hadoop.hops.ssl.hostname.verifier             ALLOW_ALL\nspark.hadoop.hops.rpc.socket.factory.class.default  io.hops.hadoop.shaded.org.apache.hadoop.net.HopsSSLSocketFactory\nspark.hadoop.client.rpc.ssl.enabled.protocol        TLSv1.2\nspark.hadoop.hops.ssl.keystores.passwd.name         material_passwd\nspark.hadoop.hops.ssl.keystore.name                 keyStore.jks\nspark.hadoop.hops.ssl.trustore.name                 trustStore.jks\nspark.sql.hive.metastore.jars                       path\nspark.sql.hive.metastore.jars.path                  [Path to the Hopsworks Hive Jars]\nspark.hadoop.hive.metastore.uris                    thrift://[metastore_ip]:[metastore_port]\n</code></pre> <p><code>spark.sql.hive.metastore.jars.path</code> should point to the path with the jars from the uncompressed Hive archive you can find in clients.tar.gz.</p>"},{"location":"user_guides/integrations/spark/#pyspark","title":"PySpark","text":"<p>To use PySpark, install the HSFS Python library which can be found on PyPi.</p> <p>Matching Hopsworks version</p> <p>The major version of <code>HSFS</code> needs to match the major version of Hopsworks.</p>"},{"location":"user_guides/integrations/spark/#generating-an-api-key","title":"Generating an API Key","text":"<p>For instructions on how to generate an API key follow this user guide. For the Spark integration to work correctly make sure you add the following scopes to your API key:</p> <ol> <li>featurestore</li> <li>project</li> <li>job</li> <li>kafka</li> </ol>"},{"location":"user_guides/integrations/spark/#connecting-to-the-feature-store","title":"Connecting to the Feature Store","text":"<p>You are now ready to connect to the Hopsworks Feature Store from Spark:</p> <pre><code>import hopsworks\nproject = hopsworks.login(\n    host='my_instance',                 # DNS of your Feature Store instance\n    port=443,                           # Port to reach your Hopsworks instance, defaults to 443\n    project='my_project',               # Name of your Hopsworks Feature Store project\n    api_key_value='api_key',            # The API key to authenticate with the feature store\n    hostname_verification=True          # Disable for self-signed certificates\n)\nfs = project.get_feature_store()           # Get the project's default feature store\n</code></pre> <p>Engine</p> <p><code>Hopsworks</code> leverages several engines depending on whether you are running using Apache Spark or Pandas/Polars. The default behaviour of the library is to use the <code>spark</code> engine if you do not specify any <code>engine</code> option in the <code>login</code> method and if the <code>PySpark</code> library is available in the environment.</p>"},{"location":"user_guides/integrations/spark/#next-steps","title":"Next Steps","text":"<p>For more information about how to connect, see the Login API. Or continue with the Data Source guide to import your own data to the Feature Store.</p>"},{"location":"user_guides/integrations/databricks/api_key/","title":"Hopsworks API key","text":"<p>In order for the Databricks cluster to be able to communicate with Hopsworks, clients running on Databricks need to be able to access a Hopsworks API key.</p>"},{"location":"user_guides/integrations/databricks/api_key/#generate-an-api-key","title":"Generate an API key","text":"<p>For instructions on how to generate an API key follow this user guide. For the Databricks integration to work make sure you add the following scopes to your API key:</p> <ol> <li>featurestore</li> <li>project</li> <li>job</li> <li>kafka</li> </ol>"},{"location":"user_guides/integrations/databricks/api_key/#quickstart-api-key-argument","title":"Quickstart API key Argument","text":"<p>API key as Argument</p> <p>To get started quickly, without saving the Hopsworks API in a secret storage, you can simply supply it as an argument when instantiating a connection:</p> <pre><code>    import hopsworks\n    project = hopsworks.login(\n        host='my_instance',                 # DNS of your Feature Store instance\n        port=443,                           # Port to reach your Hopsworks instance, defaults to 443\n        project='my_project',               # Name of your Hopsworks Feature Store project\n        api_key_value='apikey',             # The API key to authenticate with Hopsworks\n    )\n    fs = project.get_feature_store()           # Get the project's default feature store\n</code></pre>"},{"location":"user_guides/integrations/databricks/api_key/#next-steps","title":"Next Steps","text":"<p>Continue with the configuration guide to finalize the configuration of the Databricks Cluster to communicate with the Hopsworks Feature Store.</p>"},{"location":"user_guides/integrations/databricks/configuration/","title":"Databricks Integration","text":"<p>Users can configure their Databricks clusters to write the results of feature engineering pipelines in the Hopsworks Feature Store using HSFS. Configuring a Databricks cluster can be done from the Hopsworks Feature Store UI. This guide explains each step.</p>"},{"location":"user_guides/integrations/databricks/configuration/#prerequisites","title":"Prerequisites","text":"<p>In order to be able to configure a Databricks cluster to use the Feature Store of your Hopsworks instance, it is necessary to ensure networking is setup correctly between the instances and that the Databricks cluster has access to the Hopsworks API key to perform requests with HSFS from Databricks to Hopsworks.</p>"},{"location":"user_guides/integrations/databricks/configuration/#networking","title":"Networking","text":"<p>If you haven't done so already, follow the networking guides for either AWS or Azure for instructions on how to configure networking properly between Databricks' VPC (or Virtual Network on Azure) and the managed.hopsworks.ai VPC/VNet.</p>"},{"location":"user_guides/integrations/databricks/configuration/#hopsworks-api-key","title":"Hopsworks API key","text":"<p>In order for the Feature Store API to be able to communicate with the user's Hopsworks instance, the client library (HSFS) needs to have access to a previously generated API key from Hopsworks. For ways to setup and store the Hopsworks API key, please refer to the API key guide for Databricks.</p>"},{"location":"user_guides/integrations/databricks/configuration/#databricks-api-key","title":"Databricks API key","text":"<p>Hopsworks uses the Databricks REST APIs to communicate with the Databricks instance and configure clusters on behalf of users. To achieve that, the first step is to register an instance and a valid API key in Hopsworks.</p> <p>Users can get a valid Databricks API key by following the Databricks Documentation</p> <p>Cluster access control</p> <p>If users have enabled Databricks Cluster access control, it is important that the users running the cluster configuration (i.e., the user generating the API key) has <code>Can Manage</code> privileges on the cluster they are trying to configure.</p>"},{"location":"user_guides/integrations/databricks/configuration/#register-a-new-databricks-instance","title":"Register a new Databricks Instance","text":"<p>To register a new Databricks instance, first navigate to <code>Project settings</code>, which can be found on the left-hand side of a Project's landing page, then select the <code>Integrations</code> tab.</p> <p> Hopsworks's Integrations page </p> <p>Registering a Databricks instance requires adding the instance address and the Databricks API key.</p> <p>The instance name corresponds to the address of the Databricks instance and should be in the format <code>[UUID].cloud.databricks.com</code> (or <code>adb-[UUID].19.azuredatabricks.net</code> for Databricks on Azure), essentially the same web address used to reach the Databricks instance from the browser.</p> <p> Register a Databricks Instance along with a Databricks API key </p> <p>The API key will be stored in the Hopsworks secret store for the user and will be available only for that user.  If multiple users need to configure Databricks clusters, each has to generate an API key and register an instance. The Databricks instance registration does not have a project scope, meaning that once registered, the user can configure clusters for all projects they are part of.</p>"},{"location":"user_guides/integrations/databricks/configuration/#databricks-cluster","title":"Databricks Cluster","text":"<p>A cluster needs to exist before users can configure it using the Hopsworks UI. The cluster can be in any state prior to the configuration.</p> <p>Runtime limitation</p> <p>Currently Runtime 12.2 LTS is suggested to be able to use the full suite of Hopsworks Feature Store capabilities.</p>"},{"location":"user_guides/integrations/databricks/configuration/#configure-a-cluster","title":"Configure a cluster","text":"<p>Clusters are configured for a project user, which, in Hopsworks terms, means a user operating within the scope of a project. To configure a cluster, click on the <code>Configure</code> button. By default the cluster will be configured for the user making the request. If the user doesn't have <code>Can Manage</code> privilege on the cluster, they can ask a project <code>Data Owner</code> to configure it for them. Hopsworks <code>Data Owners</code> are allowed to configure clusters for other project users, as long as they have the required Databricks privileges.</p> <p> Configure a Databricks Cluster from Hopsworks </p> <p>During the cluster configuration the following steps will be taken:</p> <ul> <li>Upload an archive to DBFS containing the necessary Jars for HSFS and HopsFS to be able to read and write from the Hopsworks Feature Store</li> <li>Add an initScript to configure the Jars when the cluster is started</li> <li>Install <code>hsfs</code> python library</li> <li>Configure the necessary Spark properties to authenticate and communicate with the Feature Store</li> </ul> <p>HopsFS configuration</p> <p>It is not necessary to configure HopsFS if data is stored outside the Hopsworks file system. To do this define Data Sources and link them to Feature Groups and Training Datasets.</p> <p>When a cluster is configured for a specific project user, all the operations with the Hopsworks Feature Store will be executed as that project user. If another user needs to re-use the same cluster, the cluster can be reconfigured by following the same steps above.</p>"},{"location":"user_guides/integrations/databricks/configuration/#connecting-to-the-feature-store","title":"Connecting to the Feature Store","text":"<p>At the end of the configuration, Hopsworks will start the cluster. Once the cluster is running users can establish a connection to the Hopsworks Feature Store from Databricks:</p> <pre><code>import hopsworks\nproject = hopsworks.login(\n    host='my_instance',                 # DNS of your Hopsworks instance\n    port=443,                           # Port to reach your Hopsworks instance, defaults to 443\n    project='my_project',               # Name of your Hopsworks project\n    api_key_value='apikey',             # The API key to authenticate with Hopsworks\n)\nfs = project.get_feature_store()           # Get the project's default feature store\n</code></pre>"},{"location":"user_guides/integrations/databricks/configuration/#next-steps","title":"Next Steps","text":"<p>For more information about how to connect, see the Login API. Or continue with the Data Source guide to import your own data to the Feature Store.</p>"},{"location":"user_guides/integrations/databricks/networking/","title":"Networking","text":"<p>In order for Spark to communicate with the Feature Store from Databricks, networking needs to be set up correctly. This includes deploying the Hopsworks Instance to either the same VPC or enable VPC/VNet peering between the VPC/VNet of the Databricks Cluster and the Hopsworks Cluster.</p>"},{"location":"user_guides/integrations/databricks/networking/#aws","title":"AWS","text":""},{"location":"user_guides/integrations/databricks/networking/#step-1-ensure-network-connectivity","title":"Step 1: Ensure network connectivity","text":"<p>The DataFrame API needs to be able to connect directly to the IP on which the Feature Store is listening. This means that if you deploy the Feature Store on AWS you will either need to deploy the Feature Store in the same VPC as your Databricks cluster or to set up VPC Peering between your Databricks VPC and the Feature Store VPC.</p>"},{"location":"user_guides/integrations/databricks/networking/#option-1-deploy-the-feature-store-in-the-databricks-vpc","title":"Option 1: Deploy the Feature Store in the Databricks VPC","text":"<p>When you deploy the Feature Store Hopsworks instance, select the Databricks VPC and Availability Zone as the VPC and Availability Zone of your Feature Store cluster. Identify your Databricks VPC by searching for VPCs containing Databricks in their name in your Databricks AWS region in the AWS Management Console:</p> <p> Identify the Databricks VPC </p>"},{"location":"user_guides/integrations/databricks/networking/#option-2-set-up-vpc-peering","title":"Option 2: Set up VPC peering","text":"<p>Follow the guide VPC Peering to set up VPC peering between the Feature Store cluster and Databricks. Get your Feature Store VPC ID and CIDR by searching for the Feature Store VPC in the AWS Management Console:</p> <p>managed.hopsworks.ai</p> <p>On managed.hopsworks.ai, the VPC is shown in the cluster details.</p> <p> Identify the Feature Store VPC </p>"},{"location":"user_guides/integrations/databricks/networking/#step-2-configure-the-security-group","title":"Step 2: Configure the Security Group","text":"<p>The Feature Store Security Group needs to be configured to allow traffic from your Databricks clusters to be able to connect to the Feature Store.</p> <p>managed.hopsworks.ai</p> <p>If you deployed your Hopsworks Feature Store with managed.hopsworks.ai, you only need to enable outside access of the Feature Store, Online Feature Store, and Kafka services.</p> <p>Open your feature store instance under EC2 in the AWS Management Console and ensure that ports 443, 3306, 9083, 9085, 8020, 50010, and 9092 are reachable from the Databricks Security Group:</p> <p> Hopsworks Feature Store Security Group </p> <p>Connectivity from the Databricks Security Group can be allowed by opening the Security Group, adding a port to the Inbound rules and searching for dbe-worker in the source field. Selecting any of the dbe-worker Security Groups will be sufficient:</p> <p> Hopsworks Feature Store Security Group details </p>"},{"location":"user_guides/integrations/databricks/networking/#azure","title":"Azure","text":""},{"location":"user_guides/integrations/databricks/networking/#step-1-set-up-vnet-peering-between-hopsworks-and-databricks","title":"Step 1: Set up VNet peering between Hopsworks and Databricks","text":"<p>VNet peering between the Hopsworks and the Databricks virtual network is required to be able to connect to the Feature Store from Databricks.</p> <p>In the Azure portal, go to Azure Databricks and go to Virtual Network Peering:</p> <p> Azure Databricks </p> <p>Select Add Peering:</p> <p> Add peering </p> <p>Name the peering and select the virtual network used by your Hopsworks cluster. The virtual network is shown in the cluster details on managed.hopsworks.ai (see the next picture). Ensure to press the copy button on the bottom of the page and save the value somewhere. Press Add and create the peering:</p> <p> Configure peering </p> <p>The virtual network used by your cluster is shown under Details:</p> <p> Check the Hopsworks virtual network </p> <p>The peering connection should now be listed as initiated:</p> <p> Peering connection initiated </p> <p>On the Azure portal, go to Virtual networks and search for the virtual network used by your Hopsworks cluster:</p> <p> Virtual networks </p> <p>Open the network and select Peerings:</p> <p> Select peerings </p> <p>Choose to add a peering connection:</p> <p> Add a peering connection </p> <p>Name the peering connection and select I know my resource ID. Paste the string copied when creating the peering from Databricks Azure. If you haven't copied that string, then manually select the virtual network used by Databricks and press OK to create the peering:</p> <p> Configure peering </p> <p>The peering should now be Updating:</p> <p> Cloud account settings </p> <p>Wait for the peering to show up as Connected. There should now be bi-directional network connectivity between the Feature Store and Databricks:</p> <p> Cloud account settings </p>"},{"location":"user_guides/integrations/databricks/networking/#step-2-configure-the-network-security-group","title":"Step 2: Configure the Network Security Group","text":"<p>The virtual network peering will allow full access between the Hopsworks virtual network and the Databricks virtual network by default. However, if you have a different setup, ensure that the Network Security Group of the Feature Store is configured to allow traffic from your Databricks clusters.</p> <p>Ensure that ports 443, 9083, 9085, 8020, 50010, and 9092 are reachable from the Databricks cluster Network Security Group.</p> <p>managed.hopsworks.ai</p> <p>If you deployed your Hopsworks Feature Store instance with managed.hopsworks.ai, it suffices to enable outside access of the Feature Store, Online Feature Store, and Kafka services.</p>"},{"location":"user_guides/integrations/databricks/networking/#next-steps","title":"Next Steps","text":"<p>Continue with the Hopsworks API key guide to setup access to a Hopsworks API key from the Databricks Cluster, in order to be able to use the Hopsworks Feature Store.</p>"},{"location":"user_guides/integrations/emr/emr_configuration/","title":"Configure EMR for the Hopsworks Feature Store","text":"<p>To enable EMR to access the Hopsworks Feature Store, you need to set up a Hopsworks API key, add a bootstrap action and configurations to your EMR cluster.</p> <p>Info</p> <p>Ensure Networking is set up correctly before proceeding with this guide.</p>"},{"location":"user_guides/integrations/emr/emr_configuration/#step-1-set-up-a-hopsworks-api-key","title":"Step 1: Set up a Hopsworks API key","text":"<p>For instructions on how to generate an API key follow this user guide. For the EMR integration to work correctly make sure you add the following scopes to your API key:</p> <ol> <li>featurestore</li> <li>project</li> <li>job</li> <li>kafka</li> </ol>"},{"location":"user_guides/integrations/emr/emr_configuration/#store-the-api-key-in-the-aws-secrets-manager","title":"Store the API key in the AWS Secrets Manager","text":"<p>In the AWS management console ensure that your active region is the region you use for EMR. Go to the AWS Secrets Manager and select Store new secret. Select Other type of secrets and add api-key as the key and paste the API key created in the previous step as the value. Click next.</p> <p> Store a Hopsworks API key in the Secrets Manager </p> <p>As a secret name, enter hopsworks/featurestore. Select next twice and finally store the secret. Then click on the secret in the secrets list and take note of the Secret ARN.</p> <p> Name the secret </p>"},{"location":"user_guides/integrations/emr/emr_configuration/#grant-access-to-the-secret-to-the-emr-ec2-instance-profile","title":"Grant access to the secret to the EMR EC2 instance profile","text":"<p>Identify your EMR EC2 instance profile in the EMR cluster summary:</p> <p> Identify your EMR EC2 instance profile </p> <p>In the AWS Management Console, go to IAM, select Roles and then the EC2 instance profile used by your EMR cluster. Select Add inline policy. Choose Secrets Manager as a service, expand the Read access level and check GetSecretValue. Expand Resources and select Add ARN. Paste the ARN of the secret created in the previous step. Click on Review, give the policy a name and click on Create policy.</p> <p> Configure the access policy for the Secrets Manager </p>"},{"location":"user_guides/integrations/emr/emr_configuration/#step-2-configure-your-emr-cluster","title":"Step 2: Configure your EMR cluster","text":""},{"location":"user_guides/integrations/emr/emr_configuration/#add-the-hopsworks-feature-store-configuration-to-your-emr-cluster","title":"Add the Hopsworks Feature Store configuration to your EMR cluster","text":"<p>In order for EMR to be able to talk to the Feature Store, you need to update the Hadoop and Spark configurations. Copy the configuration below and replace ip-XXX-XX-XX-XXX.XX-XXXX-X.compute.internal with the private DNS name of your Hopsworks master node.</p> <pre><code>[\n  {\n    \"Classification\": \"hadoop-env\",\n    \"Properties\": {\n\n    },\n    \"Configurations\": [\n      {\n        \"Classification\": \"export\",\n        \"Properties\": {\n          \"HADOOP_CLASSPATH\": \"$HADOOP_CLASSPATH:/usr/lib/hopsworks/client/*\"\n        },\n        \"Configurations\": [\n\n        ]\n      }\n    ]\n  },\n  {\n    \"Classification\": \"spark-defaults\",\n    \"Properties\": {\n      \"spark.hadoop.hops.ipc.server.ssl.enabled\": true,\n      \"spark.hadoop.fs.hopsfs.impl\": \"io.hops.hopsfs.client.HopsFileSystem\",\n      \"spark.hadoop.client.rpc.ssl.enabled.protocol\": \"TLSv1.2\",\n      \"spark.hadoop.hops.ssl.hostname.verifier\": \"ALLOW_ALL\",\n      \"spark.hadoop.hops.rpc.socket.factory.class.default\": \"io.hops.hadoop.shaded.org.apache.hadoop.net.HopsSSLSocketFactory\",\n      \"spark.hadoop.hops.ssl.keystores.passwd.name\": \"/usr/lib/hopsworks/material_passwd\",\n      \"spark.hadoop.hops.ssl.keystore.name\": \"/usr/lib/hopsworks/keyStore.jks\",\n      \"spark.hadoop.hops.ssl.trustore.name\": \"/usr/lib/hopsworks/trustStore.jks\",\n      \"spark.serializer\": \"org.apache.spark.serializer.KryoSerializer\",\n      \"spark.executor.extraClassPath\": \"/usr/lib/hopsworks/client/*\",\n      \"spark.driver.extraClassPath\": \"/usr/lib/hopsworks/client/*\",\n      \"spark.sql.hive.metastore.jars\": \"path\",\n      \"spark.sql.hive.metastore.jars.path\": \"/usr/lib/hopsworks/apache-hive-bin/lib/*\",\n      \"spark.hadoop.hive.metastore.uris\": \"thrift://ip-XXX-XX-XX-XXX.XX-XXXX-X.compute.internal:9083\"\n    }\n  },\n]\n</code></pre> <p>When you create your EMR cluster, add the configuration:</p> <p>Note</p> <p>Don't forget to replace ip-XXX-XX-XX-XXX.XX-XXXX-X.compute.internal with the private DNS name of your Hopsworks master node.</p> <p> Configure EMR to access the Feature Store </p>"},{"location":"user_guides/integrations/emr/emr_configuration/#add-the-bootstrap-action-to-your-emr-cluster","title":"Add the Bootstrap Action to your EMR cluster","text":"<p>EMR requires Hopsworks connectors to be able to communicate with the Hopsworks Feature Store. These connectors can be installed with the bootstrap action shown below. Copy the content into a file and name the file <code>hopsworks.sh</code>. Copy that file into any S3 bucket that is readable by your EMR clusters and take note of the S3 URI of that file e.g., <code>s3://my-emr-init/hopsworks.sh</code>.</p> <pre><code>#!/bin/bash\nset -e\n\nif [ \"$#\" -ne 3 ]; then\n    echo \"Usage hopsworks.sh HOPSWORKS_API_KEY_SECRET, HOPSWORKS_HOST, PROJECT_NAME\"\n    exit 1\nfi\n\nSECRET_NAME=$1\nHOST=$2\nPROJECT=$3\n\nAPI_KEY=$(aws secretsmanager get-secret-value --secret-id $SECRET_NAME | jq -r .SecretString | jq -r '.[\"api-key\"]')\n\nPROJECT_ID=$(curl -H \"Authorization: ApiKey ${API_KEY}\" https://$HOST/hopsworks-api/api/project/getProjectInfo/$PROJECT | jq -r .projectId)\n\nsudo yum -y install python3-devel.x86_64 || true\n\nsudo mkdir /usr/lib/hopsworks\nsudo chown hadoop:hadoop /usr/lib/hopsworks\ncd /usr/lib/hopsworks\n\ncurl -o client.tar.gz -H \"Authorization: ApiKey ${API_KEY}\" https://$HOST/hopsworks-api/api/project/$PROJECT_ID/client\n\ntar -xvf client.tar.gz\ntar -xzf client/apache-hive-*-bin.tar.gz || true\nmv apache-hive-*-bin apache-hive-bin\nrm client.tar.gz\nrm client/apache-hive-*-bin.tar.gz\n\ncurl -H \"Authorization: ApiKey ${API_KEY}\" https://$HOST/hopsworks-api/api/project/$PROJECT_ID/credentials | jq -r .kStore | base64 -d &gt; keyStore.jks\n\ncurl -H \"Authorization: ApiKey ${API_KEY}\" https://$HOST/hopsworks-api/api/project/$PROJECT_ID/credentials | jq -r .tStore | base64 -d &gt; trustStore.jks\n\necho -n $(curl -H \"Authorization: ApiKey ${API_KEY}\" https://$HOST/hopsworks-api/api/project/$PROJECT_ID/credentials | jq -r .password) &gt; material_passwd\n\nchmod -R o-rwx /usr/lib/hopsworks\n\nsudo pip3 install --upgrade hopsworks~=X.X.0\n</code></pre> <p>Matching Hopsworks version</p> <p>We recommend that the major and minor version of the Python library match the major and minor version of the Hopsworks deployment.</p> <p><p> You find the Hopsworks version inside any of your Project's settings tab on Hopsworks </p></p> <p>Add the bootstrap actions when configuring your EMR cluster. Provide 3 arguments to the bootstrap action: The name of the API key secret e.g., <code>hopsworks/featurestore</code>, the public DNS name of your Hopsworks cluster, such as <code>ad005770-33b5-11eb-b5a7-bfabd757769f.cloud.hopsworks.ai</code>, and the name of your Hopsworks project, e.g. <code>demo_fs_meb10179</code>.</p> <p> Set the bootstrap action for EMR </p> <p>Your EMR cluster will now be able to access your Hopsworks Feature Store.</p>"},{"location":"user_guides/integrations/emr/emr_configuration/#next-steps","title":"Next Steps","text":"<p>Use the Login API to connect to the Hopsworks Feature Store. For more information about how to use the Feature Store, see the Quickstart Guide.</p>"},{"location":"user_guides/integrations/emr/networking/","title":"Networking","text":"<p>In order for Spark to communicate with the Hopsworks Feature Store from EMR, networking needs to be set up correctly. This includes deploying the Hopsworks Feature Store to either the same VPC or enable VPC peering between the VPC of the EMR cluster and the Hopsworks Feature Store.</p>"},{"location":"user_guides/integrations/emr/networking/#step-1-ensure-network-connectivity","title":"Step 1: Ensure network connectivity","text":"<p>The DataFrame API needs to be able to connect directly to the IP on which the Feature Store is listening. This means that if you deploy the Feature Store on AWS you will either need to deploy the Feature Store in the same VPC as your EMR cluster or to set up VPC Peering between your EMR VPC and the Feature Store VPC.</p>"},{"location":"user_guides/integrations/emr/networking/#option-1-deploy-the-feature-store-in-the-emr-vpc","title":"Option 1: Deploy the Feature Store in the EMR VPC","text":"<p>When deploying the Hopsworks Feature Store, select the EMR VPC and Availability Zone as the VPC and Availability Zone of your Feature Store. Identify your EMR VPC in the Summary of your EMR cluster:</p> <p> Identify the EMR VPC </p> <p> Identify the EMR VPC </p>"},{"location":"user_guides/integrations/emr/networking/#option-2-set-up-vpc-peering","title":"Option 2: Set up VPC peering","text":"<p>Follow the guide VPC Peering to set up VPC peering between the Feature Store and EMR. Get your Feature Store VPC ID and CIDR by searching for the Feature Store VPC in the AWS Management Console:</p> <p> Identify the Feature Store VPC </p>"},{"location":"user_guides/integrations/emr/networking/#step-2-configure-the-security-group","title":"Step 2: Configure the Security Group","text":"<p>The Feature Store Security Group needs to be configured to allow traffic from your EMR clusters to be able to connect to the Feature Store.</p> <p>Open your feature store instance under EC2 in the AWS Management Console and ensure that ports 443, 3306, 9083, 9085, 8020 and 30010 (443,3306,8020,30010,9083,9085) are reachable from the EMR Security Group:</p> <p> Hopsworks Feature Store Security Group </p> <p>Connectivity from the EMR Security Group can be allowed by opening the Security Group, adding a port to the Inbound rules and setting the EMR master and core security group as source:</p> <p> Hopsworks Feature Store Security Group details </p> <p>You can find your EMR security groups in the EMR cluster summary:</p> <p> EMR Security Groups </p>"},{"location":"user_guides/integrations/emr/networking/#next-steps","title":"Next Steps","text":"<p>Continue with the Configure EMR for the Hopsworks Feature Store, in order to be able to use the Hopsworks Feature Store.</p>"},{"location":"user_guides/migration/40_migration/","title":"4.0 Migration Guide","text":""},{"location":"user_guides/migration/40_migration/#breaking-changes","title":"Breaking Changes","text":"<p>With the release of Hopsworks 4.0, a number of necessary breaking changes have been put in place to improve the overall experience of using the Hopsworks platform. These breaking changes can be categorized in the following areas:</p> <ul> <li> <p>Python API</p> </li> <li> <p>Multi-Environment Docker Images</p> </li> <li> <p>On-Demand Transformation Functions</p> </li> </ul>"},{"location":"user_guides/migration/40_migration/#python-api","title":"Python API","text":"<p>A number of significant changes have been made in the Python API Hopsworks 4.0. Previously, in Hopsworks 3.X, there were 3 python libraries used (\u201chopsworks\u201d, \u201chsfs\u201d &amp; \u201chsml\u201d) to develop feature, training &amp; inference pipelines, with the 4.0 release there is now one single \u201chopsworks\u201d python library that should be used. For backwards compatibility, it is still possible to import both the \u201chsfs\u201d &amp; \u201chsml\u201d packages directly, but the proper way to import them is to use \u201chopsworks.hsfs\u201d &amp; \u201chopsworks.hsml\u201d. The direct imports will be deprecated later.</p> <p>Another significant change in the Hopsworks Python API is the use of optional extras to allow a developer to easily import exactly what is needed as part of their work. The main ones are great-expectations and polars. It is arguable whether this is a breaking change but it is important to note depending on how a particular pipeline has been written which may encounter a problem when executing using Hopsworks 4.0.</p> <p>Finally, there are a number of relatively small breaking changes and deprecated methods to improve the developer experience, these include:</p> <ul> <li> <p>connection.init() is now considered deprecated</p> </li> <li> <p>When loading arrow_flight_client, an OptionalDependencyNotFoundError can be now thrown providing more detailed information on the error than the previous ModuleNotFoundError in 3.X.</p> </li> <li> <p>DatasetApi's zip and unzip will now return False when a timeout is exceeded instead of previously throwing an Exception</p> </li> </ul>"},{"location":"user_guides/migration/40_migration/#multi-environment-docker-images","title":"Multi-Environment Docker Images","text":"<p>As part of the Hopsworks 4.0 release, an engineering team using Hopsworks can now customize the docker images that they use for their feature, training and inference pipelines. By adding this flexibility, a set of breaking changes are necessary. Instead of having one common docker image for fti pipelines, with the release of 4.0 a number of specific docker images are provided to allow an engineering team using Hopsworks to install exactly what they need to get their feature, training and inference pipelines up and running. This breaking change will require existing customers running Hopsworks 3.X to test their existing pipelines using Hopsworks 4.0 before upgrading their production environments.</p>"},{"location":"user_guides/migration/40_migration/#on-demand-transformation-functions","title":"On-Demand Transformation Functions","text":"<p>A number of changes have been made to transformation functions in the last releases of Hopsworks. With 4.0, On-Demand Transformation Functions are now better supported which has resulted in some breaking changes. The following is how transformation functions were used in previous versions of Hopsworks and the how transformation functions are used in the 4.0 release.</p> Pre-4.04.0 <pre><code>#################################################\n# Creating transformation function Hopsworks 3.8#\n#################################################\n\n# Define custom transformation function\ndef add_one(feature):\n    return feature + 1\n\n# Create transformation function\nadd_one = fs.create_transformation_function(add_one,\n    output_type=int,\n    version=1,\n)\n\n# Save transformation function\nadd_one.save()\n\n# Retrieve transformation function\nscaler = fs.get_transformation_function(\n    name=\"add_one\",\n    version=1,\n)\n\n# Create feature view\nfeature_view = fs.get_or_create_feature_view(\n    name='serving_fv',\n    version=1,\n    query=selected_features,\n    # Apply your custom transformation functions to the feature `feature_1`\n    transformation_functions={\n        \"feature_1\": add_one,\n    },\n    labels=['target'],\n)\n</code></pre> <pre><code>#################################################\n# Creating transformation function Hopsworks 4.0#\n#################################################\n\n# Define custom transformation function\n@hopsworks.udf(int)\ndef add_one(feature):\n    return feature + 1\n\n# Create feature view\nfeature_view = fs.get_or_create_feature_view(\n    name='serving_fv',\n    version=1,\n    query=selected_features,\n    # Apply the custom transformation functions defined to the feature `feature_1`\n    transformation_functions=[\n        add_one(\"feature_1\"),\n    ],\n    labels=['target'],\n)\n</code></pre> <p>Note that the number of lines of code required has been significantly reduced using the \u201c@hopsworks.udf\u201d python decorator.</p>"},{"location":"user_guides/mlops/","title":"Model Registry &amp; Serving Guides","text":"<p>This section serves to provide guides and examples for the common usage of abstractions and functionality of Models and Deployments through the Hopsworks UI and APIs.</p> <ul> <li>Model Registry</li> <li>Model Serving</li> <li>Vector Database</li> </ul>"},{"location":"user_guides/mlops/provenance/provenance/","title":"Provenance","text":""},{"location":"user_guides/mlops/provenance/provenance/#introduction","title":"Introduction","text":"<p>Hopsworks allows users to track provenance (lineage) between:</p> <ul> <li>data sources</li> <li>feature groups</li> <li>feature views</li> <li>training datasets</li> <li>models</li> </ul> <p>In the provenance pages we will call a provenance artifact or shortly artifact, any of the five entities above.</p> <p>With the following provenance graph:</p> <pre><code>data source -&gt; feature group -&gt; feature group -&gt; feature view -&gt; training dataset -&gt; model\n</code></pre> <p>we will call the parent, the artifact to the left, and the child, the artifact to the right. So a feature view has a number of feature groups as parents and can have a number of training datasets as children.</p> <p>Tracking provenance allows users to determine where and if an artifact is being used. You can track, for example, if feature groups are being used to create additional (derived) feature groups or feature views, or if their data is eventually used to train models.</p> <p>You can interact with the provenance graph using the UI or the APIs.</p>"},{"location":"user_guides/mlops/provenance/provenance/#model-provenance","title":"Model provenance","text":"<p>The relationship between feature views and models is captured in the model constructor. If you do not provide at least the feature view object to the constructor, the provenance will not capture this relation and you will not be able to navigate from model to the feature view it used or from the feature view to this model.</p> <p>You can provide the feature view object and have the training dataset version be inferred.</p> Python <pre><code># this fv object will be provided to the model constructor\nfv = hsfs.get_feature_view(...)\n\n# when calling training data related methods on the feature view, the training dataset version is cached in the feature view and is implicitly provided to the model constructor\nX_train, X_test, y_train, y_test = feature_view.train_test_split(...)\n\n# provide the feature_view object in the model constructor\nhsml.model_registry.ModelRegistry.python.create_model(\n    ...\n    feature_view = fv\n    ...)\n</code></pre> <p>You can of course explicitly provide the training dataset version.</p> Python <pre><code># this object will be provided to the model constructor\nfv = hsfs.get_feature_view(...)\n\n# this training dataset version will be provided to the model constructor\nX_train, X_test, y_train, y_test = feature_view.get_train_test_split(training_dataset_version=1)\n\n# provide the feature_view object in the model constructor\nhsml.model_registry.ModelRegistry.python.create_model(\n    ...\n    feature_view = fv,\n    training_dataset_version = 1,\n    ...)\n</code></pre> <p>Once the relation is stored in the provenance graph, you can navigate the graph from model to feature view or training dataset and the other way around.</p> <p>Users can call the <code>Model.get_feature_view_provenance</code> method or the <code>Model.get_training_dataset_provenance</code> method which will each return a provenance Link object.</p> <p>You can also retrieve directly the parent feature view object, without the need to extract them from the provenance links object, using the <code>Model.get_feature_view</code> method.</p> Python <pre><code>feature_view = model.get_feature_view()\n</code></pre> <p>This utility method also has the options to initialize the required components for batch or online retrieval of feature vectors.</p> Python <pre><code>model.get_feature_view(init: bool = True, online: Optional[bool]: None)\n</code></pre> <p>By default, the base init for feature vector retrieval is enabled. In case you have a workflow that requires more particular options, you can disable this base init by setting the <code>init</code> to <code>false</code>. The method detects if it is running within a deployment and will initialize the feature vector retrieval for the serving. If the <code>online</code> argument is provided and <code>true</code> it will initialize for online feature vector retrieval. If the <code>online</code> argument is provided and <code>false</code> it will initialize the feature vector retrieval for batch scoring.</p>"},{"location":"user_guides/mlops/provenance/provenance/#using-the-ui","title":"Using the UI","text":"<p>In the model overview UI you can explore the provenance graph of the model:</p> <p> Provenance graph of derived feature groups </p>"},{"location":"user_guides/mlops/provenance/provenance/#provenance-links","title":"Provenance Links","text":"<p>All the <code>_provenance</code> methods return a <code>Link</code> dictionary object that contains <code>accessible</code>, <code>inaccessible</code>, <code>deleted</code> lists.</p> <ul> <li><code>accessible</code> - contains any artifact from the result, that the user has access to.</li> <li><code>inaccessible</code> - contains any artifacts that might have been shared at some point in the past, but where this sharing was retracted.   Since the relation between artifacts is still maintained in the provenance, the user will only have access to limited metadata and the artifacts will be included in this <code>inaccessible</code> list.</li> <li><code>deleted</code> - contains artifacts that are deleted with children still present in the system.   There is minimum amount of metadata for the deleted allowing for some limited human readable identification.</li> </ul>"},{"location":"user_guides/mlops/registry/","title":"Model Registry Guides","text":"<p>Hopsworks Model Registry is a centralized repository, within an organization, to manage machine learning models. A model is the product of training a machine learning algorithm with training data.</p> <p>This section provides guides for creating models and publish them to the Model Registry to make them available for download for batch predictions, or deployed to serve realtime applications.</p>"},{"location":"user_guides/mlops/registry/#exporting-a-model","title":"Exporting a model","text":"<p>Follow these framework-specific guides to export a Model to the Model Registry.</p> <ul> <li> <p>TensorFlow</p> </li> <li> <p>Torch</p> </li> <li> <p>Scikit-learn</p> </li> <li> <p>LLM</p> </li> <li> <p>Other Python frameworks</p> </li> </ul>"},{"location":"user_guides/mlops/registry/#model-schema","title":"Model Schema","text":"<p>A Model schema describes the input and outputs for a model. It provides a functional description of the model which makes it simpler to get started working with it. For example if the model inputs a tensor, the model schema can define the shape and data type of the tensor.</p>"},{"location":"user_guides/mlops/registry/#input-example","title":"Input Example","text":"<p>An Input example provides an instance of a valid model input. Input examples are stored with the model as separate artifacts.</p>"},{"location":"user_guides/mlops/registry/input_example/","title":"How To Attach An Input Example","text":""},{"location":"user_guides/mlops/registry/input_example/#introduction","title":"Introduction","text":"<p>In this guide you will learn how to attach an input example to a model. An input example is simply an instance of a valid model input. Attaching an input example to your model will give other users a better understanding of what data it expects.</p>"},{"location":"user_guides/mlops/registry/input_example/#code","title":"Code","text":""},{"location":"user_guides/mlops/registry/input_example/#step-1-connect-to-hopsworks","title":"Step 1: Connect to Hopsworks","text":"Python <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n</code></pre>"},{"location":"user_guides/mlops/registry/input_example/#step-2-generate-an-input-example","title":"Step 2: Generate an input example","text":"<p>Generate an input example which corresponds to a valid input to your model. Currently we support <code>pandas.DataFrame, pandas.Series, numpy.ndarray, list</code> to be passed as input example.</p> Python <pre><code>import numpy as np\n\ninput_example = np.random.randint(0, high=256, size=784, dtype=np.uint8)\n</code></pre>"},{"location":"user_guides/mlops/registry/input_example/#step-3-set-input_example-parameter","title":"Step 3: Set input_example parameter","text":"<p>Set the <code>input_example</code> parameter in the <code>create_model</code> function and call <code>save()</code> to attaching it to the model and register it in the registry.</p> Python <pre><code>model = mr.tensorflow.create_model(name=\"mnist\",\n                                input_example=input_example)\nmodel.save(\"./model\")\n</code></pre>"},{"location":"user_guides/mlops/registry/model_evaluation_images/","title":"How To Save Model Evaluation Images","text":""},{"location":"user_guides/mlops/registry/model_evaluation_images/#introduction","title":"Introduction","text":"<p>In this guide, you will learn how to attach model evaluation images to a model. Model evaluation images are images that visually describe model performance metrics. For example, confusion matrices, ROC curves, model bias tests, and training loss curves are examples of common model evaluation images. By attaching model evaluation images to your versioned model, other users can better understand the model performance and evaluation metrics.</p>"},{"location":"user_guides/mlops/registry/model_evaluation_images/#code","title":"Code","text":""},{"location":"user_guides/mlops/registry/model_evaluation_images/#step-1-connect-to-hopsworks","title":"Step 1: Connect to Hopsworks","text":"Python <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n</code></pre>"},{"location":"user_guides/mlops/registry/model_evaluation_images/#step-2-generate-model-evaluation-images","title":"Step 2: Generate model evaluation images","text":"<p>Generate an image that visualizes model performance and evaluation metrics</p> Python <pre><code>import seaborn\nfrom sklearn.metrics import confusion_matrix\n\n# Predict the training data using the trained model\ny_pred_train = model.predict(X_train)\n\n# Predict the test data using the trained model\ny_pred_test = model.predict(X_test)\n\n# Calculate and print the confusion matrix for the test predictions\nresults = confusion_matrix(y_test, y_pred_test)\n\n# Create a DataFrame for the confusion matrix results\ndf_confusion_matrix = pd.DataFrame(\n    results,\n    ['True Normal', 'True Fraud'],\n    ['Pred Normal', 'Pred Fraud'],\n)\n\n# Create a heatmap using seaborn with annotations\nheatmap = seaborn.heatmap(df_confusion_matrix, annot=True)\n\n# Get the figure and display it\nfig = heatmap.get_figure()\nfig.show()\n</code></pre>"},{"location":"user_guides/mlops/registry/model_evaluation_images/#step-3-save-the-figure-to-a-file-inside-the-model-directory","title":"Step 3: Save the figure to a file inside the model directory","text":"<p>Save the figure to a file with a common filename extension (for example, .png or .jpeg), and place it in a directory called <code>images</code> - a subdirectory of the model directory that is registered to Hopsworks.</p> Python <pre><code># Specify the directory name for saving the model and related artifacts\nmodel_dir = \"./model\"\n\n# Create a subdirectory of model_dir called 'images' for saving the model evaluation images\nmodel_images_dir = model_dir + \"/images\"\nif not os.path.exists(model_images_dir):\n    os.mkdir(model_images_dir)\n\n# Save the figure to an image file in the images directory\nfig.savefig(model_images_dir + \"/confusion_matrix.png\")\n\n# Register the model\npy_model = mr.python.create_model(name=\"py_model\")\npy_model.save(\"./model\")\n</code></pre>"},{"location":"user_guides/mlops/registry/model_schema/","title":"How To Attach A Model Schema","text":""},{"location":"user_guides/mlops/registry/model_schema/#introduction","title":"Introduction","text":"<p>In this guide you will learn how to attach a model schema to your model. A model schema, describes the type and shape of inputs and outputs (predictions) for your model. Attaching a model schema to your model will give other users a better understanding of what data it expects.</p>"},{"location":"user_guides/mlops/registry/model_schema/#code","title":"Code","text":""},{"location":"user_guides/mlops/registry/model_schema/#step-1-connect-to-hopsworks","title":"Step 1: Connect to Hopsworks","text":"Python <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n</code></pre>"},{"location":"user_guides/mlops/registry/model_schema/#step-2-create-modelschema","title":"Step 2: Create ModelSchema","text":"<p>Create a ModelSchema for your inputs and outputs by passing in an example that your model is trained on and a valid prediction. Currently, we support <code>pandas.DataFrame, pandas.Series, numpy.ndarray, list</code>.</p> Python <pre><code># Import a Schema and ModelSchema definition\nfrom hsml.utils.model_schema import ModelSchema\nfrom hsml.utils.schema import Schema\n\n# Model inputs for MNIST dataset\ninputs = [{'type': 'uint8', 'shape': [28, 28, 1], 'description': 'grayscale representation of 28x28 MNIST images'}]\n\n# Build the input schema\ninput_schema = Schema(inputs)\n\n# Model outputs\noutputs = [{'type': 'float32', 'shape': [10]}]\n\n# Build the output schema\noutput_schema = Schema(outputs)\n\n# Create ModelSchema object\nmodel_schema = ModelSchema(input_schema=input_schema, output_schema=output_schema)\n</code></pre>"},{"location":"user_guides/mlops/registry/model_schema/#step-3-set-model_schema-parameter","title":"Step 3: Set model_schema parameter","text":"<p>Set the <code>model_schema</code> parameter in the <code>create_model</code> function and call <code>save()</code> to attaching it to the model and register it in the registry.</p> Python <pre><code>model = mr.tensorflow.create_model(name=\"mnist\",\n                                model_schema=model_schema)\nmodel.save(\"./model\")\n</code></pre>"},{"location":"user_guides/mlops/registry/frameworks/llm/","title":"How To Export a Large Language Model (LLM)","text":""},{"location":"user_guides/mlops/registry/frameworks/llm/#introduction","title":"Introduction","text":"<p>In this guide you will learn how to export a Large Language Model (LLM) and register it in the Model Registry.</p>"},{"location":"user_guides/mlops/registry/frameworks/llm/#code","title":"Code","text":""},{"location":"user_guides/mlops/registry/frameworks/llm/#step-1-connect-to-hopsworks","title":"Step 1: Connect to Hopsworks","text":"Python <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n</code></pre>"},{"location":"user_guides/mlops/registry/frameworks/llm/#step-2-download-the-llm","title":"Step 2: Download the LLM","text":"<p>Download your base or fine-tuned LLM. LLMs can typically be downloaded using the official frameworks provided by their creators (e.g., HuggingFace, Ollama, ...)</p> Python <pre><code># Download LLM (e.g., using huggingface to download Llama-3.1-8B base model)\nfrom huggingface_hub import snapshot_download\n\nmodel_dir = snapshot_download(\n                \"meta-llama/Llama-3.1-8B\",\n                ignore_patterns=\"original/*\"\n            )\n</code></pre>"},{"location":"user_guides/mlops/registry/frameworks/llm/#step-3-optional-fine-tune-llm","title":"Step 3: (Optional) Fine-tune LLM","text":"<p>If necessary, fine-tune your LLM with an instruction set. A LLM can be fine-tuned fully or using Parameter Efficient Fine Tuning (PEFT) methods such as LoRA or QLoRA.</p> Python <pre><code># Fine-tune LLM using PEFT (LoRA, QLoRA) or other methods\nmodel_dir = ...\n</code></pre>"},{"location":"user_guides/mlops/registry/frameworks/llm/#step-4-register-model-in-registry","title":"Step 4: Register model in registry","text":"<p>Use the <code>ModelRegistry.llm.create_model(..)</code> function to register a model as LLM. Define a name, and attach optional metrics for your model, then invoke the <code>save()</code> function with the parameter being the path to the local directory where the model was exported to.</p> Python <pre><code># Model evaluation metrics\nmetrics = {'f1-score': 0.8, 'perplexity': 31.62, 'bleu-score': 0.73}\n\nllm_model = mr.llm.create_model(\"llm_model\", metrics=metrics)\n\nllm_model.save(model_dir)\n</code></pre>"},{"location":"user_guides/mlops/registry/frameworks/llm/#going-further","title":"Going Further","text":"<p>You can attach an Input Example and a Model Schema to your model to document the shape and type of the data the model was trained on.</p>"},{"location":"user_guides/mlops/registry/frameworks/python/","title":"How To Export a Python Model","text":""},{"location":"user_guides/mlops/registry/frameworks/python/#introduction","title":"Introduction","text":"<p>In this guide you will learn how to export a generic Python model and register it in the Model Registry.</p>"},{"location":"user_guides/mlops/registry/frameworks/python/#code","title":"Code","text":""},{"location":"user_guides/mlops/registry/frameworks/python/#step-1-connect-to-hopsworks","title":"Step 1: Connect to Hopsworks","text":"Python <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n</code></pre>"},{"location":"user_guides/mlops/registry/frameworks/python/#step-2-train","title":"Step 2: Train","text":"<p>Define your XGBoost model and run the training loop.</p> Python <pre><code># Define a model\nmodel = XGBClassifier()\n\n# Train model\nmodel.fit(X_train, y_train)\n</code></pre>"},{"location":"user_guides/mlops/registry/frameworks/python/#step-3-export-to-local-path","title":"Step 3: Export to local path","text":"<p>Export the XGBoost model to a directory on the local filesystem.</p> Python <pre><code>model_file = \"model.json\"\n\nmodel.save_model(model_file)\n</code></pre>"},{"location":"user_guides/mlops/registry/frameworks/python/#step-4-register-model-in-registry","title":"Step 4: Register model in registry","text":"<p>Use the <code>ModelRegistry.python.create_model(..)</code> function to register a model as a Python model. Define a name, and attach optional metrics for your model, then invoke the <code>save()</code> function with the parameter being the path to the local directory where the model was exported to.</p> Python <pre><code># Model evaluation metrics\nmetrics = {'accuracy': 0.92}\n\npy_model = mr.python.create_model(\"py_model\", metrics=metrics)\n\npy_model.save(model_dir)\n</code></pre>"},{"location":"user_guides/mlops/registry/frameworks/python/#going-further","title":"Going Further","text":"<p>You can attach an Input Example and a Model Schema to your model to document the shape and type of the data the model was trained on.</p>"},{"location":"user_guides/mlops/registry/frameworks/skl/","title":"How To Export a Scikit-learn Model","text":""},{"location":"user_guides/mlops/registry/frameworks/skl/#introduction","title":"Introduction","text":"<p>In this guide you will learn how to export a Scikit-learn model and register it in the Model Registry.</p>"},{"location":"user_guides/mlops/registry/frameworks/skl/#code","title":"Code","text":""},{"location":"user_guides/mlops/registry/frameworks/skl/#step-1-connect-to-hopsworks","title":"Step 1: Connect to Hopsworks","text":"Python <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n</code></pre>"},{"location":"user_guides/mlops/registry/frameworks/skl/#step-2-train","title":"Step 2: Train","text":"<p>Define your Scikit-learn model and run the training loop.</p> Python <pre><code># Define a model\niris_knn = KNeighborsClassifier(..)\n\niris_knn.fit(..)\n</code></pre>"},{"location":"user_guides/mlops/registry/frameworks/skl/#step-3-export-to-local-path","title":"Step 3: Export to local path","text":"<p>Export the Scikit-learn model to a directory on the local filesystem.</p> Python <pre><code>model_file = \"skl_knn.pkl\"\n\njoblib.dump(iris_knn, model_file)\n</code></pre>"},{"location":"user_guides/mlops/registry/frameworks/skl/#step-4-register-model-in-registry","title":"Step 4: Register model in registry","text":"<p>Use the <code>ModelRegistry.sklearn.create_model(..)</code> function to register a model as a Scikit-learn model. Define a name, and attach optional metrics for your model, then invoke the <code>save()</code> function with the parameter being the path to the local directory where the model was exported to.</p> Python <pre><code># Model evaluation metrics\nmetrics = {'accuracy': 0.92}\n\nskl_model = mr.sklearn.create_model(\"skl_model\", metrics=metrics)\n\nskl_model.save(model_file)\n</code></pre>"},{"location":"user_guides/mlops/registry/frameworks/skl/#going-further","title":"Going Further","text":"<p>You can attach an Input Example and a Model Schema to your model to document the shape and type of the data the model was trained on.</p>"},{"location":"user_guides/mlops/registry/frameworks/tch/","title":"How To Export a Torch Model","text":""},{"location":"user_guides/mlops/registry/frameworks/tch/#introduction","title":"Introduction","text":"<p>In this guide you will learn how to export a Torch model and register it in the Model Registry.</p>"},{"location":"user_guides/mlops/registry/frameworks/tch/#code","title":"Code","text":""},{"location":"user_guides/mlops/registry/frameworks/tch/#step-1-connect-to-hopsworks","title":"Step 1: Connect to Hopsworks","text":"Python <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n</code></pre>"},{"location":"user_guides/mlops/registry/frameworks/tch/#step-2-train","title":"Step 2: Train","text":"<p>Define your Torch model and run the training loop.</p> Python <pre><code># Define the model architecture\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        ...\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        ...\n        return x\n\n# Instantiate the model\nnet = Net()\n\n# Run the training loop\nfor epoch in range(n):\n    ...\n</code></pre>"},{"location":"user_guides/mlops/registry/frameworks/tch/#step-3-export-to-local-path","title":"Step 3: Export to local path","text":"<p>Export the Torch model to a directory on the local filesystem.</p> Python <pre><code>model_dir = \"./model\"\n\ntorch.save(net.state_dict(), model_dir)\n</code></pre>"},{"location":"user_guides/mlops/registry/frameworks/tch/#step-4-register-model-in-registry","title":"Step 4: Register model in registry","text":"<p>Use the <code>ModelRegistry.torch.create_model(..)</code> function to register a model as a Torch model. Define a name, and attach optional metrics for your model, then invoke the <code>save()</code> function with the parameter being the path to the local directory where the model was exported to.</p> Python <pre><code># Model evaluation metrics\nmetrics = {'accuracy': 0.92}\n\ntch_model = mr.torch.create_model(\"tch_model\", metrics=metrics)\n\ntch_model.save(model_dir)\n</code></pre>"},{"location":"user_guides/mlops/registry/frameworks/tch/#going-further","title":"Going Further","text":"<p>You can attach an Input Example and a Model Schema to your model to document the shape and type of the data the model was trained on.</p>"},{"location":"user_guides/mlops/registry/frameworks/tf/","title":"How To Export a TensorFlow Model","text":""},{"location":"user_guides/mlops/registry/frameworks/tf/#introduction","title":"Introduction","text":"<p>In this guide you will learn how to export a TensorFlow model and register it in the Model Registry.</p> <p>Save in SavedModel format</p> <p>Make sure the model is saved in the SavedModel format to be able to deploy it on TensorFlow Serving.</p>"},{"location":"user_guides/mlops/registry/frameworks/tf/#code","title":"Code","text":""},{"location":"user_guides/mlops/registry/frameworks/tf/#step-1-connect-to-hopsworks","title":"Step 1: Connect to Hopsworks","text":"Python <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n</code></pre>"},{"location":"user_guides/mlops/registry/frameworks/tf/#step-2-train","title":"Step 2: Train","text":"<p>Define your TensorFlow model and run the training loop.</p> Python <pre><code># Define a model\nmodel = tf.keras.Sequential()\n\n# Add layers\nmodel.add(..)\n\n# Compile the model.\nmodel.compile(..)\n\n# Train the model\nmodel.fit(..)\n</code></pre>"},{"location":"user_guides/mlops/registry/frameworks/tf/#step-3-export-to-local-path","title":"Step 3: Export to local path","text":"<p>Export the TensorFlow model to a directory on the local filesystem.</p> Python <pre><code>model_dir = \"./model\"\n\ntf.saved_model.save(model, model_dir)\n</code></pre>"},{"location":"user_guides/mlops/registry/frameworks/tf/#step-4-register-model-in-registry","title":"Step 4: Register model in registry","text":"<p>Use the <code>ModelRegistry.tensorflow.create_model(..)</code> function to register a model as a TensorFlow model. Define a name, and attach optional metrics for your model, then invoke the <code>save()</code> function with the parameter being the path to the local directory where the model was exported to.</p> Python <pre><code># Model evaluation metrics\nmetrics = {'accuracy': 0.92}\n\ntf_model = mr.tensorflow.create_model(\"tf_model\", metrics=metrics)\n\ntf_model.save(model_dir)\n</code></pre>"},{"location":"user_guides/mlops/registry/frameworks/tf/#going-further","title":"Going Further","text":"<p>You can attach an Input Example and a Model Schema to your model to document the shape and type of the data the model was trained on.</p>"},{"location":"user_guides/mlops/serving/","title":"Model Serving Guide","text":""},{"location":"user_guides/mlops/serving/#deployment","title":"Deployment","text":"<p>Assuming you have already created a model in the Model Registry, a deployment can now be created to prepare a model artifact for this model and make it accessible for running predictions behind a REST or gRPC endpoint. Follow the Deployment Creation Guide to create a Deployment for your model.</p>"},{"location":"user_guides/mlops/serving/#predictor","title":"Predictor","text":"<p>Predictors are responsible for running a model server that loads a trained model, handles inference requests and returns predictions, see the Predictor Guide.</p>"},{"location":"user_guides/mlops/serving/#transformer","title":"Transformer","text":"<p>Transformers are used to apply transformations on the model inputs before sending them to the predictor for making predictions using the model, see the Transformer Guide.</p>"},{"location":"user_guides/mlops/serving/#resource-allocation","title":"Resource Allocation","text":"<p>Configure the resources to be allocated for predictor and transformer in a model deployment, see the Resource Allocation Guide.</p>"},{"location":"user_guides/mlops/serving/#inference-batcher","title":"Inference Batcher","text":"<p>Configure the predictor to batch inference requests, see the Inference Batcher Guide.</p>"},{"location":"user_guides/mlops/serving/#inference-logger","title":"Inference Logger","text":"<p>Configure the predictor to log inference requests and predictions, see the Inference Logger Guide.</p>"},{"location":"user_guides/mlops/serving/#rest-api","title":"REST API","text":"<p>Send inference requests to deployed models using REST API, see the Rest API Guide.</p>"},{"location":"user_guides/mlops/serving/#troubleshooting","title":"Troubleshooting","text":"<p>Inspect the model server logs to troubleshoot your model deployments, see the Troubleshooting Guide.</p>"},{"location":"user_guides/mlops/serving/#external-access","title":"External access","text":"<p>Grant users authenticated by an external Identity Provider access to model deployments, see the External Access Guide.</p>"},{"location":"user_guides/mlops/serving/api-protocol/","title":"How to Select the API protocol for a Deployment","text":""},{"location":"user_guides/mlops/serving/api-protocol/#introduction","title":"Introduction","text":"<p>Hopsworks supports both REST and gRPC as API protocols for sending inference requests to model deployments. While REST API protocol is supported in all types of model deployments, support for gRPC is only available for models served with KServe.</p> <p>Warning</p> <p>At the moment, the gRPC API protocol is only supported for Python model deployments (e.g., scikit-learn, xgboost). Support for Tensorflow model deployments is coming soon.</p>"},{"location":"user_guides/mlops/serving/api-protocol/#gui","title":"GUI","text":""},{"location":"user_guides/mlops/serving/api-protocol/#step-1-create-a-new-deployment","title":"Step 1: Create a new deployment","text":"<p>If you have at least one model already trained and saved in the Model Registry, navigate to the deployments page by clicking on the <code>Deployments</code> tab on the navigation menu on the left.</p> <p> Deployments navigation tab </p> <p>Once in the deployments page, you can create a new deployment by either clicking on <code>New deployment</code> (if there are no existing deployments) or on <code>Create new deployment</code> it the top-right corner. Both options will open the deployment creation form.</p>"},{"location":"user_guides/mlops/serving/api-protocol/#step-2-go-to-advanced-options","title":"Step 2: Go to advanced options","text":"<p>A simplified creation form will appear including the most common deployment fields from all available configurations. Resource allocation is part of the advanced options of a deployment. To navigate to the advanced creation form, click on <code>Advanced options</code>.</p> <p> Advanced options. Go to advanced deployment creation form </p>"},{"location":"user_guides/mlops/serving/api-protocol/#step-3-select-the-api-protocol","title":"Step 3: Select the API protocol","text":"<p>Enabling gRPC as the API protocol for a model deployment requires KServe as the serving platform for the deployment. Make sure that KServe is enabled by activating the corresponding checkbox.</p> <p> Enable KServe in the advanced deployment form </p> <p>Then, you can select the API protocol to be enabled in your model deployment.</p> <p> Select gRPC API protocol </p> <p>Only one API protocol can be enabled in a model deployment (they cannot support both gRPC and REST)</p> <p>Currently, KServe model deployments are limited to one API protocol at a time. Therefore, only one of REST or gRPC API protocols can be enabled at the same time on the same model deployment. You cannot change the API protocol of existing deployments.</p> <p>Once you are done with the changes, click on <code>Create new deployment</code> at the bottom of the page to create the deployment for your model.</p>"},{"location":"user_guides/mlops/serving/api-protocol/#code","title":"Code","text":""},{"location":"user_guides/mlops/serving/api-protocol/#step-1-connect-to-hopsworks","title":"Step 1: Connect to Hopsworks","text":"Python <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n</code></pre>"},{"location":"user_guides/mlops/serving/api-protocol/#step-2-create-a-deployment-with-a-specific-api-protocol","title":"Step 2: Create a deployment with a specific API protocol","text":"Python <pre><code>my_model = mr.get_model(\"my_model\", version=1)\n\nmy_predictor = ms.create_predictor(\n    my_model,\n    api_protocol=\"GRPC\"  # defaults to \"REST\"\n)\nmy_predictor.deploy()\n\n# or\n\nmy_deployment = ms.create_deployment(my_predictor)\nmy_deployment.save()\n</code></pre>"},{"location":"user_guides/mlops/serving/api-protocol/#api-reference","title":"API Reference","text":"<p>API Protocol</p>"},{"location":"user_guides/mlops/serving/deployment-state/","title":"How To Inspect A Deployment State","text":""},{"location":"user_guides/mlops/serving/deployment-state/#introduction","title":"Introduction","text":"<p>In this guide, you will learn how to inspect the state of a deployment.</p> <p>A state can be seen as a snapshot of the current inner workings of a deployment. The following is the state transition diagram for deployments.</p> <p> State transitions of deployments </p> <p>States are composed of a status and a condition. While a status represents a high-level view of the state, conditions contain more detailed information closely related to infrastructure terms.</p>"},{"location":"user_guides/mlops/serving/deployment-state/#gui","title":"GUI","text":""},{"location":"user_guides/mlops/serving/deployment-state/#step-1-inspect-deployment-status","title":"Step 1: Inspect deployment status","text":"<p>If you have at least one deployment already created, navigate to the deployments page by clicking on the <code>Deployments</code> tab on the navigation menu on the left.</p> <p> Deployments navigation tab </p> <p>Once in the deployments page, find the deployment you want to inspect. Next to the actions buttons, you can find an indicator showing the current status of the deployment. This indicator changes its color based on the status.</p> <p>To inspect the condition of the deployment, click on the name of the deployment to open the deployment overview page.</p>"},{"location":"user_guides/mlops/serving/deployment-state/#step-2-inspect-condition","title":"Step 2: Inspect condition","text":"<p>Once in the deployment overview page, you can find the aforementioned status indicator at the top of page. Below it, a one-line message is shown with a more detailed description of the deployment status. This message is built using the current condition of the deployment.</p> <p> Deployments status condition </p>"},{"location":"user_guides/mlops/serving/deployment-state/#step-3-check-no-of-running-instances","title":"Step 3: Check n\u00ba of running instances","text":"<p>Additionally, you can find the n\u00ba of instances currently running by scrolling down to the <code>resource allocation</code> section.</p> <p> Resource allocation for a deployment </p> <p>Scale-to-zero capabilities</p> <p>If scale-to-zero capabilities are enabled, you can see how the n\u00ba of instances of a running deployment goes to zero and the status changes to <code>idle</code>. To enable scale-to-zero in a deployment, see Resource Allocation Guide</p>"},{"location":"user_guides/mlops/serving/deployment-state/#code","title":"Code","text":""},{"location":"user_guides/mlops/serving/deployment-state/#step-1-connect-to-hopsworks","title":"Step 1: Connect to Hopsworks","text":"Python <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n</code></pre>"},{"location":"user_guides/mlops/serving/deployment-state/#step-2-retrieve-an-existing-deployment","title":"Step 2: Retrieve an existing deployment","text":"Python <pre><code>deployment = ms.get_deployment(\"mydeployment\")\n</code></pre>"},{"location":"user_guides/mlops/serving/deployment-state/#step-3-inspect-deployment-state","title":"Step 3: Inspect deployment state","text":"Python <pre><code>state = deployment.get_state()\n\nstate.describe()\n</code></pre>"},{"location":"user_guides/mlops/serving/deployment-state/#step-4-check-no-of-running-instances","title":"Step 4: Check n\u00ba of running instances","text":"Python <pre><code># n\u00ba of predictor instances\ndeployment.resources.describe()\n\n# n\u00ba of transformer instances\ndeployment.transformer.resources.describe()\n</code></pre>"},{"location":"user_guides/mlops/serving/deployment-state/#api-reference","title":"API Reference","text":"<p><code>Deployment</code></p> <p><code>PredictorState</code></p>"},{"location":"user_guides/mlops/serving/deployment-state/#deployment-status","title":"Deployment status","text":"<p>The status of a deployment is a high-level description of its current state.</p> Show deployment status Status Description CREATED Deployment has never been started STARTING Deployment is starting RUNNING Deployment is ready and running. Predictions are served without additional latencies. IDLE Deployment is ready, but idle. Higher latencies (i.e., cold-start) are expected in the first incoming inference requests FAILED Deployment is in a failed state, which can be due to multiple reasons. More details can be found in the condition UPDATING Deployment is applying updates to the running instances STOPPING Deployment is stopping STOPPED Deployment has been stopped"},{"location":"user_guides/mlops/serving/deployment-state/#deployment-conditions","title":"Deployment conditions","text":"<p>A condition contains more specific information about the status of the deployment. They are mainly useful to track the progress of starting or stopping deployments.</p> <p>Status conditions contain three pieces of information: type, status and reason. While the type describes the purpose of the condition, the status represents its progress. Additionally, a reason field is provided with a more descriptive message of the status.</p> Show deployment conditions Type Status Description STOPPED <code>Unknown</code> Deployment is stopping. <code>True</code> Deployment is stopped. Therefore, no instances are running and no resources are allocated. SCHEDULED <code>Unknown</code> Deployment is being scheduled <code>False</code> Deployment failed to be scheduled. This is commonly due to insufficient resources to satisfy the deployment requirements <code>True</code> Deployment has been scheduled successfully. At this point, resources have been already allocated for the deployment. INITIALIZED <code>Unknown</code> Deployment is initializing. This step involves initialization tasks such as pulling docker images or mounting data volumes <code>False</code> Deployment failed to initialized <code>True</code> Deployment has been initialized successfully. At this point, the docker images have been pulled and data volumes mounted STARTED <code>Unknown</code> Deployment is starting. In this step, the model server is started and predictor / transformer scripts (if provided) are executed <code>False</code> Deployment failed to start. This can be due to errors in the predictor / transformer script, missing dependencies or model server incompatibilities. <code>True</code> Deployment has been started successfully. At this point, the model server has been started and the predictor / transformer scripts (if provided) executed. READY <code>Unknown</code> Connectivity is being set up. <code>False</code> Connectivity failed to be set up, mainly due to networking issues. <code>True</code> Connectivity has been set up and the deployment is ready <p>The following are two diagrams with the state transitions of conditions in starting and stopping deployments, respectively.</p> <p> Condition transitions in starting deployments </p> <p> Condition transitions in stopping deployments </p>"},{"location":"user_guides/mlops/serving/deployment/","title":"How To Create A Model Deployment","text":""},{"location":"user_guides/mlops/serving/deployment/#introduction","title":"Introduction","text":"<p>In this guide, you will learn how to create a new deployment for a trained model.</p> <p>Warning</p> <p>This guide assumes that a model has already been trained and saved into the Model Registry. To learn how to create a model in the Model Registry, see Model Registry Guide</p> <p>Deployments are used to unify the different components involved in making one or more trained models online and accessible to compute predictions on demand. For each deployment, there are four concepts to consider:</p> <ol> <li>Model files</li> <li>Artifact files</li> <li>Predictor</li> <li>Transformer</li> </ol>"},{"location":"user_guides/mlops/serving/deployment/#gui","title":"GUI","text":""},{"location":"user_guides/mlops/serving/deployment/#step-1-create-a-deployment","title":"Step 1: Create a deployment","text":"<p>If you have at least one model already trained and saved in the Model Registry, navigate to the deployments page by clicking on the <code>Deployments</code> tab on the navigation menu on the left.</p> <p> Deployments navigation tab </p> <p>Once in the deployments page, you can create a new deployment by either clicking on <code>New deployment</code> (if there are no existing deployments) or on <code>Create new deployment</code> it the top-right corner. Both options will open the deployment creation form.</p>"},{"location":"user_guides/mlops/serving/deployment/#step-2-basic-deployment-configuration","title":"Step 2: Basic deployment configuration","text":"<p>A simplified creation form will appear including the most common deployment fields from all available configurations. We provide default values for the rest of the fields, adjusted to the type of deployment you want to create.</p> <p>In the simplified form, select the model framework used to train your model. Then, select the model you want to deploy from the list of available models under <code>pick a model</code>.</p> <p>After selecting the model, the rest of fields are filled automatically. We pick the last model version and model artifact version available in the Model Registry. Moreover, we infer the deployment name from the model name.</p> <p>Deployment name validation rules</p> <p>A valid deployment name can only contain characters a-z, A-Z and 0-9.</p> <p>Predictor script for Python models</p> <p>For Python models, you must select a custom predictor script that loads and runs the trained model by clicking on <code>From project</code> or <code>Upload new file</code>, to choose an existing script in the project file system or upload a new script, respectively.</p> <p>If you prefer, change the name of the deployment, model version or artifact version. Then, click on <code>Create new deployment</code> to create the deployment for your model.</p> <p> Select the model framework </p> <p> Select the model </p>"},{"location":"user_guides/mlops/serving/deployment/#step-3-optional-advanced-configuration","title":"Step 3 (Optional): Advanced configuration","text":"<p>Optionally, you can access and adjust other parameters of the deployment configuration by clicking on <code>Advanced options</code>.</p> <p> Advanced options. Go to advanced deployment creation form </p> <p>You will be redirected to a full-page deployment creation form where you can see all the default configuration values we selected for your deployment and adjust them according to your use case. Apart from the aforementioned simplified configuration, in this form you can setup the following components:</p> <p>Deployment advanced options</p> <ol> <li>Predictor</li> <li>Transformer</li> <li>Inference logger</li> <li>Inference batcher</li> <li>Resources</li> <li>API protocol</li> </ol> <p>Once you are done with the changes, click on <code>Create new deployment</code> at the bottom of the page to create the deployment for your model.</p>"},{"location":"user_guides/mlops/serving/deployment/#step-4-kueue-enabled-select-a-queue","title":"Step 4: (Kueue enabled) Select a Queue","text":"<p>If the cluster is installed with Kueue enabled, you will need to select a queue in which the deployment should run. This can be done from <code>Advance configuration -&gt; Scheduler section</code>.</p> <p></p>"},{"location":"user_guides/mlops/serving/deployment/#step-5-deployment-creation","title":"Step 5: Deployment creation","text":"<p>Wait for the deployment creation process to finish.</p> <p> Deployment creation in progress </p>"},{"location":"user_guides/mlops/serving/deployment/#step-6-deployment-overview","title":"Step 6: Deployment overview","text":"<p>Once the deployment is created, you will be redirected to the list of all your existing deployments in the project. You can use the filters on the top of the page to easily locate your new deployment.</p> <p> List of deployments </p> <p>After that, click on the new deployment to access the overview page.</p> <p> Deployment overview </p>"},{"location":"user_guides/mlops/serving/deployment/#code","title":"Code","text":""},{"location":"user_guides/mlops/serving/deployment/#step-1-connect-to-hopsworks","title":"Step 1: Connect to Hopsworks","text":"Python <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n</code></pre>"},{"location":"user_guides/mlops/serving/deployment/#step-2-create-deployment","title":"Step 2: Create deployment","text":"<p>Retrieve the trained model you want to deploy.</p> Python <pre><code>my_model = mr.get_model(\"my_model\", version=1)\n</code></pre>"},{"location":"user_guides/mlops/serving/deployment/#option-a-using-the-model-object","title":"Option A: Using the model object","text":"Python <pre><code>my_deployment = my_model.deploy()\n</code></pre>"},{"location":"user_guides/mlops/serving/deployment/#option-b-using-the-model-serving-handle","title":"Option B: Using the Model Serving handle","text":"Python <pre><code># get Hopsworks Model Serving handle\nms = project.get_model_serving()\n\nmy_predictor = ms.create_predictor(my_model)\nmy_deployment = my_predictor.deploy()\n\n# or\nmy_deployment = ms.create_deployment(my_predictor)\nmy_deployment.save()\n</code></pre>"},{"location":"user_guides/mlops/serving/deployment/#api-reference","title":"API Reference","text":"<p><code>ModelServing</code></p>"},{"location":"user_guides/mlops/serving/deployment/#model-files","title":"Model Files","text":"<p>Model files are the files exported when a specific version of a model is saved to the model registry (see Model Registry). These files are unique for each model version, but shared across model deployments created for the same version of the model.</p> <p>Inside a model deployment, the local path to the model files is stored in the <code>MODEL_FILES_PATH</code> environment variable (see environment variables). Moreover, you can explore the model files under the <code>/Models/&lt;model-name&gt;/&lt;model-version&gt;/Files</code> directory using the File Browser.</p> <p>Warning</p> <p>All files under <code>/Models</code> are managed by Hopsworks. Changes to model files cannot be reverted and can have an impact on existing model deployments.</p>"},{"location":"user_guides/mlops/serving/deployment/#artifact-files","title":"Artifact Files","text":"<p>Artifact files are files involved in the correct startup and running of the model deployment. The most important files are the predictor and transformer scripts. The former is used to load and run the model for making predictions. The latter is typically used to apply transformations on the model inputs at inference time before making predictions. Predictor and transformer scripts run on separate components and, therefore, scale independently of each other.</p> <p>Tip</p> <p>Whenever you provide a predictor script, you can include the transformations of model inputs in the same script as far as they don't need to be scaled independently from the model inference process.</p> <p>Additionally, artifact files can also contain a server configuration file that helps detach configuration used within the model deployment from the model server or the implementation of the predictor and transformer scripts. Inside a model deployment, the local path to the configuration file is stored in the <code>CONFIG_FILE_PATH</code> environment variable (see environment variables).</p> <p>Every model deployment runs a specific version of the artifact files, commonly referred to as artifact version. One or more model deployments can use the same artifact version (i.e., same predictor and transformer scripts). Artifact versions are unique for the same model version.</p> <p>When a new deployment is created, a new artifact version is generated in two cases:</p> <ul> <li>the artifact version in the predictor is set to <code>CREATE</code> (see Artifact Version)</li> <li>no model artifact with the same files has been created before.</li> </ul> <p>Inside a model deployment, the local path to the artifact files is stored in the <code>ARTIFACT_FILES_PATH</code> environment variable (see environment variables). Moreover, you can explore the artifact files under the <code>/Models/&lt;model-name&gt;/&lt;model-version&gt;/Artifacts/&lt;artifact-version&gt;</code> directory using the File Browser.</p> <p>Warning</p> <p>All files under <code>/Models</code> are managed by Hopsworks. Changes to artifact files cannot be reverted and can have an impact on existing model deployments.</p> <p>Additional files</p> <p>Currently, the artifact files can only include predictor and transformer scripts, and a configuration file. Support for additional files (e.g., other resources) is coming soon.</p>"},{"location":"user_guides/mlops/serving/deployment/#predictor","title":"Predictor","text":"<p>Predictors are responsible for running the model server that loads the trained model, listens to inference requests and returns prediction results. To learn more about predictors, see the Predictor Guide</p> <p>Note</p> <p>Only one predictor is supported in a deployment.</p> <p>Info</p> <p>Model artifacts are assigned an incremental version number, being <code>0</code> the version reserved for model artifacts that do not contain predictor or transformer scripts (i.e., shared artifacts containing only the model files).</p>"},{"location":"user_guides/mlops/serving/deployment/#transformer","title":"Transformer","text":"<p>Transformers are used to apply transformations on the model inputs before sending them to the predictor for making predictions using the model. To learn more about transformers, see the Transformer Guide.</p> <p>Warning</p> <p>Transformers are only supported in KServe deployments.</p>"},{"location":"user_guides/mlops/serving/external-access/","title":"How To Configure External Access To A Model Deployment","text":""},{"location":"user_guides/mlops/serving/external-access/#introduction","title":"Introduction","text":"<p>Hopsworks supports role-based access control (RBAC) for project members within a project, where a project ML assets can only be accessed by Hopsworks users that are members of that project (See governance).</p> <p>However, there are cases where you might want to grant external users with access to specific model deployments without them having to register into Hopsworks or to join the project which will give them access to all project ML assets. For these cases, Hopsworks supports fine-grained access control to model deployments based on user groups managed by an external Identity Provider.</p> <p>Authentication methods</p> <p>Hopsworks can be configured to use different types of authentication methods including OAuth2, LDAP and Kerberos. See the Authentication Methods Guide for more information.</p>"},{"location":"user_guides/mlops/serving/external-access/#gui-for-hopsworks-users","title":"GUI (for Hopsworks users)","text":""},{"location":"user_guides/mlops/serving/external-access/#step-1-navigate-to-a-model-deployment","title":"Step 1: Navigate to a model deployment","text":"<p>If you have at least one model deployment already created, navigate to the model deployments page by clicking on the <code>Deployments</code> tab on the navigation menu on the left.</p> <p> Deployments navigation tab </p> <p>Once in the model deployments page, find the model deployment you want to configure external access and click on the name of the deployment to open the model deployment overview page.</p> <p> Deployment overview </p>"},{"location":"user_guides/mlops/serving/external-access/#step-2-go-to-external-access","title":"Step 2: Go to External Access","text":"<p>You can find the external access configuration by clicking on <code>External access</code> on the navigation menu on the left or scrolling down to the external access section.</p> <p> External access configuration </p>"},{"location":"user_guides/mlops/serving/external-access/#step-3-add-or-remove-user-groups","title":"Step 3: Add or remove user groups","text":"<p>In this section, you can add and remove user groups by clicking on <code>edit external user groups</code> and typing the group name in the text-free input field or selecting one of the existing ones in the dropdown list. After that, click on the <code>save</code> button to persist the changes.</p> <p>Case sensitivity</p> <p>Inference requests are authorized using a case-sensitive exact match between the group names of the user making the request and the group names granted access to the model deployment. Therefore, a user assigned to the group <code>lab1</code> won't have access to a model deployment accessible by group <code>LAB1</code>.</p> <p> External access configuration </p>"},{"location":"user_guides/mlops/serving/external-access/#gui-for-external-users","title":"GUI (for external users)","text":""},{"location":"user_guides/mlops/serving/external-access/#step-1-login-with-the-external-identity-provider","title":"Step 1: Login with the external identity provider","text":"<p>Navigate to Hopsworks, and click on the <code>Login with</code> button to sign in using the configured external identity provider (e.g., Keycloak in this example).</p> <p> Login with External Identity Provider </p>"},{"location":"user_guides/mlops/serving/external-access/#step-2-explore-the-model-deployments-you-are-granted-access-to","title":"Step 2: Explore the model deployments you are granted access to","text":"<p>Once you sign in to Hopsworks, you can see the list of model deployments you are granted access to based on your assigned groups.</p> <p> Deployments with external access </p>"},{"location":"user_guides/mlops/serving/external-access/#step-2-inspect-your-current-groups","title":"Step 2: Inspect your current groups","text":"<p>You can find the current groups you are assigned to at the top of the page.</p> <p> External user groups </p>"},{"location":"user_guides/mlops/serving/external-access/#step-3-get-an-api-key","title":"Step 3: Get an API key","text":"<p>Inference requests to model deployments are authenticated and authorized based on your external user and user groups. You can create API keys to authenticate your inference requests by clicking on the <code>Create API Key</code> button.</p> <p>Authorization header</p> <p>API keys are set in the <code>Authorization</code> header following the format <code>ApiKey &lt;api-key-value&gt;</code></p> <p> Get API key </p>"},{"location":"user_guides/mlops/serving/external-access/#step-4-send-inference-requests","title":"Step 4: Send inference requests","text":"<p>Depending on the type of model deployment, the URI of the model server can differ (e.g., <code>/chat/completions</code> for LLM deployments or <code>/predict</code> for traditional model deployments). You can find the corresponding URI on every model deployment card.</p> <p>In addition to the <code>Authorization</code> header containing the API key, the <code>Host</code> header needs to be set according to the model deployment where the inference requests are sent to. This header is used by the ingress to route the inference requests to the corresponding model deployment. You can find the <code>Host</code> header value in the model deployment card.</p> <p>Code snippets</p> <p>For clients sending inference requests using libraries similar to curl or OpenAI API-compatible libraries (e.g., LangChain), you can find code snippet examples by clicking on the <code>Curl &gt;_</code> and <code>LangChain &gt;_</code> buttons.</p> <p> Deployment endpoint </p>"},{"location":"user_guides/mlops/serving/external-access/#refreshing-external-user-groups","title":"Refreshing External User Groups","text":"<p>Every time an external user signs in to Hopsworks using a pre-configured authentication method, Hopsworks fetches the external user groups and updates the internal state accordingly. Given that groups can be added/removed from users at any time by the Identity Provider, Hopsworks needs to periodically fetch the external user groups to keep the state updated.</p> <p>Therefore, external users that want to access model deployments are required to login periodically to ensure they are still part of the allowed groups. The timespan between logins is controlled by the configuration parameter <code>requireExternalUserLoginAfterHours</code> available during the Hopsworks installation and upgrade.</p> <p>The <code>requireExternalUserLoginAfterHours</code> configuration parameter controls the number of hours after which external users are required to sign in to Hopsworks to refresh their external user groups.</p> <p>Configuring <code>requireExternalUserLoginAfterHours</code></p> <p>Allowed values are -1, 0 and greater than 0, where -1 disables the periodic login requirement and 0 disables external access completely for every model deployment.</p>"},{"location":"user_guides/mlops/serving/inference-batcher/","title":"How To Configure Inference Batcher","text":""},{"location":"user_guides/mlops/serving/inference-batcher/#introduction","title":"Introduction","text":"<p>Inference batching can be enabled to increase inference request throughput at the cost of higher latencies. The configuration of the inference batcher depends on the serving tool and the model server used in the deployment. See the compatibility matrix.</p>"},{"location":"user_guides/mlops/serving/inference-batcher/#gui","title":"GUI","text":""},{"location":"user_guides/mlops/serving/inference-batcher/#step-1-create-new-deployment","title":"Step 1: Create new deployment","text":"<p>If you have at least one model already trained and saved in the Model Registry, navigate to the deployments page by clicking on the <code>Deployments</code> tab on the navigation menu on the left.</p> <p> Deployments navigation tab </p> <p>Once in the deployments page, you can create a new deployment by either clicking on <code>New deployment</code> (if there are no existing deployments) or on <code>Create new deployment</code> it the top-right corner. Both options will open the deployment creation form.</p>"},{"location":"user_guides/mlops/serving/inference-batcher/#step-2-go-to-advanced-options","title":"Step 2: Go to advanced options","text":"<p>A simplified creation form will appear including the most common deployment fields from all available configurations. Inference batching is part of the advanced options of a deployment. To navigate to the advanced creation form, click on <code>Advanced options</code>.</p> <p> Advanced options. Go to advanced deployment creation form </p>"},{"location":"user_guides/mlops/serving/inference-batcher/#step-3-configure-inference-batching","title":"Step 3: Configure inference batching","text":"<p>To enable inference batching, click on the <code>Request batching</code> checkbox.</p> <p> Inference batching configuration (default values) </p> <p>If your deployment uses KServe, you can optionally set three additional parameters for the inference batcher: maximum batch size, maximum latency (ms) and timeout (s).</p> <p>Once you are done with the changes, click on <code>Create new deployment</code> at the bottom of the page to create the deployment for your model.</p>"},{"location":"user_guides/mlops/serving/inference-batcher/#code","title":"Code","text":""},{"location":"user_guides/mlops/serving/inference-batcher/#step-1-connect-to-hopsworks","title":"Step 1: Connect to Hopsworks","text":"Python <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n</code></pre>"},{"location":"user_guides/mlops/serving/inference-batcher/#step-2-define-an-inference-logger","title":"Step 2: Define an inference logger","text":"Python <pre><code>from hsml.inference_batcher import InferenceBatcher\n\nmy_batcher = InferenceBatcher(enabled=True,\n                              # optional\n                              max_batch_size=32,\n                              max_latency=5000, # milliseconds\n                              timeout=5 # seconds\n                              )\n</code></pre>"},{"location":"user_guides/mlops/serving/inference-batcher/#step-3-create-a-deployment-with-the-inference-batcher","title":"Step 3: Create a deployment with the inference batcher","text":"Python <pre><code>my_model = mr.get_model(\"my_model\", version=1)\n\nmy_predictor = ms.create_predictor(my_model,\n                                  inference_batcher=my_batcher\n                                  )\nmy_predictor.deploy()\n\n# or\n\nmy_deployment = ms.create_deployment(my_predictor)\nmy_deployment.save()\n</code></pre>"},{"location":"user_guides/mlops/serving/inference-batcher/#api-reference","title":"API Reference","text":"<p><code>InferenceBatcher</code></p>"},{"location":"user_guides/mlops/serving/inference-batcher/#compatibility-matrix","title":"Compatibility matrix","text":"Show supported inference batcher configuration Serving tool Model server Inference batching Fine-grained configuration Docker Flask \u274c - TensorFlow Serving \u2705 \u274c Kubernetes Flask \u274c - TensorFlow Serving \u2705 \u274c KServe Flask \u2705 \u2705 TensorFlow Serving \u2705 \u2705"},{"location":"user_guides/mlops/serving/inference-logger/","title":"How To Configure Inference Logging","text":""},{"location":"user_guides/mlops/serving/inference-logger/#introduction","title":"Introduction","text":"<p>Once a model is deployed and starts making predictions as inference requests arrive, logging model inputs and predictions becomes essential to monitor the health of the model and take action if the model's performance degrades over time.</p> <p>Hopsworks supports logging both inference requests and predictions as events to a Kafka topic for analysis.</p> <p>Topic schemas vary depending on the serving tool. See below</p>"},{"location":"user_guides/mlops/serving/inference-logger/#gui","title":"GUI","text":""},{"location":"user_guides/mlops/serving/inference-logger/#step-1-create-new-deployment","title":"Step 1: Create new deployment","text":"<p>If you have at least one model already trained and saved in the Model Registry, navigate to the deployments page by clicking on the <code>Deployments</code> tab on the navigation menu on the left.</p> <p> Deployments navigation tab </p> <p>Once in the deployments page, you can create a new deployment by either clicking on <code>New deployment</code> (if there are no existing deployments) or on <code>Create new deployment</code> it the top-right corner. Both options will open the deployment creation form.</p>"},{"location":"user_guides/mlops/serving/inference-logger/#step-2-go-to-advanced-options","title":"Step 2: Go to advanced options","text":"<p>A simplified creation form will appear including the most common deployment fields from all available configurations. Inference logging is part of the advanced options of a deployment. To navigate to the advanced creation form, click on <code>Advanced options</code>.</p> <p> Advanced options. Go to advanced deployment creation form </p>"},{"location":"user_guides/mlops/serving/inference-logger/#step-3-configure-inference-logging","title":"Step 3: Configure inference logging","text":"<p>To enable inference logging, choose <code>CREATE</code> as Kafka topic name to create a new topic, or select an existing topic. If you prefer, you can disable inference logging by selecting <code>NONE</code>.</p> <p>If you decide to create a new topic, select the number of partitions and number of replicas for your topic, or use the default values.</p> <p> Inference logging configuration with a new kafka topic </p> <p>If the deployment is created with KServe enabled, you can specify which inference logs you want to send to the Kafka topic (i.e., <code>MODEL_INPUTS</code>, <code>PREDICTIONS</code> or both)</p> <p>Once you are done with the changes, click on <code>Create new deployment</code> at the bottom of the page to create the deployment for your model.</p>"},{"location":"user_guides/mlops/serving/inference-logger/#code","title":"Code","text":""},{"location":"user_guides/mlops/serving/inference-logger/#step-1-connect-to-hopsworks","title":"Step 1: Connect to Hopsworks","text":"Python <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n</code></pre>"},{"location":"user_guides/mlops/serving/inference-logger/#step-2-define-an-inference-logger","title":"Step 2: Define an inference logger","text":"Python <pre><code>from hsml.inference_logger import InferenceLogger\nfrom hsml.kafka_topic import KafkaTopic\n\nnew_topic = KafkaTopic(name=\"CREATE\",\n                      # optional\n                      num_partitions=1,\n                      num_replicas=1\n                      )\n\nmy_logger = InferenceLogger(kafka_topic=new_topic, mode=\"ALL\")\n</code></pre> <p>Use dict for simpler code</p> <p>Similarly, you can create the same logger with:</p> <pre><code>my_logger = InferenceLogger(kafka_topic={\"name\": \"CREATE\"}, mode=\"ALL\")\n</code></pre>"},{"location":"user_guides/mlops/serving/inference-logger/#step-3-create-a-deployment-with-the-inference-logger","title":"Step 3: Create a deployment with the inference logger","text":"Python <pre><code>my_model = mr.get_model(\"my_model\", version=1)\n\nmy_predictor = ms.create_predictor(my_model,\n                                  inference_logger=my_logger\n                                  )\nmy_predictor.deploy()\n\n# or\n\nmy_deployment = ms.create_deployment(my_predictor)\nmy_deployment.save()\n</code></pre>"},{"location":"user_guides/mlops/serving/inference-logger/#api-reference","title":"API Reference","text":"<p><code>InferenceLogger</code></p>"},{"location":"user_guides/mlops/serving/inference-logger/#topic-schema","title":"Topic schema","text":"<p>The schema of Kafka events varies depending on the serving tool. In KServe deployments, model inputs and predictions are logged in separate events, but sharing the same <code>requestId</code> field. In non-KServe deployments, the same event contains both the model input and prediction related to the same inference request.</p> Show kafka topic schemas KServeDocker / Kubernetes <pre><code>{\n    \"fields\": [\n        { \"name\": \"servingId\", \"type\": \"int\" },\n        { \"name\": \"modelName\", \"type\": \"string\" },\n        { \"name\": \"modelVersion\", \"type\": \"int\" },\n        { \"name\": \"requestTimestamp\", \"type\": \"long\" },\n        { \"name\": \"responseHttpCode\", \"type\": \"int\" },\n        { \"name\": \"inferenceId\", \"type\": \"string\" },\n        { \"name\": \"messageType\", \"type\": \"string\" },\n        { \"name\": \"payload\", \"type\": \"string\" }\n    ],\n    \"name\": \"inferencelog\",\n    \"type\": \"record\"\n}\n</code></pre> <pre><code>{\n    \"fields\": [\n        { \"name\": \"modelId\", \"type\": \"int\" },\n        { \"name\": \"modelName\", \"type\": \"string\" },\n        { \"name\": \"modelVersion\", \"type\": \"int\" },\n        { \"name\": \"requestTimestamp\", \"type\": \"long\" },\n        { \"name\": \"responseHttpCode\", \"type\": \"int\" },\n        { \"name\": \"inferenceRequest\", \"type\": \"string\" },\n        { \"name\": \"inferenceResponse\", \"type\": \"string\" },\n        { \"name\": \"modelServer\", \"type\": \"string\" },\n        { \"name\": \"servingTool\", \"type\": \"string\" }\n    ],\n    \"name\": \"inferencelog\",\n    \"type\": \"record\"\n}\n</code></pre>"},{"location":"user_guides/mlops/serving/predictor/","title":"How To Configure A Predictor","text":""},{"location":"user_guides/mlops/serving/predictor/#introduction","title":"Introduction","text":"<p>In this guide, you will learn how to configure a predictor for a trained model.</p> <p>Warning</p> <p>This guide assumes that a model has already been trained and saved into the Model Registry. To learn how to create a model in the Model Registry, see Model Registry Guide</p> <p>Predictors are the main component of deployments. They are responsible for running a model server that loads a trained model, handles inference requests and returns predictions. They can be configured to use different model servers, serving tools, log specific inference data or scale differently. In each predictor, you can configure the following components:</p> <ol> <li>Model server</li> <li>Serving tool</li> <li>User-provided script</li> <li>Server configuration file</li> <li>Python environments</li> <li>Transformer</li> <li>Inference Logger</li> <li>Inference Batcher</li> <li>Resources</li> <li>API protocol</li> </ol>"},{"location":"user_guides/mlops/serving/predictor/#gui","title":"GUI","text":""},{"location":"user_guides/mlops/serving/predictor/#step-1-create-new-deployment","title":"Step 1: Create new deployment","text":"<p>If you have at least one model already trained and saved in the Model Registry, navigate to the deployments page by clicking on the <code>Deployments</code> tab on the navigation menu on the left.</p> <p> Deployments navigation tab </p> <p>Once in the deployments page, you can create a new deployment by either clicking on <code>New deployment</code> (if there are no existing deployments) or on <code>Create new deployment</code> it the top-right corner. Both options will open the deployment creation form.</p>"},{"location":"user_guides/mlops/serving/predictor/#step-2-choose-a-backend","title":"Step 2: Choose a backend","text":"<p>A simplified creation form will appear, including the most common deployment fields from all available configurations. The first step is to choose a backend for your model deployment. The backend will filter the models shown below according to the framework that the model was registered with in the model registry.</p> <p>For example if you registered the model as a TensorFlow model using <code>ModelRegistry.tensorflow.create_model(...)</code> you select <code>Tensorflow Serving</code> in the dropdown.</p> <p> Select the backend </p> <p>All models compatible with the selected backend will be listed in the model dropdown.</p> <p> Select the model </p> <p>Moreover, you can optionally select a predictor script (see Step 3 (Optional): Select a predictor script), enable KServe (see Step 4 (Optional): Enable KServe) or change other advanced configuration (see Step 5 (Optional): Other advanced options). Otherwise, click on <code>Create new deployment</code> to create the deployment for your model.</p>"},{"location":"user_guides/mlops/serving/predictor/#step-3-optional-select-a-predictor-script","title":"Step 3 (Optional): Select a predictor script","text":"<p>For python models, if you want to use your own predictor script click on <code>From project</code> and navigate through the file system to find it, or click on <code>Upload new file</code> to upload a predictor script now.</p> <p> Select a predictor script in the simplified deployment form </p>"},{"location":"user_guides/mlops/serving/predictor/#step-4-optional-change-predictor-environment","title":"Step 4 (Optional): Change predictor environment","text":"<p>If you are using a predictor script it is also required to configure the inference environment for the predictor. This environment needs to have all the necessary dependencies installed to run your predictor script.</p> <p>By default, we provide a set of environments like <code>tensorflow-inference-pipeline</code>, <code>torch-inference-pipeline</code> and <code>pandas-inference-pipeline</code> that serves this purpose for common machine learning frameworks.</p> <p>To create your own it is recommended to clone the <code>minimal-inference-pipeline</code> and install additional dependencies for your use-case.</p> <p> Select an environment for the predictor script </p>"},{"location":"user_guides/mlops/serving/predictor/#step-5-optional-select-a-configuration-file","title":"Step 5 (Optional): Select a configuration file","text":"<p>Note</p> <p>Only available for LLM deployments.</p> <p>You can select a configuration file to be added to the artifact files. If a predictor script is provided, this configuration file will be available inside the model deployment at the local path stored in the <code>CONFIG_FILE_PATH</code> environment variable. If a predictor script is not provided, this configuration file will be directly passed to the vLLM server. You can find all configuration parameters supported by the vLLM server in the vLLM documentation.</p> <p> Select a configuration file in the simplified deployment form </p>"},{"location":"user_guides/mlops/serving/predictor/#step-6-optional-enable-kserve","title":"Step 6 (Optional): Enable KServe","text":"<p>Other configuration such as the serving tool, is part of the advanced options of a deployment. To navigate to the advanced creation form, click on <code>Advanced options</code>.</p> <p> Advanced options. Go to advanced deployment creation form </p> <p>Here, you change the serving tool for your deployment by enabling or disabling the KServe checkbox.</p> <p> KServe checkbox in the advanced deployment form </p>"},{"location":"user_guides/mlops/serving/predictor/#step-7-optional-other-advanced-options","title":"Step 7 (Optional): Other advanced options","text":"<p>Additionally, you can adjust the default values of the rest of components:</p> <p>Predictor components</p> <ol> <li>Transformer</li> <li>Inference logger</li> <li>Inference batcher</li> <li>Resources</li> <li>API protocol</li> </ol> <p>Once you are done with the changes, click on <code>Create new deployment</code> at the bottom of the page to create the deployment for your model.</p>"},{"location":"user_guides/mlops/serving/predictor/#code","title":"Code","text":""},{"location":"user_guides/mlops/serving/predictor/#step-1-connect-to-hopsworks","title":"Step 1: Connect to Hopsworks","text":"Python <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n</code></pre>"},{"location":"user_guides/mlops/serving/predictor/#step-2-optional-implement-a-predictor-script","title":"Step 2 (Optional): Implement a predictor script","text":"PredictorAsync PredictorPredictor (vLLM deployments only) <pre><code>class Predictor():\n\n    def __init__(self):\n        \"\"\" Initialization code goes here\"\"\"\n        # Model files can be found at os.environ[\"MODEL_FILES_PATH\"]\n        # self.model = ... # load your model\n\n    def predict(self, inputs):\n        \"\"\" Serve predictions using the trained model\"\"\"\n        # Use the model to make predictions\n        # return self.model.predict(inputs)\n</code></pre> <pre><code>class Predictor():\n\n    def __init__(self):\n        \"\"\" Initialization code goes here\"\"\"\n        # Model files can be found at os.environ[\"MODEL_FILES_PATH\"]\n        # self.model = ... # load your model\n\n    async def predict(self, inputs):\n        \"\"\" Asynchronously serve predictions using the trained model\"\"\"\n        # Perform async operations that required\n        # result = await some_async_preprocessing(inputs)\n\n        # Use the model to make predictions\n        # return self.model.predict(result)\n</code></pre> <pre><code>import os\nfrom vllm import **version**, AsyncEngineArgs, AsyncLLMEngine\nfrom typing import Iterable, AsyncIterator, Union, Optional\nfrom kserve.protocol.rest.openai import (\n    CompletionRequest,\n    ChatPrompt,\n    ChatCompletionRequestMessage,\n)\nfrom kserve.protocol.rest.openai.types import Completion\nfrom kserve.protocol.rest.openai.types.openapi import ChatCompletionTool\n\nclass Predictor():\n\n    def __init__(self):\n        \"\"\" Initialization code goes here\"\"\"\n\n        # (optional) if any, access the configuration file via os.environ[\"CONFIG_FILE_PATH\"]\n        config = ...\n\n        print(\"Starting vLLM backend...\")\n        engine_args = AsyncEngineArgs(\n            model=os.environ[\"MODEL_FILES_PATH\"],\n            **config\n        )\n\n        # \"self.vllm_engine\" is required as the local variable with the vllm engine handler\n        self.vllm_engine = AsyncLLMEngine.from_engine_args(engine_args)\n\n    #\n    # NOTE: Default implementations of the apply_chat_template and create_completion methods are already provided.\n    #       If needed, you can override these methods as shown below\n    #\n\n    #def apply_chat_template(\n    #    self,\n    #    messages: Iterable[ChatCompletionRequestMessage],\n    #    chat_template: Optional[str] = None,\n    #    tools: Optional[list[ChatCompletionTool]] = None,\n    #) -&gt; ChatPrompt:\n    #    \"\"\"Converts a prompt or list of messages into a single templated prompt string\"\"\"\n\n    #    prompt = ... # apply chat template on the message to build the prompt\n    #    return ChatPrompt(prompt=prompt)\n\n    #async def create_completion(\n    #    self, request: CompletionRequest\n    #) -&gt; Union[Completion, AsyncIterator[Completion]]:\n    #    \"\"\"Generate responses using the vLLM engine\"\"\"\n    #\n    #    generators = self.vllm_engine.generate(...)\n    #\n    #    # Completion: used for returning a single answer (batch)\n    #    # AsyncIterator[Completion]: used for returning a stream of answers\n    #    return ...\n</code></pre> <p>Jupyter magic</p> <p>In a jupyter notebook, you can add <code>%%writefile my_predictor.py</code> at the top of the cell to save it as a local file.</p>"},{"location":"user_guides/mlops/serving/predictor/#step-3-optional-upload-the-script-to-your-project","title":"Step 3 (Optional): Upload the script to your project","text":"<p>You can also use the UI to upload your predictor script. See above</p> Python <pre><code>uploaded_file_path = dataset_api.upload(\"my_predictor.py\", \"Resources\", overwrite=True)\npredictor_script_path = os.path.join(\"/Projects\", project.name, uploaded_file_path)\n</code></pre>"},{"location":"user_guides/mlops/serving/predictor/#step-4-define-predictor","title":"Step 4: Define predictor","text":"Python <pre><code>my_model = mr.get_model(\"my_model\", version=1)\n\nmy_predictor = ms.create_predictor(my_model,\n                                  # optional\n                                  model_server=\"PYTHON\",\n                                  serving_tool=\"KSERVE\",\n                                  script_file=predictor_script_path\n                                  )\n</code></pre>"},{"location":"user_guides/mlops/serving/predictor/#step-5-create-a-deployment-with-the-predictor","title":"Step 5: Create a deployment with the predictor","text":"Python <pre><code>my_deployment = my_predictor.deploy()\n\n# or\nmy_deployment = ms.create_deployment(my_predictor)\nmy_deployment.save()\n</code></pre>"},{"location":"user_guides/mlops/serving/predictor/#api-reference","title":"API Reference","text":"<p><code>Predictor</code></p>"},{"location":"user_guides/mlops/serving/predictor/#model-server","title":"Model Server","text":"<p>Hopsworks Model Serving supports deploying models with a Flask server for python-based models, TensorFlow Serving for TensorFlow / Keras models and vLLM for Large Language Models (LLMs). Today, you can deploy PyTorch models as python-based models.</p> Show supported model servers Model Server Supported ML Models and Frameworks Flask \u2705 python-based (scikit-learn, xgboost, pytorch...) TensorFlow Serving \u2705 keras, tensorflow TorchServe \u274c pytorch vLLM \u2705 vLLM-supported models (see list)"},{"location":"user_guides/mlops/serving/predictor/#serving-tool","title":"Serving tool","text":"<p>In Hopsworks, model servers are deployed on Kubernetes. There are two options for deploying models on Kubernetes: using KServe inference services or Kubernetes built-in deployments. KServe is the recommended way to deploy models in Hopsworks.</p> <p>The following is a comparative table showing the features supported by each of them.</p> Show serving tools comparison Feature / requirement Kubernetes (enterprise) KServe (enterprise) Autoscaling (scale-out) \u2705 \u2705 Resource allocation \u2796 min. resources \u2705 min / max. resources Inference logging \u2796 simple \u2705 fine-grained Inference batching \u2796 partially \u2705 Scale-to-zero \u274c \u2705 after 30s of inactivity Transformers \u274c \u2705 Low-latency predictions \u274c \u2705 Multiple models \u274c \u2796 (python-based) User-provided predictor required  (python-only) \u2705 \u274c"},{"location":"user_guides/mlops/serving/predictor/#user-provided-script","title":"User-provided script","text":"<p>Depending on the model server and serving platform used in the model deployment, you can (or need) to provide your own python script to load the model and make predictions. This script is referred to as predictor script, and is included in the artifact files of the model deployment.</p> <p>The predictor script needs to implement a given template depending on the model server of the model deployment. See the templates in Step 2.</p> Show supported user-provided predictors Serving tool Model server User-provided predictor script Kubernetes Flask server \u2705 (required) TensorFlow Serving \u274c KServe Fast API \u2705 (only required for artifacts with multiple models) TensorFlow Serving \u274c vLLM \u2705 (optional)"},{"location":"user_guides/mlops/serving/predictor/#server-configuration-file","title":"Server configuration file","text":"<p>Depending on the model server, a server configuration file can be selected to help detach configuration used within the model deployment from the model server or the implementation of the predictor and transformer scripts. In other words, by modifying the configuration file of an existing model deployment you can adjust its settings without making changes to the predictor or transformer scripts. Inside a model deployment, the local path to the configuration file is stored in the <code>CONFIG_FILE_PATH</code> environment variable (see environment variables).</p> <p>Configuration file format</p> <p>The configuration file can be of any format, except in vLLM deployments without a predictor script for which a YAML file is required.</p> <p>Passing arguments to vLLM via configuration file</p> <p>For vLLM deployments without a predictor script, the server configuration file is required and it is used to configure the vLLM server. For example, you can use this configuration file to specify the chat template  or LoRA modules to be loaded by the vLLM server. See all available parameters in the official documentation.</p>"},{"location":"user_guides/mlops/serving/predictor/#environment-variables","title":"Environment variables","text":"<p>A number of different environment variables is available in the predictor to ease its implementation.</p> Show environment variables Name Description MODEL_FILES_PATH Local path to the model files ARTIFACT_FILES_PATH Local path to the artifact files CONFIG_FILE_PATH Local path to the configuration file DEPLOYMENT_NAME Name of the current deployment MODEL_NAME Name of the model being served by the current deployment MODEL_VERSION Version of the model being served by the current deployment ARTIFACT_VERSION Version of the model artifact being served by the current deployment"},{"location":"user_guides/mlops/serving/predictor/#python-environments","title":"Python environments","text":"<p>Depending on the model server and serving tool used in the model deployment, you can select the Python environment where the predictor and transformer scripts will run. To create a new Python environment see Python Environments.</p> Show supported Python environments Serving tool Model server Editable Predictor Transformer Kubernetes Flask server \u274c <code>pandas-inference-pipeline</code> only \u274c TensorFlow Serving \u274c (official) tensorflow serving image \u274c KServe Fast API \u2705 any <code>inference-pipeline</code> image any <code>inference-pipeline</code> image TensorFlow Serving \u2705 (official) tensorflow serving image any <code>inference-pipeline</code> image vLLM \u2705 <code>vllm-inference-pipeline</code> or <code>vllm-openai</code> any <code>inference-pipeline</code> image <p>Note</p> <p>The selected Python environment is used for both predictor and transformer. Support for selecting a different Python environment for the predictor and transformer is coming soon.</p>"},{"location":"user_guides/mlops/serving/predictor/#transformer","title":"Transformer","text":"<p>Transformers are used to apply transformations on the model inputs before sending them to the predictor for making predictions using the model. To learn more about transformers, see the Transformer Guide.</p> <p>Note</p> <p>Transformers are only supported in KServe deployments.</p>"},{"location":"user_guides/mlops/serving/predictor/#inference-logger","title":"Inference logger","text":"<p>Inference loggers are deployment components that log inference requests into a Kafka topic for later analysis. To learn about the different logging modes, see the Inference Logger Guide</p>"},{"location":"user_guides/mlops/serving/predictor/#inference-batcher","title":"Inference batcher","text":"<p>Inference batcher are deployment component that apply batching to the incoming inference requests for a better throughput-latency trade-off. To learn about the different configuration available for the inference batcher, see the Inference Batcher Guide.</p>"},{"location":"user_guides/mlops/serving/predictor/#resources","title":"Resources","text":"<p>Resources include the number of replicas for the deployment as well as the resources (i.e., memory, CPU, GPU) to be allocated per replica. To learn about the different combinations available, see the Resources Guide.</p>"},{"location":"user_guides/mlops/serving/predictor/#api-protocol","title":"API protocol","text":"<p>Hopsworks supports both REST and gRPC as the API protocols to send inference requests to model deployments. In general, you use gRPC when you need lower latency inference requests. To learn more about the REST and gRPC API protocols for model deployments, see the API Protocol Guide.</p>"},{"location":"user_guides/mlops/serving/resources/","title":"How To Allocate Resources To A Model Deployment","text":""},{"location":"user_guides/mlops/serving/resources/#introduction","title":"Introduction","text":"<p>Depending on the serving tool used to deploy a trained model, resource allocation can be configured at different levels. While deployments on Docker containers only support a fixed number of resources (CPU and memory), using Kubernetes or KServe allows a better exploitation of the resources available in the platform, by enabling you to specify how many CPUs, GPUs, and memory are allocated to a deployment. See the compatibility matrix.</p>"},{"location":"user_guides/mlops/serving/resources/#gui","title":"GUI","text":""},{"location":"user_guides/mlops/serving/resources/#step-1-create-new-deployment","title":"Step 1: Create new deployment","text":"<p>If you have at least one model already trained and saved in the Model Registry, navigate to the deployments page by clicking on the <code>Deployments</code> tab on the navigation menu on the left.</p> <p> Deployments navigation tab </p> <p>Once in the deployments page, you can create a new deployment by either clicking on <code>New deployment</code> (if there are no existing deployments) or on <code>Create new deployment</code> it the top-right corner. Both options will open the deployment creation form.</p>"},{"location":"user_guides/mlops/serving/resources/#step-2-go-to-advanced-options","title":"Step 2: Go to advanced options","text":"<p>A simplified creation form will appear including the most common deployment fields from all available configurations. Resource allocation is part of the advanced options of a deployment. To navigate to the advanced creation form, click on <code>Advanced options</code>.</p> <p> Advanced options. Go to advanced deployment creation form </p>"},{"location":"user_guides/mlops/serving/resources/#step-3-configure-resource-allocation","title":"Step 3: Configure resource allocation","text":"<p>In the <code>Resource allocation</code> section of the form, you can optionally set the resources to be allocated to the predictor and/or the transformer (if available). Moreover, you can choose the minimum number of replicas for each of these components.</p> Scale-to-zero capabilities <p>Deployments with KServe enabled can scale to zero by choosing <code>0</code> as the number of instances.</p> <p> Resource allocation for the predictor and transformer </p> <p>Once you are done with the changes, click on <code>Create new deployment</code> at the bottom of the page to create the deployment for your model.</p>"},{"location":"user_guides/mlops/serving/resources/#code","title":"Code","text":""},{"location":"user_guides/mlops/serving/resources/#step-1-connect-to-hopsworks","title":"Step 1: Connect to Hopsworks","text":"Python <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n</code></pre>"},{"location":"user_guides/mlops/serving/resources/#step-2-define-the-predictor-resource-configuration","title":"Step 2: Define the predictor resource configuration","text":"Python <pre><code>from hsml.resources import PredictorResources, Resources\n\nminimum_res = Resources(cores=1, memory=128, gpus=1)\nmaximum_res = Resources(cores=2, memory=256, gpus=1)\n\npredictor_res = PredictorResources(num_instances=1, requests=minimum_res, limits=maximum_res)\n</code></pre>"},{"location":"user_guides/mlops/serving/resources/#step-3-optional-define-the-transformer-resource-configuration","title":"Step 3 (Optional): Define the transformer resource configuration","text":"Python <pre><code>from hsml.resources import TransformerResources\n\nminimum_res = Resources(cores=1, memory=128, gpus=1)\nmaximum_res = Resources(cores=2, memory=256, gpus=1)\n\ntransformer_res = TransformerResources(num_instances=2, requests=minimum_res, limits=maximum_res)\n</code></pre>"},{"location":"user_guides/mlops/serving/resources/#step-4-create-a-deployment-with-the-resource-configuration","title":"Step 4: Create a deployment with the resource configuration","text":"Python <pre><code>my_model = mr.get_model(\"my_model\", version=1)\n\nmy_predictor = ms.create_predictor(my_model,\n                                  resources=predictor_res,\n                                  # transformer=Transformer(script_file,\n                                  #                         resources=transformer_res)\n                                  )\nmy_predictor.deploy()\n\n# or\n\nmy_deployment = ms.create_deployment(my_predictor)\nmy_deployment.save()\n</code></pre>"},{"location":"user_guides/mlops/serving/resources/#api-reference","title":"API Reference","text":"<p><code>Resources</code></p>"},{"location":"user_guides/mlops/serving/resources/#compatibility-matrix","title":"Compatibility matrix","text":"Show supported resource allocation configuration Serving tool Component Resources Docker Predictor Fixed Transformer \u274c Kubernetes Predictor Minimum resources Transformer \u274c KServe Predictor Minimum / maximum resources Transformer Minimum / maximum resources"},{"location":"user_guides/mlops/serving/rest-api/","title":"Hopsworks Model Serving REST API","text":""},{"location":"user_guides/mlops/serving/rest-api/#introduction","title":"Introduction","text":"<p>Hopsworks provides model serving capabilities by leveraging KServe as the model serving platform and Istio as the ingress gateway to the model deployments.</p> <p>This document explains how to interact with a model deployment via REST API.</p>"},{"location":"user_guides/mlops/serving/rest-api/#base-url","title":"Base URL","text":"<p>Deployed models are accessible through the Istio ingress gateway. The URL to interact with a model deployment is provided on the model deployment page in the Hopsworks UI.</p> <p>The URL follows the format <code>http://&lt;ISTIO_GATEWAY_IP&gt;/&lt;RESOURCE_PATH&gt;</code>, where <code>RESOURCE_PATH</code> depends on the <code>Predictor.model_server</code> (e.g., vLLM, TensorFlow Serving, SKLearn ModelServer).</p> <p> Deployment Endpoints </p>"},{"location":"user_guides/mlops/serving/rest-api/#authentication","title":"Authentication","text":"<p>All requests must include an API Key for authentication. You can create an API by following this guide.</p> <p>Include the key in the Authorization header:</p> <pre><code>Authorization: ApiKey &lt;API_KEY_VALUE&gt;\n</code></pre>"},{"location":"user_guides/mlops/serving/rest-api/#headers","title":"Headers","text":"Header Description Example Value <code>Host</code> Model\u2019s hostname, provided in Hopsworks UI. <code>fraud.test.hopsworks.ai</code> <code>Authorization</code> API key for authentication. <code>ApiKey &lt;your_api_key&gt;</code> <code>Content-Type</code> Request payload type (always JSON). <code>application/json</code>"},{"location":"user_guides/mlops/serving/rest-api/#request-format","title":"Request Format","text":"<p>The request format depends on the model sever being used.</p> <p>For predictive inference (i.e., for Tensorflow or SkLearn or Python Serving). The request must be sent as a JSON object containing an <code>inputs</code> or <code>instances</code> field. See more information on the request format. An example for this is given below.</p> PythonCurl <p>REST API example for Predictive Inference (Tensorflow or SkLearn or Python Serving)</p> <pre><code>import requests\n\ndata = {\n    \"inputs\": [\n        [\n            4641025220953719,\n            4920355418495856\n        ]\n    ]\n}\n\nheaders = {\n    \"Host\": \"fraud.test.hopsworks.ai\",\n    \"Authorization\": \"ApiKey 8kDOlnRlJU4kiV1Y.RmFNJY3XKAUSqmJZ03kbUbXKMQSHveSBgMIGT84qrM5qXMjLib7hdlfGeg8fBQZp\",\n    \"Content-Type\": \"application/json\"\n}\n\nresponse = requests.post(\n    \"http://10.87.42.108/v1/models/fraud:predict\",\n    headers=headers,\n    json=data\n)\nprint(response.json())\n</code></pre> <p>REST API example for Predictive Inference (Tensorflow or SkLearn or Python Serving)</p> <pre><code>curl -X POST \"http://10.87.42.108/v1/models/fraud:predict\" \\\n  -H \"Host: fraud.test.hopsworks.ai\" \\\n  -H \"Authorization: ApiKey 8kDOlnRlJU4kiV1Y.RmFNJY3XKAUSqmJZ03kbUbXKMQSHveSBgMIGT84qrM5qXMjLib7hdlfGeg8fBQZp\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n        \"inputs\": [\n          [\n            4641025220953719,\n            4920355418495856\n          ]\n        ]\n      }'\n</code></pre> <p>For generative inference (i.e vLLM) the response follows the OpenAI specification.</p>"},{"location":"user_guides/mlops/serving/rest-api/#response","title":"Response","text":"<p>The model returns predictions in a JSON object. The response depends on the model server implementation. You can find more information regarding specific model servers in the Kserve documentation.</p>"},{"location":"user_guides/mlops/serving/transformer/","title":"How To Configure A Transformer","text":""},{"location":"user_guides/mlops/serving/transformer/#introduction","title":"Introduction","text":"<p>In this guide, you will learn how to configure a transformer in a deployment.</p> <p>Transformers are used to apply transformations on the model inputs before sending them to the predictor for making predictions using the model. They run on a built-in Flask server provided by Hopsworks and require a user-provided python script implementing the Transformer class.</p> Warning <p>Transformers are only supported in deployments using KServe as serving tool.</p> <p>A transformer has two configurable components:</p> <ol> <li>User-provided script</li> <li>Resources</li> </ol> <p>See examples of transformer scripts in the serving example notebooks.</p>"},{"location":"user_guides/mlops/serving/transformer/#gui","title":"GUI","text":""},{"location":"user_guides/mlops/serving/transformer/#step-1-create-new-deployment","title":"Step 1: Create new deployment","text":"<p>If you have at least one model already trained and saved in the Model Registry, navigate to the deployments page by clicking on the <code>Deployments</code> tab on the navigation menu on the left.</p> <p> Deployments navigation tab </p> <p>Once in the deployments page, you can create a new deployment by either clicking on <code>New deployment</code> (if there are no existing deployments) or on <code>Create new deployment</code> it the top-right corner. Both options will open the deployment creation form.</p>"},{"location":"user_guides/mlops/serving/transformer/#step-2-go-to-advanced-options","title":"Step 2: Go to advanced options","text":"<p>A simplified creation form will appear including the most common deployment fields from all available configurations. Transformers are part of the advanced options of a deployment. To navigate to the advanced creation form, click on <code>Advanced options</code>.</p> <p> Advanced options. Go to advanced deployment creation form </p>"},{"location":"user_guides/mlops/serving/transformer/#step-3-select-a-transformer-script","title":"Step 3: Select a transformer script","text":"<p>Transformers require KServe as the serving platform for the deployment. Make sure that KServe is enabled for this deployment by activating the corresponding checkbox.</p> <p> Enable KServe in the advanced deployment form </p> <p>Then, if the transformer script is already located in Hopsworks, click on <code>From project</code> and navigate through the file system to find your script. Otherwise, you can click on <code>Upload new file</code> to upload the transformer script now.</p> <p> Choose a transformer script in the advanced deployment form </p> <p>After selecting the transformer script, you can optionally configure resource allocation for your transformer (see Step 4). Otherwise, click on <code>Create new deployment</code> to create the deployment for your model.</p>"},{"location":"user_guides/mlops/serving/transformer/#step-4-optional-configure-resource-allocation","title":"Step 4 (Optional): Configure resource allocation","text":"<p>At the end of the page, you can configure the resources to be allocated for the transformer, as well as the minimum and maximum number of replicas to be deployed.</p> Scale-to-zero capabilities <p>Deployments with KServe enabled can scale to zero by choosing <code>0</code> as the number of instances.</p> <p> Resource allocation for the transformer </p> <p>Once you are done with the changes, click on <code>Create new deployment</code> at the bottom of the page to create the deployment for your model.</p>"},{"location":"user_guides/mlops/serving/transformer/#code","title":"Code","text":""},{"location":"user_guides/mlops/serving/transformer/#step-1-connect-to-hopsworks","title":"Step 1: Connect to Hopsworks","text":"Python <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# get Dataset API instance\ndataset_api = project.get_dataset_api()\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n</code></pre>"},{"location":"user_guides/mlops/serving/transformer/#step-2-implement-transformer-script","title":"Step 2: Implement transformer script","text":"Transformer <pre><code>class Transformer():\n    def __init__(self):\n        \"\"\" Initialization code goes here\"\"\"\n        pass\n\n    def preprocess(self, inputs):\n        \"\"\" Transform the requests inputs here. The object returned by this method will be used as model input to make predictions. \"\"\"\n        return inputs\n\n    def postprocess(self, outputs):\n        \"\"\" Transform the predictions computed by the model before returning a response \"\"\"\n        return outputs\n</code></pre> <p>Jupyter magic</p> <p>In a jupyter notebook, you can add <code>%%writefile my_transformer.py</code> at the top of the cell to save it as a local file.</p>"},{"location":"user_guides/mlops/serving/transformer/#step-3-upload-the-script-to-your-project","title":"Step 3: Upload the script to your project","text":"<p>You can also use the UI to upload your transformer script. See above</p> Python <pre><code>uploaded_file_path = dataset_api.upload(\"my_transformer.py\", \"Resources\", overwrite=True)\ntransformer_script_path = os.path.join(\"/Projects\", project.name, uploaded_file_path)\n</code></pre>"},{"location":"user_guides/mlops/serving/transformer/#step-4-define-a-transformer","title":"Step 4: Define a transformer","text":"Python <pre><code>my_transformer = ms.create_transformer(script_file=uploaded_file_path)\n\n# or\n\nfrom hsml.transformer import Transformer\n\nmy_transformer = Transformer(script_file)\n</code></pre>"},{"location":"user_guides/mlops/serving/transformer/#step-5-create-a-deployment-with-the-transformer","title":"Step 5: Create a deployment with the transformer","text":"Python <pre><code>my_predictor = ms.create_predictor(transformer=my_transformer)\nmy_deployment = my_predictor.deploy()\n\n# or\nmy_deployment = ms.create_deployment(my_predictor, transformer=my_transformer)\nmy_deployment.save()\n</code></pre>"},{"location":"user_guides/mlops/serving/transformer/#api-reference","title":"API Reference","text":"<p><code>Transformer</code></p>"},{"location":"user_guides/mlops/serving/transformer/#resources","title":"Resources","text":"<p>Resources include the number of replicas for the deployment as well as the resources (i.e., memory, CPU, GPU) to be allocated per replica. To learn about the different combinations available, see the Resources Guide.</p>"},{"location":"user_guides/mlops/serving/transformer/#environment-variables","title":"Environment variables","text":"<p>A number of different environment variables is available in the transformer to ease its implementation.</p> Show environment variables Name Description ARTIFACT_FILES_PATH Local path to the model artifact files DEPLOYMENT_NAME Name of the current deployment MODEL_NAME Name of the model being served by the current deployment MODEL_VERSION Version of the model being served by the current deployment ARTIFACT_VERSION Version of the model artifact being served by the current deployment"},{"location":"user_guides/mlops/serving/troubleshooting/","title":"How To Troubleshoot A Model Deployment","text":""},{"location":"user_guides/mlops/serving/troubleshooting/#introduction","title":"Introduction","text":"<p>In this guide, you will learn how to troubleshoot a deployment that is having issues to serve a trained model. But before that, it is important to understand how deployment states are defined and the possible transitions between conditions.</p> <p>When a deployment is starting, it follows an ordered sequence of states before becoming ready for serving predictions. Similarly, it follows an ordered sequence of states when being stopped, although with fewer steps.</p>"},{"location":"user_guides/mlops/serving/troubleshooting/#gui","title":"GUI","text":""},{"location":"user_guides/mlops/serving/troubleshooting/#step-1-inspect-deployment-status","title":"Step 1: Inspect deployment status","text":"<p>If you have at least one deployment already created, navigate to the deployments page by clicking on the <code>Deployments</code> tab on the navigation menu on the left.</p> <p> Deployments navigation tab </p> <p>Once in the deployments page, find the deployment you want to inspect. Next to the actions buttons, you can find an indicator showing the current status of the deployment. For a more descriptive representation, this indicator changes its color based on the status.</p> <p>To inspect the condition of the deployment, click on the name of the deployment to open the deployment overview page.</p>"},{"location":"user_guides/mlops/serving/troubleshooting/#step-2-inspect-condition","title":"Step 2: Inspect condition","text":"<p>At the top of page, you can find the same status indicator mentioned in the previous step. Below it, a one-line message is shown with a more detailed description of the deployment status. This message is built using the current status condition of the deployment.</p> <p>Oftentimes, the status and the one-line description are enough to understand the current state of a deployment. For instance, when the cluster lacks enough allocatable resources to meet the deployment requirements, a meaningful error message will be shown with the root cause.</p> <p> Condition of a deployment that cannot be scheduled </p> <p>However, when the deployment fails to start further details might be needed depending on the source of failure. For example, failures in the initialization or starting steps will show a less relevant message. In those cases, you can explore the deployments logs in search of the cause of the problem.</p> <p> Condition of a deployment that fails to start </p>"},{"location":"user_guides/mlops/serving/troubleshooting/#step-3-explore-transient-logs","title":"Step 3: Explore transient logs","text":"<p>Each deployment is composed of several components depending on its configuration and the model being served. Transient logs refer to component-specific logs that are directly retrieved from the component itself. Therefore, these logs can only be retrieved as long as the deployment components are reachable.</p> <p>Transient logs are informative and fast to retrieve, facilitating the troubleshooting of deployment components at a glance</p> <p>Transient logs are convenient when access to the most recent logs of a deployment is needed.</p> <p>Info</p> <p>When a deployment is in idle state, there are no components running (i.e., scaled to zero) and, thus, no transient logs are available.</p> <p>Note</p> <p>In the current version of Hopsworks, transient logs can only be accessed using the Hopsworks Machine Learning Python library. See an example.</p>"},{"location":"user_guides/mlops/serving/troubleshooting/#step-4-explore-historical-logs","title":"Step 4: Explore historical logs","text":"<p>Transient logs are continuously collected and stored in OpenSearch, where they become historical logs accessible using the integrated OpenSearch Dashboards. Therefore, historical logs contain the same information than transient logs. However, there might be cases where transient logs could not be collected in time for a specific component and, thus, not included in the historical logs.</p> <p>Historical logs are persisted transient logs that can be queried, filtered and sorted using OpenSearch Dashboards, facilitating a more sophisticated exploration of past records.</p> <p>Historical logs are convenient when a deployment fails occasionally, either at inference time or without a clear reason. In this case, narrowing the inspection of component-specific logs at a concrete point in time and searching for keywords can be helpful.</p> <p>To access the OpenSearch Dashboards, click on the <code>See logs</code> button at the top of the deployment overview page.</p> <p> Access to historical logs of a deployment </p> <p>Note</p> <p>In case you are not familiar with the interface, you may find the official documentation useful.</p> <p>Once in the OpenSearch Dashboards, you can search for keywords, apply multiple filters and sort the records by timestamp.</p> Show available filters Filter Description component Name of the deployment component (i.e., predictor or transformer) container_name Name of the container within a component (i.e., kserve-container, storage-initializer, inference-logger) serving_name Name of the deployment model_name Name of the model being served model_version Version of the model being served timestamp Timestamp when the record was reported"},{"location":"user_guides/mlops/serving/troubleshooting/#code","title":"Code","text":""},{"location":"user_guides/mlops/serving/troubleshooting/#step-1-connect-to-hopsworks","title":"Step 1: Connect to Hopsworks","text":"Python <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n</code></pre>"},{"location":"user_guides/mlops/serving/troubleshooting/#step-2-retrieve-an-existing-deployment","title":"Step 2: Retrieve an existing deployment","text":"Python <pre><code>deployment = ms.get_deployment(\"mydeployment\")\n</code></pre>"},{"location":"user_guides/mlops/serving/troubleshooting/#step-3-get-current-deployments-predictor-state","title":"Step 3: Get current deployment's predictor state","text":"Python <pre><code>state = deployment.get_state()\n\nstate.describe()\n</code></pre>"},{"location":"user_guides/mlops/serving/troubleshooting/#step-4-explore-transient-logs","title":"Step 4: Explore transient logs","text":"Python <pre><code>deployment.get_logs(component=\"predictor|transformer\", tail=10)\n</code></pre>"},{"location":"user_guides/mlops/serving/troubleshooting/#api-reference","title":"API Reference","text":"<p><code>Deployment</code></p> <p><code>PredictorState</code></p>"},{"location":"user_guides/projects/","title":"Projects Guides","text":"<p>This section serves to provide guides and examples for the common usage of services in a Project through the Hopsworks UI and APIs.</p> <ul> <li>Projects</li> <li>Authentication</li> <li>API Keys</li> <li>Jupyter</li> <li>Jobs</li> <li>Git</li> <li>Python Environment</li> <li>Kafka</li> <li>OpenSearch</li> <li>Secrets</li> </ul>"},{"location":"user_guides/projects/airflow/airflow/","title":"Orchestrate Jobs using Apache Airflow","text":""},{"location":"user_guides/projects/airflow/airflow/#introduction","title":"Introduction","text":"<p>Hopsworks jobs can be orchestrated using Apache Airflow. You can define a Airflow DAG (Directed Acyclic Graph) containing the dependencies between Hopsworks jobs. You can then schedule the DAG to be executed at a specific schedule using a cron expression.</p> <p>Airflow DAGs are defined as Python files. Within the Python file, different operators can be used to trigger different actions. Hopsworks provides an operator to execute jobs on Hopsworks and a sensor to wait for a specific job to finish.</p>"},{"location":"user_guides/projects/airflow/airflow/#use-apache-airflow-in-hopsworks","title":"Use Apache Airflow in Hopsworks","text":"<p>Hopsworks deployments include a deployment of Apache Airflow. You can access it from the Hopsworks UI by clicking on the Airflow button on the left menu.</p> <p>Airflow is configured to enforce Role Based Access Control (RBAC) to the Airflow DAGs. Admin users on Hopsworks have access to all the DAGs in the deployment. Regular users can access all the DAGs of the projects they are a member of.</p> <p>Access Control</p> <p>Airflow does not have any knowledge of the Hopsworks project you are currently working on. As such, when opening the Airflow UI, you will see all the DAGs all of the projects you are a member of.</p>"},{"location":"user_guides/projects/airflow/airflow/#hopsworks-dag-builder","title":"Hopsworks DAG Builder","text":"Airflow DAG Builder <p>You can create a new Airflow DAG to orchestrate jobs using the Hopsworks DAG builder tool. Click on New Workflow to create a new Airflow DAG. You should provide a name for the DAG as well as a schedule interval. You can define the schedule using the dropdown menus or by providing a cron expression.</p> <p>You can add to the DAG Hopsworks operators and sensors:</p> <ul> <li> <p>Operator: The operator is used to trigger a job execution. When configuring the operator you select the job you want to execute and you can optionally provide execution arguments. You can decide whether or not the operator should wait for the execution to be completed. If you select the wait option, the operator will block and Airflow will not execute any parallel task. If you select the wait option the Airflow task fails if the job fails. If you want to execute tasks in parallel, you should not select the wait option but instead use the sensor. When configuring the operator, you can can also provide which other Airflow tasks it depends on. If you add a dependency, the task will be executed only after the upstream tasks have been executed successfully.</p> </li> <li> <p>Sensor: The sensor can be used to wait for executions to be completed. Similarly to the wait option of the operator, the sensor blocks until the job execution is completed. The sensor can be used to launch several jobs in parallel and wait for their execution to be completed. Please note that the sensor is defined at the job level rather than the execution level. The sensor will wait for the most recent execution to be completed and it will fail the Airflow task if the execution was not successful.</p> </li> </ul> <p>You can then create the DAG and Hopsworks will generate the Python file.</p>"},{"location":"user_guides/projects/airflow/airflow/#write-your-own-dag","title":"Write your own DAG","text":"<p>If you prefer to code the DAGs or you want to edit a DAG built with the builder tool, you can do so. The Airflow DAGs are stored in the Airflow dataset which you can access using the file browser in the project settings.</p> <p>When writing the code for the DAG you can invoke the operator as follows:</p> <pre><code>HopsworksLaunchOperator(dag=dag,\n task_id=\"profiles_fg_0\",\n project_name=\"airflow_doc\",\n job_name=\"profiles_fg\",\n job_arguments=\"\",\n wait_for_completion=True)\n</code></pre> <p>You should provide the name of the Airflow task (<code>task_id</code>) and the Hopsworks job information (<code>project_name</code>, <code>job_name</code>, <code>job_arguments</code>). You can set the <code>wait_for_completion</code> flag to <code>True</code> if you want the operator to block and wait for the job execution to be finished.</p> <p>Similarly, you can invoke the sensor as shown below. You should provide the name of the Airflow task (<code>task_id</code>) and the Hopsworks job information (<code>project_name</code>, <code>job_name</code>)</p> <pre><code>HopsworksJobSuccessSensor(dag=dag,\n    task_id='wait_for_profiles_fg',\n project_name=\"airflow_doc\",\n    job_name='profiles_fg')\n</code></pre> <p>When writing the DAG file, you should also add the <code>access_control</code> parameter to the DAG configuration. The <code>access_control</code> parameter specifies which projects have access to the DAG and which actions the project members can perform on it. If you do not specify the <code>access_control</code> option, project members will not be able to see the DAG in the Airflow UI.</p> <p>Admin access</p> <p>The <code>access_control</code> configuration does not apply to Hopsworks admin users which have full access to all the DAGs even if they are not member of the project.</p> <pre><code>    dag = DAG(\n        dag_id = \"example_dag\",\n        default_args = args,\n        access_control = {\n            \"project_name\": {\"can_dag_read\", \"can_dag_edit\"},\n        },\n\n        schedule_interval = \"0 4 * * *\"\n    )\n</code></pre> <p>Project Name</p> <p>You should replace the <code>project_name</code> in the snippet above with the name of your own project</p>"},{"location":"user_guides/projects/airflow/airflow/#manage-airflow-dags-using-git","title":"Manage Airflow DAGs using Git","text":"<p>You can leverage the Git integration to track your Airflow DAGs in a git repository. Airflow will only consider the DAG files which are stored in the Airflow Dataset in Hopsworks. After cloning the git repository in Hopsworks, you can automate the process of copying the DAG file in the Airflow Dataset using <code>DatasetApi.copy</code> of the Hopsworks API.</p>"},{"location":"user_guides/projects/api_key/create_api_key/","title":"How To Create An API Key","text":""},{"location":"user_guides/projects/api_key/create_api_key/#introduction","title":"Introduction","text":"<p>An API key allows a user or a program to make API calls without having to authenticate with a username and password. To access an endpoint using an API key, a client should send the access token using the ApiKey authentication scheme.</p> <p>The API Key can now be used when connecting to your Hopsworks instance using the <code>hopsworks</code>, <code>hsfs</code> or <code>hsml</code> python library or set in the <code>ApiKey</code> header for the REST API.</p> <pre><code>GET /resource HTTP/1.1\nHost: server.hopsworks.ai\nAuthorization: ApiKey &lt;api_key&gt;\n</code></pre>"},{"location":"user_guides/projects/api_key/create_api_key/#ui","title":"UI","text":"<p>In this guide, you will learn how to create an API key.</p>"},{"location":"user_guides/projects/api_key/create_api_key/#step-1-navigate-to-api-keys","title":"Step 1: Navigate to API Keys","text":"<p>In the Account Settings page you can find the API section showing a list of all API keys.</p> <p> List of API Keys </p>"},{"location":"user_guides/projects/api_key/create_api_key/#step-2-create-an-api-key","title":"Step 2: Create an API Key","text":"<p>Click <code>New Api key</code>, select the required scopes and create it by clicking <code>Create Api Key</code>.</p> <p>Copy the value and save it in a secure location, such as a password manager.</p> <p> Create new API Key </p>"},{"location":"user_guides/projects/api_key/create_api_key/#login-with-api-key-using-sdk","title":"Login with API Key using SDK","text":"<p>In this guide you learned how to create an API Key. You can now use the API Key to login using the <code>hopsworks</code> python SDK.</p>"},{"location":"user_guides/projects/auth/krb/","title":"Login using Kerberos","text":""},{"location":"user_guides/projects/auth/krb/#introduction","title":"Introduction","text":"<p>Hopsworks supports different methods of authentication. Here we will look at authentication using Kerberos.</p>"},{"location":"user_guides/projects/auth/krb/#prerequisites","title":"Prerequisites","text":"<p>A Hopsworks cluster with Kerberos authentication. See Configure Kerberos on how to configure Kerberos on your cluster.</p>"},{"location":"user_guides/projects/auth/krb/#step-1-log-in-with-kerberos","title":"Step 1: Log in with Kerberos","text":"<p>If Kerberos is configured you will see a Log in using alternative on the login page. Choose Kerberos and click on Go to Hopsworks to login.</p> Log in using Kerberos <p>If password login is disabled you only see the Log in using Kerberos/SSO alternative. Click on Go to Hopsworks to login.</p> Kerberos only authentication <p>To be able to authenticate with Kerberos you need to configure your browser to use Kerberos. Note that without a properly configured browser, the Kerberos token is not sent to the server and so SSO will not work.</p> <p>If Kerberos is not configured properly you will see Wrong credentials message when trying to log in.</p> Missing Kerberos ticket"},{"location":"user_guides/projects/auth/krb/#step-2-give-consent","title":"Step 2: Give consent","text":"<p>When logging in with Kerberos for the first time Hopsworks will retrieve and save consented claims (firstname, lastname and email), about the logged in end-user. If you have multiple email addresses registered in Kerberos you can choose one to use with Hopsworks.</p> <p>If you do not want your information to be saved in Hopsworks you can click Cancel. This will redirect you back to the login page.</p> Give consent <p>After clicking on Register you will be redirected to the landing page:  Landing page </p> <p>In the landing page, you will find two buttons. Use these buttons to either create a demo project or a new project.</p>"},{"location":"user_guides/projects/auth/ldap/","title":"Login using LDAP","text":""},{"location":"user_guides/projects/auth/ldap/#introduction","title":"Introduction","text":"<p>Hopsworks supports different methods of authentication. Here we will look at authentication using LDAP.</p>"},{"location":"user_guides/projects/auth/ldap/#prerequisites","title":"Prerequisites","text":"<p>A Hopsworks cluster with LDAP authentication. See Configure LDAP on how to configure LDAP on your cluster.</p>"},{"location":"user_guides/projects/auth/ldap/#step-1-log-in-with-ldap","title":"Step 1: Log in with LDAP","text":"<p>If LDAP is configured you will see a Log in using alternative on the login page. Choose LDAP and type in your username and password then click on Login.</p> <p>Note that you need to use your LDAP credentials.</p> Log in using LDAP"},{"location":"user_guides/projects/auth/ldap/#step-2-give-consent","title":"Step 2: Give consent","text":"<p>When logging in with LDAP for the first time Hopsworks will retrieve and save consented claims (firstname, lastname and email), about the logged in end-user. If you have multiple email addresses registered in LDAP you can choose one to use with Hopsworks.</p> <p>If you do not want your information to be saved in Hopsworks you can click Cancel. This will redirect you back to the login page.</p> Give consent <p>After clicking on Register you will be redirected to the landing page:  Landing page </p> <p>In the landing page, you will find two buttons. Use these buttons to either create a demo project or a new project.</p>"},{"location":"user_guides/projects/auth/login/","title":"Log in To Hopsworks","text":""},{"location":"user_guides/projects/auth/login/#introduction","title":"Introduction","text":"<p>Hopsworks supports different methods of authentication. Here we will look at authentication using username and password.</p>"},{"location":"user_guides/projects/auth/login/#prerequisites","title":"Prerequisites","text":"<p>An account on a Hopsworks cluster.</p>"},{"location":"user_guides/projects/auth/login/#step-1-log-in-with-email-and-password","title":"Step 1: Log in with email and password","text":"<p>After your account is validated by an administrator you can use your email and password to login.</p> Login with password"},{"location":"user_guides/projects/auth/login/#step-2-two-factor-authentication","title":"Step 2: Two-factor authentication","text":"<p>If two-factor authentication is enabled you will be presented with a two-factor authentication window after you enter your password. Use your authenticator app (example. Google Authenticator) on your phone to get a one-time password.</p> One time password <p>Upon successful login, you will arrive at the landing page:</p> Landing page <p>In the landing page, you will find two buttons. Use these buttons to either create a demo project or a new project.</p>"},{"location":"user_guides/projects/auth/oauth/","title":"Login Using A Third-party Identity Provider","text":""},{"location":"user_guides/projects/auth/oauth/#introduction","title":"Introduction","text":"<p>Hopsworks supports different methods of authentication. Here we will look at authentication using Third-party Identity Provider.</p>"},{"location":"user_guides/projects/auth/oauth/#prerequisites","title":"Prerequisites","text":"<p>A Hopsworks cluster with OAuth authentication. See Configure OAuth2 on how to configure OAuth on your cluster.</p>"},{"location":"user_guides/projects/auth/oauth/#step-1-log-in-with-oauth","title":"Step 1: Log in with OAuth","text":"<p>If OAuth is configured a Login with button will appear in the login page. Use this button to log in to Hopsworks using your OAuth credentials.</p> Login with OAuth2"},{"location":"user_guides/projects/auth/oauth/#step-2-give-consent","title":"Step 2: Give consent","text":"<p>When logging in with OAuth for the first time Hopsworks will retrieve and save consented claims (firstname, lastname and email), about the logged in end-user.</p> Give consent <p>After clicking on Register you will be redirected to the landing page:  Landing page </p> <p>In the landing page, you will find two buttons. Use these buttons to either create a demo project or a new project.</p>"},{"location":"user_guides/projects/auth/profile/","title":"Update Your Profile and Credentials","text":""},{"location":"user_guides/projects/auth/profile/#introduction","title":"Introduction","text":"<p>A profile is required to access Hopsworks. A profile is created when a user registers and can be updated via Account settings.</p>"},{"location":"user_guides/projects/auth/profile/#prerequisites","title":"Prerequisites","text":"<p>An account on a Hopsworks cluster.</p> <p>Updating profile and credentials is not supported if you are using Third-party Identity Providers like Kerberos, LDAP, or OAuth to authenticate to Hopsworks.</p>"},{"location":"user_guides/projects/auth/profile/#step-1-go-to-your-account-settings","title":"Step 1: Go to your Account settings","text":"<p>After you have logged in, in the upper right-hand corner of the screen, you will see your name. Click on your name, then click on the menu item Account settings. The account settings page will open with profile tab selected. In this tab you can change your first and last name. You cannot change your email address and will need to create a new account if you wish to change your email address.</p> Update profile"},{"location":"user_guides/projects/auth/profile/#step-2-update-credential","title":"Step 2: Update credential","text":"<p>To update your credential go to the Authentication tab as shown in the image below.  Update credential </p>"},{"location":"user_guides/projects/auth/profile/#step-3-enablereset-two-factor-authentication","title":"Step 3: Enable/Reset Two-factor Authentication","text":"<p>You can also change your two-factor setting in the Authentication tab. Two-factor authentication is only available if it is enabled from the cluster administration page.</p> Enable Two-factor Authentication <p>After enabling or resetting two-factor you will be presented with a QR Code. You will then need to scan the QR code to add it on your phone's authenticator application (example. Google Authenticator).</p> <p>If you miss this step, you will have to recover your smartphone credentials at a later stage.</p> Register Two-factor Authentication <p>Use the one time password generated by your authenticator app to confirm the registration.</p>"},{"location":"user_guides/projects/auth/recovery/","title":"Password Recovery","text":""},{"location":"user_guides/projects/auth/recovery/#introduction","title":"Introduction","text":"<p>This topic describes how to recover a forgotten password.</p>"},{"location":"user_guides/projects/auth/recovery/#prerequisites","title":"Prerequisites","text":"<p>An account on a Hopsworks cluster.</p>"},{"location":"user_guides/projects/auth/recovery/#step-1-request-password-reset","title":"Step 1: Request password reset","text":"<p>If you forget your password start by clicking on Forgot password on the login page. Enter your email and click on the Send reset link button.  Password reset </p>"},{"location":"user_guides/projects/auth/recovery/#step-2-use-the-password-reset-link","title":"Step 2: Use the password reset link","text":"<p>A password reset link will be sent to the email address you entered if the email is found in the system. Click on the reset link to set your new password.</p>"},{"location":"user_guides/projects/auth/registration/","title":"Register A New Account On Hopsworks","text":""},{"location":"user_guides/projects/auth/registration/#introduction","title":"Introduction","text":"<p>Hopsworks supports different methods of authentication. To use username and password as the method of authentication, you first need to register.</p>"},{"location":"user_guides/projects/auth/registration/#prerequisites","title":"Prerequisites","text":"<p>Registration enabled Hopsworks cluster.</p> <p>The process for registering a new account is as follows</p>"},{"location":"user_guides/projects/auth/registration/#step-1-register-a-new-account","title":"Step 1: Register a new account","text":"<p>Click on the Register button on the login page and register your email address and details.</p> Register new account"},{"location":"user_guides/projects/auth/registration/#step-2-enable-two-factor-authentication","title":"Step 2: Enable Two-Factor Authentication","text":"<p>If two-factor authentication is required you will be presented with a page like in the figure below. Scan the QR code or type the code in bold to register your account in your authenticator app (example. Google Authenticator).</p> Add two-factor authentication"},{"location":"user_guides/projects/auth/registration/#step-3-validate-your-email-address","title":"Step 3: Validate your email address","text":"<p>Validate your email address by clicking on the link in the validation email you received. After your account is created an administrator needs to validate your account before you can log in.</p> Account created"},{"location":"user_guides/projects/git/clone_repo/","title":"How To Clone a Git Repository","text":""},{"location":"user_guides/projects/git/clone_repo/#introduction","title":"Introduction","text":"<p>Repositories are cloned and managed within the scope of a project. The content of the repository will reside on the Hopsworks File System. The content of the repository can be edited from Jupyter notebooks and can for example be used to configure Jobs. Repositories can be managed from the Git section in the project settings. The Git overview in the project settings provides a list of repositories currently cloned within the project, the location of their content as well which branch and commit their HEAD is currently at.</p>"},{"location":"user_guides/projects/git/clone_repo/#prerequisites","title":"Prerequisites","text":"<ul> <li>For cloning a private repository, you should configure a Git Provider with your git credentials. You can clone a GitHub and GitLab public repository without configuring the provider. However, for BitBucket you always need to configure the username and token to clone a repository.</li> </ul>"},{"location":"user_guides/projects/git/clone_repo/#ui","title":"UI","text":""},{"location":"user_guides/projects/git/clone_repo/#step-1-navigate-to-repositories","title":"Step 1: Navigate to repositories","text":"<p>In the left-hand sidebar found in your project click on <code>Project settings</code>, and then navigate to the <code>Git</code> section.</p> <p>This page lists all the cloned git repositories under <code>Repositories</code>, while operations performed on those repositories, e.g <code>push</code>/<code>pull</code>/<code>commit</code> are listed under <code>Git Executions</code>.</p> <p> Git repository overview </p>"},{"location":"user_guides/projects/git/clone_repo/#step-2-clone-a-repository","title":"Step 2: Clone a repository","text":"<p>To clone a new repository, click on the <code>Clone repository</code> button on the Git overview page.</p> <p> Git clone </p> <p>You should first choose the git provider e.g., GitHub, GitLab or BitBucket. If you are cloning a private repository, remember to configure the username and token for the provider first in Git Provider. The clone dialog also asks you to specify the URL of the repository to clone. The supported protocol is HTTPS. As an example, if the repository is hosted on GitHub, the URL should look like: <code>https://github.com/logicalclocks/hops-examples.git</code>.</p> <p>Then specify which branch you want to clone. By default the <code>main</code> branch will be used, however a different branch or commit can be specified by selecting <code>Clone from a specific branch</code>.</p> <p>You can select the folder, within your project, in which the repository should be cloned. By default, the repository is going to be cloned within the <code>Jupyter</code> dataset. However, by clicking on the location button, a different location can be selected.</p> <p>Finally, click on the <code>Clone repository</code> button to trigger the cloning of the repository.</p>"},{"location":"user_guides/projects/git/clone_repo/#step-3-track-progress-of-the-clone","title":"Step 3: Track progress of the clone","text":"<p>The progress of the git clone can be tracked under <code>Git Executions</code>.</p> <p> Track progress of clone </p>"},{"location":"user_guides/projects/git/clone_repo/#step-4-browse-repository-files","title":"Step 4: Browse repository files","text":"<p>In the <code>File browser</code> page you can now browse the files of the cloned repository. In the figure below, the repository is located in <code>Jupyter/hops-examples</code> directory.</p> <p> Browse repository files </p>"},{"location":"user_guides/projects/git/clone_repo/#code","title":"Code","text":"<p>You can also clone a repository through the hopsworks git API in python.</p>"},{"location":"user_guides/projects/git/clone_repo/#step-1-get-the-git-api","title":"Step 1: Get the git API","text":"<pre><code>import hopsworks\n\nproject = hopsworks.login()\n\ngit_api = project.get_git_api()\n</code></pre>"},{"location":"user_guides/projects/git/clone_repo/#step-2-clone-the-repository","title":"Step 2: Clone the repository","text":"<pre><code>REPO_URL=\"https://github.com/logicalclocks/hops-examples.git\" # git repository\nHOPSWORKS_FOLDER=\"Jupyter\" # path in Hopsworks filesystem to clone to\nPROVIDER=\"GitHub\"\nBRANCH=\"master\" # optional branch to clone\n\nexamples_repo = git_api.clone(REPO_URL, HOPSWORKS_FOLDER, PROVIDER, branch=BRANCH)\n</code></pre>"},{"location":"user_guides/projects/git/clone_repo/#api-reference","title":"API Reference","text":"<p>Api reference for git repositories is available here: <code>GitRepo</code></p> <p>A notebook for managing git can be found in the Git Management Tutorial.</p>"},{"location":"user_guides/projects/git/clone_repo/#errors-and-troubleshooting","title":"Errors and Troubleshooting","text":""},{"location":"user_guides/projects/git/clone_repo/#invalid-credentials","title":"Invalid credentials","text":"<p>This might happen when the credentials entered for the provider are incorrect. Try the following:</p> <ul> <li>Confirm that the settings for the provider ( in Account Settings &gt; Git providers) are correct. You must enter both your Git provider username and token.</li> <li>Confirm that you have selected the correct Git provider when cloning the repository.</li> <li>Ensure your personal access token has the correct repository access rights.</li> <li>Ensure your personal access token has not expired.</li> </ul>"},{"location":"user_guides/projects/git/clone_repo/#timeout-errors","title":"Timeout errors","text":"<p>Cloning a large repo or checking out a large branch may hit timeout errors. You can try again later if the system was under heavy load at the time.</p>"},{"location":"user_guides/projects/git/clone_repo/#symlink-errors","title":"Symlink errors","text":"<p>Git repositories with symlinks are not yet supported, therefore cloning repositories with symlinks will fail. You can create a separate branch to remove the symlinks, and clone from this branch.</p>"},{"location":"user_guides/projects/git/clone_repo/#going-further","title":"Going Further","text":"<p>You can now start Jupyter from the cloned git repository path to work with the files.</p>"},{"location":"user_guides/projects/git/configure_git_provider/","title":"How To Configure a Git Provider","text":""},{"location":"user_guides/projects/git/configure_git_provider/#introduction","title":"Introduction","text":"<p>When you perform Git operations on Hopsworks that need to interact with the remote repository, Hopsworks relies on the Git HTTPS protocol to perform those operations. Authentication with the remote repository happens through a token generated by the Git repository hosting service (GitHub, GitLab, BitBucket).</p> <p>Token permissions</p> <p>The token permissions should grant access to public and private repositories including read and write access to repository contents and commit statuses. If you are using the new GitHub access tokens, make sure you choose the correct <code>Resource owner</code> when generating the token for the repositories you will want to clone. For the <code>Repository permissions</code> of the new GitHub fine-grained token, you should at least give read and write access to <code>Commit statuses</code> and <code>Contents</code>.</p>"},{"location":"user_guides/projects/git/configure_git_provider/#ui","title":"UI","text":"<p>Documentation on how to generate a token for the supported Git hosting services is available here:</p> <ul> <li>GitHub</li> <li>GitLab</li> <li>BitBucket</li> </ul>"},{"location":"user_guides/projects/git/configure_git_provider/#step-1-navigate-to-git-providers","title":"Step 1: Navigate to Git Providers","text":"<p>You can access the <code>Git Providers</code> page of your Hopsworks cluster by clicking on your name, in the top right corner, and choosing <code>Account Settings</code> from the dropdown menu. The <code>Git providers</code> section displays which providers have been already configured and can be used to clone new repositories.</p> <p> Git provider configuration list </p>"},{"location":"user_guides/projects/git/configure_git_provider/#step-2-configure-a-provider","title":"Step 2: Configure a provider","text":"<p>Click on <code>Edit Configuration</code> to change a provider username or token, or to configure a new provider.</p> <p>Tick the checkbox next to the provider you want to configure and insert the username and the token to use for that provider.</p> <p> Git provider configuration </p> <p>Click <code>Create Configuration</code> to save the configuration.</p>"},{"location":"user_guides/projects/git/configure_git_provider/#step-3-provider-is-configured","title":"Step 3: Provider is configured","text":"<p>The configured provider should now be marked as configured.</p> <p> Git provider configured </p>"},{"location":"user_guides/projects/git/configure_git_provider/#code","title":"Code","text":"<p>You can also configure a git provider using the hopsworks git API in python.</p>"},{"location":"user_guides/projects/git/configure_git_provider/#step-1-get-the-git-api","title":"Step 1: Get the git API","text":"<pre><code>import hopsworks\n\nproject = hopsworks.login()\n\ngit_api = project.get_git_api()\n</code></pre>"},{"location":"user_guides/projects/git/configure_git_provider/#step-2-configure-git-provider","title":"Step 2: Configure git provider","text":"<pre><code>PROVIDER=\"GitHub\"\nGITHUB_USER=\"my_user\"\nAPI_TOKEN=\"my_token\"\n\ngit_api.set_provider(PROVIDER, GITHUB_USER, API_TOKEN)\n</code></pre>"},{"location":"user_guides/projects/git/configure_git_provider/#api-reference","title":"API Reference","text":"<p><code>GitProvider</code></p>"},{"location":"user_guides/projects/git/configure_git_provider/#going-further","title":"Going Further","text":"<p>You can now use the credentials to clone a repository from the configured provider.</p>"},{"location":"user_guides/projects/git/repository_actions/","title":"Repository actions","text":""},{"location":"user_guides/projects/git/repository_actions/#introduction","title":"Introduction","text":"<p>This section explains the git operations or commands you can perform on hopsworks git repositories. These commands include commit, pull, push, create branches and many more.</p> <p>Repository permissions</p> <p>Git repositories are private. Only the owner of the repository can perform git actions on the repository such as commit, push, pull e.t.c.</p>"},{"location":"user_guides/projects/git/repository_actions/#ui","title":"UI","text":"<p>The operations to perform on the cloned repository can be found in the dropdown as shown below.</p> <p> Repository actions </p> <p>Note that some repository actions will require the username and token to be configured first depending on the provider. For example to be able to perform a push action in any repository, you must configure the provider for the repository first. To be able to perform a pull action for the for a GitLab repository, you must configure the GitLab provider first. You will see the dialog below in the case you need to configure the provider first to perform the repository action.</p> <p> Configure provider prompt </p>"},{"location":"user_guides/projects/git/repository_actions/#read-only-repositories","title":"Read only repositories","text":"<p>In read only repositories, the following actions are disabled: commit, push and file checkout. The read only property can be enabled or disabled in the Cluster settings &gt; Configuration, by updating the <code>enable_read_only_git_repositories</code> variable to true or false. Note that you need administrator privileges to update this property.</p>"},{"location":"user_guides/projects/git/repository_actions/#code","title":"Code","text":"<p>You can also perform the repository actions using the hopsworks git API in python.</p>"},{"location":"user_guides/projects/git/repository_actions/#step-1-get-the-git-api","title":"Step 1: Get the git API","text":"<pre><code>import hopsworks\n\nproject = hopsworks.login()\n\ngit_api = project.get_git_api()\n</code></pre>"},{"location":"user_guides/projects/git/repository_actions/#step-2-get-the-git-repository","title":"Step 2: Get the git repository","text":"<pre><code>git_repo = git_api.get_repo(REPOSITORY_NAME)\n</code></pre>"},{"location":"user_guides/projects/git/repository_actions/#step-3-perform-the-git-repository-action-eg-commit","title":"Step 3: Perform the git repository action e.g commit","text":"<pre><code>git_repo = git_api.commit(\"Test commit\")\n</code></pre>"},{"location":"user_guides/projects/git/repository_actions/#api-reference","title":"API Reference","text":"<p>Api reference for repository actions is available here: <code>GitRepo</code></p>"},{"location":"user_guides/projects/iam_role/iam_role_chaining/","title":"How To Use AWS IAM Roles on EC2 instances","text":""},{"location":"user_guides/projects/iam_role/iam_role_chaining/#introduction","title":"Introduction","text":"<p>When deploying Hopsworks on EC2 instances you might need to assume different roles to access resources on AWS. These roles can be configured in AWS and mapped to a project in Hopsworks.</p>"},{"location":"user_guides/projects/iam_role/iam_role_chaining/#prerequisites","title":"Prerequisites","text":"<p>Before you begin this guide you'll need the following:</p> <ul> <li>A Hopsworks cluster running on EC2.</li> <li>Role chaining setup in AWS.</li> <li>Configure role mappings in Hopsworks.   For a guide on how to configure this see AWS IAM Role Chaining.</li> </ul>"},{"location":"user_guides/projects/iam_role/iam_role_chaining/#ui","title":"UI","text":"<p>In this guide, you will learn how to use a mapped IAM role in your project.</p>"},{"location":"user_guides/projects/iam_role/iam_role_chaining/#step-1-navigate-to-your-projects-iam-role-chaining-tab","title":"Step 1: Navigate to your project's IAM Role Chaining tab","text":"<p>In the Project Settings page you can find the IAM Role Chaining section showing a list of all IAM roles mapped to your project.</p> Role Chaining"},{"location":"user_guides/projects/iam_role/iam_role_chaining/#step-2-use-the-iam-role","title":"Step 2: Use the IAM role","text":"<p>You can now use the IAM roles listed in your project when creating a Data Source with Temporary Credentials.</p>"},{"location":"user_guides/projects/jobs/notebook_job/","title":"How To Run A Jupyter Notebook Job","text":""},{"location":"user_guides/projects/jobs/notebook_job/#introduction","title":"Introduction","text":"<p>This guide describes how to configure a job to execute a Jupyter Notebook (.ipynb) and visualize the evaluated notebook.</p> <p>All members of a project in Hopsworks can launch the following types of applications through a project's Jobs service:</p> <ul> <li>Python</li> <li>Apache Spark</li> <li>Ray</li> </ul> <p>Launching a job of any type is very similar process, what mostly differs between job types is the various configuration parameters each job type comes with. Hopsworks support scheduling jobs to run on a regular basis, e.g backfilling a Feature Group by running your feature engineering pipeline nightly. Scheduling can be done both through the UI and the python API, checkout our Scheduling guide.</p>"},{"location":"user_guides/projects/jobs/notebook_job/#ui","title":"UI","text":""},{"location":"user_guides/projects/jobs/notebook_job/#step-1-jobs-overview","title":"Step 1: Jobs overview","text":"<p>The image below shows the Jobs overview page in Hopsworks and is accessed by clicking <code>Jobs</code> in the sidebar.</p> <p> Jobs overview </p>"},{"location":"user_guides/projects/jobs/notebook_job/#step-2-create-new-job-dialog","title":"Step 2: Create new job dialog","text":"<p>Click <code>New Job</code> and the following dialog will appear.</p> <p> Create new job dialog </p>"},{"location":"user_guides/projects/jobs/notebook_job/#step-3-set-the-job-type","title":"Step 3: Set the job type","text":"<p>By default, the dialog will create a Spark job. To instead configure a Jupyter Notebook job, select <code>PYTHON</code>.</p> <p> Select Python job type </p>"},{"location":"user_guides/projects/jobs/notebook_job/#step-4-set-the-notebook","title":"Step 4: Set the notebook","text":"<p>Next step is to select the Jupyter Notebook to run. You can either select <code>From project</code>, if the file was previously uploaded to Hopsworks, or <code>Upload new file</code> which lets you select a file from your local filesystem as demonstrated below. By default, the job name is the same as the file name, but you can customize it as shown.</p> <p> Configure program </p> <p>Then click <code>Create job</code> to create the job.</p>"},{"location":"user_guides/projects/jobs/notebook_job/#step-5-optional-set-the-jupyter-notebook-arguments","title":"Step 5 (optional): Set the Jupyter Notebook arguments","text":"<p>In the job settings, you can specify arguments for your notebook script. Arguments must be in the format of <code>-p arg1 value1 -p arg2 value2</code>. For each argument, you must first provide <code>-p</code>, followed by the parameter name (e.g. <code>arg1</code>), followed by its value (e.g. <code>value1</code>). The next step is to read the arguments in the notebook which is explained in this guide.</p> <p> Configure notebook arguments </p>"},{"location":"user_guides/projects/jobs/notebook_job/#step-6-optional-additional-configuration","title":"Step 6 (optional): Additional configuration","text":"<p>It is possible to also set following configuration settings for a <code>PYTHON</code> job.</p> <ul> <li><code>Environment</code>: The python environment to use</li> <li><code>Container memory</code>: The amount of memory in MB to be allocated to the Jupyter Notebook script</li> <li><code>Container cores</code>: The number of cores to be allocated for the Jupyter Notebook script</li> <li><code>Additional files</code>: List of files that will be locally accessible in the working directory of the application. Only recommended to use if project datasets are not mounted under <code>/hopsfs</code>. You can always modify the arguments in the job settings.</li> </ul> <p> Set the job type </p>"},{"location":"user_guides/projects/jobs/notebook_job/#step-7-kueue-enabled-select-a-queue","title":"Step 7: (Kueue enabled) Select a Queue","text":"<p>If the cluster is installed with Kueue enabled, you will need to select a queue in which the job should run. This can be done from <code>Advance configuration -&gt; Scheduler section</code>.</p> <p></p>"},{"location":"user_guides/projects/jobs/notebook_job/#step-8-execute-the-job","title":"Step 8: Execute the job","text":"<p>Now click the <code>Run</code> button to start the execution of the job. You will be redirected to the <code>Executions</code> page where you can see the list of all executions.</p> <p> Start job execution </p>"},{"location":"user_guides/projects/jobs/notebook_job/#step-9-visualize-output-notebook","title":"Step 9: Visualize output notebook","text":"<p>Once the execution is finished, click <code>Logs</code> and then <code>notebook out</code> to see the logs for the execution.</p> <p> Visualize output notebook </p> <p>You can directly edit and save the output notebook by clicking <code>Open Notebook</code>.</p>"},{"location":"user_guides/projects/jobs/notebook_job/#code","title":"Code","text":""},{"location":"user_guides/projects/jobs/notebook_job/#step-1-upload-the-jupyter-notebook-script","title":"Step 1: Upload the Jupyter Notebook script","text":"<p>This snippet assumes the Jupyter Notebook script is in the current working directory and named <code>notebook.ipynb</code>.</p> <p>It will upload the Jupyter Notebook script to the <code>Resources</code> dataset in your project.</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\ndataset_api = project.get_dataset_api()\n\nuploaded_file_path = dataset_api.upload(\"notebook.ipynb\", \"Resources\")\n</code></pre>"},{"location":"user_guides/projects/jobs/notebook_job/#step-2-create-jupyter-notebook-job","title":"Step 2: Create Jupyter Notebook job","text":"<p>In this snippet we get the <code>JobsApi</code> object to get the default job configuration for a <code>PYTHON</code> job, set the jupyter notebook file and override the environment to run in, and finally create the <code>Job</code> object.</p> <pre><code>jobs_api = project.get_job_api()\n\nnotebook_job_config = jobs_api.get_configuration(\"PYTHON\")\n\n# Set the application file\nnotebook_job_config['appPath'] = uploaded_file_path\n\n# Override the python job environment\nnotebook_job_config['environmentName'] = \"python-feature-pipeline\"\n\njob = jobs_api.create_job(\"notebook_job\", notebook_job_config)\n</code></pre>"},{"location":"user_guides/projects/jobs/notebook_job/#step-3-execute-the-job","title":"Step 3: Execute the job","text":"<p>In this code snippet, we execute the job with arguments and wait until it reaches a terminal state.</p> <pre><code># Run the job\nexecution = job.run(args='-p a 2 -p b 5', await_termination=True)\n</code></pre>"},{"location":"user_guides/projects/jobs/notebook_job/#configuration","title":"Configuration","text":"<p>The following table describes the job configuration parameters for a PYTHON job.</p> <p><code>conf = jobs_api.get_configuration(\"PYTHON\")</code></p> Field Type Description Default <code>conf['type']</code> string Type of the job configuration <code>\"pythonJobConfiguration\"</code> <code>conf['appPath']</code> string Project relative path to notebook (e.g., <code>Resources/foo.ipynb</code>) <code>null</code> <code>conf['defaultArgs']</code> string Arguments to pass to the notebook.Will be overridden if arguments are passed explicitly via <code>Job.run(args=\"...\")</code>.Must conform to Papermill format <code>-p arg1 val1</code> <code>null</code> <code>conf['environmentName']</code> string Name of the project Python environment to use <code>\"pandas-training-pipeline\"</code> <code>conf['resourceConfig']['cores']</code> float Number of CPU cores to be allocated <code>1.0</code> <code>conf['resourceConfig']['memory']</code> int Number of MBs to be allocated <code>2048</code> <code>conf['resourceConfig']['gpus']</code> int Number of GPUs to be allocated <code>0</code> <code>conf['logRedirection']</code> boolean Whether logs are redirected <code>true</code> <code>conf['jobType']</code> string Type of job <code>\"PYTHON\"</code> <code>conf['files']</code> string Comma-separated string of HDFS path(s) to files to be made available to the application. Example: <code>hdfs:///Project/&lt;project&gt;/Resources/file1.py,...</code> <code>null</code>"},{"location":"user_guides/projects/jobs/notebook_job/#accessing-project-data","title":"Accessing project data","text":"<p>Recommended approach if <code>/hopsfs</code> is mounted</p> <p>If your Hopsworks installation is configured to mount the project datasets under <code>/hopsfs</code>, which it is in most cases, then please refer to this section instead of the <code>Additional files</code> property to reference file resources.</p>"},{"location":"user_guides/projects/jobs/notebook_job/#absolute-paths","title":"Absolute paths","text":"<p>The project datasets are mounted under <code>/hopsfs</code>, so you can access <code>data.csv</code> from the <code>Resources</code> dataset using <code>/hopsfs/Resources/data.csv</code> in your notebook.</p>"},{"location":"user_guides/projects/jobs/notebook_job/#relative-paths","title":"Relative paths","text":"<p>The notebook's working directory is the folder it is located in. For example, if it is located in the <code>Resources</code> dataset, and you have a file named <code>data.csv</code> in that dataset, you simply access it using <code>data.csv</code>. Also, if you write a local file, for example <code>output.txt</code>, it will be saved in the <code>Resources</code> dataset.</p>"},{"location":"user_guides/projects/jobs/notebook_job/#api-reference","title":"API Reference","text":"<p><code>Job</code></p> <p><code>Execution</code></p>"},{"location":"user_guides/projects/jobs/pyspark_job/","title":"How To Run A PySpark Job","text":""},{"location":"user_guides/projects/jobs/pyspark_job/#introduction","title":"Introduction","text":"<p>This guide will describe how to configure a job to execute a pyspark script inside the cluster.</p> <p>All members of a project in Hopsworks can launch the following types of applications through a project's Jobs service:</p> <ul> <li>Python</li> <li>Apache Spark</li> <li>Ray</li> </ul> <p>Launching a job of any type is very similar process, what mostly differs between job types is the various configuration parameters each job type comes with. Hopsworks clusters support scheduling to run jobs on a regular basis, e.g backfilling a Feature Group by running your feature engineering pipeline nightly. Scheduling can be done both through the UI and the python API, checkout our Scheduling guide.</p> <p>PySpark program can either be a <code>.py</code> script or a <code>.ipynb</code> file, however be mindful of how to access/create the spark session based on the extension you provide.</p> <p>Instantiate the SparkSession</p> <p>For a <code>.py</code> file, remember to instantiate the SparkSession i.e <code>spark=SparkSession.builder.getOrCreate()</code></p> <p>For a <code>.ipynb</code> file, the <code>SparkSession</code> is already available as <code>spark</code> when the job is started.</p>"},{"location":"user_guides/projects/jobs/pyspark_job/#ui","title":"UI","text":""},{"location":"user_guides/projects/jobs/pyspark_job/#step-1-jobs-overview","title":"Step 1: Jobs overview","text":"<p>The image below shows the Jobs overview page in Hopsworks and is accessed by clicking <code>Jobs</code> in the sidebar.</p> <p> Jobs overview </p>"},{"location":"user_guides/projects/jobs/pyspark_job/#step-2-create-new-job-dialog","title":"Step 2: Create new job dialog","text":"<p>Click <code>New Job</code> and the following dialog will appear.</p> <p> Create new job dialog </p>"},{"location":"user_guides/projects/jobs/pyspark_job/#step-3-set-the-job-type","title":"Step 3: Set the job type","text":"<p>By default, the dialog will create a Spark job. Make sure <code>SPARK</code> is chosen.</p>"},{"location":"user_guides/projects/jobs/pyspark_job/#step-4-set-the-script","title":"Step 4: Set the script","text":"<p>Next step is to select the program to run. You can either select <code>From project</code>, if the file was previously uploaded to Hopsworks, or <code>Upload new file</code> which lets you select a file from your local filesystem as demonstrated below. By default, the job name is the same as the file name, but you can customize it as shown.</p> <p> Configure program </p> <p>Then click <code>Create job</code> to create the job.</p>"},{"location":"user_guides/projects/jobs/pyspark_job/#step-5-optional-set-the-pyspark-script-arguments","title":"Step 5 (optional): Set the PySpark script arguments","text":"<p>In the job settings, you can specify arguments for your PySpark script. Remember to handle the arguments inside your PySpark script.</p> <p> Configure PySpark script arguments </p>"},{"location":"user_guides/projects/jobs/pyspark_job/#step-6-optional-advanced-configuration","title":"Step 6 (optional): Advanced configuration","text":"<p>Resource allocation for the Spark driver and executors can be configured, also the number of executors and whether dynamic execution should be enabled.</p> <ul> <li> <p><code>Environment</code>: The python environment to use, must be based on <code>spark-feature-pipeline</code></p> </li> <li> <p><code>Driver memory</code>: Number of cores to allocate for the Spark driver</p> </li> <li> <p><code>Driver virtual cores</code>: Number of MBs to allocate for the Spark driver</p> </li> <li> <p><code>Executor memory</code>: Number of cores to allocate for each Spark executor</p> </li> <li> <p><code>Executor virtual cores</code>: Number of MBs to allocate for each Spark executor</p> </li> <li> <p><code>Dynamic/Static</code>: Run the Spark application in static or dynamic allocation mode (see spark docs for details).</p> </li> </ul> <p> Resource configuration for the PySpark job </p> <p>Additional files or dependencies required for the Spark job can be configured.</p> <ul> <li> <p><code>Additional archives</code>: List of archives to be extracted into the working directory of each executor.</p> </li> <li> <p><code>Additional jars</code>: List of jars to be placed in the working directory of each executor.</p> </li> <li> <p><code>Additional python dependencies</code>: List of python files and archives to be placed on each executor and added to PATH.</p> </li> <li> <p><code>Additional files</code>: List of files to be placed in the working directory of each executor.</p> </li> </ul> <p> File configuration for the PySpark job </p> <p>Line-separates properties to be set for the Spark application. For example, changing the configuration variables for the Kryo Serializer or setting environment variables for the driver, you can set the properties as shown below.</p> <p> Additional Spark configuration </p>"},{"location":"user_guides/projects/jobs/pyspark_job/#step-7-kueue-enabled-select-a-queue","title":"Step 7: (Kueue enabled) Select a Queue","text":"<p>Currently we do not have Kueue support for Spark. You do not need to select a queue to run the job in.</p>"},{"location":"user_guides/projects/jobs/pyspark_job/#step-8-execute-the-job","title":"Step 8: Execute the job","text":"<p>Now click the <code>Run</code> button to start the execution of the job. You will be redirected to the <code>Executions</code> page where you can see the list of all executions.</p> <p> Start job execution </p>"},{"location":"user_guides/projects/jobs/pyspark_job/#step-9-application-logs","title":"Step 9: Application logs","text":"<p>To monitor logs while the execution is running, click <code>Spark UI</code> to open the Spark UI in a separate tab.</p> <p>Once the execution is finished, you can click on <code>Logs</code> to see the full logs for execution.</p> <p> Access Spark logs </p>"},{"location":"user_guides/projects/jobs/pyspark_job/#code","title":"Code","text":""},{"location":"user_guides/projects/jobs/pyspark_job/#step-1-upload-the-pyspark-program","title":"Step 1: Upload the PySpark program","text":"<p>This snippet assumes the program to run is in the current working directory and named <code>script.py</code>.</p> <p>It will upload the python script to the <code>Resources</code> dataset in your project.</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\ndataset_api = project.get_dataset_api()\n\nuploaded_file_path = dataset_api.upload(\"script.py\", \"Resources\")\n</code></pre>"},{"location":"user_guides/projects/jobs/pyspark_job/#step-2-create-pyspark-job","title":"Step 2: Create PySpark job","text":"<p>In this snippet we get the <code>JobsApi</code> object to get the default job configuration for a <code>PYSPARK</code> job, set the pyspark script and override the environment to run in, and finally create the <code>Job</code> object.</p> <pre><code>jobs_api = project.get_job_api()\n\nspark_config = jobs_api.get_configuration(\"PYSPARK\")\n\n# Set the application file\nspark_config['appPath'] = uploaded_file_path\n\n# Override the python job environment\nspark_config['environmentName'] = \"spark-feature-pipeline\"\n\njob = jobs_api.create_job(\"pyspark_job\", spark_config)\n</code></pre>"},{"location":"user_guides/projects/jobs/pyspark_job/#step-3-execute-the-job","title":"Step 3: Execute the job","text":"<p>In this snippet we execute the job synchronously, that is wait until it reaches a terminal state, and then download and print the logs.</p> <pre><code>execution = job.run(await_termination=True)\n\nout, err = execution.download_logs()\n\nf_out = open(out, \"r\")\nprint(f_out.read())\n\nf_err = open(err, \"r\")\nprint(f_err.read())\n</code></pre>"},{"location":"user_guides/projects/jobs/pyspark_job/#configuration","title":"Configuration","text":"<p>The following table describes the job configuration parameters for a PYSPARK job.</p> <p><code>conf = jobs_api.get_configuration(\"PYSPARK\")</code></p> Field Type Description Default <code>conf['type']</code> string Type of the job configuration <code>\"sparkJobConfiguration\"</code> <code>conf['appPath']</code> string Project path to spark program (e.g <code>Resources/foo.py</code>) <code>null</code> <code>conf['defaultArgs']</code> string Arguments to pass to the program. Will be overridden if arguments are passed explicitly via <code>Job.run(args=\"...\")</code> <code>null</code> <code>conf['environmentName']</code> string Name of the project spark environment to use <code>\"spark-feature-pipeline\"</code> <code>conf['spark.driver.cores']</code> float Number of CPU cores allocated for the driver <code>1.0</code> <code>conf['spark.driver.memory']</code> int Memory allocated for the driver (in MB) <code>2048</code> <code>conf['spark.executor.instances']</code> int Number of executor instances <code>1</code> <code>conf['spark.executor.cores']</code> float Number of CPU cores per executor <code>1.0</code> <code>conf['spark.executor.memory']</code> int Memory allocated per executor (in MB) <code>4096</code> <code>conf['spark.dynamicAllocation.enabled']</code> boolean Enable dynamic allocation of executors <code>true</code> <code>conf['spark.dynamicAllocation.minExecutors']</code> int Minimum number of executors with dynamic allocation <code>1</code> <code>conf['spark.dynamicAllocation.maxExecutors']</code> int Maximum number of executors with dynamic allocation <code>2</code> <code>conf['spark.dynamicAllocation.initialExecutors']</code> int Initial number of executors with dynamic allocation <code>1</code> <code>conf['spark.blacklist.enabled']</code> boolean Whether executor/node blacklisting is enabled <code>false</code> <code>conf['files']</code> string Comma-separated string of HDFS path(s) to files to be made available to the application. Example: <code>hdfs:///Project/&lt;project_name&gt;/Resources/file1.py,...</code> <code>null</code> <code>conf['pyFiles']</code> string Comma-separated string of HDFS path(s) to python modules to be made available to the application. Example: <code>hdfs:///Project/&lt;project_name&gt;/Resources/file1.py,...</code> <code>null</code> <code>conf['jars']</code> string Comma-separated string of HDFS path(s) to jars to be included in CLASSPATH. Example: <code>hdfs:///Project/&lt;project_name&gt;/Resources/app.jar,...</code> <code>null</code> <code>conf['archives']</code> string Comma-separated string of HDFS path(s) to archives to be made available to the application. Example: <code>hdfs:///Project/&lt;project_name&gt;/Resources/archive.zip,...</code> <code>null</code> <code>conf['properties']</code> string A new line separated (<code>\\n</code>) list of properties to pass to the Spark application. The properties should be in the format <code>name=value</code> <code>null</code>"},{"location":"user_guides/projects/jobs/pyspark_job/#accessing-project-data","title":"Accessing project data","text":""},{"location":"user_guides/projects/jobs/pyspark_job/#read-directly-from-the-filesystem-recommended","title":"Read directly from the filesystem (recommended)","text":"<p>To read a dataset in your project using Spark, use the full filesystem path where the data is stored. For example, to read a CSV file named <code>data.csv</code> located in the <code>Resources</code> dataset of a project called <code>my_project</code>:</p> <pre><code>df = spark.read.csv(\"/Projects/my_project/Resources/data.csv\", header=True, inferSchema=True)\ndf.show()\n</code></pre>"},{"location":"user_guides/projects/jobs/pyspark_job/#additional-files","title":"Additional files","text":"<p>Different file types can be attached to the spark job and made available in the <code>/srv/hops/artifacts</code> folder when the PySpark job is started. This configuration is mainly useful when you need to add additional setup, such as jars that needs to be added to the CLASSPATH.</p> <p>When reading data in your Spark job it is recommended to use the Spark read API as previously demonstrated, since this reads from the filesystem directly, whereas <code>Additional files</code> configuration options will download the files in its entirety and is not a scalable option.</p>"},{"location":"user_guides/projects/jobs/pyspark_job/#api-reference","title":"API Reference","text":"<p><code>Job</code></p> <p><code>Execution</code></p>"},{"location":"user_guides/projects/jobs/python_job/","title":"How To Run A Python Job","text":""},{"location":"user_guides/projects/jobs/python_job/#introduction","title":"Introduction","text":"<p>This guide will describe how to configure a job to execute a python script inside the cluster.</p> <p>All members of a project in Hopsworks can launch the following types of applications through a project's Jobs service:</p> <ul> <li>Python</li> <li>Apache Spark</li> <li>Ray</li> </ul> <p>Launching a job of any type is very similar process, what mostly differs between job types is the various configuration parameters each job type comes with. Hopsworks support scheduling jobs to run on a regular basis, e.g backfilling a Feature Group by running your feature engineering pipeline nightly. Scheduling can be done both through the UI and the python API, checkout our Scheduling guide.</p>"},{"location":"user_guides/projects/jobs/python_job/#ui","title":"UI","text":""},{"location":"user_guides/projects/jobs/python_job/#step-1-jobs-overview","title":"Step 1: Jobs overview","text":"<p>The image below shows the Jobs overview page in Hopsworks and is accessed by clicking <code>Jobs</code> in the sidebar.</p> <p> Jobs overview </p>"},{"location":"user_guides/projects/jobs/python_job/#step-2-create-new-job-dialog","title":"Step 2: Create new job dialog","text":"<p>Click <code>New Job</code> and the following dialog will appear.</p> <p> Create new job dialog </p>"},{"location":"user_guides/projects/jobs/python_job/#step-3-set-the-job-type","title":"Step 3: Set the job type","text":"<p>By default, the dialog will create a Spark job. To instead configure a Python job, select <code>PYTHON</code>.</p> <p> Select Python job type </p>"},{"location":"user_guides/projects/jobs/python_job/#step-4-set-the-script","title":"Step 4: Set the script","text":"<p>Next step is to select the python script to run. You can either select <code>From project</code>, if the file was previously uploaded to Hopsworks, or <code>Upload new file</code> which lets you select a file from your local filesystem as demonstrated below. By default, the job name is the same as the file name, but you can customize it as shown.</p> <p> Configure program </p>"},{"location":"user_guides/projects/jobs/python_job/#step-5-optional-set-the-python-script-arguments","title":"Step 5 (optional): Set the Python script arguments","text":"<p>In the job settings, you can specify arguments for your Python script. Remember to handle the arguments inside your Python script.</p> <p> Configure Python script arguments </p>"},{"location":"user_guides/projects/jobs/python_job/#step-6-optional-additional-configuration","title":"Step 6 (optional): Additional configuration","text":"<p>It is possible to also set following configuration settings for a <code>PYTHON</code> job.</p> <ul> <li><code>Environment</code>: The python environment to use</li> <li><code>Container memory</code>: The amount of memory in MB to be allocated to the Python script</li> <li><code>Container cores</code>: The number of cores to be allocated for the Python script</li> <li><code>Additional files</code>: List of files that will be locally accessible in the working directory of the application.   Only recommended to use if project datasets are not mounted under <code>/hopsfs</code>.   You can always modify the arguments in the job settings.</li> </ul> <p> Additional configuration </p>"},{"location":"user_guides/projects/jobs/python_job/#step-7-kueue-enabled-select-a-queue","title":"Step 7: (Kueue enabled) Select a Queue","text":"<p>If the cluster is installed with Kueue enabled, you will need to select a queue in which the job should run. This can be done from <code>Advance configuration -&gt; Scheduler section</code>.</p> <p></p>"},{"location":"user_guides/projects/jobs/python_job/#step-8-execute-the-job","title":"Step 8: Execute the job","text":"<p>Now click the <code>Run</code> button to start the execution of the job. You will be redirected to the <code>Executions</code> page where you can see the list of all executions.</p> <p>Once the execution is finished, click on <code>Logs</code> to see the logs for the execution.</p> <p> Start job execution </p>"},{"location":"user_guides/projects/jobs/python_job/#code","title":"Code","text":""},{"location":"user_guides/projects/jobs/python_job/#step-1-upload-the-python-script","title":"Step 1: Upload the Python script","text":"<p>This snippet assumes the python script is in the current working directory and named <code>script.py</code>.</p> <p>It will upload the python script to the <code>Resources</code> dataset in your project.</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\ndataset_api = project.get_dataset_api()\n\nuploaded_file_path = dataset_api.upload(\"script.py\", \"Resources\")\n</code></pre>"},{"location":"user_guides/projects/jobs/python_job/#step-2-create-python-job","title":"Step 2: Create Python job","text":"<p>In this snippet we get the <code>JobsApi</code> object to get the default job configuration for a <code>PYTHON</code> job, set the python script and override the environment to run in, and finally create the <code>Job</code> object.</p> <pre><code>jobs_api = project.get_job_api()\n\npy_job_config = jobs_api.get_configuration(\"PYTHON\")\n\n# Set the application file\npy_job_config['appPath'] = uploaded_file_path\n\n# Override the python job environment\npy_job_config['environmentName'] = \"python-feature-pipeline\"\n\njob = jobs_api.create_job(\"py_job\", py_job_config)\n</code></pre>"},{"location":"user_guides/projects/jobs/python_job/#step-3-execute-the-job","title":"Step 3: Execute the job","text":"<p>In this snippet we execute the job synchronously, that is wait until it reaches a terminal state, and then download and print the logs.</p> <pre><code># Run the job\nexecution = job.run(await_termination=True)\n\n# Download logs\nout, err = execution.download_logs()\n\nf_out = open(out, \"r\")\nprint(f_out.read())\n\nf_err = open(err, \"r\")\nprint(f_err.read())\n</code></pre>"},{"location":"user_guides/projects/jobs/python_job/#configuration","title":"Configuration","text":"<p>The following table describes the job configuration parameters for a PYTHON job.</p> <p><code>conf = jobs_api.get_configuration(\"PYTHON\")</code></p> Field Type Description Default <code>conf['type']</code> string Type of the job configuration <code>\"pythonJobConfiguration\"</code> <code>conf['appPath']</code> string Project relative path to script (e.g., <code>Resources/foo.py</code>) <code>null</code> <code>conf['defaultArgs']</code> string Arguments to pass to the script. Will be overridden if arguments are passed explicitly via <code>Job.run(args=\"...\")</code> <code>null</code> <code>conf['environmentName']</code> string Name of the project Python environment to use <code>\"pandas-training-pipeline\"</code> <code>conf['resourceConfig']['cores']</code> float Number of CPU cores to be allocated <code>1.0</code> <code>conf['resourceConfig']['memory']</code> int Number of MBs to be allocated <code>2048</code> <code>conf['resourceConfig']['gpus']</code> int Number of GPUs to be allocated <code>0</code> <code>conf['logRedirection']</code> boolean Whether logs are redirected <code>true</code> <code>conf['jobType']</code> string Type of job <code>\"PYTHON\"</code> <code>conf['files']</code> string Comma-separated string of HDFS path(s) to files to be made available to the application. Example: <code>hdfs:///Project/&lt;project&gt;/Resources/file1.py,...</code> <code>null</code>"},{"location":"user_guides/projects/jobs/python_job/#accessing-project-data","title":"Accessing project data","text":"<p>Recommended approach if <code>/hopsfs</code> is mounted</p> <p>If your Hopsworks installation is configured to mount the project datasets under <code>/hopsfs</code>, which it is in most cases, then please refer to this section instead of the <code>Additional files</code> property to reference file resources.</p>"},{"location":"user_guides/projects/jobs/python_job/#absolute-paths","title":"Absolute paths","text":"<p>The project datasets are mounted under <code>/hopsfs</code>, so you can access <code>data.csv</code> from the <code>Resources</code> dataset using <code>/hopsfs/Resources/data.csv</code> in your script.</p>"},{"location":"user_guides/projects/jobs/python_job/#relative-paths","title":"Relative paths","text":"<p>The script's working directory is the folder it is located in. For example, if it is located in the <code>Resources</code> dataset, and you have a file named <code>data.csv</code> in that dataset, you simply access it using <code>data.csv</code>. Also, if you write a local file, for example <code>output.txt</code>, it will be saved in the <code>Resources</code> dataset.</p>"},{"location":"user_guides/projects/jobs/python_job/#api-reference","title":"API Reference","text":"<p><code>Job</code></p> <p><code>Execution</code></p>"},{"location":"user_guides/projects/jobs/ray_job/","title":"How To Run A Ray Job","text":""},{"location":"user_guides/projects/jobs/ray_job/#introduction","title":"Introduction","text":"<p>This guide will describe how to configure a job to execute a ray program inside the cluster.</p> <p>All members of a project in Hopsworks can launch the following types of applications through a project's Jobs service:</p> <ul> <li>Python</li> <li>Apache Spark</li> <li>Ray</li> </ul> <p>Launching a job of any type is very similar process, what mostly differs between job types is the various configuration parameters each job type comes with. Hopsworks support scheduling to run jobs on a regular basis, e.g backfilling a Feature Group by running your feature engineering pipeline nightly. Scheduling can be done both through the UI and the python API, checkout our Scheduling guide.</p> <p>Enable Ray</p> <p>Support for Ray needs to be explicitly enabled by adding the following option in the <code>values.yaml</code> file for the deployment:</p> <pre><code>global:\n  ray:\n    enabled: true\n</code></pre>"},{"location":"user_guides/projects/jobs/ray_job/#ui","title":"UI","text":""},{"location":"user_guides/projects/jobs/ray_job/#step-1-jobs-overview","title":"Step 1: Jobs overview","text":"<p>The image below shows the Jobs overview page in Hopsworks and is accessed by clicking <code>Jobs</code> in the sidebar.</p> <p> Jobs overview </p>"},{"location":"user_guides/projects/jobs/ray_job/#step-2-create-new-job-dialog","title":"Step 2: Create new job dialog","text":"<p>Click <code>New Job</code> and the following dialog will appear.</p> <p> Create new job dialog </p>"},{"location":"user_guides/projects/jobs/ray_job/#step-3-set-the-job-type","title":"Step 3: Set the job type","text":"<p>By default, the dialog will create a Spark job. Make sure <code>RAY</code> is chosen.</p>"},{"location":"user_guides/projects/jobs/ray_job/#step-4-set-the-script","title":"Step 4: Set the script","text":"<p>Next step is to select the program to run. You can either select <code>From project</code>, if the file was previously uploaded to Hopsworks, or <code>Upload new file</code> which lets you select a file from your local filesystem as demonstrated below. After that set the name for the job. By default, the job name is the same as the file name, but you can customize it here.</p> <p> Configure program </p>"},{"location":"user_guides/projects/jobs/ray_job/#step-5-optional-advanced-configuration","title":"Step 5 (optional): Advanced configuration","text":"<p>Resource allocation for the Driver and Workers can be configured.</p> <p>Using the resources in the Ray script</p> <p>The resource configurations describe the cluster that will be provisioned when launching the Ray job. User can still provide extra configurations in the job script using <code>ScalingConfig</code>, i.e. <code>ScalingConfig(num_workers=4, trainer_resources={\"CPU\": 1}, use_gpu=True)</code>.</p> <ul> <li> <p><code>Driver memory</code>: Memory in MBs to allocate for Driver</p> </li> <li> <p><code>Driver virtual cores</code>: Number of cores to allocate for the Driver</p> </li> <li> <p><code>Worker memory</code>: Memory in MBs to allocate for each worker</p> </li> <li> <p><code>Worker cores</code>: Number of cores to allocate for each worker</p> </li> <li> <p><code>Min workers</code>: Minimum number of workers to start with</p> </li> <li> <p><code>Max workers</code>: Maximum number of workers to scale up to</p> </li> </ul> <p> Resource configuration for the Ray Job </p> <p>Runtime environment and Additional files required for the Ray job can also be provided.</p> <ul> <li> <p><code>Runtime Environment (Optional)</code>:  A runtime environment describes the dependencies required for the Ray job including files, packages, environment variables, and more.   This is useful when you need to install specific packages and set environment variables for this particular Ray job.   It should be provided as a YAML file.   You can select the file from the project or upload a new one.</p> </li> <li> <p><code>Additional files</code>: List of other files required for the Ray job.   These files will be placed in <code>/srv/hops/ray/job</code>.</p> </li> </ul> <p> Runtime configuration and additional files for Ray job </p>"},{"location":"user_guides/projects/jobs/ray_job/#step-6-kueue-enabled-select-a-queue","title":"Step 6: (Kueue enabled) Select a Queue","text":"<p>If the cluster is installed with Kueue enabled, you will need to select a queue in which the job should run. This can be done from <code>Advance configuration -&gt; Scheduler section</code>.</p> <p></p>"},{"location":"user_guides/projects/jobs/ray_job/#step-7-execute-the-job","title":"Step 7: Execute the job","text":"<p>Now click the <code>Run</code> button to start the execution of the job, and then click on <code>Executions</code> to see the list of all executions.</p> <p> Start job execution </p>"},{"location":"user_guides/projects/jobs/ray_job/#step-8-ray-dashboard","title":"Step 8: Ray Dashboard","text":"<p>When the Ray job is running, you can access the Ray dashboard to monitor the job. The Ray dashboard is accessible from the <code>Executions</code> page. Please note that the Ray dashboard is only available when the job execution is running. In the Ray Dashboard, you can monitor the resources used by the job, the number of workers, logs, and the tasks that are running.</p> <p> Access Ray Dashboard </p>"},{"location":"user_guides/projects/jobs/ray_job/#step-9-application-logs","title":"Step 9: Application logs","text":"<p>Once the execution is finished, you can click on <code>Logs</code> to see the full logs for execution.</p> <p> Access Ray job execution logs </p>"},{"location":"user_guides/projects/jobs/ray_job/#code","title":"Code","text":""},{"location":"user_guides/projects/jobs/ray_job/#step-1-upload-the-ray-script","title":"Step 1: Upload the Ray script","text":"<p>This snippet assumes the Ray program is in the current working directory and named <code>ray_job.py</code>. If the file is already in the project, you can skip this step.</p> <p>It will upload the jar to the <code>Resources</code> dataset in your project.</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\ndataset_api = project.get_dataset_api()\n\nuploaded_file_path = dataset_api.upload(\"ray_job.py\", \"Resources\")\n</code></pre>"},{"location":"user_guides/projects/jobs/ray_job/#step-2-create-ray-job","title":"Step 2: Create Ray job","text":"<p>In this snippet we get the <code>JobsApi</code> object to get the default job configuration for a <code>RAY</code> job, set the python script to run and create the <code>Job</code> object.</p> <pre><code>jobs_api = project.get_job_api()\n\nray_config = jobs_api.get_configuration(\"RAY\")\n\nray_config['appPath'] = uploaded_file_path\nray_config['environmentName'] = \"ray-training-pipeline\"\nray_config['driverCores'] = 2\nray_config['driverMemory'] = 2048\nray_config['workerCores'] = 2\nray_config['workerMemory'] = 4096\nray_config['minWorkers'] = 1\nray_config['maxWorkers'] = 4\n\njob = jobs_api.create_job(\"ray_job\", ray_config)\n</code></pre>"},{"location":"user_guides/projects/jobs/ray_job/#step-3-execute-the-job","title":"Step 3: Execute the job","text":"<p>In this snippet we execute the job synchronously, that is wait until it reaches a terminal state, and then download and print the logs.</p> <pre><code>execution = job.run(await_termination=True)\n\nout, err = execution.download_logs()\n\nf_out = open(out, \"r\")\nprint(f_out.read())\n\nf_err = open(err, \"r\")\nprint(f_err.read())\n</code></pre>"},{"location":"user_guides/projects/jobs/ray_job/#configuration","title":"Configuration","text":"<p>The following table describes the job configuration parameters for a RAY job.</p> <p><code>conf = jobs_api.get_configuration(\"RAY\")</code></p> Field Type Description Default <code>conf['type']</code> string Type of the job configuration <code>\"rayJobConfiguration\"</code> <code>conf['appPath']</code> string Project relative path to script (e.g., <code>Resources/foo.py</code>) <code>null</code> <code>conf['defaultArgs']</code> string Arguments to pass to the script. Will be overridden if arguments are passed explicitly via <code>Job.run(args=\"...\")</code> <code>null</code> <code>conf['environmentName']</code> string Name of the project Python environment to use <code>\"pandas-training-pipeline\"</code> <code>conf['driverCores']</code> float Number of CPU cores to be allocated for the Ray head process <code>1.0</code> <code>conf['driverMemory']</code> int Number of MBs to be allocated for the Ray head process <code>2048</code> <code>conf['driverGpus']</code> int Number of GPUs to be allocated for the Ray head process <code>0</code> <code>conf['workerCores']</code> float Number of CPU cores to be allocated for each Ray worker process <code>1.0</code> <code>conf['workerMemory']</code> int Number of MBs to be allocated for each Ray worker process <code>4096</code> <code>conf['workerGpus']</code> int Number of GPUs to be allocated for each Ray worker process <code>0</code> <code>conf['workerMinInstances']</code> int Minimum number of Ray workers <code>1</code> <code>conf['workerMaxInstances']</code> int Maximum number of Ray workers <code>1</code> <code>conf['jobType']</code> string Type of job <code>\"RAY\"</code> <code>conf['files']</code> string Comma-separated string of HDFS path(s) to files to be made available to the application. Example: <code>hdfs:///Project/&lt;project&gt;/Resources/file1.py,...</code> <code>null</code>"},{"location":"user_guides/projects/jobs/ray_job/#accessing-project-data","title":"Accessing project data","text":"<p>The project datasets are mounted under <code>/hopsfs</code>, so you can access <code>data.csv</code> from the <code>Resources</code> dataset using <code>/hopsfs/Resources/data.csv</code> in your script.</p>"},{"location":"user_guides/projects/jobs/ray_job/#api-reference","title":"API Reference","text":"<p><code>Job</code></p> <p><code>Execution</code></p>"},{"location":"user_guides/projects/jobs/schedule_job/","title":"How To Schedule a Job","text":""},{"location":"user_guides/projects/jobs/schedule_job/#introduction","title":"Introduction","text":"<p>Hopsworks clusters can run jobs on a schedule, allowing you to automate the execution. Whether you need to backfill your feature groups on a nightly basis or run a model training pipeline every week, the Hopsworks scheduler will help you automate these tasks. Each job can be configured to have a single schedule. For more advanced use cases, Hopsworks integrates with any DAG manager and directly with the open-source Apache Airflow, check out our Airflow Guide.</p> <p>Schedules can be defined using the drop down menus in the UI or a Quartz cron expression.</p> <p>Schedule frequency</p> <p>The Hopsworks scheduler runs every minute. As such, the scheduling frequency should be of at least 1 minute.</p> <p>Parallel executions</p> <p>If a job execution needs to be scheduled, the scheduler will first check that there are no active executions for that job. If there is an execution running, the scheduler will postpone the execution until the running one is done.</p>"},{"location":"user_guides/projects/jobs/schedule_job/#ui","title":"UI","text":""},{"location":"user_guides/projects/jobs/schedule_job/#scheduling-jobs","title":"Scheduling Jobs","text":"<p>You can define a schedule for a job during the creation of the job itself or after the job has been created from the job overview UI.</p> <p> Schedule a Job </p> <p>The add schedule prompt requires you to select a frequency either through the drop down menus or by using a cron expression. You can also provide a start time to specify from when the schedule should have effect. The start time can also be in the past. If that's the case, the scheduler will backfill the executions from the specified start time. As mentioned above, the execution backfilling will happen one execution at the time.</p> <p>You can optionally provide an end date time to specify until when the scheduling should continue. The end time can also be in the past.</p> <p>In the job overview, you can see the current scheduling configuration, whether or not it is enabled and when the next execution is planned for.</p> <p>All times will be considered as UTC time.</p> <p> Job scheduling overview </p>"},{"location":"user_guides/projects/jobs/schedule_job/#job-argument","title":"Job argument","text":"<p>When a job execution is triggered by the scheduler, a <code>-start_time</code> argument is added to the job arguments. The <code>-start_time</code> value will be the time of the scheduled execution in UTC in the ISO-8601 format (e.g.: <code>-start_time 2023-08-19T18:00:00Z</code>).</p> <p>The <code>-start_time</code> value passed as argument represents the time when the execution was scheduled, not when the execution was started. For example, if the scheduled execution time was in the past (e.g., in the case of backfilling), the <code>-start_time</code> passed to the execution is the time in the past, not the current time when the execution is running. Similarly, if the scheduler was not running for a period of time, when it comes back online, it will start the executions it missed to schedule while offline. Even in that case, the <code>-start_time</code> value will contain the time at which the execution was supposed to be started, not the current time.</p>"},{"location":"user_guides/projects/jobs/schedule_job/#disable-enable-a-schedule","title":"Disable / Enable a schedule","text":"<p>You can decide to pause the scheduling of a job and avoid new executions to be started. You can later on re-enable the same scheduling configuration, and the scheduler will run the executions that were skipped while the schedule was disabled, if any, sequentially. In this way you will backfill the executions in between.</p> <p>You can skip the backfilling of the executions by editing the scheduling configuration and bringing forward the schedule start time for the job.</p>"},{"location":"user_guides/projects/jobs/schedule_job/#delete-a-scheduling","title":"Delete a scheduling","text":"<p>You can remove the schedule for a job using the UI and by clicking on the trash icon on the schedule section of the job overview. If you re-schedule a job after having deleted the previous schedule, even with the same options, it will not take into account previous scheduled executions.</p>"},{"location":"user_guides/projects/jobs/spark_job/","title":"How To Run A Spark Job","text":""},{"location":"user_guides/projects/jobs/spark_job/#introduction","title":"Introduction","text":"<p>This guide will describe how to configure a job to execute a spark program inside the cluster.</p> <p>All members of a project in Hopsworks can launch the following types of applications through a project's Jobs service:</p> <ul> <li>Python</li> <li>Apache Spark</li> <li>Ray</li> </ul> <p>Launching a job of any type is very similar process, what mostly differs between job types is the various configuration parameters each job type comes with. Hopsworks support scheduling to run jobs on a regular basis, e.g backfilling a Feature Group by running your feature engineering pipeline nightly. Scheduling can be done both through the UI and the python API, checkout our Scheduling guide.</p>"},{"location":"user_guides/projects/jobs/spark_job/#ui","title":"UI","text":""},{"location":"user_guides/projects/jobs/spark_job/#step-1-jobs-overview","title":"Step 1: Jobs overview","text":"<p>The image below shows the Jobs overview page in Hopsworks and is accessed by clicking <code>Jobs</code> in the sidebar.</p> <p> Jobs overview </p>"},{"location":"user_guides/projects/jobs/spark_job/#step-2-create-new-job-dialog","title":"Step 2: Create new job dialog","text":"<p>Click <code>New Job</code> and the following dialog will appear.</p> <p> Create new job dialog </p>"},{"location":"user_guides/projects/jobs/spark_job/#step-3-set-the-job-type","title":"Step 3: Set the job type","text":"<p>By default, the dialog will create a Spark job. Make sure <code>SPARK</code> is chosen.</p>"},{"location":"user_guides/projects/jobs/spark_job/#step-4-set-the-jar","title":"Step 4: Set the jar","text":"<p>Next step is to select the program to run. You can either select <code>From project</code>, if the file was previously uploaded to Hopsworks, or <code>Upload new file</code> which lets you select a file from your local filesystem as demonstrated below. After that set the name for the job. By default, the job name is the same as the file name, but you can customize it here.</p> <p> Configure program </p>"},{"location":"user_guides/projects/jobs/spark_job/#step-5-set-the-main-class","title":"Step 5: Set the main class","text":"<p>Next step is to set the main class for the application. Then specify advanced configuration or click <code>Create New Job</code> to create the job.</p> <p> Set the main class </p> <p>Then click <code>Create job</code> to create the job.</p>"},{"location":"user_guides/projects/jobs/spark_job/#step-6-optional-set-the-spark-script-arguments","title":"Step 6 (optional): Set the Spark script arguments","text":"<p>In the job settings, you can specify arguments for your Spark script. Remember to handle the arguments inside your Spark script.</p> <p> Configure Spark script arguments </p>"},{"location":"user_guides/projects/jobs/spark_job/#step-7-optional-advanced-configuration","title":"Step 7 (optional): Advanced configuration","text":"<p>Resource allocation for the Spark driver and executors can be configured, also the number of executors and whether dynamic execution should be enabled.</p> <ul> <li> <p><code>Environment</code>: The environment to use, must be based on <code>spark-feature-pipeline</code></p> </li> <li> <p><code>Driver memory</code>: Number of cores to allocate for the Spark driver</p> </li> <li> <p><code>Driver virtual cores</code>: Number of MBs to allocate for the Spark driver</p> </li> <li> <p><code>Executor memory</code>: Number of cores to allocate for each Spark executor</p> </li> <li> <p><code>Executor virtual cores</code>: Number of MBs to allocate for each Spark executor</p> </li> <li> <p><code>Dynamic/Static</code>: Run the Spark application in static or dynamic allocation mode (see spark docs for details).</p> </li> </ul> <p> Resource configuration for the Spark job </p> <p>Additional files or dependencies required for the Spark job can be configured.</p> <ul> <li> <p><code>Additional archives</code>: List of archives to be extracted into the working directory of each executor.</p> </li> <li> <p><code>Additional jars</code>: List of jars to be placed in the working directory of each executor.</p> </li> <li> <p><code>Additional python dependencies</code>: List of python files and archives to be placed on each executor and added to PATH.</p> </li> <li> <p><code>Additional files</code>: List of files to be placed in the working directory of each executor.</p> </li> </ul> <p> File configuration for the Spark job </p> <p>Line-separates properties to be set for the Spark application. For example, changing the configuration variables for the Kryo Serializer or setting environment variables for the driver, you can set the properties as shown below.</p> <p> Additional Spark configuration </p>"},{"location":"user_guides/projects/jobs/spark_job/#step-8-kueue-enabled-select-a-queue","title":"Step 8: (Kueue enabled) Select a Queue","text":"<p>Currently we do not have Kueue support for Spark. You do not need to select a queue to run the job in.</p>"},{"location":"user_guides/projects/jobs/spark_job/#step-9-execute-the-job","title":"Step 9: Execute the job","text":"<p>Now click the <code>Run</code> button to start the execution of the job, and then click on <code>Executions</code> to see the list of all executions.</p> <p> Start job execution </p>"},{"location":"user_guides/projects/jobs/spark_job/#step-10-application-logs","title":"Step 10: Application logs","text":"<p>To monitor logs while the execution is running, click <code>Spark UI</code> to open the Spark UI in a separate tab.</p> <p>Once the execution is finished, you can click on <code>Logs</code> to see the full logs for execution.</p> <p> Access Spark logs </p>"},{"location":"user_guides/projects/jobs/spark_job/#code","title":"Code","text":""},{"location":"user_guides/projects/jobs/spark_job/#step-1-upload-the-spark-jar","title":"Step 1: Upload the Spark jar","text":"<p>This snippet assumes the Spark program is in the current working directory and named <code>sparkpi.jar</code>.</p> <p>It will upload the jar to the <code>Resources</code> dataset in your project.</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\ndataset_api = project.get_dataset_api()\n\nuploaded_file_path = dataset_api.upload(\"sparkpi.jar\", \"Resources\")\n</code></pre>"},{"location":"user_guides/projects/jobs/spark_job/#step-2-create-spark-job","title":"Step 2: Create Spark job","text":"<p>In this snippet we get the <code>JobsApi</code> object to get the default job configuration for a <code>SPARK</code> job, set the python script to run and create the <code>Job</code> object.</p> <pre><code>jobs_api = project.get_job_api()\n\nspark_config = jobs_api.get_configuration(\"SPARK\")\n\nspark_config['appPath'] = uploaded_file_path\nspark_config['mainClass'] = 'org.apache.spark.examples.SparkPi'\n\njob = jobs_api.create_job(\"pyspark_job\", spark_config)\n</code></pre>"},{"location":"user_guides/projects/jobs/spark_job/#step-3-execute-the-job","title":"Step 3: Execute the job","text":"<p>In this snippet we execute the job synchronously, that is wait until it reaches a terminal state, and then download and print the logs.</p> <pre><code>execution = job.run(await_termination=True)\n\nout, err = execution.download_logs()\n\nf_out = open(out, \"r\")\nprint(f_out.read())\n\nf_err = open(err, \"r\")\nprint(f_err.read())\n</code></pre>"},{"location":"user_guides/projects/jobs/spark_job/#configuration","title":"Configuration","text":"<p>The following table describes the job configuration parameters for a SPARK job.</p> <p><code>conf = jobs_api.get_configuration(\"SPARK\")</code></p> Field Type Description Default <code>conf['type']</code> string Type of the job configuration <code>\"sparkJobConfiguration\"</code> <code>conf['appPath']</code> string Project path to spark program (e.g., <code>Resources/foo.jar</code>) <code>null</code> <code>conf['mainClass']</code> string Name of the main class to run (e.g., <code>org.company.Main</code>) <code>null</code> <code>conf['defaultArgs']</code> string Arguments to pass to the program. Will be overridden if arguments are passed explicitly via <code>Job.run(args=\"...\")</code> <code>null</code> <code>conf['environmentName']</code> string Name of the project spark environment to use <code>\"spark-feature-pipeline\"</code> <code>conf['spark.driver.cores']</code> float Number of CPU cores allocated for the driver <code>1.0</code> <code>conf['spark.driver.memory']</code> int Memory allocated for the driver (in MB) <code>2048</code> <code>conf['spark.executor.instances']</code> int Number of executor instances <code>1</code> <code>conf['spark.executor.cores']</code> float Number of CPU cores per executor <code>1.0</code> <code>conf['spark.executor.memory']</code> int Memory allocated per executor (in MB) <code>4096</code> <code>conf['spark.dynamicAllocation.enabled']</code> boolean Enable dynamic allocation of executors <code>true</code> <code>conf['spark.dynamicAllocation.minExecutors']</code> int Minimum number of executors with dynamic allocation <code>1</code> <code>conf['spark.dynamicAllocation.maxExecutors']</code> int Maximum number of executors with dynamic allocation <code>2</code> <code>conf['spark.dynamicAllocation.initialExecutors']</code> int Initial number of executors with dynamic allocation <code>1</code> <code>conf['spark.blacklist.enabled']</code> boolean Whether executor/node blacklisting is enabled <code>false</code> <code>conf['files']</code> string Comma-separated string of HDFS path(s) to files to be made available to the application. Example: <code>hdfs:///Project/&lt;project_name&gt;/Resources/file1.py,...</code> <code>null</code> <code>conf['pyFiles']</code> string Comma-separated string of HDFS path(s) to Python modules to be made available to the application. Example: <code>hdfs:///Project/&lt;project_name&gt;/Resources/file1.py,...</code> <code>null</code> <code>conf['jars']</code> string Comma-separated string of HDFS path(s) to jars to be included in CLASSPATH. Example: <code>hdfs:///Project/&lt;project_name&gt;/Resources/app.jar,...</code> <code>null</code> <code>conf['archives']</code> string Comma-separated string of HDFS path(s) to archives to be made available to the application. Example: <code>hdfs:///Project/&lt;project_name&gt;/Resources/archive.zip,...</code> <code>null</code> <code>conf['properties']</code> string A new line separated (<code>\\n</code>) list of properties to pass to the Spark application. The properties should be in the format <code>name=value</code> <code>null</code>"},{"location":"user_guides/projects/jobs/spark_job/#accessing-project-data","title":"Accessing project data","text":""},{"location":"user_guides/projects/jobs/spark_job/#read-directly-from-the-filesystem-recommended","title":"Read directly from the filesystem (recommended)","text":"<p>To read a dataset in your project using Spark, use the full filesystem path where the data is stored. For example, to read a CSV file named <code>data.csv</code> located in the <code>Resources</code> dataset of a project called <code>my_project</code>:</p> <pre><code>Dataset&lt;Row&gt; df = spark.read()\n    .option(\"header\", \"true\")       // CSV has header\n    .option(\"inferSchema\", \"true\")  // Infer data types\n    .csv(\"/Projects/my_project/Resources/data.csv\");\n\ndf.show();\n</code></pre>"},{"location":"user_guides/projects/jobs/spark_job/#additional-files","title":"Additional files","text":"<p>Different file types can be attached to the spark job and made available in the <code>/srv/hops/artifacts</code> folder when the Spark job is started. This configuration is mainly useful when you need to add additional configuration such as jars that needs to be added to the CLASSPATH.</p> <p>When reading data in your Spark job it is recommended to use the Spark read API as previously demonstrated, since this reads from the filesystem directly, whereas <code>Additional files</code> configuration options will download the files in its entirety and is not a scalable option.</p>"},{"location":"user_guides/projects/jobs/spark_job/#api-reference","title":"API Reference","text":"<p><code>Job</code></p> <p><code>Execution</code></p>"},{"location":"user_guides/projects/jupyter/python_notebook/","title":"How To Run A Python Notebook","text":""},{"location":"user_guides/projects/jupyter/python_notebook/#introduction","title":"Introduction","text":"<p>Jupyter is provided as a service in Hopsworks, providing the same user experience and features as if run on your laptop.</p> <ul> <li>Supports JupyterLab and the classic Jupyter front-end</li> <li>Configured with Python3, PySpark and Ray kernels</li> </ul>"},{"location":"user_guides/projects/jupyter/python_notebook/#step-1-jupyter-dashboard","title":"Step 1: Jupyter dashboard","text":"<p>The image below shows the Jupyter service page in Hopsworks and is accessed by clicking <code>Jupyter</code> in the sidebar.</p> <p> Jupyter dashboard in Hopsworks </p> <p>From this page, you can configure various options and settings to start Jupyter with as described in the sections below.</p>"},{"location":"user_guides/projects/jupyter/python_notebook/#step-2-optional-configure-resources","title":"Step 2 (Optional): Configure resources","text":"<p>Next step is to configure Jupyter, Click <code>edit configuration</code> to get to the configuration page and select <code>Python</code>.</p> <ul> <li> <p><code>Container cores</code>: Number of cores to allocate for the Jupyter instance</p> </li> <li> <p><code>Container memory</code>: Number of MBs to allocate for the Jupyter instance</p> </li> </ul> <p>Configured resource pool is shared by all running kernels. If a kernel crashes while executing a cell, try increasing the Container memory.</p> <p> Resource configuration for the Python kernel </p> <p>Click <code>Save</code> to save the new configuration.</p>"},{"location":"user_guides/projects/jupyter/python_notebook/#step-3-optional-configure-environment-root-folder-and-automatic-shutdown","title":"Step 3 (Optional): Configure environment, root folder and automatic shutdown","text":"<p>Before starting the server there are three additional configurations that can be set next to the <code>Run Jupyter</code> button.</p> <p>The environment that Jupyter should run in needs to be configured. Select the environment that contains the necessary dependencies for your code.</p> <p> Configure environment </p> <p>The runtime of the Jupyter instance can be configured, this is useful to ensure that idle instances will not be hanging around and keep allocating resources. If a limited runtime is not desirable, this can be disabled by setting <code>no limit</code>.</p> <p> Configure maximum runtime </p> <p>The root path from which to start the Jupyter instance can be configured. By default it starts by setting the <code>/Jupyter</code> folder as the root.</p> <p> Configure root folder </p>"},{"location":"user_guides/projects/jupyter/python_notebook/#step-4-kueue-enabled-select-a-queue","title":"Step 4: (Kueue enabled) Select a Queue","text":"<p>If the cluster is installed with Kueue enabled, you will need to select a queue in which the notebook should run. This can be done from <code>Advance configuration -&gt; Scheduler section</code>.</p> <p></p>"},{"location":"user_guides/projects/jupyter/python_notebook/#step-5-start-jupyter","title":"Step 5: Start Jupyter","text":"<p>Start the Jupyter instance by clicking the <code>Run Jupyter</code> button.</p> <p> Starting Jupyter and running a Python notebook </p>"},{"location":"user_guides/projects/jupyter/python_notebook/#accessing-project-data","title":"Accessing project data","text":"<p>Recommended approach if <code>/hopsfs</code> is mounted</p> <p>If your Hopsworks installation is configured to mount the project datasets under <code>/hopsfs</code>, which it is in most cases, then please refer to this section. If the file system is not mounted, then project files can be localized using the download api to localize files in the current working directory.</p>"},{"location":"user_guides/projects/jupyter/python_notebook/#absolute-paths","title":"Absolute paths","text":"<p>The project datasets are mounted under <code>/hopsfs</code>, so you can access <code>data.csv</code> from the <code>Resources</code> dataset using <code>/hopsfs/Resources/data.csv</code> in your notebook.</p>"},{"location":"user_guides/projects/jupyter/python_notebook/#relative-paths","title":"Relative paths","text":"<p>The notebook's working directory is the folder it is located in. For example, if it is located in the <code>Resources</code> dataset, and you have a file named <code>data.csv</code> in that dataset, you simply access it using <code>data.csv</code>. Also, if you write a local file, for example <code>output.txt</code>, it will be saved in the <code>Resources</code> dataset.</p>"},{"location":"user_guides/projects/jupyter/python_notebook/#going-further","title":"Going Further","text":"<p>You can learn how to install a library so that it can be used in a notebook.</p>"},{"location":"user_guides/projects/jupyter/ray_notebook/","title":"How To Run A Ray Notebook","text":""},{"location":"user_guides/projects/jupyter/ray_notebook/#introduction","title":"Introduction","text":"<p>Jupyter is provided as a service in Hopsworks, providing the same user experience and features as if run on your laptop.</p> <ul> <li>Supports JupyterLab and the classic Jupyter front-end</li> <li>Configured with Python3, PySpark and Ray kernels</li> </ul> <p>Enable Ray</p> <p>Support for Ray needs to be explicitly enabled by adding the following option in the <code>values.yaml</code> file for the deployment:</p> <pre><code>global:\n  ray:\n    enabled: true\n</code></pre>"},{"location":"user_guides/projects/jupyter/ray_notebook/#step-1-jupyter-dashboard","title":"Step 1: Jupyter dashboard","text":"<p>The image below shows the Jupyter service page in Hopsworks and is accessed by clicking <code>Jupyter</code> in the sidebar.</p> <p> Jupyter dashboard in Hopsworks </p> <p>From this page, you can configure various options and settings to start Jupyter with as described in the sections below.</p>"},{"location":"user_guides/projects/jupyter/ray_notebook/#step-2-optional-configure-ray","title":"Step 2 (Optional): Configure Ray","text":"<p>Next step is to configure the Ray cluster configuration that will be created when you start Ray session later in Jupyter. Click <code>edit configuration</code> to get to the configuration page and select <code>Ray</code>.</p>"},{"location":"user_guides/projects/jupyter/ray_notebook/#resource-and-compute","title":"Resource and compute","text":"<p>Resource allocation for the Driver and Workers can be configured.</p> <p>Using the resources in the Ray script</p> <p>The resource configurations describe the cluster that will be provisioned when launching the Ray job. User can still provide extra configurations in the job script using <code>ScalingConfig</code>, i.e. <code>ScalingConfig(num_workers=4, trainer_resources={\"CPU\": 1}, use_gpu=True)</code>.</p> <ul> <li> <p><code>Driver memory</code>: Memory in MBs to allocate for Driver</p> </li> <li> <p><code>Driver virtual cores</code>: Number of cores to allocate for the Driver</p> </li> <li> <p><code>Worker memory</code>: Memory in MBs to allocate for each worker</p> </li> <li> <p><code>Worker cores</code>: Number of cores to allocate for each worker</p> </li> <li> <p><code>Min workers</code>: Minimum number of workers to start with</p> </li> <li> <p><code>Max workers</code>: Maximum number of workers to scale up to</p> </li> </ul> <p> Resource configuration for the Ray kernels </p> <p>Runtime environment and Additional files required for the Ray job can also be provided.</p> <ul> <li> <p><code>Runtime Environment (Optional)</code>:  A runtime environment describes the dependencies required for the Ray job including files, packages, environment variables, and more.   This is useful when you need to install specific packages and set environment variables for this particular Ray job.   It should be provided as a YAML file.   You can select the file from the project or upload a new one.</p> </li> <li> <p><code>Additional files</code>: List of other files required for the Ray job.   These files will be placed in <code>/srv/hops/ray/job</code>.</p> </li> </ul> <p> Runtime configuration and additional files for Ray jupyter session </p> <p>Click <code>Save</code> to save the new configuration.</p>"},{"location":"user_guides/projects/jupyter/ray_notebook/#step-3-optional-configure-max-runtime-and-root-path","title":"Step 3 (Optional): Configure max runtime and root path","text":"<p>Before starting the server there are two additional configurations that can be set next to the <code>Run Jupyter</code> button.</p> <p>The runtime of the Jupyter instance can be configured, this is useful to ensure that idle instances will not be hanging around and keep allocating resources. If a limited runtime is not desirable, this can be disabled by setting <code>no limit</code>.</p> <p> Configure maximum runtime </p> <p>The root path from which to start the Jupyter instance can be configured. By default it starts by setting the <code>/Jupyter</code> folder as the root.</p> <p> Configure root folder </p>"},{"location":"user_guides/projects/jupyter/ray_notebook/#step-4-select-the-environment","title":"Step 4: Select the environment","text":"<p>Hopsworks provides a variety of environments to run Jupyter notebooks. Select the environment you want to use by clicking on the dropdown menu. In order to be able to run a Ray notebook, you need to select the environment that has the Ray kernel installed. Environment with Ray kernel have a <code>Ray Enabled</code> label next to them.</p>"},{"location":"user_guides/projects/jupyter/ray_notebook/#step-5-kueue-enabled-select-a-queue","title":"Step 5: (Kueue enabled) Select a Queue","text":"<p>If the cluster is installed with Kueue enabled, you will need to select a queue in which the notebook should run. This can be done from <code>Advance configuration -&gt; Scheduler section</code>.</p> <p></p>"},{"location":"user_guides/projects/jupyter/ray_notebook/#step-6-start-jupyter","title":"Step 6: Start Jupyter","text":"<p>Start the Jupyter instance by clicking the <code>Run Jupyter</code> button.</p>"},{"location":"user_guides/projects/jupyter/ray_notebook/#running-ray-code-in-jupyter","title":"Running Ray code in Jupyter","text":"<p>Once the Jupyter instance is started, you can create a new notebook by clicking on the <code>New</code> button and selecting <code>Ray</code> kernel. You can now write and run Ray code in the notebook. When you first run a cell with Ray code, a Ray session will be started and you can monitor the resources used by the job in the Ray dashboard.</p> <p> Ray Kernel </p>"},{"location":"user_guides/projects/jupyter/ray_notebook/#step-7-access-ray-dashboard","title":"Step 7: Access Ray Dashboard","text":"<p>When you start a Ray session in Jupyter, a new application will appear in the Jupyter page. The notebook name from which the session was started is displayed. You can access the Ray UI by clicking on the <code>Ray Dashboard</code> and a new tab will be opened. The Ray dashboard is only available while the Ray kernel is running. You can kill the Ray session to free up resources by shutting down the kernel in Jupyter. In the Ray Dashboard, you can monitor the resources used  by code you are running, the number of workers, logs, and the tasks that are running.</p> <p> Access Ray Dashboard for Jupyter Ray session </p>"},{"location":"user_guides/projects/jupyter/ray_notebook/#accessing-project-data","title":"Accessing project data","text":"<p>The project datasets are mounted under <code>/hopsfs</code> in the Ray containers, so you can access <code>data.csv</code> from the <code>Resources</code> dataset using <code>/hopsfs/Resources/data.csv</code>.</p>"},{"location":"user_guides/projects/jupyter/remote_filesystem_driver/","title":"Configuring remote filesystem driver","text":""},{"location":"user_guides/projects/jupyter/remote_filesystem_driver/#introduction","title":"Introduction","text":"<p>We provide two ways to access and persist files in HopsFS from a jupyter notebook:</p> <ul> <li><code>hdfscontentsmanager</code>: With <code>hdfscontentsmanager</code> you interact with the project datasets using the dataset api.   When you start a notebook using the <code>hdfscontentsmanager</code> you will only see the files in the configured root path.</li> <li><code>hopsfsmount</code>: With <code>hopsfsmount</code> all the project datasets are available in the jupyter notebook as a local filesystem.   This means you can use native Python file I/O operations (copy, move, create, open, etc.) to interact with the project datasets.   When you open the jupyter notebook you will see all the project datasets.</li> </ul>"},{"location":"user_guides/projects/jupyter/remote_filesystem_driver/#configuring-the-driver","title":"Configuring the driver","text":"<p>To configure the driver you need to have admin role and set the <code>jupyter_remote_fs_driver</code> to either <code>hdfscontentsmanager</code> or <code>hopsfsmount</code>. The default driver is <code>hdfscontentsmanager</code>.</p>"},{"location":"user_guides/projects/jupyter/spark_notebook/","title":"How To Run A PySpark Notebook","text":""},{"location":"user_guides/projects/jupyter/spark_notebook/#introduction","title":"Introduction","text":"<p>Jupyter is provided as a service in Hopsworks, providing the same user experience and features as if run on your laptop.</p> <ul> <li>Supports JupyterLab and the classic Jupyter front-end</li> <li>Configured with Python3, PySpark and Ray kernels</li> </ul>"},{"location":"user_guides/projects/jupyter/spark_notebook/#step-1-jupyter-dashboard","title":"Step 1: Jupyter dashboard","text":"<p>The image below shows the Jupyter service page in Hopsworks and is accessed by clicking <code>Jupyter</code> in the sidebar.</p> <p> Jupyter dashboard in Hopsworks </p> <p>From this page, you can configure various options and settings to start Jupyter with as described in the sections below.</p>"},{"location":"user_guides/projects/jupyter/spark_notebook/#step-2-a-spark-environment-must-be-configured","title":"Step 2: A Spark environment must be configured","text":"<p>The PySpark kernel will only be available if Jupyter is configured to use the <code>spark-feature-pipeline</code> or an environment cloned from it. You can easily refer to the green ticks as to what kernels are available in which environment.</p> <p> Select an environment with PySpark kernel enabled </p>"},{"location":"user_guides/projects/jupyter/spark_notebook/#step-3-optional-configure-spark-properties","title":"Step 3 (Optional): Configure spark properties","text":"<p>Next step is to configure the Ray properties to be used in Jupyter, Click <code>edit configuration</code> to get to the configuration page and select <code>Ray</code>.</p>"},{"location":"user_guides/projects/jupyter/spark_notebook/#resource-and-compute","title":"Resource and compute","text":"<p>Resource allocation for the Spark driver and executors can be configured, also the number of executors and whether dynamic execution should be enabled.</p> <ul> <li> <p><code>Driver memory</code>: Number of cores to allocate for the Spark driver</p> </li> <li> <p><code>Driver virtual cores</code>: Number of MBs to allocate for the Spark driver</p> </li> <li> <p><code>Executor memory</code>: Number of cores to allocate for each Spark executor</p> </li> <li> <p><code>Executor virtual cores</code>: Number of MBs to allocate for each Spark executor</p> </li> <li> <p><code>Dynamic/Static</code>: Run the Spark application in static or dynamic allocation mode (see spark docs for details).</p> </li> </ul> <p> Resource configuration for the Spark kernels </p>"},{"location":"user_guides/projects/jupyter/spark_notebook/#attach-files-or-dependencies","title":"Attach files or dependencies","text":"<p>Additional files or dependencies required for the Spark job can be configured.</p> <ul> <li> <p><code>Additional archives</code>: List of zip or .tgz files that will be locally accessible by the application</p> </li> <li> <p><code>Additional jars</code>: List of .jar files to add to the CLASSPATH of the application</p> </li> <li> <p><code>Additional python dependencies</code>: List of .py, .zip or .egg files that will be locally accessible by the application</p> </li> <li> <p><code>Additional files</code>: List of files that will be locally accessible by the application</p> </li> </ul> <p> File configuration for the Spark kernels </p> <p>Line-separates properties to be set for the Spark application. For example, changing the configuration variables for the Kryo Serializer or setting environment variables for the driver, you can set the properties as shown below.</p> <p> Additional Spark configuration </p> <p>Click <code>Save</code> to save the new configuration.</p>"},{"location":"user_guides/projects/jupyter/spark_notebook/#step-4-optional-configure-root-folder-and-automatic-shutdown","title":"Step 4 (Optional): Configure root folder and automatic shutdown","text":"<p>Before starting the server there are two additional configurations that can be set next to the <code>Run Jupyter</code> button.</p> <p>The runtime of the Jupyter instance can be configured, this is useful to ensure that idle instances will not be hanging around and keep allocating resources. If a limited runtime is not desirable, this can be disabled by setting <code>no limit</code>.</p> <p> Configure maximum runtime </p> <p>The root path from which to start the Jupyter instance can be configured. By default it starts by setting the <code>/Jupyter</code> folder as the root.</p> <p> Configure root folder </p>"},{"location":"user_guides/projects/jupyter/spark_notebook/#step-5-kueue-enabled-select-a-queue","title":"Step 5: (Kueue enabled) Select a Queue","text":"<p>Currently we do not have Kueue support for Spark. You do not need to select a queue to run the notebook in.</p>"},{"location":"user_guides/projects/jupyter/spark_notebook/#step-5-start-jupyter","title":"Step 5: Start Jupyter","text":"<p>Start the Jupyter instance by clicking the <code>Run Jupyter</code> button.</p> <p> Starting Jupyter and running a Spark notebook </p>"},{"location":"user_guides/projects/jupyter/spark_notebook/#step-6-access-spark-ui","title":"Step 6: Access Spark UI","text":"<p>Navigate back to Hopsworks and a Spark session will have appeared, click on the <code>Spark UI</code> button to go to the Spark UI.</p> <p> Access Spark UI and see application logs </p>"},{"location":"user_guides/projects/jupyter/spark_notebook/#accessing-project-data","title":"Accessing project data","text":""},{"location":"user_guides/projects/jupyter/spark_notebook/#read-directly-from-the-filesystem-recommended","title":"Read directly from the filesystem (recommended)","text":"<p>To read a dataset in your project using Spark, use the full filesystem path where the data is stored. For example, to read a CSV file named <code>data.csv</code> located in the <code>Resources</code> dataset of a project called <code>my_project</code>:</p> <pre><code>df = spark.read.csv(\"/Projects/my_project/Resources/data.csv\", header=True, inferSchema=True)\ndf.show()\n</code></pre>"},{"location":"user_guides/projects/jupyter/spark_notebook/#additional-files","title":"Additional files","text":"<p>Different files can be attached to the jupyter session and made available in the <code>/srv/hops/artifacts</code> folder when the PySpark kernel is started. This configuration is mainly useful when you need to add additional configuration such as jars that needs to be added to the CLASSPATH.</p> <p>When reading data in your Spark application, it is recommended to use the Spark read API as previously demonstrated, since this reads from the filesystem directly, whereas <code>Additional files</code> configuration options will download the files in its entirety and is not a scalable option.</p>"},{"location":"user_guides/projects/jupyter/spark_notebook/#going-further","title":"Going Further","text":"<p>You can learn how to install a library so that it can be used in a notebook.</p>"},{"location":"user_guides/projects/kafka/consume_messages/","title":"How To Consume Message From A Topic","text":""},{"location":"user_guides/projects/kafka/consume_messages/#introduction","title":"Introduction","text":"<p>A Consumer is a process which reads messages from a Kafka topic. In Hopsworks, all user roles are capable of performing 'Read' and 'Describe' actions on Kafka topics within projects that they are a member of or are shared with them.</p>"},{"location":"user_guides/projects/kafka/consume_messages/#prerequisites","title":"Prerequisites","text":"<p>This guide requires that you have previously produced messages to a kafka topic.</p>"},{"location":"user_guides/projects/kafka/consume_messages/#code","title":"Code","text":"<p>In this guide, you will learn how to consume messages from a kafka topic.</p>"},{"location":"user_guides/projects/kafka/consume_messages/#step-1-get-the-kafka-api","title":"Step 1: Get the Kafka API","text":"<pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nkafka_api = project.get_kafka_api()\n</code></pre>"},{"location":"user_guides/projects/kafka/consume_messages/#step-2-configure-confluent-kafka-client","title":"Step 2: Configure confluent-kafka client","text":"<pre><code>consumer_config = kafka_api.get_default_config()\nconsumer_config['default.topic.config'] = {'auto.offset.reset': 'earliest'}\n\nfrom confluent_kafka import Consumer\n\nconsumer = Consumer(consumer_config)\n</code></pre>"},{"location":"user_guides/projects/kafka/consume_messages/#step-3-consume-messages-from-a-topic","title":"Step 3: Consume messages from a topic","text":"<pre><code># Subscribe to topic\nconsumer.subscribe([\"my_topic\"])\n\nfor i in range(0, 10):\n    msg = consumer.poll(timeout=10.0)\n    print(msg.value())\n</code></pre>"},{"location":"user_guides/projects/kafka/consume_messages/#api-reference","title":"API Reference","text":"<p><code>KafkaTopic</code></p>"},{"location":"user_guides/projects/kafka/create_schema/","title":"How To Create A Kafka Schema","text":""},{"location":"user_guides/projects/kafka/create_schema/#introduction","title":"Introduction","text":""},{"location":"user_guides/projects/kafka/create_schema/#code","title":"Code","text":"<p>In this guide, you will learn how to create a Kafka Avro Schema in the Hopsworks Schema Registry.</p>"},{"location":"user_guides/projects/kafka/create_schema/#step-1-get-the-kafka-api","title":"Step 1: Get the Kafka API","text":"<pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nkafka_api = project.get_kafka_api()\n</code></pre>"},{"location":"user_guides/projects/kafka/create_schema/#step-2-define-the-schema","title":"Step 2: Define the schema","text":"<p>Define the Avro Schema, see types for the format of the schema.</p> <pre><code>schema = {\n    \"type\": \"record\",\n    \"name\": \"tutorial\",\n    \"fields\": [\n        {\n            \"name\": \"id\",\n            \"type\": \"int\"\n        },\n        {\n            \"name\": \"data\",\n            \"type\": \"string\"\n        }\n    ]\n}\n</code></pre>"},{"location":"user_guides/projects/kafka/create_schema/#step-3-create-the-schema","title":"Step 3: Create the schema","text":"<p>Create the schema in the Schema Registry.</p> <pre><code>SCHEMA_NAME=\"schema_example\"\n\nmy_schema = kafka_api.create_schema(SCHEMA_NAME, schema)\n</code></pre>"},{"location":"user_guides/projects/kafka/create_schema/#api-reference","title":"API Reference","text":"<p><code>KafkaSchema</code></p>"},{"location":"user_guides/projects/kafka/create_topic/","title":"How To Create A Kafka Topic","text":""},{"location":"user_guides/projects/kafka/create_topic/#introduction","title":"Introduction","text":"<p>A Topic is a queue to which records are stored and published. Producer applications write data to topics and consumer applications read from topics.</p>"},{"location":"user_guides/projects/kafka/create_topic/#prerequisites","title":"Prerequisites","text":"<p>This guide requires that you have 'Data owner' role and have previously created a Kafka Schema to be used for the topic.</p>"},{"location":"user_guides/projects/kafka/create_topic/#code","title":"Code","text":"<p>In this guide, you will learn how to create a Kafka Topic.</p>"},{"location":"user_guides/projects/kafka/create_topic/#step-1-get-the-kafka-api","title":"Step 1: Get the Kafka API","text":"<pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nkafka_api = project.get_kafka_api()\n</code></pre>"},{"location":"user_guides/projects/kafka/create_topic/#step-2-define-the-schema","title":"Step 2: Define the schema","text":"<pre><code>TOPIC_NAME=\"topic_example\"\nSCHEMA_NAME=\"schema_example\"\n\nmy_topic = kafka_api.create_topic(TOPIC_NAME, SCHEMA_NAME, 1, replicas=1, partitions=1)\n</code></pre>"},{"location":"user_guides/projects/kafka/create_topic/#api-reference","title":"API Reference","text":"<p><code>KafkaTopic</code></p>"},{"location":"user_guides/projects/kafka/produce_messages/","title":"How To Produce To A Topic","text":""},{"location":"user_guides/projects/kafka/produce_messages/#introduction","title":"Introduction","text":"<p>A Producer is a process which produces messages to a Kafka topic. In Hopsworks, only users with the 'Data owner' role are capable of performing the 'Write' action on Kafka topics within the project that they are a member of.</p>"},{"location":"user_guides/projects/kafka/produce_messages/#prerequisites","title":"Prerequisites","text":"<p>This guide requires that you have 'Data owner' role and have previously created a Kafka Topic.</p>"},{"location":"user_guides/projects/kafka/produce_messages/#code","title":"Code","text":"<p>In this guide, you will learn how to produce messages to a kafka topic.</p>"},{"location":"user_guides/projects/kafka/produce_messages/#step-1-get-the-kafka-api","title":"Step 1: Get the Kafka API","text":"<pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nkafka_api = project.get_kafka_api()\n</code></pre>"},{"location":"user_guides/projects/kafka/produce_messages/#step-2-configure-confluent-kafka-client","title":"Step 2: Configure confluent-kafka client","text":"<pre><code>producer_config = kafka_api.get_default_config()\n\nfrom confluent_kafka import Producer\n\nproducer = Producer(producer_config)\n</code></pre>"},{"location":"user_guides/projects/kafka/produce_messages/#step-3-produce-messages-to-topic","title":"Step 3: Produce messages to topic","text":"<pre><code>import uuid\nimport json\n\n# Send a few messages\nfor i in range(0, 10):\n    producer.produce(\"my_topic\", json.dumps({\"id\": i, \"data\": str(uuid.uuid1())}), \"key\")\n\n# Trigger the sending of all messages to the brokers, 10 sec timeout\nproducer.flush(10)\n</code></pre>"},{"location":"user_guides/projects/kafka/produce_messages/#api-reference","title":"API Reference","text":"<p><code>KafkaTopic</code></p>"},{"location":"user_guides/projects/kafka/produce_messages/#going-further","title":"Going Further","text":"<p>Now you can create a Consumer to read the messages from the topic.</p>"},{"location":"user_guides/projects/opensearch/connect/","title":"How To Connect To OpenSearch","text":""},{"location":"user_guides/projects/opensearch/connect/#introduction","title":"Introduction","text":"<p>Text here</p> <p>Limited to internal Jobs and Notebooks</p> <p>Currently it's only possible to configure the opensearch-py client in a job or jupyter notebook running inside the Hopsworks cluster.</p>"},{"location":"user_guides/projects/opensearch/connect/#code","title":"Code","text":"<p>In this guide, you will learn how to connect to the OpenSearch cluster using an opensearch-py client.</p>"},{"location":"user_guides/projects/opensearch/connect/#step-1-get-the-opensearch-api","title":"Step 1: Get the OpenSearch API","text":"<pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nopensearch_api = project.get_opensearch_api()\n</code></pre>"},{"location":"user_guides/projects/opensearch/connect/#step-2-configure-the-opensearch-py-client","title":"Step 2: Configure the opensearch-py client","text":"<pre><code>from opensearchpy import OpenSearch\n\nclient = OpenSearch(**opensearch_api.get_default_py_config())\n</code></pre>"},{"location":"user_guides/projects/opensearch/connect/#api-reference","title":"API Reference","text":"<p><code>OpenSearchApi</code></p>"},{"location":"user_guides/projects/opensearch/connect/#going-further","title":"Going Further","text":"<p>You can now use the client to interact directly with the OpenSearch cluster, such as vector database.</p>"},{"location":"user_guides/projects/opensearch/knn/","title":"How To Use OpenSearch k-NN plugin","text":""},{"location":"user_guides/projects/opensearch/knn/#introduction","title":"Introduction","text":"<p>The k-NN plugin enables users to search for the k-nearest neighbors to a query point across an index of vectors. To determine the neighbors, you can specify the space (the distance function) you want to use to measure the distance between points.</p> <p>Use cases include recommendations (for example, an \u201cother songs you might like\u201d feature in a music application), image recognition, and fraud detection.</p> <p>Limited to internal Jobs and Notebooks</p> <p>Currently it's only possible to configure the opensearch-py client in a job or jupyter notebook running inside the Hopsworks cluster.</p>"},{"location":"user_guides/projects/opensearch/knn/#code","title":"Code","text":"<p>In this guide, you will learn how to create a simple recommendation application, using the <code>k-NN plugin</code> in OpenSearch.</p>"},{"location":"user_guides/projects/opensearch/knn/#step-1-get-the-opensearch-api","title":"Step 1: Get the OpenSearch API","text":"Python <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nopensearch_api = project.get_opensearch_api()\n</code></pre>"},{"location":"user_guides/projects/opensearch/knn/#step-2-configure-the-opensearch-py-client","title":"Step 2: Configure the opensearch-py client","text":"Python <pre><code>from opensearchpy import OpenSearch\n\nclient = OpenSearch(**opensearch_api.get_default_py_config())\n</code></pre>"},{"location":"user_guides/projects/opensearch/knn/#step-3-create-an-index","title":"Step 3: Create an index","text":"<p>Create an index to use by calling <code>opensearch_api.get_project_index(..)</code>.</p> Python <pre><code>knn_index_name = opensearch_api.get_project_index(\"demo_knn_index\")\n\nindex_body = {\n    \"settings\": {\n        \"knn\": True,\n        \"knn.algo_param.ef_search\": 100,\n    },\n    \"mappings\": {\n        \"properties\": {\n            \"my_vector1\": {\n                \"type\": \"knn_vector\",\n                \"dimension\": 2\n            }\n        }\n    }\n}\n\nresponse = client.indices.create(knn_index_name, body=index_body)\n\nprint(response)\n</code></pre>"},{"location":"user_guides/projects/opensearch/knn/#step-4-bulk-ingestion-of-vectors","title":"Step 4: Bulk ingestion of vectors","text":"<p>Ingest 10 vectors in a bulk fashion to the index. These vectors represent the list of vectors to calculate the similarity for.</p> Python <pre><code>from opensearchpy.helpers import bulk\nimport random\n\nactions = [\n    {\n        \"_index\": knn_index_name,\n        \"_id\": count,\n        \"_source\": {\n            \"my_vector1\": [random.uniform(0, 10), random.uniform(0, 10)],\n        }\n    }\n    for count in range(0, 10)\n]\n\nbulk(\n    client,\n    actions,\n)\n</code></pre>"},{"location":"user_guides/projects/opensearch/knn/#step-5-score-vector-similarity","title":"Step 5: Score vector similarity","text":"<p>Score the vector <code>[2.5, 3]</code> and find the 3 most similar vectors.</p> Python <pre><code># Define the search request\nquery = {\n    \"size\": 3,\n    \"query\": {\n        \"knn\": {\n            \"my_vector1\": {\n                \"vector\": [2.5, 3],\n                \"k\": 3\n            }\n        }\n    }\n}\n\n# Perform the similarity search\nresponse = client.search(\n    body = query,\n    index = knn_index_name\n)\n\n# Pretty print response\nimport pprint\npp = pprint.PrettyPrinter()\npp.pprint(response)\n</code></pre> <p><code>Output</code> from the above script shows the score for each of the three most similar vectors that have been indexed.</p> <p><code>[4.798869166444522, 4.069064892468535]</code> is the most similar vector to <code>[2.5, 3]</code> with a score of <code>0.1346312</code>.</p> Bash <pre><code>2022-05-30 09:55:50,529 INFO: POST https://10.0.2.15:9200/my_project_demo_knn_index/_search [status:200 request:0.017s]\n{'_shards': {'failed': 0, 'skipped': 0, 'successful': 1, 'total': 1},\n 'hits': {'hits': [{'_id': '9',\n                    '_index': 'my_project_demo_knn_index',\n                    '_score': 0.1346312,\n                    '_source': {'my_vector1': [4.798869166444522,\n                                               4.069064892468535]},\n                    '_type': '_doc'},\n                   {'_id': '0',\n                    '_index': 'my_project_demo_knn_index',\n                    '_score': 0.040784083,\n                    '_source': {'my_vector1': [6.267438489652193,\n                                               6.0538134453735175]},\n                    '_type': '_doc'},\n                   {'_id': '7',\n                    '_index': 'my_project_demo_knn_index',\n                    '_score': 0.03222388,\n                    '_source': {'my_vector1': [7.973873201006634,\n                                               2.7361877621502115]},\n                    '_type': '_doc'}],\n          'max_score': 0.1346312,\n          'total': {'relation': 'eq', 'value': 3}},\n 'timed_out': False,\n 'took': 9}\n</code></pre>"},{"location":"user_guides/projects/opensearch/knn/#api-reference","title":"API Reference","text":"<p>k-NN plugin</p> <p><code>OpenSearchApi</code></p>"},{"location":"user_guides/projects/project/create_project/","title":"How To Create A Project","text":""},{"location":"user_guides/projects/project/create_project/#introduction","title":"Introduction","text":"<p>In this guide, you will learn how to create a new project.</p> <p>Project name validation rules</p> <p>A valid project name can only contain characters a-z, A-Z, 0-9 and special characters \u2018_\u2019 and \u2018.\u2019 but not \u2018__\u2019 (double underscore). There is also a number of reserved project names that can not be used.</p>"},{"location":"user_guides/projects/project/create_project/#gui","title":"GUI","text":""},{"location":"user_guides/projects/project/create_project/#step-1-create-a-project","title":"Step 1: Create a project","text":"<p>If you log in to the platform and do not have any projects, you are presented with the following view. To run the Feature Store tour click <code>Run a demo project</code>, to create a new project click <code>Create new project</code>.</p> <p>For this guide click <code>Create new project</code> to continue.</p> <p> Landing page </p>"},{"location":"user_guides/projects/project/create_project/#step-2-project-creation-form","title":"Step 2: Project creation form","text":"<p>In the creation form in which you enter the project name, an optional description and set of members to invite to the project.</p> <p> Project creation form </p>"},{"location":"user_guides/projects/project/create_project/#step-3-project-creation","title":"Step 3: Project creation","text":"<p>Then wait for the project creation process to finish.</p> <p> List of created API Keys </p>"},{"location":"user_guides/projects/project/create_project/#step-4-project-overview","title":"Step 4: Project overview","text":"<p>Once the project is created the overview page for it will appear.</p> <p> List of created API Keys </p>"},{"location":"user_guides/projects/project/create_project/#code","title":"Code","text":""},{"location":"user_guides/projects/project/create_project/#step-1-connect-to-hopsworks","title":"Step 1: Connect to Hopsworks","text":"<pre><code>import hopsworks\n\nhopsworks.login()\n</code></pre>"},{"location":"user_guides/projects/project/create_project/#step-2-create-project","title":"Step 2: Create project","text":"<pre><code>project = hopsworks.create_project(\"my_project\")\n</code></pre>"},{"location":"user_guides/projects/project/create_project/#api-reference","title":"API Reference","text":"<p><code>Project</code></p>"},{"location":"user_guides/projects/project/create_project/#reserved-project-names","title":"Reserved project names","text":"<pre><code>PROJECTS, HOPS-SYSTEM, HOPSWORKS, INFORMATION_SCHEMA, AIRFLOW, GLASSFISH_TIMERS, GRAFANA, HOPS, METASTORE,\nMYSQL, NDBINFO, RONDB_REPLICATION, PERFORMANCE_SCHEMA, SQOOP, SYS, GLASSFISH_TIMERS, GRAFANA, HOPS, METASTORE, MYSQL, NDBINFO,\nPERFORMANCE_SCHEMA, SQOOP, SYS, BIGINT, BINARY, BOOLEAN, BOTH, BY, CASE, CAST, CHAR, COLUMN, CONF, CREATE, CROSS, CUBE, CURRENT, CURRENT_DATE,\nCURRENT_TIMESTAMP, CURSOR, DATABASE, DATE, DECIMAL, DELETE, DESCRIBE, DISTINCT, DOUBLE, DROP, ELSE, END,\nEXCHANGE, EXISTS, EXTENDED, EXTERNAL, FALSE, FETCH, FLOAT, FOLLOWING, FOR, FROM, FULL, FUNCTION, GRANT, GROUP,\nGROUPING, HAVING, IF, IMPORT, IN, INNER, INSERT, INT, INTERSECT, INTERVAL, INTO, IS, JOIN, LATERAL, LEFT, LESS,\nLIKE, LOCAL, MACRO, MAP, MORE, NONE, NOT, NULL, OF, ON, OR, ORDER, OUT, OUTER, OVER, PARTIALSCAN, PARTITION,\nPERCENT, PRECEDING, PRESERVE, PROCEDURE, RANGE, READS, REDUCE, REVOKE, RIGHT, ROLLUP, ROW, ROWS, SELECT, SET,\nSMALLINT, TABLE, TABLESAMPLE, THEN, TIMESTAMP, TO, TRANSFORM, TRIGGER, TRUE, TRUNCATE, UNBOUNDED, UNION,\nUNIQUEJOIN, UPDATE, USER, USING, UTC_TMESTAMP, VALUES, VARCHAR, WHEN, WHERE, WINDOW, WITH, COMMIT, ONLY,\nREGEXP, RLIKE, ROLLBACK, START, CACHE, CONSTRAINT, FOREIGN, PRIMARY, REFERENCES, DAYOFWEEK, EXTRACT, FLOOR,\nINTEGER, PRECISION, VIEWS, TIME, NUMERIC, SYNC, BASE, PYTHON37, FILEBEAT.\n\nAnd any word containing _FEATURESTORE.\n</code></pre>"},{"location":"user_guides/projects/project/manage_members/","title":"How To Manage Members To A Project","text":""},{"location":"user_guides/projects/project/manage_members/#introduction","title":"Introduction","text":"<p>In this guide, you will learn how to add new members to your project and understand the different roles available within a project.</p>"},{"location":"user_guides/projects/project/manage_members/#step-1-view-the-members-list","title":"Step 1: View the members list","text":"<p>Navigate to the <code>Project settings</code> page and locate the <code>General</code> section, which displays the current members of the project.</p> <p> List of project members </p>"},{"location":"user_guides/projects/project/manage_members/#step-2-add-a-new-member","title":"Step 2: Add a new member","text":"<p>Click <code>Add members</code> to open a dialog where you can invite users. Select one or more users to invite.</p> <p> Add new member dialog </p> <p>Each member can be assigned one of three roles, depending on the level of access they need.</p>"},{"location":"user_guides/projects/project/manage_members/#data-owner","title":"Data owner","text":"<p>Data owners hold the highest authority in the project, with full control over its contents.</p> <p>They can:</p> <ul> <li>Share the project with other projects</li> <li>Manage project settings and members</li> <li>Create, read, update, and delete all feature store resources (feature groups, feature views, training datasets, etc.)</li> </ul> <p>Project author</p> <p>The project creator is a special type of Data owner. Only the creator can delete the project, and their role cannot be changed.</p>"},{"location":"user_guides/projects/project/manage_members/#data-scientist","title":"Data scientist","text":"<p>Data scientists are consumers and creators of feature views and training datasets.</p> <p>They can:</p> <ul> <li>Create feature views and training datasets using existing feature groups</li> <li>Manage the feature views and training datasets they have created</li> <li>Read feature groups created by Data owners</li> </ul>"},{"location":"user_guides/projects/project/manage_members/#feature-store-restricted","title":"Feature store restricted","text":"<p>Feature store restricted users have the most limited access, designed for users who only need to consume specific features that have been explicitly shared with them.</p> <p>They can:</p> <ul> <li>Access and use feature groups that have been explicitly shared with them</li> <li>Create feature views and training datasets using shared features</li> </ul>"},{"location":"user_guides/projects/project/manage_members/#step-3-confirm-member-invitation","title":"Step 3: Confirm member invitation","text":"<p>The invited user will now appear in the members list and will have immediate access to the project based on their assigned role.</p> <p> List of project members </p>"},{"location":"user_guides/projects/project/manage_members/#step-4-manage-members","title":"Step 4: Manage members","text":"<p>To change a member's role or remove them from the project, click the <code>Manage members</code> button. From there, you can modify roles or delete members as needed.</p> <p> Managing members of project </p>"},{"location":"user_guides/projects/python/custom_commands/","title":"Adding extra configuration with generic bash commands","text":""},{"location":"user_guides/projects/python/custom_commands/#introduction","title":"Introduction","text":"<p>Hopsworks comes with several prepackaged Python environments that contain libraries for data engineering, machine learning, and more general data science use-cases. Hopsworks also offers the ability to install additional packages from various sources, such as using the pip or conda package managers and public or private git repository.</p> <p>Some Python libraries require the installation of some OS-Level libraries. In some cases, you may need to add more complex configuration to your environment. This demands writing your own commands and executing them on top of the existing environment.</p> <p>In this guide, you will learn how to run custom bash commands that can be used to add more complex configuration to your environment e.g., installing OS-Level packages or configuring an oracle database.</p>"},{"location":"user_guides/projects/python/custom_commands/#prerequisites","title":"Prerequisites","text":"<p>In order to install a custom dependency one of the base environments must first be cloned, follow this guide for that.</p>"},{"location":"user_guides/projects/python/custom_commands/#running-bash-commands","title":"Running bash commands","text":"<p>In this section, we will see how you can run custom bash commands in Hopsworks to configure your Python environment.</p> <p>In Hopsworks, we maintain a docker image built on top of Ubuntu Linux distribution. You can run generic bash commands on top of the project environment from the UI or REST API.</p>"},{"location":"user_guides/projects/python/custom_commands/#setting-up-the-bash-script-and-artifacts-from-the-ui","title":"Setting up the bash script and artifacts from the UI","text":"<p>To use the UI, navigate to the Python environment in the Project settings. In the Python environment page, navigate to custom commands. From the UI, you can write the bash commands in the textbox provided. These bash commands will be uploaded and executed when building your new environment. You can include build artifacts e.g., binaries that you would like to execute or include when building the environment. See Figure 1.</p> <p> Figure 1: You can write custom commands and upload build artifacts from the UI </p>"},{"location":"user_guides/projects/python/custom_commands/#code","title":"Code","text":"<p>You can also run the custom commands using the REST API. From the REST API, you should provide the path, in HOPSFS, to the bash script and the artifacts(comma separated string of paths in HopsFs). The REST API endpoint for running custom commands is: <code>hopsworks-api/api/project/&lt;projectId&gt;/python/environments/&lt;environmentName&gt;/commands/custom</code> and the body should look like this:</p> <pre><code>{\n    \"commandsFile\": \"&lt;pathToYourBashScriptInHopsFS&gt;\",\n    \"artifacts\": \"&lt;commaSeparatedListOfPathsToArtifactsInHopsFS&gt;\"\n}\n</code></pre>"},{"location":"user_guides/projects/python/custom_commands/#what-to-include-in-the-bash-script","title":"What to include in the bash script","text":"<p>There are few important things to be aware of when writing the bash script:</p> <ul> <li>The first line of your bash script should always be <code>#!/bin/bash</code> (known as shebang) so that the script can be interpreted and executed using the Bash shell.</li> <li>You can use <code>apt</code>, <code>apt-get</code> and <code>deb</code> commands to install packages.   You should always run these commands with <code>sudo</code>.   In some cases, these commands will ask for user input, therefore you should provide the input of what the command expects, e.g., <code>sudo apt -y install</code>, otherwise the build will fail.   We have already configured <code>apt-get</code> to be non-interactive</li> <li>The build artifacts will be copied to <code>srv/hops/build</code>.   You can use them in your script via this path.   This path is also available via the environmental variable <code>BUILD_PATH</code>.   If you want to use many artifacts it is advisable to create a zip file and upload it to HopsFS in one of your project datasets.   You can then include the zip file as one of the artifacts.</li> <li>The conda environment is located in <code>/srv/hops/anaconda/envs/hopsworks_environment</code>.   You can install or uninstall packages in the conda environment using pip like: <code>/srv/hops/anaconda/envs/hopsworks_environment/bin/pip install spotify==0.10.2</code>.   If the command requires some input, write the command together with the expected input otherwise the build will fail.</li> </ul>"},{"location":"user_guides/projects/python/environment_history/","title":"Environment History","text":"<p>Hopsworks comes with several prepackaged Python environments that contain libraries for data engineering, machine learning, and more general data science use-cases. Hopsworks also offers the ability to install additional packages from various sources, such as using the pip or conda package managers and public or private git repository.</p> <p>The Python virtual environment is shared by different members of the project. When a member of the project introduces a change to the environment i.e., installs/uninstalls a library, a new environment is created and it becomes a defacto environment for everyone in the project. It is therefore important to track how the environment has been changing over time i.e., what libraries were installed, uninstalled, upgraded, or downgraded when the environment was created and who introduced the changes.</p> <p>In this guide, you will learn how you can track the changes of your Python environment.</p>"},{"location":"user_guides/projects/python/environment_history/#viewing-python-environment-history-in-the-ui","title":"Viewing python environment history in the UI","text":"<p>The Python environment evolves over time as libraries are installed, uninstalled, upgraded, and downgraded. To assist in tracking the changes in the environment, you can see the environment history in the UI. You can view what changes were introduced at each point a new environment was created. Hopsworks will keep a version of a YAML file for each environment so that if you want to restore an older environment you can use it. To see the differences between environments click on the button as shown in figure 1. You will then see the difference between the environment and the previous environment it was created from.</p> <p> Figure 1: You can see the difference between the two environments by clicking on the button pointed.  </p> <p>If you had built the environment using custom commands you can go back to see what commands were run during the build as shown in figure 2.</p> <p> Figure 2:  You can see custom commands that were used to create the environment by clicking on the button pointed.  </p>"},{"location":"user_guides/projects/python/python_env_clone/","title":"How To Clone Python Environment","text":""},{"location":"user_guides/projects/python/python_env_clone/#introduction","title":"Introduction","text":"<p>Cloning an environment in Hopsworks means creating a copy of one of the base environments. The base environments are immutable, meaning that it is required to clone an environment before you can make any change to it, such as installing your own libraries. This ensures that the project maintains a set of stable environments that are tested with the capabilities of the platform, meanwhile through cloning, allowing users to further customize an environment without affecting the base environments.</p> <p>In this guide, you will learn how to clone an environment.</p>"},{"location":"user_guides/projects/python/python_env_clone/#step-1-select-an-environment","title":"Step 1: Select an environment","text":"<p>Under the <code>Project settings</code> section you can find the <code>Python environment</code> setting.</p> <p>First select an environment, for example the <code>python-feature-pipeline</code>.</p> <p> Select a base environment </p>"},{"location":"user_guides/projects/python/python_env_clone/#step-2-clone-environment","title":"Step 2: Clone environment","text":"<p>The environment can now be cloned by clicking <code>Clone env</code> and entering a name and description. The interface will show <code>Syncing packages</code> while creating the environment.</p> <p> Clone a base environment </p>"},{"location":"user_guides/projects/python/python_env_clone/#step-3-environment-is-now-ready","title":"Step 3: Environment is now ready","text":"<p> Environment is now cloned </p> <p>What does the CUSTOM mean?</p> <p>Notice that the cloned environment is tagged as <code>CUSTOM</code>, it means that it is a base environment which has been cloned.</p> <p>Base environment also marked</p> <p>When you select a <code>CUSTOM</code> environment the base environment it was cloned from is also shown.</p>"},{"location":"user_guides/projects/python/python_env_clone/#concerning-upgrades","title":"Concerning upgrades","text":"<p>Please note</p> <p>The base environments are automatically upgraded when Hopsworks is upgraded and application code should keep functioning provided that no breaking changes were made in the upgraded version of the environment. A <code>CUSTOM</code> environment is not automatically upgraded and the users is recommended to reapply the modifications to a base environment if they encounter issues after an upgrade.</p>"},{"location":"user_guides/projects/python/python_env_clone/#next-steps","title":"Next steps","text":"<p>In this guide you learned how to clone a new environment. The next step is to install a library in the environment.</p>"},{"location":"user_guides/projects/python/python_env_export/","title":"How To Export Python Environment","text":""},{"location":"user_guides/projects/python/python_env_export/#introduction","title":"Introduction","text":"<p>Each of the python environments in a project can be exported to an <code>environment.yml</code> file. It can be useful to export it to keep a snapshot of all the installed libraries and their versions.</p> <p>In this guide, you will learn how to export a python environment.</p>"},{"location":"user_guides/projects/python/python_env_export/#step-1-go-to-environment","title":"Step 1: Go to environment","text":"<p>Under the <code>Project settings</code> section you can find the <code>Python environment</code> setting.</p>"},{"location":"user_guides/projects/python/python_env_export/#step-2-select-a-custom-environment","title":"Step 2: Select a CUSTOM environment","text":"<p>Select the environment that you have previously cloned and want to export. Only a <code>CUSTOM</code> environment can be exported.</p>"},{"location":"user_guides/projects/python/python_env_export/#step-3-click-export-env","title":"Step 3: Click Export env","text":"<p>An existing Anaconda environment can be exported as a yml file, clicking the <code>Export env</code> will download the <code>environment.yml</code> file in your browser.</p> <p> Export environment </p>"},{"location":"user_guides/projects/python/python_env_overview/","title":"Python Environments","text":""},{"location":"user_guides/projects/python/python_env_overview/#introduction","title":"Introduction","text":"<p>Hopsworks postulates that building ML systems following the FTI pipeline architecture is best practice. This architecture consists of three independently developed and operated ML pipelines:</p> <ul> <li>Feature Pipeline: takes as input raw data that it transforms into features (and labels)</li> <li>Training Pipeline: takes as input features (and labels) and outputs a trained model</li> <li>Inference Pipeline: takes new feature data and a trained model and makes predictions.</li> </ul> <p>In order to facilitate the development of these pipelines Hopsworks bundles several python environments containing necessary dependencies. Each environment can also be customized further by installing additional dependencies from PyPi, Conda, Wheel files, GitHub repos or applying custom Dockerfiles on top.</p>"},{"location":"user_guides/projects/python/python_env_overview/#step-1-go-to-environments-page","title":"Step 1: Go to environments page","text":"<p>Under the <code>Project settings</code> section you can find the <code>Python environment</code> setting.</p>"},{"location":"user_guides/projects/python/python_env_overview/#step-2-list-available-environments","title":"Step 2: List available environments","text":"<p>Environments listed under <code>FEATURE ENGINEERING</code> corresponds to environments you would use in a feature pipeline, <code>MODEL TRAINING</code> maps to environments used in a training pipeline and <code>MODEL INFERENCE</code> are what you would use in inference pipelines.</p> <p> Bundled python environments </p> <p>Python version</p> <p>The python version used in all the environments is 3.11.</p>"},{"location":"user_guides/projects/python/python_env_overview/#feature-engineering","title":"Feature engineering","text":"<p>The <code>FEATURE ENGINEERING</code> environments can be used in Jupyter notebooks, a Python job or a PySpark job.</p> <ul> <li><code>python-feature-pipeline</code> for writing feature pipelines using Python</li> <li><code>spark-feature-pipeline</code> for writing feature pipelines using PySpark</li> </ul>"},{"location":"user_guides/projects/python/python_env_overview/#model-training","title":"Model training","text":"<p>The <code>MODEL TRAINING</code> environments can be used in Jupyter notebooks or a Python job or in a Ray job.</p> <ul> <li><code>tensorflow-training-pipeline</code> to train TensorFlow models</li> <li><code>torch-training-pipeline</code> to train PyTorch models</li> <li><code>pandas-training-pipeline</code> to train XGBoost, Catboost and Sklearn models</li> <li><code>ray_training_pipeline</code> a general purpose environment for distributed training using Ray framework to train XGBoost and Sklearn models.   Should be used in Ray job.   It can be customized to install additional dependencies of your choice.</li> <li><code>ray_torch_training_pipeline</code> for distributed training of PyTorch models using Ray framework in a Ray job</li> <li><code>ray_tensorflow_training_pipeline</code> for distributed training of TensorFlow models using Ray framework in a Ray job</li> </ul>"},{"location":"user_guides/projects/python/python_env_overview/#model-inference","title":"Model inference","text":"<p>The <code>MODEL INFERENCE</code> environments can be used in a deployment using a custom predictor script.</p> <ul> <li><code>tensorflow-inference-pipeline</code> to load and serve TensorFlow models</li> <li><code>torch-inference-pipeline</code> to load and serve PyTorch models</li> <li><code>pandas-inference-pipeline</code> to load and serve XGBoost, Catboost and Sklearn models</li> <li><code>vllm-inference-pipeline</code> to load and serve LLMs with vLLM inference engine</li> <li><code>minimal-inference-pipeline</code> to install your own custom framework, contains a minimal set of dependencies</li> </ul>"},{"location":"user_guides/projects/python/python_env_overview/#next-steps","title":"Next steps","text":"<p>In this guide you learned how to find the bundled python environments and where they can be used. Now you can test out the environment in a Jupyter notebook.</p>"},{"location":"user_guides/projects/python/python_install/","title":"How To Install Python Libraries","text":""},{"location":"user_guides/projects/python/python_install/#introduction","title":"Introduction","text":"<p>Hopsworks comes with several prepackaged Python environments that contain libraries for data engineering, machine learning, and more general data science use-cases. Hopsworks also offers the ability to install additional packages from various sources, such as using the pip or conda package managers and public or private git repository.</p> <p>In this guide, you will learn how to install Python packages using these different options.</p> <ul> <li>PyPi, using pip package manager</li> <li>A conda channel, using conda package manager</li> <li>Packages contained in .whl format</li> <li>A public or private git repository</li> <li>A requirements.txt file to install multiple libraries at the same time using pip</li> </ul> <p>Notice</p> <p>If your libraries require installing some extra OS-Level packages, refer to the guide custom commands guide on how to install OS-Level packages.</p>"},{"location":"user_guides/projects/python/python_install/#prerequisites","title":"Prerequisites","text":"<p>In order to install a custom dependency one of the base environments must first be cloned, follow this guide for that.</p>"},{"location":"user_guides/projects/python/python_install/#step-1-go-to-environments-page","title":"Step 1: Go to environments page","text":"<p>Under the <code>Project settings</code> section select the <code>Python environment</code> setting.</p>"},{"location":"user_guides/projects/python/python_install/#step-2-select-a-custom-environment","title":"Step 2: Select a CUSTOM environment","text":"<p>Select the environment that you have previously cloned and want to modify.</p>"},{"location":"user_guides/projects/python/python_install/#step-3-installation-options","title":"Step 3: Installation options","text":""},{"location":"user_guides/projects/python/python_install/#name-and-version","title":"Name and version","text":"<p>Enter the name and, optionally, the desired version to install.</p> <p> Installing library by name and version </p>"},{"location":"user_guides/projects/python/python_install/#search","title":"Search","text":"<p>Enter the search term and select a library and version to install.</p> <p> Installing library using search </p>"},{"location":"user_guides/projects/python/python_install/#distribution-whl-egg","title":"Distribution (.whl, .egg..)","text":"<p>Install a python package by uploading the corresponding package file and selecting it in the file browser.</p> <p> Installing library from file </p>"},{"location":"user_guides/projects/python/python_install/#git-source","title":"Git source","text":"<p>The URL you should provide is the same as you would enter on the command line using <code>pip install git+{repo_url}</code>, where <code>repo_url</code> is the part that you enter in <code>Git URL</code>.</p> <p>For example to install matplotlib 3.7.2, the following are correct inputs:</p> <p><code>matplotlib @ git+https://github.com/matplotlib/matplotlib@v3.7.2</code></p> <p><code>git+https://github.com/matplotlib/matplotlib@v3.7.2</code></p> <p>In the case of a private git repository, also select whether it is a GitHub or GitLab repository and the preconfigured access token for the repository.</p> <p>Keep your secrets safe</p> <p>If you are installing from a git repository which is not GitHub or GitLab simply supply the access token in the URL. Keep in mind that in this case the access token may be visible in logs for other users in the same project to see.</p> <p> Installing library from git repo </p>"},{"location":"user_guides/projects/python/python_install/#going-further","title":"Going Further","text":"<p>Now you can use the library in a Jupyter notebook or a Job.</p>"},{"location":"user_guides/projects/scheduling/kube_scheduler/","title":"Scheduler","text":""},{"location":"user_guides/projects/scheduling/kube_scheduler/#introduction","title":"Introduction","text":"<p>Hopsworks allows users to configure some Kubernetes scheduler abstractions, such as Affinity and Priority Classes. Hopsworks also supports additional scheduling abstractions backed by Kueue. This includes Queues, Cohorts and Topologies. All these scheduling abstractions are supported in jobs, jupyter notebooks and model deployments. Kueue abstractions however, are currently not supported for Spark jobs.</p> <p>Hopsworks Admins can control which labels and priority classes can be used in the cluster (see Admin configuration section) and by which project (see Project Configuration section)</p> <p>Within a project, data owners can set defaults for jobs and Jupyter notebooks running within that project (see: Project defaults section).</p>"},{"location":"user_guides/projects/scheduling/kube_scheduler/#node-labels-node-affinity-and-node-anti-affinity","title":"Node Labels, Node Affinity and Node Anti-Affinity","text":"<p>Labels in Kubernetes are key-value pairs used to organize and select resources. Hopsworks relies on labels applied to nodes for pod-node affinity to determine where the pod can (or cannot) run. Some uses cases where labels and affinity can be used include:</p> <ul> <li>Hardware constraints (GPU, SSD)</li> <li>Environment separation (prod/dev)</li> <li>Co-locating related pods</li> <li>Spreading pods for high availability</li> </ul> <p>Hopsworks uses the node affinity <code>IN</code> operator for the Hopsworks Node Affinity and the <code>NOT IN</code> operator for the Hopsworks Node Anti Affinity.</p> <p>For more information on Kubernetes Affinity, you can check the Kubernetes Affinity documentation page.</p>"},{"location":"user_guides/projects/scheduling/kube_scheduler/#priority-classes","title":"Priority Classes","text":"<p>Priority classes in Kubernetes determine the scheduling and eviction priority of pods.</p> <p>Pods with higher priority:</p> <ul> <li>Get scheduled first</li> <li>Can preempt (evict) lower priority pods</li> <li>Less likely to be evicted under resource pressure</li> </ul> <p>Common uses:</p> <ul> <li>Protecting critical workloads</li> <li>Ensuring core services stay running</li> <li>Managing resource competition</li> <li>Guaranteeing QoS for important applications</li> </ul> <p>For more information on Priority Classes, you can check the Kubernetes Priority Classes documentation page.</p>"},{"location":"user_guides/projects/scheduling/kube_scheduler/#kueue","title":"Kueue","text":"<p>Hopsworks adds the integration with Kueue to offer more advanced scheduling abstractions such as queues, cohorts and topologies.</p> <p>For a more detailed view on how Hopsworks uses the Kueue abstractions you can check the Kueue details section.</p>"},{"location":"user_guides/projects/scheduling/kube_scheduler/#queues-cohorts","title":"Queues, Cohorts","text":"<p>Jobs, notebooks and model deployments are submitted to these queues. Hopsworks administrator can define quotas on how many resources a queue can use. Queues can be grouped together in cohorts in order to add the ability to borrow resources from each other when the other queue does not use its resources.</p> <p>When creating a new job, the user can select a queue for the job in the <code>Advance configuration -&gt; Scheduler section</code>.</p> <p></p>"},{"location":"user_guides/projects/scheduling/kube_scheduler/#topologies","title":"Topologies","text":"<p>The integration of Hopsworks with Kueue, also provides access to the topology abstraction. Topologies can be defined, so that the user can decide for the pods of jobs or model deployments to run somehow grouped together. The user could decide for example, that all pods of a job should run on the same host, because the pods need to transfer a lot of data between each other, and we want to avoid network traffic to lower the latency.</p> <p>The user can select the topology unit for jobs, notebooks and model deployments in the <code>Advance configuration -&gt; Scheduler section</code>.</p> <p></p>"},{"location":"user_guides/projects/scheduling/kube_scheduler/#admin-configuration","title":"Admin configuration","text":""},{"location":"user_guides/projects/scheduling/kube_scheduler/#affinity-and-priority-classes","title":"Affinity and priority classes","text":"<p>Hopsworks admins can control the affinity labels and priority classes available on the Hopsworks cluster from the <code>Cluster Settings -&gt; Scheduler</code> page:</p> <p></p> <p>Hopsworks Cluster can run within a shared Kubernetes Cluster. The first configuration level is to limit the subset of labels and priority classes that can be used within the Hopsworks Cluster. This can be done from the <code>Available in Hopsworks</code> sub-section.</p> <p>Permissions</p> <p>In order to be able to list all the Kubernetes Node Labels, Hopsworks requires the following cluster role:</p> <pre><code>    - apiGroups: [\"\"]\n    resources: [\"nodes\"]\n    verbs: [\"get\", \"list\"]\n</code></pre> <p>In order to be able to list all the Kubernetes Cluster Priority Classes, Hopsworsk requires this cluster role:</p> <pre><code>    - apiGroups: [\"scheduling.k8s.io\"]\n    resources: [\"priorityclasses\"]\n    verbs: [\"get\", \"list\"]\n</code></pre> <p>If the roles above are configured properly (default behaviour), admins can only select values from the drop down menu. If the roles are missing, admins would be required to enter them as free text and should be careful about typos. Any typos here will be propagated in the other configuration and use levels leading to errors or missbehaviour when running computation.</p>"},{"location":"user_guides/projects/scheduling/kube_scheduler/#queues","title":"Queues","text":"<p>Every new project gets automatic access to the default Hopsworks queue. An administrator can define the default queue for projects user jobs and system jobs.</p> <p></p>"},{"location":"user_guides/projects/scheduling/kube_scheduler/#project-configuration","title":"Project Configuration","text":"<p>Hopsworks admins can configure the labels and priority classes that can be used by default within a project. This will be a subset of the ones configured for Hopsworks. In the figure above, in the sub-section <code>Available in Project</code> Hopsworks admins can configure the labels and priority classes available by default in any Hopsworks Project.</p> <p>Hopsworks admins can also override the default project configuration on a per-project basis. That is, Hopsworks admins can make certain labels and priority classes available only to certain projects. This can be achieved from the <code>Cluster Settings -&gt; Project -&gt; &lt;ProjectName&gt; -&gt; edit configuration</code> configuration page:</p> <p></p>"},{"location":"user_guides/projects/scheduling/kube_scheduler/#project-defaults","title":"Project defaults","text":"<p>Within a project, different jobs, Jupyter notebooks and model deployments can run with different labels and/or priority classes. <code>Data Owners</code> in a project can specify the default values from the project settings: The default Label will be used for the default Node Affinity for jobs, notebooks, and model deployments.</p> <p></p>"},{"location":"user_guides/projects/scheduling/kube_scheduler/#configuration-of-jobs-notebooks-and-deployments","title":"Configuration of Jobs, Notebooks, and Deployments","text":"<p>In the advanced configuration sections for job, notebook, and model deployments, users can set affinity, anti affinity and priority class. The Affinity and Anti Affinity can be selected from the list of allowed labels.</p> <p><code>Affinity</code> configures on which nodes this pod can run. If a node has any of the labels present in the Affinity option, the pod can be scheduler to run to run there.</p> <p><code>Anti Affinity</code> configures on which nodes this pod will not run on. If a node has any of the labels present in the Anti Affinity option, the pod will not be scheduler to run there.</p> <p><code>Priority Class</code> specifies with which priority a pod will run.</p> <p></p>"},{"location":"user_guides/projects/scheduling/kueue_details/","title":"Kueue","text":""},{"location":"user_guides/projects/scheduling/kueue_details/#introduction","title":"Introduction","text":"<p>Hopsworks provides the integration with Kueue to provide the additional scheduling abstractions. Hopsworks currently acts only as a \"reader\" to the Kueue abstractions and currently does not manage the lifecycle of Kueue abstraction with the exception of the default localqueue for each namespace. All the other abstractions are expected to be managed by the administrators of Hopsworks, directly on the Kubernetes cluster.</p> <p>However Hopsworks and Kueue integration currently only supports frameworks python and ray for jobs, notebooks and model deployments. The same queues are also used for Hopsworks internal jobs (zipping, git operations, python library installation). Spark is currently not supported, and thus will not be managed by Kueue for scheduling, and instead it will bypass the queues setup (important to note when thinking about queue quotas) and instead are managed directly by the Kubernetes Scheduler.</p>"},{"location":"user_guides/projects/scheduling/kueue_details/#resource-flavors","title":"Resource flavors","text":"<p>When trying to define queues in Kueue, the first abstraction that needs to be defined is a Resource Flavor. The resource flavor defines the resources that a queue will later manage. Hopsworks helm chart installs and uses a default ResourceFlavor</p> <pre><code>apiVersion: kueue.x-k8s.io/v1beta1\nkind: ResourceFlavor\nmetadata:\n  name: default-flavor\nspec:\n  nodeLabels:\n    cloud.provider.com/region: europe\n  topologyName: default\n</code></pre> <p>Node labels filter the available nodes to this resource flavor and is required for topologies</p>"},{"location":"user_guides/projects/scheduling/kueue_details/#cluster-queues","title":"Cluster Queues","text":"<p>Cluster Queues are the actual queues for submitting jobs and model deployments to. The default hopsworks queue looks like:</p> <pre><code>apiVersion: kueue.x-k8s.io/v1beta1\nkind: ClusterQueue\nmetadata:\n  name: other\nspec:\n  cohort: cluster\n  namespaceSelector: {}\n  preemption:\n    borrowWithinCohort:\n      policy: Never\n    reclaimWithinCohort: Never\n    withinClusterQueue: Never\n  queueingStrategy: BestEffortFIFO\n  resourceGroups:\n  - coveredResources:\n    - cpu\n    - memory\n    - pods\n    - nvidia.com/gpu\n    flavors:\n    - name: default-flavor\n      resources:\n      - name: cpu\n        nominalQuota: \"0\"\n      - name: memory\n        nominalQuota: \"0\"\n      - name: pods\n        nominalQuota: \"0\"\n      - name: nvidia.com/gpu\n        nominalQuota: \"0\"\n</code></pre> <p>The preemption and nominal quotas are set to the minimal as this queue is designed to have lowest priority in getting resources allocated. If a cluster is underutilized and there are resources available, it can still borrow up to the maximum resources present in the parent cohort, but by design this queue has no dedicated resources. The presumption is that other, more important queues, defined by the cluster administrator will have higher preference in getting resources.</p>"},{"location":"user_guides/projects/scheduling/kueue_details/#local-queues","title":"Local Queues","text":"<p>Local Queues are the mechanism to provide access to a queue (cluster queue) to a specific project in Hopsworks (Kubernetes namespace).</p> <p>Every new project gets automatic access to the default Hopsworks queue. An administrator can define the default queue for projects user jobs and system jobs.</p> <p></p>"},{"location":"user_guides/projects/scheduling/kueue_details/#cohorts","title":"Cohorts","text":"<p>Cohorts are groupings of cluster queues that have some meaning together and can share resources. Hopsworks defines a default <code>cluster</code> cohort</p> <pre><code>apiVersion: kueue.x-k8s.io/v1alpha1\nkind: Cohort\nmetadata:\n  name: cluster\nspec:\n  resourceGroups:\n  - coveredResources:\n    - cpu\n    - memory\n    - pods\n    - nvidia.com/gpu\n    flavors:\n    - name: default-flavor\n      resources:\n      - name: cpu\n        nominalQuota: 100\n      - name: memory\n        nominalQuota: 200Gi\n      - name: pods\n        nominalQuota: 100\n      - name: nvidia.com/gpu\n        nominalQuota: 50\n</code></pre> <p>Cohorts can contain other cohorts and thus you can create a hierarchy of cohorts. Cohorts can set fair sharing weight where using</p> <pre><code>  fairSharing:\n    weight\n</code></pre> <p>in the definition of a cohort, the user can control a priority towards borrowing resources from other cohorts.</p>"},{"location":"user_guides/projects/scheduling/kueue_details/#topologies","title":"Topologies","text":"<p>Topologies defines a way of grouping together pods belonging to the same job/deployment so that they are colocated within the same topology unit. Hopsworks defines a default topology:</p> <pre><code>apiVersion: kueue.x-k8s.io/v1alpha1\nkind: Topology\nmetadata:\n  name: default\nspec:\n  levels:\n  - nodeLabel: cloud.provider.com/region\n  - nodeLabel: cloud.provider.com/zone\n  - nodeLabel: kubernetes.io/hostname\n</code></pre> <p>The topology is defined in the Resource Flavor used by a Cluster Queue.</p> <p>When creating a new job, the user can select a topology unit for the job to run in and thus decide if all pods of a job should run on the same hostname, in the same zone or in the same region. The user can select the topology for jobs, notebooks and deployments in the <code>Advance configuration -&gt; Scheduler section</code>.</p> <p></p>"},{"location":"user_guides/projects/secrets/create_secret/","title":"How To Create A Secret","text":""},{"location":"user_guides/projects/secrets/create_secret/#introduction","title":"Introduction","text":"<p>A Secret is a key-value pair used to store encrypted information accessible only to the owner of the secret. Also if you wish to, you can share the same secret API key with all the members of a Project.</p>"},{"location":"user_guides/projects/secrets/create_secret/#ui","title":"UI","text":""},{"location":"user_guides/projects/secrets/create_secret/#step-1-navigate-to-secrets","title":"Step 1: Navigate to Secrets","text":"<p>In the <code>Account Settings</code> page you can find the <code>Secrets</code> section showing a list of all secrets.</p> <p> List of secrets </p>"},{"location":"user_guides/projects/secrets/create_secret/#step-2-create-a-secret","title":"Step 2: Create a Secret","text":"<p>Click <code>New Secret</code> to bring up the dialog for secret creation. Enter a name for the secret to be used for lookup, and the secret value.</p> <p>If the secret should be private to this user, select <code>Private</code>, to share the secret with all members of a project select <code>Project</code> and enter the project name.</p> <p> Create new secret dialog </p>"},{"location":"user_guides/projects/secrets/create_secret/#step-3-secret-created","title":"Step 3: Secret created","text":"<p>Click <code>New Secret</code> to bring up the dialog for secret creation. Enter a name for the secret to be used for lookup, and the secret value.</p> <p>If the secret should be private to this user, select <code>Private</code>, to share the secret with all members of a project select <code>Project</code> and enter the project name.</p> <p> Secret is now created </p>"},{"location":"user_guides/projects/secrets/create_secret/#code","title":"Code","text":""},{"location":"user_guides/projects/secrets/create_secret/#step-1-get-secrets-api","title":"Step 1: Get secrets API","text":"<pre><code>import hopsworks\n\nhopsworks.login()\n\nsecrets_api = hopsworks.get_secrets_api()\n</code></pre>"},{"location":"user_guides/projects/secrets/create_secret/#step-2-create-secret","title":"Step 2: Create secret","text":"<pre><code>secret = secrets_api.create_secret(\"my_secret\", \"Fk3MoPlQXCQvPo\")\n</code></pre>"},{"location":"user_guides/projects/secrets/create_secret/#api-reference","title":"API Reference","text":"<p><code>SecretsApi</code></p>"},{"location":"python-api/hopsworks/","title":"hopsworks","text":""},{"location":"python-api/hopsworks/#hopsworks","title":"hopsworks","text":""},{"location":"python-api/hopsworks/#hopsworks.login","title":"[source]  login","text":"<pre><code>login(\n    host: str | None = None,\n    port: int = 443,\n    project: str | None = None,\n    api_key_value: str | None = None,\n    api_key_file: str | None = None,\n    hostname_verification: bool = False,\n    trust_store_path: str | None = None,\n    cert_folder: str | None = None,\n    engine: Literal[\n        \"spark\",\n        \"python\",\n        \"training\",\n        \"spark-no-metastore\",\n        \"spark-delta\",\n    ]\n    | None = None,\n) -&gt; project.Project\n</code></pre> <p>Connect to Serverless Hopsworks by calling the <code>hopsworks.login()</code> function with no arguments.</p> Connect to Serverless <pre><code>import hopsworks\n\nproject = hopsworks.login()\n</code></pre> <p>Alternatively, connect to your own Hopsworks installation by specifying the host, port and API key.</p> Connect to your Hopsworks cluster <pre><code>import hopsworks\n\nproject = hopsworks.login(\n    host=\"my.hopsworks.server\",\n    port=8181,\n    api_key_value=\"DKN8DndwaAjdf98FFNSxwdVKx\",\n)\n</code></pre> <p>In addition to setting function arguments directly, <code>hopsworks.login()</code> also reads the environment variables: <code>HOPSWORKS_HOST</code>, <code>HOPSWORKS_PORT</code>, <code>HOPSWORKS_PROJECT</code>, <code>HOPSWORKS_API_KEY</code>, <code>HOPSWORKS_HOSTNAME_VERIFICATION</code>, <code>HOPSWORKS_TRUST_STORE_PATH</code>, <code>HOPSWORKS_CERT_FOLDER</code> and <code>HOPSWORKS_ENGINE</code>.</p> <p>The function arguments do however take precedence over the environment variables in case both are set.</p> PARAMETER DESCRIPTION <code>host</code> <p>The hostname of the Hopsworks instance.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>port</code> <p>The port on which the Hopsworks instance can be reached.</p> <p> TYPE: <code>int</code> DEFAULT: <code>443</code> </p> <code>project</code> <p>Name of the project to access. If used inside a Hopsworks environment it always gets the current project. If not provided you will be prompted to enter it.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>api_key_value</code> <p>Value of the API Key</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>api_key_file</code> <p>Path to file wih API Key</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>hostname_verification</code> <p>Whether to verify Hopsworks' certificate</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>trust_store_path</code> <p>Path on the file system containing the Hopsworks certificates</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>cert_folder</code> <p>The directory to store downloaded certificates. Defaults to the system temp directory.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>engine</code> <p>Specifies the engine to use. The default value is <code>None</code>, which automatically selects the engine based on the environment:</p> <ul> <li><code>spark</code>: Used if Spark is available, such as in Hopsworks or Databricks environments.</li> <li><code>python</code>: Used in local Python environments or AWS SageMaker when Spark is not available.</li> <li><code>training</code>: Used when only feature store metadata is needed, such as for obtaining training dataset locations and label information during Hopsworks training experiments.</li> <li><code>spark-no-metastore</code>: Functions like <code>spark</code> but does not rely on the Hive metastore.</li> <li><code>spark-delta</code>: Minimizes dependencies further by avoiding both Hive metastore and HopsFS.</li> </ul> <p> TYPE: <code>Literal['spark', 'python', 'training', 'spark-no-metastore', 'spark-delta'] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>project.Project</code> <p>The Project object to perform operations on.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p> <code>hopsworks.client.exceptions.HopsworksSSLClientError</code> <p>If SSLError is raised from underlying requests library.</p>"},{"location":"python-api/hopsworks/#hopsworks.create_project","title":"[source]  create_project","text":"<pre><code>create_project(\n    name: str,\n    description: str | None = None,\n    feature_store_topic: str | None = None,\n) -&gt; project.Project | None\n</code></pre> <p>Create a new project.</p> Not supported <p>The function does not work if you are connected to Serverless Hopsworks.</p> Example for creating a new project <pre><code>import hopsworks\n\nhopsworks.login(...)\n\nhopsworks.create_project(\"my_project\", description=\"An example Hopsworks project\")\n</code></pre> PARAMETER DESCRIPTION <code>name</code> <p>The name of the project.</p> <p> TYPE: <code>str</code> </p> <code>description</code> <p>Description of the project.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>feature_store_topic</code> <p>Feature store topic name.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>project.Project | None</code> <p>The Project object to perform operations on.</p>"},{"location":"python-api/hopsworks/#hopsworks.get_current_project","title":"[source]  get_current_project","text":"<pre><code>get_current_project() -&gt; project.Project\n</code></pre> <p>Get a reference to the current logged in project.</p> Example for getting the project reference <pre><code>import hopsworks\n\nhopsworks.login()\n\nproject = hopsworks.get_current_project()\n</code></pre> RETURNS DESCRIPTION <code>project.Project</code> <p>The Project object to perform operations on.</p>"},{"location":"python-api/hopsworks/#hopsworks.get_secrets_api","title":"[source]  get_secrets_api","text":"<pre><code>get_secrets_api() -&gt; secret_api.SecretsApi\n</code></pre> <p>Get the secrets api.</p> RETURNS DESCRIPTION <code>secret_api.SecretsApi</code> <p>The Secrets API handle.</p>"},{"location":"python-api/hopsworks/core/alerts_api/","title":"hopsworks.core.alerts_api","text":""},{"location":"python-api/hopsworks/core/alerts_api/#hopsworks.core.alerts_api","title":"hopsworks.core.alerts_api","text":""},{"location":"python-api/hopsworks/core/alerts_api/#hopsworks_common.core.alerts_api.AlertsApi","title":"[source]  AlertsApi","text":"<p>Alerts API handle.</p> <p>To obtain an object of this type, use <code>project.get_alerts_api</code>.</p> Returned by <ul> <li> <code><code></code>Project.<code></code>get_alerts_api</code> </li> </ul>"},{"location":"python-api/hopsworks/core/alerts_api/#hopsworks_common.core.alerts_api.AlertsApi.get_alerts","title":"[source]  get_alerts","text":"<pre><code>get_alerts() -&gt; list[alert.ProjectAlert]\n</code></pre> <p>Get all project alerts.</p> Example <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nalerts_api = project.get_alerts_api()\n\nalert = alerts_api.get_alerts(alert_id=1)\n</code></pre> RETURNS DESCRIPTION <code>list[alert.ProjectAlert]</code> <p>List of ProjectAlert objects.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request</p>"},{"location":"python-api/hopsworks/core/alerts_api/#hopsworks_common.core.alerts_api.AlertsApi.get_alert","title":"[source]  get_alert","text":"<pre><code>get_alert(alert_id: int) -&gt; alert.ProjectAlert | None\n</code></pre> <p>Get a specific project alert by ID.</p> Example <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nalerts_api = project.get_alerts_api()\n\nalert = alerts_api.get_alert(alert_id=1)\n</code></pre> PARAMETER DESCRIPTION <code>alert_id</code> <p>The ID of the alert to retrieve.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>alert.ProjectAlert | None</code> <p>The ProjectAlert object.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request</p>"},{"location":"python-api/hopsworks/core/alerts_api/#hopsworks_common.core.alerts_api.AlertsApi.get_job_alerts","title":"[source]  get_job_alerts","text":"<pre><code>get_job_alerts(job_name: str) -&gt; list[alert.JobAlert]\n</code></pre> <p>Get all job alerts.</p> Example <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nalerts_api = project.get_alerts_api()\n\njob_alerts = alerts_api.get_job_alerts(job_name=\"my_job\")\n</code></pre> PARAMETER DESCRIPTION <code>job_name</code> <p>The name of the job.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>list[alert.JobAlert]</code> <p>List of JobAlert objects.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request</p>"},{"location":"python-api/hopsworks/core/alerts_api/#hopsworks_common.core.alerts_api.AlertsApi.get_job_alert","title":"[source]  get_job_alert","text":"<pre><code>get_job_alert(\n    job_name: str, alert_id: int\n) -&gt; alert.JobAlert | None\n</code></pre> <p>Get a specific job alert by ID.</p> Example <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nalerts_api = project.get_alerts_api()\n\njob_alerts = alerts_api.get_job_alert(job_name=\"my_job\", alert_id=1)\n</code></pre> PARAMETER DESCRIPTION <code>job_name</code> <p>The name of the job.</p> <p> TYPE: <code>str</code> </p> <code>alert_id</code> <p>The ID of the alert to retrieve.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>alert.JobAlert | None</code> <p>The JobAlert object.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request</p>"},{"location":"python-api/hopsworks/core/alerts_api/#hopsworks_common.core.alerts_api.AlertsApi.get_feature_group_alerts","title":"[source]  get_feature_group_alerts","text":"<pre><code>get_feature_group_alerts(\n    feature_store_id: int, feature_group_id: int\n) -&gt; list[alert.FeatureGroupAlert]\n</code></pre> <p>Get all feature group alerts.</p> Example <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nalerts_api = project.get_alerts_api()\n\nfeature_group_alerts = alerts_api.get_feature_group_alerts(feature_store_id=1, feature_group_id=1)\n</code></pre> PARAMETER DESCRIPTION <code>feature_store_id</code> <p>The ID of the feature store.</p> <p> TYPE: <code>int</code> </p> <code>feature_group_id</code> <p>The ID of the feature group.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>list[alert.FeatureGroupAlert]</code> <p>List of FeatureGroupAlert objects.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request</p>"},{"location":"python-api/hopsworks/core/alerts_api/#hopsworks_common.core.alerts_api.AlertsApi.get_feature_group_alert","title":"[source]  get_feature_group_alert","text":"<pre><code>get_feature_group_alert(\n    feature_store_id: int,\n    feature_group_id: int,\n    alert_id: int,\n) -&gt; alert.FeatureGroupAlert | None\n</code></pre> <p>Get a specific feature group alert by ID.</p> Example <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nalerts_api = project.get_alerts_api()\n\nfeature_group_alerts = alerts_api.get_feature_group_alert(feature_store_id=1, feature_group_id=1, alert_id=1)\n</code></pre> PARAMETER DESCRIPTION <code>feature_store_id</code> <p>The ID of the feature store.</p> <p> TYPE: <code>int</code> </p> <code>feature_group_id</code> <p>The ID of the feature group.</p> <p> TYPE: <code>int</code> </p> <code>alert_id</code> <p>The ID of the alert to retrieve.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>alert.FeatureGroupAlert | None</code> <p>The FeatureGroupAlert object.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request</p>"},{"location":"python-api/hopsworks/core/alerts_api/#hopsworks_common.core.alerts_api.AlertsApi.get_feature_view_alerts","title":"[source]  get_feature_view_alerts","text":"<pre><code>get_feature_view_alerts(\n    feature_store_id: int,\n    feature_view_name: str,\n    feature_view_version: int,\n) -&gt; list[alert.FeatureViewAlert]\n</code></pre> <p>Get all feature view alerts.</p> Example <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nalerts_api = project.get_alerts_api()\n\nfeature_view_alerts = alerts_api.get_feature_view_alerts(feature_store_id=1, feature_view_name=\"my_feature_view\", feature_view_version=1, alert_id=1)\n</code></pre> PARAMETER DESCRIPTION <code>feature_store_id</code> <p>The ID of the feature store.</p> <p> TYPE: <code>int</code> </p> <code>feature_view_name</code> <p>The name of the feature view.</p> <p> TYPE: <code>str</code> </p> <code>feature_view_version</code> <p>The version of the feature view.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>list[alert.FeatureViewAlert]</code> <p>List of FeatureViewAlert objects.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request</p>"},{"location":"python-api/hopsworks/core/alerts_api/#hopsworks_common.core.alerts_api.AlertsApi.get_feature_view_alert","title":"[source]  get_feature_view_alert","text":"<pre><code>get_feature_view_alert(\n    feature_store_id: int,\n    feature_view_name: str,\n    feature_view_version: int,\n    alert_id: int,\n) -&gt; alert.FeatureViewAlert | None\n</code></pre> <p>Get a specific feature view alert by ID.</p> Example <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nalerts_api = project.get_alerts_api()\n\nfeature_view_alerts = alerts_api.get_feature_view_alert(feature_store_id=1, feature_view_name=\"my_feature_view\", feature_view_version=1, alert_id=1)\n</code></pre> PARAMETER DESCRIPTION <code>feature_store_id</code> <p>The ID of the feature store.</p> <p> TYPE: <code>int</code> </p> <code>feature_view_name</code> <p>The name of the feature view.</p> <p> TYPE: <code>str</code> </p> <code>feature_view_version</code> <p>The version of the feature view.</p> <p> TYPE: <code>int</code> </p> <code>alert_id</code> <p>The ID of the alert to retrieve.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>alert.FeatureViewAlert | None</code> <p>The FeatureViewAlert object.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request</p>"},{"location":"python-api/hopsworks/core/alerts_api/#hopsworks_common.core.alerts_api.AlertsApi.create_project_alert","title":"[source]  create_project_alert","text":"<pre><code>create_project_alert(\n    receiver: str,\n    status: _PROJECT_FS_STATUS_ARG\n    | _PROJECT_JOB_STATUS_ARG,\n    severity: _SEVERITY_ARG,\n    service: _SERVICES_ARG,\n    threshold=0,\n) -&gt; alert.ProjectAlert\n</code></pre> <p>Create a new alert.</p> Example <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nalerts_api = project.get_alerts_api()\n\nnew_alert = alerts_api.create_project_alert(receiver=\"email\", status=\"job_finished\", severity=\"warning\", service=\"Jobs\")\n</code></pre> PARAMETER DESCRIPTION <code>status</code> <p>The status that will trigger the alert (job_finished, job_failed, job_killed, job_long_running, feature_validation_success, feature_validation_warning, feature_validation_failure, feature_monitor_shift_undetected, feature_monitor_shift_detected).</p> <p> TYPE: <code>_PROJECT_FS_STATUS_ARG | _PROJECT_JOB_STATUS_ARG</code> </p> <code>severity</code> <p>The severity of the alert (warning, critical, info).</p> <p> TYPE: <code>_SEVERITY_ARG</code> </p> <code>receiver</code> <p>The receiver of the alert (e.g., email, webhook).</p> <p> TYPE: <code>str</code> </p> <code>service</code> <p>The service associated with the alert (Featurestore, Jobs).</p> <p> TYPE: <code>_SERVICES_ARG</code> </p> <code>threshold</code> <p>The threshold for the alert.</p> <p> DEFAULT: <code>0</code> </p> RETURNS DESCRIPTION <code>alert.ProjectAlert</code> <p>The created ProjectAlert object.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the service is not Featurestore or Jobs, or if the status is not valid for the specified service.</p> <code>ValueError</code> <p>If the severity is not valid.</p> <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request</p>"},{"location":"python-api/hopsworks/core/alerts_api/#hopsworks_common.core.alerts_api.AlertsApi.create_feature_group_alert","title":"[source]  create_feature_group_alert","text":"<pre><code>create_feature_group_alert(\n    feature_store_id: int,\n    feature_group_id: int,\n    receiver: str,\n    status: _VALIDATION_STATUS_ARG | _MONITORING_STATUS_ARG,\n    severity: _SEVERITY_ARG,\n) -&gt; alert.FeatureGroupAlert\n</code></pre> <p>Create a new feature group alert.</p> Example <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nalerts_api = project.get_alerts_api()\n\nnew_alert = alerts_api.create_feature_group_alert(67, 1, receiver=\"email\", status=\"feature_validation_warning\", severity=\"warning\")\n</code></pre> PARAMETER DESCRIPTION <code>feature_store_id</code> <p>The ID of the feature store.</p> <p> TYPE: <code>int</code> </p> <code>feature_group_id</code> <p>The ID of the feature group.</p> <p> TYPE: <code>int</code> </p> <code>receiver</code> <p>The receiver of the alert (e.g., email, webhook).</p> <p> TYPE: <code>str</code> </p> <code>status</code> <p>The status that will trigger the alert (feature_validation_success, feature_validation_warning, feature_validation_failure, feature_monitor_shift_undetected, feature_monitor_shift_detected).</p> <p> TYPE: <code>_VALIDATION_STATUS_ARG | _MONITORING_STATUS_ARG</code> </p> <code>severity</code> <p>The severity of the alert (warning, critical, info).</p> <p> TYPE: <code>_SEVERITY_ARG</code> </p> RETURNS DESCRIPTION <code>alert.FeatureGroupAlert</code> <p>The created FeatureGroupAlert object.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the status is not valid.</p> <code>ValueError</code> <p>If the severity is not valid.</p> <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request</p>"},{"location":"python-api/hopsworks/core/alerts_api/#hopsworks_common.core.alerts_api.AlertsApi.create_feature_view_alert","title":"[source]  create_feature_view_alert","text":"<pre><code>create_feature_view_alert(\n    feature_store_id: int,\n    feature_view_name: str,\n    feature_view_version: int,\n    receiver: str,\n    status: _MONITORING_STATUS_ARG,\n    severity: _SEVERITY_ARG,\n) -&gt; alert.FeatureViewAlert\n</code></pre> <p>Create a new feature view alert.</p> Example <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nalerts_api = project.get_alerts_api()\n\nnew_alert = alerts_api.create_feature_view_alert(67, \"fv\", 1, receiver=\"email\", status=\"feature_monitor_shift_undetected\", severity=\"warning\")\n</code></pre> PARAMETER DESCRIPTION <code>feature_store_id</code> <p>The ID of the feature store.</p> <p> TYPE: <code>int</code> </p> <code>feature_view_name</code> <p>The name of the feature view.</p> <p> TYPE: <code>str</code> </p> <code>feature_view_version</code> <p>The version of the feature view.</p> <p> TYPE: <code>int</code> </p> <code>receiver</code> <p>The receiver of the alert (e.g., email, webhook).</p> <p> TYPE: <code>str</code> </p> <code>status</code> <p>The status that will trigger the alert (feature_monitor_shift_undetected, feature_monitor_shift_detected).</p> <p> TYPE: <code>_MONITORING_STATUS_ARG</code> </p> <code>severity</code> <p>The severity of the alert (warning, critical, info).</p> <p> TYPE: <code>_SEVERITY_ARG</code> </p> RETURNS DESCRIPTION <code>alert.FeatureViewAlert</code> <p>The created FeatureViewAlert object.</p> RAISES DESCRIPTION <code>ValueError</code> <p>if status is not valid.</p> <code>ValueError</code> <p>if severity is not valid.</p> <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request</p>"},{"location":"python-api/hopsworks/core/alerts_api/#hopsworks_common.core.alerts_api.AlertsApi.create_job_alert","title":"[source]  create_job_alert","text":"<pre><code>create_job_alert(\n    job_name: str,\n    receiver: str,\n    status: _JOB_STATUS_ARG,\n    severity: _SEVERITY_ARG,\n) -&gt; alert.JobAlert\n</code></pre> <p>Create a new job alert.</p> Example <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nalerts_api = project.get_alerts_api()\n\nnew_alert = alerts_api.create_job_alert(job_name=\"my_job\", receiver=\"email\", status=\"finished\", severity=\"warning\")\n</code></pre> PARAMETER DESCRIPTION <code>job_name</code> <p>The name of the job.</p> <p> TYPE: <code>str</code> </p> <code>receiver</code> <p>The receiver of the alert (e.g., email, webhook).</p> <p> TYPE: <code>str</code> </p> <code>status</code> <p>The status of the alert (finished, failed, killed, long_running).</p> <p> TYPE: <code>_JOB_STATUS_ARG</code> </p> <code>severity</code> <p>The severity of the alert (warning, critical, info).</p> <p> TYPE: <code>_SEVERITY_ARG</code> </p> RETURNS DESCRIPTION <code>alert.JobAlert</code> <p>The created JobAlert object.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the job name is not provided.</p> <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request</p>"},{"location":"python-api/hopsworks/core/alerts_api/#hopsworks_common.core.alerts_api.AlertsApi.get_alert_receivers","title":"[source]  get_alert_receivers","text":"<pre><code>get_alert_receivers() -&gt; list[alert_receiver.AlertReceiver]\n</code></pre> <p>Get all alert receivers.</p> Example <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nalerts_api = project.get_alerts_api()\n\nalert_receivers = alerts_api.get_alert_receivers()\n</code></pre> RETURNS DESCRIPTION <code>list[alert_receiver.AlertReceiver]</code> <p>List of alert receivers.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request</p>"},{"location":"python-api/hopsworks/core/alerts_api/#hopsworks_common.core.alerts_api.AlertsApi.get_alert_receiver","title":"[source]  get_alert_receiver","text":"<pre><code>get_alert_receiver(\n    name: str,\n) -&gt; alert_receiver.AlertReceiver | None\n</code></pre> <p>Get a specific alert receivers by name.</p> Example <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nalerts_api = project.get_alerts_api()\n\nalert_receiver = alerts_api.get_alert_receiver(\"email\")\n</code></pre> PARAMETER DESCRIPTION <code>name</code> <p>The name of the alert receiver to retrieve.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>alert_receiver.AlertReceiver | None</code> <p>The alert receiver object.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request</p>"},{"location":"python-api/hopsworks/core/alerts_api/#hopsworks_common.core.alerts_api.AlertsApi.create_alert_receiver","title":"[source]  create_alert_receiver","text":"<pre><code>create_alert_receiver(\n    name: str,\n    email_configs: list[alert_receiver.EmailConfig]\n    | None = None,\n    slack_configs: list[alert_receiver.SlackConfig]\n    | None = None,\n    pagerduty_configs: list[alert_receiver.PagerDutyConfig]\n    | None = None,\n    webhook_configs: list[alert_receiver.WebhookConfig]\n    | None = None,\n) -&gt; alert_receiver.AlertReceiver\n</code></pre> <p>Create a new alert receiver.</p> Example <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nalerts_api = project.get_alerts_api()\n\nnew_alert_receiver = alerts_api.create_alert_receiver(name=\"email\", email_configs=[{\"to\": \"email@mail.com\"}])\n</code></pre> PARAMETER DESCRIPTION <code>name</code> <p>The name of the alert receiver (e.g., email, webhook).</p> <p> TYPE: <code>str</code> </p> <code>email_configs</code> <p>List of email configurations.</p> <p> TYPE: <code>list[alert_receiver.EmailConfig] | None</code> DEFAULT: <code>None</code> </p> <code>slack_configs</code> <p>List of Slack configurations.</p> <p> TYPE: <code>list[alert_receiver.SlackConfig] | None</code> DEFAULT: <code>None</code> </p> <code>pagerduty_configs</code> <p>List of PagerDuty configurations.</p> <p> TYPE: <code>list[alert_receiver.PagerDutyConfig] | None</code> DEFAULT: <code>None</code> </p> <code>webhook_configs</code> <p>List of webhook configurations.</p> <p> TYPE: <code>list[alert_receiver.WebhookConfig] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>alert_receiver.AlertReceiver</code> <p>The created alert receiver object.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If multiple configurations are provided.</p> <code>ValueError</code> <p>If the global channel for the configuration is not configured.</p> <code>TimeoutError</code> <p>If the alert receiver creation times out.</p> <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request</p>"},{"location":"python-api/hopsworks/core/alerts_api/#hopsworks_common.core.alerts_api.AlertsApi.delete_alert","title":"[source]  delete_alert","text":"<pre><code>delete_alert(alert_id: int)\n</code></pre> <p>Delete an alert by ID.</p> Example <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nalerts_api = project.get_alerts_api()\n\nalerts_api.delete_alert(alert_id=1)\n</code></pre> PARAMETER DESCRIPTION <code>alert_id</code> <p>The ID of the alert to delete.</p> <p> TYPE: <code>int</code> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request</p>"},{"location":"python-api/hopsworks/core/alerts_api/#hopsworks_common.core.alerts_api.AlertsApi.trigger_alert","title":"[source]  trigger_alert","text":"<pre><code>trigger_alert(\n    receiver_name: str,\n    title: str,\n    summary: str,\n    description: str,\n    severity: _SEVERITY_ARG,\n    status: str,\n    name: str,\n    generator_url: str | None = None,\n    expire_after_sec: int | None = None,\n)\n</code></pre> <p>Trigger an alert.</p> Example <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nalerts_api = project.get_alerts_api()\n\nalerts_api.trigger_alert(receiver_name=\"email\", title=\"Title\", summary=\"Alert summary\", description=\"Alert description\", severity=\"info\", status=\"script_finished\", name=\"my_alert\")\n</code></pre> PARAMETER DESCRIPTION <code>receiver_name</code> <p>The receiver of the alert (e.g., email, webhook).</p> <p> TYPE: <code>str</code> </p> <code>summary</code> <p>The summary of the alert.</p> <p> TYPE: <code>str</code> </p> <code>description</code> <p>The description of the alert.</p> <p> TYPE: <code>str</code> </p> <code>severity</code> <p>The severity of the alert (warning, critical, info).</p> <p> TYPE: <code>_SEVERITY_ARG</code> </p> <code>status</code> <p>The status of the alert.</p> <p> TYPE: <code>str</code> </p> <code>name</code> <p>The name of the alert.</p> <p> TYPE: <code>str</code> </p> <code>generator_url</code> <p>The URL of the alert generator.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>expire_after_sec</code> <p>The time in seconds after which the alert should expire.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request</p>"},{"location":"python-api/hopsworks/core/alerts_api/#hopsworks_common.core.alerts_api.AlertsApi.get_triggered_alerts","title":"[source]  get_triggered_alerts","text":"<pre><code>get_triggered_alerts(\n    active: bool = True,\n    silenced: bool = False,\n    inhibited: bool = False,\n) -&gt; list[triggered_alert.TriggeredAlert]\n</code></pre> <p>Get triggered alerts.</p> Example <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nalerts_api = project.get_alerts_api()\n\ntriggered_alerts = alerts_api.get_triggered_alerts()\n</code></pre> PARAMETER DESCRIPTION <code>active</code> <p>Whether to include active alerts.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>silenced</code> <p>Whether to include silenced alerts.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>inhibited</code> <p>Whether to include inhibited alerts.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>list[triggered_alert.TriggeredAlert]</code> <p>The triggered alert objects.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request</p>"},{"location":"python-api/hopsworks/core/dataset_api/","title":"hopsworks.core.dataset_api","text":""},{"location":"python-api/hopsworks/core/dataset_api/#hopsworks.core.dataset_api","title":"hopsworks.core.dataset_api","text":""},{"location":"python-api/hopsworks/core/dataset_api/#hopsworks_common.core.dataset_api.DatasetApi","title":"[source]  DatasetApi","text":"<p>                 For backwards compatibility                 <code>hopsworks.core.dataset_api.DatasetApi</code>                 is still available as <code>hsfs.core.dataset_api.DatasetApi</code>, <code>hsml.core.dataset_api.DatasetApi</code>.                   The use of these aliases is discouraged as they are to be deprecated.               </p> Returned by <ul> <li> <code><code></code>Project.<code></code>get_dataset_api</code> </li> </ul>"},{"location":"python-api/hopsworks/core/dataset_api/#hopsworks_common.core.dataset_api.DatasetApi.download","title":"[source]  download","text":"<pre><code>download(\n    path: str,\n    local_path: str | None = None,\n    overwrite: bool | None = False,\n    chunk_size: int = DEFAULT_DOWNLOAD_FLOW_CHUNK_SIZE,\n) -&gt; str\n</code></pre> <p>Download file from Hopsworks Filesystem to the current working directory.</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\ndataset_api = project.get_dataset_api()\n\ndownloaded_file_path = dataset_api.download(\"Resources/my_local_file.txt\")\n</code></pre> PARAMETER DESCRIPTION <code>path</code> <p>Path in Hopsworks filesystem to the file.</p> <p> TYPE: <code>str</code> </p> <code>local_path</code> <p>Path where to download the file in the local filesystem.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>overwrite</code> <p>Overwrite local file if exists.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>False</code> </p> <code>chunk_size</code> <p>Upload chunk size in bytes, defaults to 1 MB.</p> <p> TYPE: <code>int</code> DEFAULT: <code>DEFAULT_DOWNLOAD_FLOW_CHUNK_SIZE</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The path to the downloaded file.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/core/dataset_api/#hopsworks_common.core.dataset_api.DatasetApi.upload","title":"[source]  upload","text":"<pre><code>upload(\n    local_path: str,\n    upload_path: str,\n    overwrite: bool = False,\n    chunk_size: int = DEFAULT_UPLOAD_FLOW_CHUNK_SIZE,\n    simultaneous_uploads: int = DEFAULT_UPLOAD_SIMULTANEOUS_UPLOADS,\n    simultaneous_chunks: int = DEFAULT_UPLOAD_SIMULTANEOUS_CHUNKS,\n    max_chunk_retries: int = DEFAULT_UPLOAD_MAX_CHUNK_RETRIES,\n    chunk_retry_interval: int = 1,\n) -&gt; str\n</code></pre> <p>Upload a file or directory to the Hopsworks filesystem.</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\ndataset_api = project.get_dataset_api()\n\n# upload a file to Resources dataset\nuploaded_file_path = dataset_api.upload(\"my_local_file.txt\", \"Resources\")\n\n# upload a directory to Resources dataset\nuploaded_file_path = dataset_api.upload(\"my_dir\", \"Resources\")\n</code></pre> PARAMETER DESCRIPTION <code>local_path</code> <p>Local path to file or directory to upload, can be relative or absolute.</p> <p> TYPE: <code>str</code> </p> <code>upload_path</code> <p>Path to directory where to upload the file in Hopsworks Filesystem.</p> <p> TYPE: <code>str</code> </p> <code>overwrite</code> <p>Overwrite file or directory if exists.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>chunk_size</code> <p>Upload chunk size in bytes, defaults to 10 MB.</p> <p> TYPE: <code>int</code> DEFAULT: <code>DEFAULT_UPLOAD_FLOW_CHUNK_SIZE</code> </p> <code>simultaneous_chunks</code> <p>Number of simultaneous chunks to upload for each file upload.</p> <p> TYPE: <code>int</code> DEFAULT: <code>DEFAULT_UPLOAD_SIMULTANEOUS_CHUNKS</code> </p> <code>simultaneous_uploads</code> <p>Number of simultaneous files to be uploaded for directories.</p> <p> TYPE: <code>int</code> DEFAULT: <code>DEFAULT_UPLOAD_SIMULTANEOUS_UPLOADS</code> </p> <code>max_chunk_retries</code> <p>Maximum retry for a chunk.</p> <p> TYPE: <code>int</code> DEFAULT: <code>DEFAULT_UPLOAD_MAX_CHUNK_RETRIES</code> </p> <code>chunk_retry_interval</code> <p>Chunk retry interval in seconds.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The path to the uploaded file or directory.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/core/dataset_api/#hopsworks_common.core.dataset_api.DatasetApi.exists","title":"[source]  exists","text":"<pre><code>exists(path: str) -&gt; bool\n</code></pre> <p>Check if a file exists in the Hopsworks Filesystem.</p> PARAMETER DESCRIPTION <code>path</code> <p>Path to check.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>bool</code> <p><code>True</code> if exists, otherwise <code>False</code>.</p>"},{"location":"python-api/hopsworks/core/dataset_api/#hopsworks_common.core.dataset_api.DatasetApi.remove","title":"[source]  remove","text":"<pre><code>remove(path: str)\n</code></pre> <p>Remove a path in the Hopsworks Filesystem.</p> PARAMETER DESCRIPTION <code>path</code> <p>Path to remove.</p> <p> TYPE: <code>str</code> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/core/dataset_api/#hopsworks_common.core.dataset_api.DatasetApi.mkdir","title":"[source]  mkdir","text":"<pre><code>mkdir(path: str) -&gt; str\n</code></pre> <p>Create a directory in the Hopsworks Filesystem.</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\ndataset_api = project.get_dataset_api()\n\ndirectory_path = dataset_api.mkdir(\"Resources/my_dir\")\n</code></pre> PARAMETER DESCRIPTION <code>path</code> <p>Path to directory.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Path to the created directory.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/core/dataset_api/#hopsworks_common.core.dataset_api.DatasetApi.copy","title":"[source]  copy","text":"<pre><code>copy(\n    source_path: str,\n    destination_path: str,\n    overwrite: bool = False,\n)\n</code></pre> <p>Copy a file or directory in the Hopsworks Filesystem.</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\ndataset_api = project.get_dataset_api()\n\ndirectory_path = dataset_api.copy(\"Resources/myfile.txt\", \"Logs/myfile.txt\")\n</code></pre> PARAMETER DESCRIPTION <code>source_path</code> <p>The source path to copy.</p> <p> TYPE: <code>str</code> </p> <code>destination_path</code> <p>The destination path.</p> <p> TYPE: <code>str</code> </p> <code>overwrite</code> <p>Overwrite destination if exists.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.DatasetException</code> <p>If the destination path already exists and overwrite is not set to <code>True</code>.</p> <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/core/dataset_api/#hopsworks_common.core.dataset_api.DatasetApi.move","title":"[source]  move","text":"<pre><code>move(\n    source_path: str,\n    destination_path: str,\n    overwrite: bool = False,\n)\n</code></pre> <p>Move a file or directory in the Hopsworks Filesystem.</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\ndataset_api = project.get_dataset_api()\n\ndirectory_path = dataset_api.move(\"Resources/myfile.txt\", \"Logs/myfile.txt\")\n</code></pre> PARAMETER DESCRIPTION <code>source_path</code> <p>The source path to move.</p> <p> TYPE: <code>str</code> </p> <code>destination_path</code> <p>The destination path.</p> <p> TYPE: <code>str</code> </p> <code>overwrite</code> <p>Overwrite destination if exists.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.DatasetException</code> <p>If the destination path already exists and overwrite is not set to <code>True</code>.</p> <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/core/dataset_api/#hopsworks_common.core.dataset_api.DatasetApi.upload_feature_group","title":"[source]  upload_feature_group","text":"<pre><code>upload_feature_group(feature_group, path, dataframe)\n</code></pre> <p>Upload a dataframe to a path in Parquet format using a feature group metadata.</p> <p>Note</p> <p>This method is a legacy method kept for backwards-compatibility; do not use it in new code.</p>"},{"location":"python-api/hopsworks/core/dataset_api/#hopsworks_common.core.dataset_api.DatasetApi.list","title":"[source]  list","text":"<pre><code>list(\n    path: str, offset: int = 0, limit: int = 1000\n) -&gt; list[str]\n</code></pre> <p>List the files and directories from a path in the Hopsworks Filesystem.</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\ndataset_api = project.get_dataset_api()\n\n# list all files in the Resources dataset\nfiles = dataset_api.list(\"/Resources\")\n\n# list all datasets in the project\nfiles = dataset_api.list(\"/\")\n</code></pre> PARAMETER DESCRIPTION <code>path</code> <p>Path in Hopsworks filesystem to the directory.</p> <p> TYPE: <code>str</code> </p> <code>offset</code> <p>The number of entities to skip.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>limit</code> <p>Max number of the returned entities.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> RETURNS DESCRIPTION <code>list[str]</code> <p>List of path to files and directories in the provided path.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/core/dataset_api/#hopsworks_common.core.dataset_api.DatasetApi.read_content","title":"[source]  read_content","text":"<pre><code>read_content(path: str, dataset_type: str = 'DATASET')\n</code></pre> <p>Read the content of a file.</p> PARAMETER DESCRIPTION <code>path</code> <p>The path to the file to read.</p> <p> TYPE: <code>str</code> </p> <code>dataset_type</code> <p>The type of dataset, can be <code>DATASET</code> or <code>HIVEDB</code>; defaults to <code>DATASET</code>. <code>HIVEDB</code> type is used to read files from Apache Hive.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'DATASET'</code> </p> RETURNS DESCRIPTION <p>An object with <code>content</code> attribute containing the file content as bytes, or <code>None</code> if the file was not found.</p>"},{"location":"python-api/hopsworks/core/dataset_api/#hopsworks_common.core.dataset_api.DatasetApi.chmod","title":"[source]  chmod","text":"<pre><code>chmod(remote_path: str, permissions: str) -&gt; dict\n</code></pre> <p>Change permissions of a file or a directory in the Hopsworks Filesystem.</p> PARAMETER DESCRIPTION <code>remote_path</code> <p>Path to change the permissions of.</p> <p> TYPE: <code>str</code> </p> <code>permissions</code> <p>Permissions string, for example <code>\"u+x\"</code>.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>The updated dataset metadata.</p> <p> TYPE: <code>dict</code> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/core/dataset_api/#hopsworks_common.core.dataset_api.DatasetApi.unzip","title":"[source]  unzip","text":"<pre><code>unzip(\n    remote_path: str,\n    block: bool = False,\n    timeout: int | None = 120,\n)\n</code></pre> <p>Unzip an archive in the dataset.</p> PARAMETER DESCRIPTION <code>remote_path</code> <p>path to file or directory to unzip.</p> <p> TYPE: <code>str</code> </p> <code>block</code> <p>if the operation should be blocking until complete.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>timeout</code> <p>timeout in seconds for the blocking, defaults to 120; if <code>None</code>, the blocking is unbounded.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>120</code> </p> RETURNS DESCRIPTION <p>Whether the operation completed in the specified timeout; if non-blocking, always returns <code>True</code>.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/core/dataset_api/#hopsworks_common.core.dataset_api.DatasetApi.zip","title":"[source]  zip","text":"<pre><code>zip(\n    remote_path: str,\n    destination_path: str | None = None,\n    block: bool = False,\n    timeout: int | None = 120,\n) -&gt; bool\n</code></pre> <p>Zip a file or directory in the dataset.</p> PARAMETER DESCRIPTION <code>remote_path</code> <p>Path to file or directory to zip.</p> <p> TYPE: <code>str</code> </p> <code>destination_path</code> <p>Path to upload the zip.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>block</code> <p>Whether the operation should be blocking until complete.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>timeout</code> <p>Timeout in seconds for the blocking, defaults to 120; if <code>None</code>, the blocking is unbounded.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>120</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>Whether the operation completed in the specified timeout; if non-blocking, always returns <code>True</code>.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/core/environment_api/","title":"hopsworks.core.environment_api","text":""},{"location":"python-api/hopsworks/core/environment_api/#hopsworks.core.environment_api","title":"hopsworks.core.environment_api","text":""},{"location":"python-api/hopsworks/core/environment_api/#hopsworks_common.core.environment_api.EnvironmentApi","title":"[source]  EnvironmentApi","text":"Returned by <ul> <li> <code><code></code>Project.<code></code>get_environment_api</code> </li> </ul>"},{"location":"python-api/hopsworks/core/environment_api/#hopsworks_common.core.environment_api.EnvironmentApi.create_environment","title":"[source]  create_environment","text":"<pre><code>create_environment(\n    name: str,\n    description: str | None = None,\n    base_environment_name: str\n    | None = \"python-feature-pipeline\",\n    await_creation: bool | None = True,\n) -&gt; environment.Environment\n</code></pre> <p>Create Python environment for the project.</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nenv_api = project.get_environment_api()\n\nnew_env = env_api.create_environment(\"my_custom_environment\", base_environment_name=\"python-feature-pipeline\")\n</code></pre> PARAMETER DESCRIPTION <code>name</code> <p>Name of the environment.</p> <p> TYPE: <code>str</code> </p> <code>base_environment_name</code> <p>The name of the environment to clone from.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>'python-feature-pipeline'</code> </p> <code>await_creation</code> <p>Whether the method returns only when the creation is finished.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>environment.Environment</code> <p>The Environment object.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/core/environment_api/#hopsworks_common.core.environment_api.EnvironmentApi.get_environment","title":"[source]  get_environment","text":"<pre><code>get_environment(\n    name: str,\n) -&gt; environment.Environment | None\n</code></pre> <p>Get handle for a Python environment in the project.</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nenv_api = project.get_environment_api()\n\nenv = env_api.get_environment(\"my_custom_environment\")\n</code></pre> PARAMETER DESCRIPTION <code>name</code> <p>Name of the environment.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>environment.Environment | None</code> <p>The Environment object or <code>None</code> if it does not exist.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/core/flink_cluster_api/","title":"hopsworks.core.flink_cluster_api","text":""},{"location":"python-api/hopsworks/core/flink_cluster_api/#hopsworks.core.flink_cluster_api","title":"hopsworks.core.flink_cluster_api","text":""},{"location":"python-api/hopsworks/core/flink_cluster_api/#hopsworks_common.core.flink_cluster_api.FlinkClusterApi","title":"[source]  FlinkClusterApi","text":"Returned by <ul> <li> <code><code></code>Project.<code></code>get_flink_cluster_api</code> </li> </ul>"},{"location":"python-api/hopsworks/core/flink_cluster_api/#hopsworks_common.core.flink_cluster_api.FlinkClusterApi.setup_cluster","title":"[source]  setup_cluster","text":"<pre><code>setup_cluster(\n    name: str, config=None\n) -&gt; flink_cluster.FlinkCluster\n</code></pre> <p>Create a new flink job representing a flink cluster, or update an existing one.</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nflink_cluster_api = project.get_flink_cluster_api()\n\nflink_config = flink_cluster_api.get_configuration()\n\nflink_config['appName'] = \"myFlinkCluster\"\n\nflink_cluster = flink_cluster_api.setup_cluster(name=\"myFlinkCluster\", config=flink_config)\n</code></pre> PARAMETER DESCRIPTION <code>name</code> <p>Name of the cluster.</p> <p> TYPE: <code>str</code> </p> <code>config</code> <p>Configuration of the cluster.</p> <p> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>flink_cluster.FlinkCluster</code> <p>The FlinkCluster object representing the cluster.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/core/flink_cluster_api/#hopsworks_common.core.flink_cluster_api.FlinkClusterApi.get_cluster","title":"[source]  get_cluster","text":"<pre><code>get_cluster(name: str) -&gt; flink_cluster.FlinkCluster | None\n</code></pre> <p>Get the job corresponding to the flink cluster.</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nflink_cluster_api = project.get_flink_cluster_api()\n\nflink_cluster = flink_cluster_api.get_cluster(name=\"myFlinkCluster\")\n</code></pre> PARAMETER DESCRIPTION <code>name</code> <p>Name of the cluster.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>flink_cluster.FlinkCluster | None</code> <p>The FlinkCluster object representing the cluster or <code>None</code> if it does not exist.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/core/git_api/","title":"hopsworks.core.git_api","text":""},{"location":"python-api/hopsworks/core/git_api/#hopsworks.core.git_api","title":"hopsworks.core.git_api","text":""},{"location":"python-api/hopsworks/core/git_api/#hopsworks_common.core.git_api.GitApi","title":"[source]  GitApi","text":"Returned by <ul> <li> <code><code></code>Project.<code></code>get_git_api</code> </li> </ul>"},{"location":"python-api/hopsworks/core/git_api/#hopsworks_common.core.git_api.GitApi.clone","title":"[source]  clone","text":"<pre><code>clone(\n    url: str,\n    path: str,\n    provider: Literal[\"GitHub\", \"GitLab\", \"BitBucket\"]\n    | None = None,\n    branch: str = None,\n) -&gt; git_repo.GitRepo\n</code></pre> <p>Clone a new Git Repo in to Hopsworks Filesystem.</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\ngit_api = project.get_git_api()\n\ngit_repo = git_api.clone(\"https://github.com/logicalclocks/hops-examples.git\", \"Resources\", \"GitHub\")\n</code></pre> PARAMETER DESCRIPTION <code>url</code> <p>URL to the git repository.</p> <p> TYPE: <code>str</code> </p> <code>path</code> <p>Path in Hopsworks Filesystem to clone the repo to.</p> <p> TYPE: <code>str</code> </p> <code>provider</code> <p>The git provider where the repo is currently hosted.</p> <p> TYPE: <code>Literal['GitHub', 'GitLab', 'BitBucket'] | None</code> DEFAULT: <code>None</code> </p> <code>branch</code> <p>The branch to clone, defaults to the configured default branch.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>git_repo.GitRepo</code> <p>Git repository object.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/core/git_api/#hopsworks_common.core.git_api.GitApi.get_repos","title":"[source]  get_repos","text":"<pre><code>get_repos() -&gt; list[git_repo.GitRepo]\n</code></pre> <p>Get the existing Git repositories.</p> RETURNS DESCRIPTION <code>list[git_repo.GitRepo]</code> <p>List of git repository objects.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/core/git_api/#hopsworks_common.core.git_api.GitApi.get_providers","title":"[source]  get_providers","text":"<pre><code>get_providers() -&gt; list[git_provider.GitProvider]\n</code></pre> <p>Get the configured Git providers.</p> RETURNS DESCRIPTION <code>list[git_provider.GitProvider]</code> <p>List of git provider objects.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/core/git_api/#hopsworks_common.core.git_api.GitApi.get_provider","title":"[source]  get_provider","text":"<pre><code>get_provider(\n    provider: Literal[\"GitHub\", \"GitLab\", \"BitBucket\"],\n    host: str = None,\n) -&gt; git_provider.GitProvider | None\n</code></pre> <p>Get the configured Git provider.</p> PARAMETER DESCRIPTION <code>provider</code> <p>Name of the git provider.</p> <p> TYPE: <code>Literal['GitHub', 'GitLab', 'BitBucket']</code> </p> <code>host</code> <p>Optional host for the git provider, e.g., <code>\"github.com\"</code> for GitHub, <code>\"gitlab.com\"</code> for GitLab, <code>\"bitbucket.org\"</code> for BitBucket.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>git_provider.GitProvider | None</code> <p>The git provider or <code>None</code> if it does not exist.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/core/git_api/#hopsworks_common.core.git_api.GitApi.set_provider","title":"[source]  set_provider","text":"<pre><code>set_provider(\n    provider: Literal[\"GitHub\", \"GitLab\", \"BitBucket\"],\n    username: str,\n    token: str,\n    host: str = None,\n)\n</code></pre> <p>Configure a Git provider.</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\ngit_api = project.get_git_api()\n\ngit_api.set_provider(\"GitHub\", \"my_user\", \"my_token\", host=\"github.com\")\n</code></pre> PARAMETER DESCRIPTION <code>provider</code> <p>Name of the git provider.</p> <p> TYPE: <code>Literal['GitHub', 'GitLab', 'BitBucket']</code> </p> <code>username</code> <p>Username for the git provider service.</p> <p> TYPE: <code>str</code> </p> <code>token</code> <p>Token to set for the git provider service.</p> <p> TYPE: <code>str</code> </p> <code>host</code> <p>Host for the git provider, e.g., <code>\"github.com\"</code> for GitHub, <code>\"gitlab.com\"</code> for GitLab, <code>\"bitbucket.org\"</code> for BitBucket.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/core/git_api/#hopsworks_common.core.git_api.GitApi.get_repo","title":"[source]  get_repo","text":"<pre><code>get_repo(\n    name: str, path: str = None\n) -&gt; git_repo.GitRepo | None\n</code></pre> <p>Get the cloned Git repository.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the git repository.</p> <p> TYPE: <code>str</code> </p> <code>path</code> <p>Optional path to specify if multiple git repositories with the same name exist in the project.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>git_repo.GitRepo | None</code> <p>The git repository or <code>None</code> if it does not exist.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/core/job_api/","title":"hopsworks.core.job_api","text":""},{"location":"python-api/hopsworks/core/job_api/#hopsworks.core.job_api","title":"hopsworks.core.job_api","text":""},{"location":"python-api/hopsworks/core/job_api/#hopsworks_common.core.job_api.JobApi","title":"[source]  JobApi","text":"<p>                 For backwards compatibility                 <code>hopsworks.core.job_api.JobApi</code>                 is still available as <code>hopsworks.core.job_api.JobsApi</code>, <code>hsfs.core.job_api.JobApi</code>.                   The use of these aliases is discouraged as they are to be deprecated.               </p> Returned by <ul> <li> <code><code></code>Project.<code></code>get_job_api</code> </li> </ul>"},{"location":"python-api/hopsworks/core/job_api/#hopsworks_common.core.job_api.JobApi.create_job","title":"[source]  create_job","text":"<pre><code>create_job(name: str, config: dict) -&gt; job.Job\n</code></pre> <p>Create a new job or update an existing one.</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\njob_api = project.get_job_api()\n\nspark_config = job_api.get_configuration(\"PYSPARK\")\n\nspark_config['appPath'] = \"/Resources/my_app.py\"\n\njob = job_api.create_job(\"my_spark_job\", spark_config)\n</code></pre> PARAMETER DESCRIPTION <code>name</code> <p>Name of the job.</p> <p> TYPE: <code>str</code> </p> <code>config</code> <p>Configuration of the job.</p> <p> TYPE: <code>dict</code> </p> RETURNS DESCRIPTION <code>job.Job</code> <p>The created job.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/core/job_api/#hopsworks_common.core.job_api.JobApi.get_job","title":"[source]  get_job","text":"<pre><code>get_job(name: str) -&gt; job.Job | None\n</code></pre> <p>Get a job.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the job.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>job.Job | None</code> <p>The Job object or <code>None</code> if it does not exist.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/core/job_api/#hopsworks_common.core.job_api.JobApi.get_jobs","title":"[source]  get_jobs","text":"<pre><code>get_jobs() -&gt; list[job.Job]\n</code></pre> <p>Get all jobs.</p> RETURNS DESCRIPTION <code>list[job.Job]</code> <p>List of all jobs.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/core/job_api/#hopsworks_common.core.job_api.JobApi.get_configuration","title":"[source]  get_configuration","text":"<pre><code>get_configuration(\n    type: Literal[\n        \"SPARK\", \"PYSPARK\", \"PYTHON\", \"DOCKER\", \"FLINK\"\n    ],\n) -&gt; dict\n</code></pre> <p>Get configuration for the specific job type.</p> PARAMETER DESCRIPTION <code>type</code> <p>The job type to retrieve the configuration of.</p> <p> TYPE: <code>Literal['SPARK', 'PYSPARK', 'PYTHON', 'DOCKER', 'FLINK']</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>The default job configuration for the specific job type.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/core/job_configuration/","title":"hopsworks.core.job_configuration","text":""},{"location":"python-api/hopsworks/core/job_configuration/#hopsworks.core.job_configuration","title":"hopsworks.core.job_configuration","text":""},{"location":"python-api/hopsworks/core/job_configuration/#hopsworks_common.core.job_configuration.JobConfiguration","title":"[source]  JobConfiguration","text":"<p>Configuration of a Hopsworks job.</p> <p>Each job has a <code>config</code> attribute, which can be used in combination with <code>job.save()</code> to update the job's configuration.</p> <p>                 For backwards compatibility                 <code>hopsworks.core.job_configuration.JobConfiguration</code>                 is still available as <code>hsfs.core.job_configuration.JobConfiguration</code>.                   The use of this alias is discouraged as it is to be deprecated.               </p> Returned by"},{"location":"python-api/hopsworks/core/kafka_api/","title":"hopsworks.core.kafka_api","text":""},{"location":"python-api/hopsworks/core/kafka_api/#hopsworks.core.kafka_api","title":"hopsworks.core.kafka_api","text":""},{"location":"python-api/hopsworks/core/kafka_api/#hopsworks_common.core.kafka_api.KafkaApi","title":"[source]  KafkaApi","text":"<p>                 For backwards compatibility                 <code>hopsworks.core.kafka_api.KafkaApi</code>                 is still available as <code>hsfs.core.kafka_api.KafkaApi</code>.                   The use of this alias is discouraged as it is to be deprecated.               </p> Returned by <ul> <li> <code><code></code>Project.<code></code>get_kafka_api</code> </li> </ul>"},{"location":"python-api/hopsworks/core/kafka_api/#hopsworks_common.core.kafka_api.KafkaApi.create_topic","title":"[source]  create_topic","text":"<pre><code>create_topic(\n    name: str,\n    schema: str,\n    schema_version: int,\n    replicas: int = 1,\n    partitions: int = 1,\n) -&gt; kafka_topic.KafkaTopic\n</code></pre> <p>Create a new kafka topic.</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nkafka_api = project.get_kafka_api()\n\nkafka_topic = kafka_api.create_topic(\"my_topic\", \"my_schema\", 1)\n</code></pre> PARAMETER DESCRIPTION <code>name</code> <p>Name of the topic.</p> <p> TYPE: <code>str</code> </p> <code>schema</code> <p>Subject name of the schema.</p> <p> TYPE: <code>str</code> </p> <code>schema_version</code> <p>Version of the schema.</p> <p> TYPE: <code>int</code> </p> <code>replicas</code> <p>Replication factor for the topic.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>partitions</code> <p>Number of partitions for the topic.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>kafka_topic.KafkaTopic</code> <p>The KafkaTopic object.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/core/kafka_api/#hopsworks_common.core.kafka_api.KafkaApi.create_schema","title":"[source]  create_schema","text":"<pre><code>create_schema(\n    subject: str, schema: dict\n) -&gt; kafka_schema.KafkaSchema\n</code></pre> <p>Create a new kafka schema.</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nkafka_api = project.get_kafka_api()\n\navro_schema = {\n  \"type\": \"record\",\n  \"name\": \"tutorial\",\n  \"fields\": [\n    {\n      \"name\": \"id\",\n      \"type\": \"int\"\n    },\n    {\n      \"name\": \"data\",\n      \"type\": \"string\"\n    }\n  ]\n}\n\nkafka_topic = kafka_api.create_schema(\"my_schema\", avro_schema)\n</code></pre> PARAMETER DESCRIPTION <code>subject</code> <p>Subject name of the schema.</p> <p> TYPE: <code>str</code> </p> <code>schema</code> <p>Avro schema definition.</p> <p> TYPE: <code>dict</code> </p> RETURNS DESCRIPTION <code>kafka_schema.KafkaSchema</code> <p>The KafkaSchema object.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/core/kafka_api/#hopsworks_common.core.kafka_api.KafkaApi.get_topic","title":"[source]  get_topic","text":"<pre><code>get_topic(name: str) -&gt; kafka_topic.KafkaTopic | None\n</code></pre> <p>Get kafka topic by name.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the topic.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>kafka_topic.KafkaTopic | None</code> <p>The KafkaTopic object or <code>None</code> if not found.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/core/kafka_api/#hopsworks_common.core.kafka_api.KafkaApi.get_topics","title":"[source]  get_topics","text":"<pre><code>get_topics() -&gt; list[kafka_topic.KafkaTopic]\n</code></pre> <p>Get all kafka topics.</p> RETURNS DESCRIPTION <code>list[kafka_topic.KafkaTopic]</code> <p>List of KafkaTopic objects.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/core/kafka_api/#hopsworks_common.core.kafka_api.KafkaApi.get_subjects","title":"[source]  get_subjects","text":"<pre><code>get_subjects() -&gt; list[str]\n</code></pre> <p>Get all subjects.</p> RETURNS DESCRIPTION <code>list[str]</code> <p>List of registered subjects.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/core/kafka_api/#hopsworks_common.core.kafka_api.KafkaApi.get_schemas","title":"[source]  get_schemas","text":"<pre><code>get_schemas(subject: str) -&gt; list[kafka_schema.KafkaSchema]\n</code></pre> <p>Get all schema versions for the subject.</p> PARAMETER DESCRIPTION <code>subject</code> <p>Subject name.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>list[kafka_schema.KafkaSchema]</code> <p>List of KafkaSchema objects.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/core/kafka_api/#hopsworks_common.core.kafka_api.KafkaApi.get_schema","title":"[source]  get_schema","text":"<pre><code>get_schema(\n    subject: str, version: int\n) -&gt; kafka_schema.KafkaSchema | None\n</code></pre> <p>Get schema given subject name and version.</p> PARAMETER DESCRIPTION <code>subject</code> <p>Subject name.</p> <p> TYPE: <code>str</code> </p> <code>version</code> <p>Version number.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>kafka_schema.KafkaSchema | None</code> <p>KafkaSchema object or <code>None</code> if it does not exist.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/core/kafka_api/#hopsworks_common.core.kafka_api.KafkaApi.get_default_config","title":"[source]  get_default_config","text":"<pre><code>get_default_config(\n    internal_kafka: bool | None = None,\n) -&gt; dict\n</code></pre> <p>Get the configuration to set up a Producer or Consumer for a Kafka broker using confluent-kafka.</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nkafka_api = project.get_kafka_api()\n\nkafka_conf = kafka_api.get_default_config()\n\nfrom confluent_kafka import Producer\n\nproducer = Producer(kafka_conf)\n</code></pre> RETURNS DESCRIPTION <code>dict</code> <p>The kafka configuration.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/core/opensearch_api/","title":"hopsworks.core.opensearch_api","text":""},{"location":"python-api/hopsworks/core/opensearch_api/#hopsworks.core.opensearch_api","title":"hopsworks.core.opensearch_api","text":""},{"location":"python-api/hopsworks/core/opensearch_api/#hopsworks_common.core.opensearch_api.OpenSearchApi","title":"[source]  OpenSearchApi","text":"<p>                 For backwards compatibility                 <code>hopsworks.core.opensearch_api.OpenSearchApi</code>                 is still available as <code>hsfs.core.opensearch_api.OpenSearchApi</code>.                   The use of this alias is discouraged as it is to be deprecated.               </p> Returned by <ul> <li> <code><code></code>Project.<code></code>get_opensearch_api</code> </li> </ul>"},{"location":"python-api/hopsworks/core/opensearch_api/#hopsworks_common.core.opensearch_api.OpenSearchApi.get_project_index","title":"[source]  get_project_index","text":"<pre><code>get_project_index(index: str) -&gt; str\n</code></pre> <p>This helper method prefixes the supplied index name with the project name to avoid index name clashes.</p> PARAMETER DESCRIPTION <code>index</code> <p>The opensearch index to interact with.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>str</code> <p>A valid opensearch index name.</p>"},{"location":"python-api/hopsworks/core/opensearch_api/#hopsworks_common.core.opensearch_api.OpenSearchApi.get_default_py_config","title":"[source]  get_default_py_config","text":"<pre><code>get_default_py_config(\n    feature_store_id: int = None,\n) -&gt; dict[str, Any]\n</code></pre> <p>Get the required opensearch configuration to setup a connection using the opensearch-py library.</p> <pre><code>import hopsworks\nfrom opensearchpy import OpenSearch\n\nproject = hopsworks.login()\n\nopensearch_api = project.get_opensearch_api()\n\nclient = OpenSearch(**opensearch_api.get_default_py_config())\n</code></pre> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>A dictionary with required configuration.</p>"},{"location":"python-api/hopsworks/core/search_api/","title":"hopsworks.core.search_api","text":""},{"location":"python-api/hopsworks/core/search_api/#hopsworks.core.search_api","title":"hopsworks.core.search_api","text":""},{"location":"python-api/hopsworks/core/search_api/#hopsworks_common.core.search_api.SearchApi","title":"[source]  SearchApi","text":"Returned by <ul> <li> <code><code></code>Project.<code></code>get_search_api</code> </li> </ul>"},{"location":"python-api/hopsworks/core/search_api/#hopsworks_common.search_results.FeatureGroupSearchResult","title":"[source]  FeatureGroupSearchResult","text":"<p>               Bases: <code>SearchResultItem</code></p> <p>Search result for a Feature Group.</p> Returned by <ul> <li> <code><code></code>FeaturestoreSearchResult.<code></code>feature_groups</code> </li> </ul>"},{"location":"python-api/hopsworks/core/search_api/#hopsworks_common.search_results.FeatureSearchResult","title":"[source]  FeatureSearchResult","text":"<p>               Bases: <code>SearchResultItem</code></p> <p>Search result for a Feature.</p> Returned by <ul> <li> <code><code></code>FeaturestoreSearchResult.<code></code>features</code> </li> </ul>"},{"location":"python-api/hopsworks/core/search_api/#hopsworks_common.search_results.FeatureViewSearchResult","title":"[source]  FeatureViewSearchResult","text":"<p>               Bases: <code>SearchResultItem</code></p> <p>Search result for a Feature View.</p> Returned by <ul> <li> <code><code></code>FeaturestoreSearchResult.<code></code>feature_views</code> </li> </ul>"},{"location":"python-api/hopsworks/core/search_api/#hopsworks_common.search_results.FeaturestoreSearchResult","title":"[source]  FeaturestoreSearchResult","text":"<p>Container for all featurestore search results.</p> Returned by"},{"location":"python-api/hopsworks/core/search_api/#hopsworks_common.search_results.FeaturestoreSearchResult.feature_groups","title":"[source]  feature_groups  <code>property</code>","text":"<pre><code>feature_groups: list[FeatureGroupSearchResult]\n</code></pre> <p>List of Feature Group search results.</p>"},{"location":"python-api/hopsworks/core/search_api/#hopsworks_common.search_results.FeaturestoreSearchResult.feature_views","title":"[source]  feature_views  <code>property</code>","text":"<pre><code>feature_views: list[FeatureViewSearchResult]\n</code></pre> <p>List of Feature View search results.</p>"},{"location":"python-api/hopsworks/core/search_api/#hopsworks_common.search_results.FeaturestoreSearchResult.training_datasets","title":"[source]  training_datasets  <code>property</code>","text":"<pre><code>training_datasets: list[TrainingDatasetSearchResult]\n</code></pre> <p>List of Training Dataset search results.</p>"},{"location":"python-api/hopsworks/core/search_api/#hopsworks_common.search_results.FeaturestoreSearchResult.features","title":"[source]  features  <code>property</code>","text":"<pre><code>features: list[FeatureSearchResult]\n</code></pre> <p>List of Feature search results.</p>"},{"location":"python-api/hopsworks/core/search_api/#hopsworks_common.search_results.FeaturestoreSearchResult.feature_groups_offset","title":"[source]  feature_groups_offset  <code>property</code>","text":"<pre><code>feature_groups_offset: int\n</code></pre> <p>Total offset for the return list of feature groups within the whole result.</p>"},{"location":"python-api/hopsworks/core/search_api/#hopsworks_common.search_results.FeaturestoreSearchResult.feature_views_offset","title":"[source]  feature_views_offset  <code>property</code>","text":"<pre><code>feature_views_offset: int\n</code></pre> <p>Total offset for the return list of feature views within the whole result.</p>"},{"location":"python-api/hopsworks/core/search_api/#hopsworks_common.search_results.FeaturestoreSearchResult.training_datasets_offset","title":"[source]  training_datasets_offset  <code>property</code>","text":"<pre><code>training_datasets_offset: int\n</code></pre> <p>Total offset for the return list of training datasets within the whole result.</p>"},{"location":"python-api/hopsworks/core/search_api/#hopsworks_common.search_results.FeaturestoreSearchResult.features_offset","title":"[source]  features_offset  <code>property</code>","text":"<pre><code>features_offset: int\n</code></pre> <p>Total offset for the return list of features within the whole result.</p>"},{"location":"python-api/hopsworks/core/search_api/#hopsworks_common.search_results.FeaturestoreSearchResult.feature_groups_total","title":"[source]  feature_groups_total  <code>property</code>","text":"<pre><code>feature_groups_total: int\n</code></pre> <p>Total number of Feature Groups matching the search.</p>"},{"location":"python-api/hopsworks/core/search_api/#hopsworks_common.search_results.FeaturestoreSearchResult.feature_views_total","title":"[source]  feature_views_total  <code>property</code>","text":"<pre><code>feature_views_total: int\n</code></pre> <p>Total number of Feature Views matching the search.</p>"},{"location":"python-api/hopsworks/core/search_api/#hopsworks_common.search_results.FeaturestoreSearchResult.training_datasets_total","title":"[source]  training_datasets_total  <code>property</code>","text":"<pre><code>training_datasets_total: int\n</code></pre> <p>Total number of Training Datasets matching the search.</p>"},{"location":"python-api/hopsworks/core/search_api/#hopsworks_common.search_results.FeaturestoreSearchResult.features_total","title":"[source]  features_total  <code>property</code>","text":"<pre><code>features_total: int\n</code></pre> <p>Total number of Features matching the search.</p>"},{"location":"python-api/hopsworks/core/search_api/#hopsworks_common.search_results.TrainingDatasetSearchResult","title":"[source]  TrainingDatasetSearchResult","text":"<p>               Bases: <code>SearchResultItem</code></p> <p>Search result for a Training Dataset.</p> Returned by <ul> <li> <code><code></code>FeaturestoreSearchResult.<code></code>training_datasets</code> </li> </ul>"},{"location":"python-api/hopsworks/core/secret_api/","title":"hopsworks.core.secret_api","text":""},{"location":"python-api/hopsworks/core/secret_api/#hopsworks.core.secret_api","title":"hopsworks.core.secret_api","text":""},{"location":"python-api/hopsworks/core/secret_api/#hopsworks_common.core.secret_api.SecretsApi","title":"[source]  SecretsApi","text":"<p>API for managing secrets in Hopsworks.</p> <p>You can get an instance of this class with <code>hopsworks.get_secrets_api</code>.</p> Returned by <ul> <li> <code><code></code>hopsworks.<code></code>get_secrets_api</code> </li> </ul>"},{"location":"python-api/hopsworks/core/secret_api/#hopsworks_common.core.secret_api.SecretsApi.get_secrets","title":"[source]  get_secrets","text":"<pre><code>get_secrets() -&gt; list[secret.Secret]\n</code></pre> <p>Get all secrets.</p> RETURNS DESCRIPTION <code>list[secret.Secret]</code> <p><code>List[Secret]</code>: List of all accessible secrets</p> <p>Raises:     hopsworks.client.exceptions.RestAPIError: If the backend encounters an error when handling the request</p>"},{"location":"python-api/hopsworks/core/secret_api/#hopsworks_common.core.secret_api.SecretsApi.get_secret","title":"[source]  get_secret","text":"<pre><code>get_secret(\n    name: str, owner: str = None\n) -&gt; secret.Secret | None\n</code></pre> <p>Get a secret.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the secret.</p> <p> TYPE: <code>str</code> </p> <code>owner</code> <p>Username of the owner for a secret shared with the current project. Users can find their username in the Account Settings &gt; Profile section.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>secret.Secret | None</code> <p>The Secret object or <code>None</code> if it does not exist.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/core/secret_api/#hopsworks_common.core.secret_api.SecretsApi.get","title":"[source]  get","text":"<pre><code>get(name: str, owner: str = None) -&gt; str\n</code></pre> <p>Get the secret's value.</p> <p>If the secret does not exist, it prompts the user to create the secret if the application is running interactively.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the secret.</p> <p> TYPE: <code>str</code> </p> <code>owner</code> <p>Email of the owner for a secret shared with the current project.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The secret value.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/core/secret_api/#hopsworks_common.core.secret_api.SecretsApi.create_secret","title":"[source]  create_secret","text":"<pre><code>create_secret(\n    name: str, value: str, project: str = None\n) -&gt; secret.Secret\n</code></pre> <p>Create a new secret.</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nsecrets_api = hopsworks.get_secrets_api()\n\nsecret = secrets_api.create_secret(\"my_secret\", \"Fk3MoPlQXCQvPo\")\n</code></pre> PARAMETER DESCRIPTION <code>name</code> <p>Name of the secret.</p> <p> TYPE: <code>str</code> </p> <code>value</code> <p>The secret value.</p> <p> TYPE: <code>str</code> </p> <code>project</code> <p>Name of the project to share the secret with.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>secret.Secret</code> <p>The Secret object.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/environment/","title":"hopsworks.environment","text":""},{"location":"python-api/hopsworks/environment/#hopsworks.environment","title":"hopsworks.environment","text":""},{"location":"python-api/hopsworks/environment/#hopsworks_common.environment.Environment","title":"[source]  Environment","text":"Returned by <ul> <li> <code><code></code>EnvironmentApi.<code></code>create_environment</code> </li><li> <code><code></code>EnvironmentApi.<code></code>get_environment</code> </li> </ul>"},{"location":"python-api/hopsworks/environment/#hopsworks_common.environment.Environment.python_version","title":"[source]  python_version  <code>property</code>","text":"<p>Python version of the environment.</p>"},{"location":"python-api/hopsworks/environment/#hopsworks_common.environment.Environment.name","title":"[source]  name  <code>property</code>","text":"<p>Name of the environment.</p>"},{"location":"python-api/hopsworks/environment/#hopsworks_common.environment.Environment.description","title":"[source]  description  <code>property</code>","text":"<p>Description of the environment.</p>"},{"location":"python-api/hopsworks/environment/#hopsworks_common.environment.Environment.install_wheel","title":"[source]  install_wheel","text":"<pre><code>install_wheel(\n    path: str, await_installation: bool | None = True\n) -&gt; Library\n</code></pre> <p>Install a python library packaged in a wheel file.</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# Upload to Hopsworks\nds_api = project.get_dataset_api()\nwhl_path = ds_api.upload(\"matplotlib-3.1.3-cp38-cp38-manylinux1_x86_64.whl\", \"Resources\")\n\n# Install\nenv_api = project.get_environment_api()\nenv = env_api.get_environment(\"my_custom_environment\")\n\nenv.install_wheel(whl_path)\n</code></pre> PARAMETER DESCRIPTION <code>path</code> <p>The path in Hopsworks where the wheel file is located.</p> <p> TYPE: <code>str</code> </p> <code>await_installation</code> <p>If <code>True</code> the method returns only when the installation finishes.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Library</code> <p>The library object.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/environment/#hopsworks_common.environment.Environment.install_requirements","title":"[source]  install_requirements","text":"<pre><code>install_requirements(\n    path: str, await_installation: bool | None = True\n) -&gt; Library\n</code></pre> <p>Install libraries specified in a <code>requirements.txt</code> file.</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# Upload to Hopsworks\nds_api = project.get_dataset_api()\nrequirements_path = ds_api.upload(\"requirements.txt\", \"Resources\")\n\n# Install\nenv_api = project.get_environment_api()\nenv = env_api.get_environment(\"my_custom_environment\")\n\nenv.install_requirements(requirements_path)\n</code></pre> PARAMETER DESCRIPTION <code>path</code> <p>The path in Hopsworks where the <code>requirements.txt</code> file is located.</p> <p> TYPE: <code>str</code> </p> <code>await_installation</code> <p>If <code>True</code> the method returns only when the installation is finished.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Library</code> <p>The library object.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/environment/#hopsworks_common.environment.Environment.delete","title":"[source]  delete","text":"<pre><code>delete()\n</code></pre> <p>Delete the environment.</p> Potentially dangerous operation <p>This operation deletes the python environment.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/execution/","title":"hopsworks.execution","text":""},{"location":"python-api/hopsworks/execution/#hopsworks.execution","title":"hopsworks.execution","text":""},{"location":"python-api/hopsworks/execution/#hopsworks_common.execution.Execution","title":"[source]  Execution","text":"<p>                 For backwards compatibility                 <code>hopsworks.execution.Execution</code>                 is still available as <code>hsfs.core.execution.Execution</code>.                   The use of this alias is discouraged as it is to be deprecated.               </p> Returned by"},{"location":"python-api/hopsworks/execution/#hopsworks_common.execution.Execution.id","title":"[source]  id  <code>property</code>","text":"<p>Id of the execution.</p>"},{"location":"python-api/hopsworks/execution/#hopsworks_common.execution.Execution.job_name","title":"[source]  job_name  <code>property</code>","text":"<p>Name of the job the execution belongs to.</p>"},{"location":"python-api/hopsworks/execution/#hopsworks_common.execution.Execution.job_type","title":"[source]  job_type  <code>property</code>","text":"<p>Type of the job the execution belongs to.</p>"},{"location":"python-api/hopsworks/execution/#hopsworks_common.execution.Execution.state","title":"[source]  state  <code>property</code>","text":"<p>Current state of the execution.</p> <p>Can be: <code>INITIALIZING</code>, <code>INITIALIZATION_FAILED</code>, <code>FINISHED</code>, <code>RUNNING</code>, <code>ACCEPTED</code>, <code>FAILED</code>, <code>KILLED</code>, <code>NEW</code>, <code>NEW_SAVING</code>, <code>SUBMITTED</code>, <code>AGGREGATING_LOGS</code>, <code>FRAMEWORK_FAILURE</code>, <code>STARTING_APP_MASTER</code>, <code>APP_MASTER_START_FAILED</code>, <code>GENERATING_SECURITY_MATERIAL</code>, or <code>CONVERTING_NOTEBOOK</code>.</p>"},{"location":"python-api/hopsworks/execution/#hopsworks_common.execution.Execution.final_status","title":"[source]  final_status  <code>property</code>","text":"<p>Final status of the execution. Can be UNDEFINED, SUCCEEDED, FAILED or KILLED.</p>"},{"location":"python-api/hopsworks/execution/#hopsworks_common.execution.Execution.submission_time","title":"[source]  submission_time  <code>property</code>","text":"<p>Timestamp when the execution was submitted.</p>"},{"location":"python-api/hopsworks/execution/#hopsworks_common.execution.Execution.stdout_path","title":"[source]  stdout_path  <code>property</code>","text":"<p>Path in Hopsworks Filesystem to stdout log file.</p>"},{"location":"python-api/hopsworks/execution/#hopsworks_common.execution.Execution.stderr_path","title":"[source]  stderr_path  <code>property</code>","text":"<p>Path in Hopsworks Filesystem to stderr log file.</p>"},{"location":"python-api/hopsworks/execution/#hopsworks_common.execution.Execution.app_id","title":"[source]  app_id  <code>property</code>","text":"<p>Application id for the execution.</p>"},{"location":"python-api/hopsworks/execution/#hopsworks_common.execution.Execution.hdfs_user","title":"[source]  hdfs_user  <code>property</code>","text":"<p>Filesystem user for the execution.</p>"},{"location":"python-api/hopsworks/execution/#hopsworks_common.execution.Execution.args","title":"[source]  args  <code>property</code>","text":"<p>Arguments set for the execution.</p>"},{"location":"python-api/hopsworks/execution/#hopsworks_common.execution.Execution.progress","title":"[source]  progress  <code>property</code>","text":"<p>Progress of the execution.</p>"},{"location":"python-api/hopsworks/execution/#hopsworks_common.execution.Execution.user","title":"[source]  user  <code>property</code>","text":"<p>User that submitted the execution.</p>"},{"location":"python-api/hopsworks/execution/#hopsworks_common.execution.Execution.duration","title":"[source]  duration  <code>property</code>","text":"<p>Duration in milliseconds the execution ran.</p>"},{"location":"python-api/hopsworks/execution/#hopsworks_common.execution.Execution.success","title":"[source]  success  <code>property</code>","text":"<p>Boolean to indicate if execution ran successfully or failed.</p> RETURNS DESCRIPTION <p><code>bool</code>. True if execution ran successfully. False if execution failed or was killed.</p>"},{"location":"python-api/hopsworks/execution/#hopsworks_common.execution.Execution.download_logs","title":"[source]  download_logs","text":"<pre><code>download_logs(\n    path: str | None = None,\n) -&gt; tuple[str | None, str | None]\n</code></pre> <p>Download stdout and stderr logs for the execution.</p> Downloading and printing the logs <pre><code># Download logs\nout_log_path, err_log_path = execution.download_logs()\n\nout_fd = open(out_log_path, \"r\")\nprint(out_fd.read())\n\nerr_fd = open(err_log_path, \"r\")\nprint(err_fd.read())\n</code></pre> PARAMETER DESCRIPTION <code>path</code> <p>path to download the logs.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>stdout</code> <p>Path to downloaded log for stdout.</p> <p> TYPE: <code>str | None</code> </p> <code>stderr</code> <p>Path to downloaded log for stderr.</p> <p> TYPE: <code>str | None</code> </p>"},{"location":"python-api/hopsworks/execution/#hopsworks_common.execution.Execution.delete","title":"[source]  delete","text":"<pre><code>delete()\n</code></pre> <p>Delete the execution.</p> Potentially dangerous operation <p>This operation deletes the execution.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/execution/#hopsworks_common.execution.Execution.stop","title":"[source]  stop","text":"<pre><code>stop()\n</code></pre> <p>Stop the execution.</p> Potentially dangerous operation <p>This operation stops the execution.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/execution/#hopsworks_common.execution.Execution.await_termination","title":"[source]  await_termination","text":"<pre><code>await_termination(timeout: float | None = None)\n</code></pre> <p>Wait until execution terminates.</p> PARAMETER DESCRIPTION <code>timeout</code> <p>The maximum waiting time in seconds. If <code>None</code> the waiting time is unbounded. Note: the actual waiting time may be bigger by approximately 3 seconds.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/execution/#hopsworks_common.execution.Execution.get_url","title":"[source]  get_url","text":"<pre><code>get_url()\n</code></pre> <p>Get url to view execution details in Hopsworks UI.</p>"},{"location":"python-api/hopsworks/flink_cluster/","title":"hopsworks.flink_cluster","text":""},{"location":"python-api/hopsworks/flink_cluster/#hopsworks.flink_cluster","title":"hopsworks.flink_cluster","text":""},{"location":"python-api/hopsworks/flink_cluster/#hopsworks_common.flink_cluster.FlinkCluster","title":"[source]  FlinkCluster","text":"Returned by <ul> <li> <code><code></code>FlinkClusterApi.<code></code>get_cluster</code> </li><li> <code><code></code>FlinkClusterApi.<code></code>setup_cluster</code> </li> </ul>"},{"location":"python-api/hopsworks/flink_cluster/#hopsworks_common.flink_cluster.FlinkCluster.id","title":"[source]  id  <code>property</code>","text":"<p>Id of the cluster.</p>"},{"location":"python-api/hopsworks/flink_cluster/#hopsworks_common.flink_cluster.FlinkCluster.name","title":"[source]  name  <code>property</code>","text":"<p>Name of the cluster.</p>"},{"location":"python-api/hopsworks/flink_cluster/#hopsworks_common.flink_cluster.FlinkCluster.creation_time","title":"[source]  creation_time  <code>property</code>","text":"<p>Date of creation for the cluster.</p>"},{"location":"python-api/hopsworks/flink_cluster/#hopsworks_common.flink_cluster.FlinkCluster.config","title":"[source]  config  <code>property</code>","text":"<p>Configuration for the cluster.</p>"},{"location":"python-api/hopsworks/flink_cluster/#hopsworks_common.flink_cluster.FlinkCluster.creator","title":"[source]  creator  <code>property</code>","text":"<p>Creator of the cluster.</p>"},{"location":"python-api/hopsworks/flink_cluster/#hopsworks_common.flink_cluster.FlinkCluster.state","title":"[source]  state  <code>property</code>","text":"<p>State of the cluster.</p>"},{"location":"python-api/hopsworks/flink_cluster/#hopsworks_common.flink_cluster.FlinkCluster.start","title":"[source]  start","text":"<pre><code>start(await_time=1800)\n</code></pre> <p>Start the flink cluster and wait until it reaches RUNNING state.</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nflink_cluster_api = project.get_flink_cluster_api()\n\nflink_cluster = flink_cluster_api.get_cluster(name=\"myFlinkCluster\")\n\nflink_cluster.start()\n</code></pre> PARAMETER DESCRIPTION <code>await_time</code> <p>defaults to 1800 seconds to account for auto-scale mechanisms.</p> <p> DEFAULT: <code>1800</code> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/flink_cluster/#hopsworks_common.flink_cluster.FlinkCluster.get_jobs","title":"[source]  get_jobs","text":"<pre><code>get_jobs() -&gt; list[dict]\n</code></pre> <p>Get jobs from the flink cluster.</p> <pre><code># log in to hopsworks\nimport hopsworks\nproject = hopsworks.login()\n\n# fetch flink cluster handle\nflink_cluster_api = project.get_flink_cluster_api()\nflink_cluster = flink_cluster_api.get_cluster(name=\"myFlinkCluster\")\n\n# get jobs from this flink cluster\nflink_cluster.get_jobs()\n</code></pre> RETURNS DESCRIPTION <code>list[dict]</code> <p>The array of dictionaries with flink job id and status of the job.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/flink_cluster/#hopsworks_common.flink_cluster.FlinkCluster.get_job","title":"[source]  get_job","text":"<pre><code>get_job(job_id) -&gt; dict\n</code></pre> <p>Get specific job from the flink cluster.</p> <pre><code># log in to hopsworks\nimport hopsworks\nproject = hopsworks.login()\n\n# fetch flink cluster handle\nflink_cluster_api = project.get_flink_cluster_api()\nflink_cluster = flink_cluster_api.get_cluster(name=\"myFlinkCluster\")\n\n# get jobs from this cluster\njob_id = '113a2af5b724a9b92085dc2d9245e1d6'\nflink_cluster.get_job(job_id)\n</code></pre> PARAMETER DESCRIPTION <code>job_id</code> <p>ID of the job within this cluster.</p> <p> </p> RETURNS DESCRIPTION <code>dict</code> <p>A dictionary with flink job id and status of the job.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/flink_cluster/#hopsworks_common.flink_cluster.FlinkCluster.stop_job","title":"[source]  stop_job","text":"<pre><code>stop_job(job_id)\n</code></pre> <p>Stop specific job in the flink cluster.</p> <pre><code># log in to hopsworks\nimport hopsworks\nproject = hopsworks.login()\n\n# fetch flink cluster handle\nflink_cluster_api = project.get_flink_cluster_api()\nflink_cluster = flink_cluster_api.get_cluster(name=\"myFlinkCluster\")\n\n# stop the job\njob_id = '113a2af5b724a9b92085dc2d9245e1d6'\nflink_cluster.stop_job(job_id)\n</code></pre> PARAMETER DESCRIPTION <code>job_id</code> <p>ID of the job within this flink cluster.</p> <p> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/flink_cluster/#hopsworks_common.flink_cluster.FlinkCluster.get_jars","title":"[source]  get_jars","text":"<pre><code>get_jars() -&gt; list[dict]\n</code></pre> <p>Get already uploaded jars from the flink cluster.</p> <pre><code># log in to hopsworks\nimport hopsworks\nproject = hopsworks.login()\n\n# fetch flink cluster handle\nflink_cluster_api = project.get_flink_cluster_api()\nflink_cluster = flink_cluster_api.get_cluster(name=\"myFlinkCluster\")\n\n# get jar files from this cluster\nflink_cluster.get_jars()\n</code></pre> RETURNS DESCRIPTION <code>list[dict]</code> <p>The array of dictionaries with jar metadata.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/flink_cluster/#hopsworks_common.flink_cluster.FlinkCluster.upload_jar","title":"[source]  upload_jar","text":"<pre><code>upload_jar(jar_file)\n</code></pre> <p>Upload jar file to the flink cluster.</p> <pre><code># log in to hopsworks\nimport hopsworks\nproject = hopsworks.login()\n\n# fetch flink cluster handle\nflink_cluster_api = project.get_flink_cluster_api()\nflink_cluster = flink_cluster_api.get_cluster(name=\"myFlinkCluster\")\n\n# upload jar file to this cluster\njar_file_path = \"./flink-example.jar\"\nflink_cluster.upload_jar(jar_file_path)\n</code></pre> PARAMETER DESCRIPTION <code>jar_file</code> <p>path to the jar file.</p> <p> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/flink_cluster/#hopsworks_common.flink_cluster.FlinkCluster.submit_job","title":"[source]  submit_job","text":"<pre><code>submit_job(jar_id, main_class, job_arguments=None) -&gt; str\n</code></pre> <p>Submit job using the specific jar file uploaded to the flink cluster.</p> <pre><code># log in to hopsworks\nimport hopsworks\nproject = hopsworks.login()\n\n# fetch flink cluster handle\nflink_cluster_api = project.get_flink_cluster_api()\nflink_cluster = flink_cluster_api.get_cluster(name=\"myFlinkCluster\")\n\n# upload jar file to this cluster\nmain_class = \"com.example.Main\"\njob_arguments = \"-arg1 arg1 -arg2 arg2\"\njar_file_path = \"./flink-example.jar\"\nflink_cluster.upload_jar(jar_file_path)\n\n#get jar file metadata (and select the 1st one for demo purposes)\njar_metadata = flink_cluster.get_jars()[0]\njar_id = jar_metadata[\"id\"]\nflink_cluster.submit_job(jar_id, main_class, job_arguments=job_arguments)\n</code></pre> PARAMETER DESCRIPTION <code>jar_id</code> <p>ID of the jar file.</p> <p> </p> <code>main_class</code> <p>Path to the main class of the jar file.</p> <p> </p> <code>job_arguments</code> <p>Job arguments, if any.</p> <p> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Job ID.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/flink_cluster/#hopsworks_common.flink_cluster.FlinkCluster.job_state","title":"[source]  job_state","text":"<pre><code>job_state(\n    job_id,\n) -&gt; Literal[\n    \"INITIALIZING\",\n    \"CREATED\",\n    \"RUNNING\",\n    \"FAILING\",\n    \"FAILED\",\n    \"CANCELLING\",\n    \"CANCELED\",\n    \"FINISHED\",\n    \"RESTARTING\",\n    \"SUSPENDED\",\n    \"RECONCILING\",\n]\n</code></pre> <p>Gets state of the job submitted to the flink cluster.</p> <pre><code># log in to hopsworks\nimport hopsworks\nproject = hopsworks.login()\n\n# fetch flink cluster handle\nflink_cluster_api = project.get_flink_cluster_api()\nflink_cluster = flink_cluster_api.get_cluster(name=\"myFlinkCluster\")\n\n# get jobs from this flink cluster\njob_id = '113a2af5b724a9b92085dc2d9245e1d6'\nflink_cluster.job_state(job_id)\n</code></pre> PARAMETER DESCRIPTION <code>job_id</code> <p>ID of the job within this flink cluster.</p> <p> </p> RETURNS DESCRIPTION <code>Literal['INITIALIZING', 'CREATED', 'RUNNING', 'FAILING', 'FAILED', 'CANCELLING', 'CANCELED', 'FINISHED', 'RESTARTING', 'SUSPENDED', 'RECONCILING']</code> <p>Status of the job.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/flink_cluster/#hopsworks_common.flink_cluster.FlinkCluster.stop","title":"[source]  stop","text":"<pre><code>stop()\n</code></pre> <p>Stop this cluster.</p> <pre><code># log in to hopsworks\nimport hopsworks\nproject = hopsworks.login()\n\n# fetch flink cluster handle\nflink_cluster_api = project.get_flink_cluster_api()\n\nflink_cluster = flink_cluster_api.get_cluster(name=\"myFlinkCluster\")\n\nflink_cluster.stop()\n</code></pre> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/flink_cluster/#hopsworks_common.flink_cluster.FlinkCluster.get_url","title":"[source]  get_url","text":"<pre><code>get_url()\n</code></pre> <p>Get url to the flink cluster in Hopsworks.</p>"},{"location":"python-api/hopsworks/git_provider/","title":"hopsworks.git_provider","text":""},{"location":"python-api/hopsworks/git_provider/#hopsworks.git_provider","title":"hopsworks.git_provider","text":""},{"location":"python-api/hopsworks/git_provider/#hopsworks_common.git_provider.GitProvider","title":"[source]  GitProvider","text":"Returned by <ul> <li> <code><code></code>GitApi.<code></code>get_provider</code> </li><li> <code><code></code>GitApi.<code></code>get_providers</code> </li> </ul>"},{"location":"python-api/hopsworks/git_provider/#hopsworks_common.git_provider.GitProvider.username","title":"[source]  username  <code>property</code>","text":"<p>Username set for the provider.</p>"},{"location":"python-api/hopsworks/git_provider/#hopsworks_common.git_provider.GitProvider.git_provider","title":"[source]  git_provider  <code>property</code>","text":"<p>Name of the provider, can be GitHub, GitLab or BitBucket.</p>"},{"location":"python-api/hopsworks/git_provider/#hopsworks_common.git_provider.GitProvider.host","title":"[source]  host  <code>property</code>","text":"<p>Host of the provider, can be for example github.com for GitHub, gitlab.com for GitLab or bitbucket.org for BitBucket.</p>"},{"location":"python-api/hopsworks/git_provider/#hopsworks_common.git_provider.GitProvider.delete","title":"[source]  delete","text":"<pre><code>delete()\n</code></pre> <p>Remove the git provider configuration.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request</p>"},{"location":"python-api/hopsworks/git_remote/","title":"hopsworks.git_remote","text":""},{"location":"python-api/hopsworks/git_remote/#hopsworks.git_remote","title":"hopsworks.git_remote","text":""},{"location":"python-api/hopsworks/git_remote/#hopsworks_common.git_remote.GitRemote","title":"[source]  GitRemote","text":"Returned by <ul> <li> <code><code></code>GitRepo.<code></code>add_remote</code> </li><li> <code><code></code>GitRepo.<code></code>get_remote</code> </li><li> <code><code></code>GitRepo.<code></code>get_remotes</code> </li> </ul>"},{"location":"python-api/hopsworks/git_remote/#hopsworks_common.git_remote.GitRemote.name","title":"[source]  name  <code>property</code>","text":"<p>Name of the remote.</p>"},{"location":"python-api/hopsworks/git_remote/#hopsworks_common.git_remote.GitRemote.url","title":"[source]  url  <code>property</code>","text":"<p>Url of the remote.</p>"},{"location":"python-api/hopsworks/git_remote/#hopsworks_common.git_remote.GitRemote.delete","title":"[source]  delete","text":"<pre><code>delete()\n</code></pre> <p>Remove the git remote from the repo.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request</p>"},{"location":"python-api/hopsworks/git_repo/","title":"hopsworks.git_repo","text":""},{"location":"python-api/hopsworks/git_repo/#hopsworks.git_repo","title":"hopsworks.git_repo","text":""},{"location":"python-api/hopsworks/git_repo/#hopsworks_common.git_repo.GitRepo","title":"[source]  GitRepo","text":"Returned by <ul> <li> <code><code></code>GitApi.<code></code>clone</code> </li><li> <code><code></code>GitApi.<code></code>get_repo</code> </li><li> <code><code></code>GitApi.<code></code>get_repos</code> </li> </ul>"},{"location":"python-api/hopsworks/git_repo/#hopsworks_common.git_repo.GitRepo.id","title":"[source]  id  <code>property</code>","text":"<p>Id of the git repo.</p>"},{"location":"python-api/hopsworks/git_repo/#hopsworks_common.git_repo.GitRepo.name","title":"[source]  name  <code>property</code>","text":"<p>Name of the git repo.</p>"},{"location":"python-api/hopsworks/git_repo/#hopsworks_common.git_repo.GitRepo.path","title":"[source]  path  <code>property</code>","text":"<p>Path to the git repo in the Hopsworks Filesystem.</p>"},{"location":"python-api/hopsworks/git_repo/#hopsworks_common.git_repo.GitRepo.creator","title":"[source]  creator  <code>property</code>","text":"<p>Creator of the git repo.</p>"},{"location":"python-api/hopsworks/git_repo/#hopsworks_common.git_repo.GitRepo.provider","title":"[source]  provider  <code>property</code>","text":"<p>Git provider for the repo, can be GitHub, GitLab or BitBucket.</p>"},{"location":"python-api/hopsworks/git_repo/#hopsworks_common.git_repo.GitRepo.current_branch","title":"[source]  current_branch  <code>property</code>","text":"<p>The current branch for the git repo.</p>"},{"location":"python-api/hopsworks/git_repo/#hopsworks_common.git_repo.GitRepo.current_commit","title":"[source]  current_commit  <code>property</code>","text":"<p>The current commit for the git repo.</p>"},{"location":"python-api/hopsworks/git_repo/#hopsworks_common.git_repo.GitRepo.read_only","title":"[source]  read_only  <code>property</code>","text":"<p>If True then the repository functions <code>GitRepo.commit</code>, <code>GitRepo.push</code> and <code>GitRepo.checkout_files</code> are forbidden.</p>"},{"location":"python-api/hopsworks/git_repo/#hopsworks_common.git_repo.GitRepo.status","title":"[source]  status","text":"<pre><code>status()\n</code></pre> <p>Get the status of the repo.</p> RETURNS DESCRIPTION <p><code>List[GitFileStatus]</code></p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/git_repo/#hopsworks_common.git_repo.GitRepo.delete","title":"[source]  delete","text":"<pre><code>delete()\n</code></pre> <p>Delete the git repo from the filesystem.</p> Potentially dangerous operation <p>This operation deletes the cloned git repository from the filesystem.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request</p>"},{"location":"python-api/hopsworks/git_repo/#hopsworks_common.git_repo.GitRepo.checkout_branch","title":"[source]  checkout_branch","text":"<pre><code>checkout_branch(branch: str, create: bool = False)\n</code></pre> <p>Checkout a branch.</p> PARAMETER DESCRIPTION <code>branch</code> <p>Name of the branch.</p> <p> TYPE: <code>str</code> </p> <code>create</code> <p>If <code>True</code>, creates a new branch and checks it out.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/git_repo/#hopsworks_common.git_repo.GitRepo.checkout_commit","title":"[source]  checkout_commit","text":"<pre><code>checkout_commit(commit: str)\n</code></pre> <p>Checkout a commit.</p> PARAMETER DESCRIPTION <code>commit</code> <p>Hash of the commit.</p> <p> TYPE: <code>str</code> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/git_repo/#hopsworks_common.git_repo.GitRepo.checkout_files","title":"[source]  checkout_files","text":"<pre><code>checkout_files(files: list[str] | list[GitFileStatus])\n</code></pre> <p>Checkout a list of files.</p> PARAMETER DESCRIPTION <code>files</code> <p>List of files or GitFileStatus objects to checkout.</p> <p> TYPE: <code>list[str] | list[GitFileStatus]</code> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/git_repo/#hopsworks_common.git_repo.GitRepo.delete_branch","title":"[source]  delete_branch","text":"<pre><code>delete_branch(branch: str)\n</code></pre> <p>Delete a branch from local repository.</p> PARAMETER DESCRIPTION <code>branch</code> <p>Name of the branch to delete.</p> <p> TYPE: <code>str</code> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/git_repo/#hopsworks_common.git_repo.GitRepo.commit","title":"[source]  commit","text":"<pre><code>commit(\n    message: str, all: bool = True, files: list[str] = None\n)\n</code></pre> <p>Add changes and new files, and then commit them.</p> PARAMETER DESCRIPTION <code>message</code> <p>Commit message.</p> <p> TYPE: <code>str</code> </p> <code>all</code> <p>Automatically stage files that have been modified and deleted, but new files are not affected.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>files</code> <p>List of new files to add and commit.</p> <p> TYPE: <code>list[str]</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/git_repo/#hopsworks_common.git_repo.GitRepo.push","title":"[source]  push","text":"<pre><code>push(branch: str, remote: str = 'origin')\n</code></pre> <p>Push changes to the remote branch.</p> PARAMETER DESCRIPTION <code>branch</code> <p>Name of the branch.</p> <p> TYPE: <code>str</code> </p> <code>remote</code> <p>Name of the remote.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'origin'</code> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/git_repo/#hopsworks_common.git_repo.GitRepo.pull","title":"[source]  pull","text":"<pre><code>pull(branch: str, remote: str = 'origin')\n</code></pre> <p>Pull changes from remote branch.</p> PARAMETER DESCRIPTION <code>branch</code> <p>Name of the branch.</p> <p> TYPE: <code>str</code> </p> <code>remote</code> <p>Name of the remote.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'origin'</code> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/git_repo/#hopsworks_common.git_repo.GitRepo.fetch","title":"[source]  fetch","text":"<pre><code>fetch(remote: str = None, branch: str = None)\n</code></pre> <p>Fetch changes from remote.</p> PARAMETER DESCRIPTION <code>remote</code> <p>Name of the remote.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>branch</code> <p>Name of the branch.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/git_repo/#hopsworks_common.git_repo.GitRepo.reset","title":"[source]  reset","text":"<pre><code>reset(\n    remote: str = None,\n    branch: str = None,\n    commit: str = None,\n)\n</code></pre> <p>Reset the branch to a specific commit or to a local branch or to a remote branch.</p> PARAMETER DESCRIPTION <code>remote</code> <p>Name of the remote.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>branch</code> <p>Name of the branch.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>commit</code> <p>Hash of the commit.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/git_repo/#hopsworks_common.git_repo.GitRepo.get_commits","title":"[source]  get_commits","text":"<pre><code>get_commits(branch: str) -&gt; list[git_commit.GitCommit]\n</code></pre> <p>Get the commits for the repo and branch.</p> PARAMETER DESCRIPTION <code>branch</code> <p>Name of the branch.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>list[git_commit.GitCommit]</code> <p>The list of commits for this repo.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/git_repo/#hopsworks_common.git_repo.GitRepo.add_remote","title":"[source]  add_remote","text":"<pre><code>add_remote(name: str, url: str) -&gt; GitRemote\n</code></pre> <p>Add a remote for the repo.</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\ngit_api = project.get_git_api()\n\nrepo = git_api.get_repo(\"my_repo\")\n\nrepo.add_remote(\"upstream\", \"https://github.com/organization/repo.git\")\n</code></pre> PARAMETER DESCRIPTION <code>name</code> <p>Name of the remote.</p> <p> TYPE: <code>str</code> </p> <code>url</code> <p>Url of the remote.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>GitRemote</code> <p>The created remote.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/git_repo/#hopsworks_common.git_repo.GitRepo.get_remote","title":"[source]  get_remote","text":"<pre><code>get_remote(name: str) -&gt; GitRemote\n</code></pre> <p>Get a remote by name for the repo.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the remote.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>GitRemote</code> <p>The git remote metadata object or <code>None</code> if it does not exist.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/git_repo/#hopsworks_common.git_repo.GitRepo.get_remotes","title":"[source]  get_remotes","text":"<pre><code>get_remotes() -&gt; list[GitRemote]\n</code></pre> <p>Get the configured remotes for the repo.</p> RETURNS DESCRIPTION <code>list[GitRemote]</code> <p>All remotes of the git repo.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/job/","title":"hopsworks.job","text":""},{"location":"python-api/hopsworks/job/#hopsworks.job","title":"hopsworks.job","text":""},{"location":"python-api/hopsworks/job/#hopsworks_common.job.Job","title":"[source]  Job","text":"<p>                 For backwards compatibility                 <code>hopsworks.job.Job</code>                 is still available as <code>hsfs.core.job.Job</code>.                   The use of this alias is discouraged as it is to be deprecated.               </p> Returned by <ul> <li> <code><code></code>JobApi.<code></code>create_job</code> </li><li> <code><code></code>JobApi.<code></code>get_job</code> </li><li> <code><code></code>JobApi.<code></code>get_jobs</code> </li><li> <code><code></code>FeatureGroup.<code></code>sink_job</code> </li><li> <code><code></code>Job.<code></code>save</code> </li> <li> <code><code></code>FeatureGroup.<code></code>materialization_job</code> </li><li> <code><code></code>FeatureGroup.<code></code>insert</code> </li><li> <code><code></code>FeatureGroup.<code></code>multi_part_insert</code> </li><li> <code><code></code>FeatureGroup.<code></code>save</code> </li><li> <code><code></code>FeatureView.<code></code>create_train_test_split</code> </li><li> <code><code></code>FeatureView.<code></code>create_train_validation_test_split</code> </li><li> <code><code></code>FeatureView.<code></code>create_training_data</code> </li><li> <code><code></code>FeatureView.<code></code>log</code> </li><li> <code><code></code>FeatureView.<code></code>materialize_log</code> </li><li> <code><code></code>FeatureView.<code></code>recreate_training_dataset</code> </li> </ul>"},{"location":"python-api/hopsworks/job/#hopsworks_common.job.Job.id","title":"[source]  id  <code>property</code>","text":"<p>Id of the job.</p>"},{"location":"python-api/hopsworks/job/#hopsworks_common.job.Job.name","title":"[source]  name  <code>property</code>","text":"<p>Name of the job.</p>"},{"location":"python-api/hopsworks/job/#hopsworks_common.job.Job.creation_time","title":"[source]  creation_time  <code>property</code>","text":"<p>Date of creation for the job.</p>"},{"location":"python-api/hopsworks/job/#hopsworks_common.job.Job.job_type","title":"[source]  job_type  <code>property</code>","text":"<p>Type of the job.</p>"},{"location":"python-api/hopsworks/job/#hopsworks_common.job.Job.creator","title":"[source]  creator  <code>property</code>","text":"<p>Creator of the job.</p>"},{"location":"python-api/hopsworks/job/#hopsworks_common.job.Job.job_schedule","title":"[source]  job_schedule  <code>property</code>","text":"<p>Return the Job schedule.</p>"},{"location":"python-api/hopsworks/job/#hopsworks_common.job.Job.executions","title":"[source]  executions  <code>property</code>","text":"<p>List of executions for the job.</p>"},{"location":"python-api/hopsworks/job/#hopsworks_common.job.Job.href","title":"[source]  href  <code>property</code>","text":"<p>The URL of the job in Hopsworks UI, use <code>get_url</code> instead.</p>"},{"location":"python-api/hopsworks/job/#hopsworks_common.job.Job.config","title":"[source]  config  <code>property</code>","text":"<p>Configuration for the job.</p>"},{"location":"python-api/hopsworks/job/#hopsworks_common.job.Job.run","title":"[source]  run","text":"<pre><code>run(args: str = None, await_termination: bool = True)\n</code></pre> <p>Run the job.</p> <p>Run the job, by default awaiting its completion, with the option of passing runtime arguments.</p> Example <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instances\nfg = fs.get_or_create_feature_group(...)\n\n# insert in to feature group\njob, _ = fg.insert(df, write_options={\"start_offline_materialization\": False})\n\n# run job\nexecution = job.run()\n\n# True if job executed successfully\nprint(execution.success)\n\n# Download logs\nout_log_path, err_log_path = execution.download_logs()\n</code></pre> PARAMETER DESCRIPTION <code>args</code> <p>Optional runtime arguments for the job.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>await_termination</code> <p>Identifies if the client should wait for the job to complete.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <p><code>Execution</code>: The execution object for the submitted run.</p>"},{"location":"python-api/hopsworks/job/#hopsworks_common.job.Job.get_state","title":"[source]  get_state","text":"<pre><code>get_state() -&gt; Literal[\n    \"UNDEFINED\",\n    \"INITIALIZING\",\n    \"INITIALIZATION_FAILED\",\n    \"FINISHED\",\n    \"RUNNING\",\n    \"ACCEPTED\",\n    \"FAILED\",\n    \"KILLED\",\n    \"NEW\",\n    \"NEW_SAVING\",\n    \"SUBMITTED\",\n    \"AGGREGATING_LOGS\",\n    \"FRAMEWORK_FAILURE\",\n    \"STARTING_APP_MASTER\",\n    \"APP_MASTER_START_FAILED\",\n    \"GENERATING_SECURITY_MATERIAL\",\n    \"CONVERTING_NOTEBOOK\",\n]\n</code></pre> <p>Get the state of the job.</p> RETURNS DESCRIPTION <code>Literal['UNDEFINED', 'INITIALIZING', 'INITIALIZATION_FAILED', 'FINISHED', 'RUNNING', 'ACCEPTED', 'FAILED', 'KILLED', 'NEW', 'NEW_SAVING', 'SUBMITTED', 'AGGREGATING_LOGS', 'FRAMEWORK_FAILURE', 'STARTING_APP_MASTER', 'APP_MASTER_START_FAILED', 'GENERATING_SECURITY_MATERIAL', 'CONVERTING_NOTEBOOK']</code> <p>The current state of the job.</p> <code>Literal['UNDEFINED', 'INITIALIZING', 'INITIALIZATION_FAILED', 'FINISHED', 'RUNNING', 'ACCEPTED', 'FAILED', 'KILLED', 'NEW', 'NEW_SAVING', 'SUBMITTED', 'AGGREGATING_LOGS', 'FRAMEWORK_FAILURE', 'STARTING_APP_MASTER', 'APP_MASTER_START_FAILED', 'GENERATING_SECURITY_MATERIAL', 'CONVERTING_NOTEBOOK']</code> <p>If no executions are found for the job, a warning is raised and it returns <code>UNDEFINED</code>.</p>"},{"location":"python-api/hopsworks/job/#hopsworks_common.job.Job.get_final_state","title":"[source]  get_final_state","text":"<pre><code>get_final_state() -&gt; Literal[\n    \"UNDEFINED\",\n    \"FINISHED\",\n    \"FAILED\",\n    \"KILLED\",\n    \"FRAMEWORK_FAILURE\",\n    \"APP_MASTER_START_FAILED\",\n    \"INITIALIZATION_FAILED\",\n]\n</code></pre> <p>Get the final state of the job.</p> RETURNS DESCRIPTION <code>Literal['UNDEFINED', 'FINISHED', 'FAILED', 'KILLED', 'FRAMEWORK_FAILURE', 'APP_MASTER_START_FAILED', 'INITIALIZATION_FAILED']</code> <p>The final state of the job.</p> <code>Literal['UNDEFINED', 'FINISHED', 'FAILED', 'KILLED', 'FRAMEWORK_FAILURE', 'APP_MASTER_START_FAILED', 'INITIALIZATION_FAILED']</code> <p><code>UNDEFINED</code> indicates that the job is still running.</p>"},{"location":"python-api/hopsworks/job/#hopsworks_common.job.Job.get_executions","title":"[source]  get_executions","text":"<pre><code>get_executions()\n</code></pre> <p>Retrieves all executions for the job ordered by submission time.</p> RETURNS DESCRIPTION <p><code>List[Execution]</code>: List of Execution objects.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/job/#hopsworks_common.job.Job.save","title":"[source]  save","text":"<pre><code>save() -&gt; Job\n</code></pre> <p>Save the job.</p> <p>This function should be called after changing a property such as the job configuration to save it persistently.</p> <pre><code>job.config['appPath'] = \"Resources/my_app.py\"\njob.save()\n</code></pre> RETURNS DESCRIPTION <code>Job</code> <p>The updated job object.</p>"},{"location":"python-api/hopsworks/job/#hopsworks_common.job.Job.delete","title":"[source]  delete","text":"<pre><code>delete()\n</code></pre> <p>Delete the job.</p> Potentially dangerous operation <p>This operation deletes the job and all executions.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/job/#hopsworks_common.job.Job.schedule","title":"[source]  schedule","text":"<pre><code>schedule(\n    cron_expression: str,\n    start_time: datetime = None,\n    end_time: datetime = None,\n) -&gt; JobSchedule\n</code></pre> <p>Schedule the execution of the job.</p> <p>If a schedule for this job already exists, the method updates it.</p> <pre><code># Schedule the job\njob.schedule(\n    cron_expression=\"0 */5 * ? * * *\",\n    start_time=datetime.datetime.now(tz=timezone.utc)\n)\n\n# Retrieve the next execution time\nprint(job.job_schedule.next_execution_date_time)\n</code></pre> PARAMETER DESCRIPTION <code>cron_expression</code> <p>The quartz cron expression.</p> <p> TYPE: <code>str</code> </p> <code>start_time</code> <p>The schedule start time in UTC. If <code>None</code>, the current time is used. The <code>start_time</code> can be a value in the past.</p> <p> TYPE: <code>datetime</code> DEFAULT: <code>None</code> </p> <code>end_time</code> <p>The schedule end time in UTC. If <code>None</code>, the schedule will continue running indefinitely. The <code>end_time</code> can be a value in the past.</p> <p> TYPE: <code>datetime</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>JobSchedule</code> <p>The schedule of the job</p>"},{"location":"python-api/hopsworks/job/#hopsworks_common.job.Job.unschedule","title":"[source]  unschedule","text":"<pre><code>unschedule()\n</code></pre> <p>Unschedule the exceution of a Job.</p>"},{"location":"python-api/hopsworks/job/#hopsworks_common.job.Job.resume_schedule","title":"[source]  resume_schedule","text":"<pre><code>resume_schedule()\n</code></pre> <p>Resumes the schedule of a Job execution.</p>"},{"location":"python-api/hopsworks/job/#hopsworks_common.job.Job.pause_schedule","title":"[source]  pause_schedule","text":"<pre><code>pause_schedule()\n</code></pre> <p>Pauses the schedule of a Job execution.</p>"},{"location":"python-api/hopsworks/job/#hopsworks_common.job.Job.get_alerts","title":"[source]  get_alerts","text":"<pre><code>get_alerts() -&gt; list[alert.JobAlert]\n</code></pre> <p>Get all alerts for the job.</p> RETURNS DESCRIPTION <code>list[alert.JobAlert]</code> <p>List of JobAlert objects.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/job/#hopsworks_common.job.Job.get_alert","title":"[source]  get_alert","text":"<pre><code>get_alert(alert_id: int) -&gt; alert.JobAlert\n</code></pre> <p>Get an alert for the job by ID.</p> PARAMETER DESCRIPTION <code>alert_id</code> <p>ID of the alert.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>alert.JobAlert</code> <p>The JobAlert object.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/job/#hopsworks_common.job.Job.create_alert","title":"[source]  create_alert","text":"<pre><code>create_alert(\n    receiver: str,\n    status: Literal[\n        \"long_running\", \"failed\", \"finished\", \"killed\"\n    ],\n    severity: Literal[\"critical\", \"warning\", \"info\"],\n) -&gt; alert.JobAlert\n</code></pre> <p>Create an alert for the job.</p> <pre><code># Create alert for the job\njob.create_alert(\n    receiver=\"email\",\n    status=\"failed\",\n    severity=\"critical\"\n)\n</code></pre> PARAMETER DESCRIPTION <code>receiver</code> <p>The receiver of the alert.</p> <p> TYPE: <code>str</code> </p> <code>status</code> <p>The status of the alert.</p> <p> TYPE: <code>Literal['long_running', 'failed', 'finished', 'killed']</code> </p> <code>severity</code> <p>The severity of the alert.</p> <p> TYPE: <code>Literal['critical', 'warning', 'info']</code> </p> RETURNS DESCRIPTION <code>alert.JobAlert</code> <p>The created JobAlert object.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If <code>status</code> or <code>severity</code> is not valid.</p> <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/job/#hopsworks_common.job.Job.get_url","title":"[source]  get_url","text":"<pre><code>get_url()\n</code></pre> <p>Get url to the job in Hopsworks.</p>"},{"location":"python-api/hopsworks/kafka_schema/","title":"hopsworks.kafka_schema","text":""},{"location":"python-api/hopsworks/kafka_schema/#hopsworks.kafka_schema","title":"hopsworks.kafka_schema","text":""},{"location":"python-api/hopsworks/kafka_schema/#hopsworks_common.kafka_schema.KafkaSchema","title":"[source]  KafkaSchema","text":"Returned by <ul> <li> <code><code></code>KafkaApi.<code></code>create_schema</code> </li><li> <code><code></code>KafkaApi.<code></code>get_schema</code> </li><li> <code><code></code>KafkaApi.<code></code>get_schemas</code> </li> </ul>"},{"location":"python-api/hopsworks/kafka_schema/#hopsworks_common.kafka_schema.KafkaSchema.id","title":"[source]  id  <code>property</code>","text":"<p>Id of the kafka schema.</p>"},{"location":"python-api/hopsworks/kafka_schema/#hopsworks_common.kafka_schema.KafkaSchema.subject","title":"[source]  subject  <code>property</code>","text":"<p>Name of the subject for the schema.</p>"},{"location":"python-api/hopsworks/kafka_schema/#hopsworks_common.kafka_schema.KafkaSchema.version","title":"[source]  version  <code>property</code>","text":"<p>Version of the schema.</p>"},{"location":"python-api/hopsworks/kafka_schema/#hopsworks_common.kafka_schema.KafkaSchema.schema","title":"[source]  schema  <code>property</code>","text":"<p>Schema definition.</p>"},{"location":"python-api/hopsworks/kafka_schema/#hopsworks_common.kafka_schema.KafkaSchema.delete","title":"[source]  delete","text":"<pre><code>delete()\n</code></pre> <p>Delete the schema.</p> Potentially dangerous operation <p>This operation deletes the schema.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request</p>"},{"location":"python-api/hopsworks/kafka_topic/","title":"hopsworks.kafka_topic","text":""},{"location":"python-api/hopsworks/kafka_topic/#hopsworks.kafka_topic","title":"hopsworks.kafka_topic","text":""},{"location":"python-api/hopsworks/kafka_topic/#hopsworks_common.kafka_topic.KafkaTopic","title":"[source]  KafkaTopic","text":"<p>Configuration for a Kafka topic.</p> <p>                 For backwards compatibility                 <code>hopsworks.kafka_topic.KafkaTopic</code>                 is still available as <code>hsml.kafka_topic.KafkaTopic</code>.                   The use of this alias is discouraged as it is to be deprecated.               </p> Returned by <ul> <li> <code><code></code>KafkaApi.<code></code>create_topic</code> </li><li> <code><code></code>KafkaApi.<code></code>get_topic</code> </li><li> <code><code></code>KafkaApi.<code></code>get_topics</code> </li> </ul>"},{"location":"python-api/hopsworks/kafka_topic/#hopsworks_common.kafka_topic.KafkaTopic.name","title":"[source]  name  <code>property</code> <code>writable</code>","text":"<p>Name of the Kafka topic.</p>"},{"location":"python-api/hopsworks/kafka_topic/#hopsworks_common.kafka_topic.KafkaTopic.replicas","title":"[source]  replicas  <code>property</code>","text":"<p>Number of replicas of the Kafka topic.</p>"},{"location":"python-api/hopsworks/kafka_topic/#hopsworks_common.kafka_topic.KafkaTopic.num_replicas","title":"[source]  num_replicas  <code>property</code> <code>writable</code>","text":"<p>Number of replicas of the Kafka topic.</p>"},{"location":"python-api/hopsworks/kafka_topic/#hopsworks_common.kafka_topic.KafkaTopic.partitions","title":"[source]  partitions  <code>property</code>","text":"<p>Number of partitions of the Kafka topic.</p>"},{"location":"python-api/hopsworks/kafka_topic/#hopsworks_common.kafka_topic.KafkaTopic.num_partitions","title":"[source]  num_partitions  <code>property</code>","text":"<p>Number of partitions of the Kafka topic.</p>"},{"location":"python-api/hopsworks/kafka_topic/#hopsworks_common.kafka_topic.KafkaTopic.schema","title":"[source]  schema  <code>property</code>","text":"<p>Schema for the topic.</p>"},{"location":"python-api/hopsworks/kafka_topic/#hopsworks_common.kafka_topic.KafkaTopic.describe","title":"[source]  describe","text":"<pre><code>describe()\n</code></pre> <p>Print a JSON description of the Kafka topic.</p>"},{"location":"python-api/hopsworks/kafka_topic/#hopsworks_common.kafka_topic.KafkaTopic.delete","title":"[source]  delete","text":"<pre><code>delete()\n</code></pre> <p>Delete the topic.</p> Potentially dangerous operation <p>This operation deletes the topic.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request</p>"},{"location":"python-api/hopsworks/project/","title":"hopsworks.project","text":""},{"location":"python-api/hopsworks/project/#hopsworks.project","title":"hopsworks.project","text":""},{"location":"python-api/hopsworks/project/#hopsworks_common.project.Project","title":"[source]  Project","text":"<p>Class representing a Hopsworks project.</p> <p>Use <code>hopsworks.login</code> to get the current project after logging in.</p> <p>Use <code>hopsworks.create_project</code> to create a new project and get the project object.</p> Returned by <ul> <li> <code><code></code>hopsworks.<code></code>create_project</code> </li><li> <code><code></code>hopsworks.<code></code>get_current_project</code> </li><li> <code><code></code>hopsworks.<code></code>login</code> </li> </ul>"},{"location":"python-api/hopsworks/project/#hopsworks_common.project.Project.id","title":"[source]  id  <code>property</code>","text":"<p>Id of the project.</p>"},{"location":"python-api/hopsworks/project/#hopsworks_common.project.Project.name","title":"[source]  name  <code>property</code>","text":"<p>Name of the project.</p>"},{"location":"python-api/hopsworks/project/#hopsworks_common.project.Project.owner","title":"[source]  owner  <code>property</code>","text":"<p>Owner of the project.</p>"},{"location":"python-api/hopsworks/project/#hopsworks_common.project.Project.description","title":"[source]  description  <code>property</code>","text":"<p>Description of the project.</p>"},{"location":"python-api/hopsworks/project/#hopsworks_common.project.Project.created","title":"[source]  created  <code>property</code>","text":"<p>Timestamp when the project was created.</p>"},{"location":"python-api/hopsworks/project/#hopsworks_common.project.Project.project_namespace","title":"[source]  project_namespace  <code>property</code>","text":"<p>Kubernetes namespace used by project.</p>"},{"location":"python-api/hopsworks/project/#hopsworks_common.project.Project.get_feature_store","title":"[source]  get_feature_store","text":"<pre><code>get_feature_store(name: str | None = None)\n</code></pre> <p>Connect to Project's Feature Store.</p> <p>Defaulting to the project name of default feature store. To get a shared feature store, the project name of the feature store is required.</p> Example for getting the Feature Store API of a project <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nfs = project.get_feature_store()\n</code></pre> PARAMETER DESCRIPTION <code>name</code> <p>Project name of the feature store.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <p>hsfs.feature_store.FeatureStore: The Feature Store API.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/project/#hopsworks_common.project.Project.get_model_registry","title":"[source]  get_model_registry","text":"<pre><code>get_model_registry()\n</code></pre> <p>Connect to Project's Model Registry API.</p> Example for getting the Model Registry API of a project <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nmr = project.get_model_registry()\n</code></pre> RETURNS DESCRIPTION <p>hsml.model_registry.ModelRegistry: The Model Registry API.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/project/#hopsworks_common.project.Project.get_model_serving","title":"[source]  get_model_serving","text":"<pre><code>get_model_serving()\n</code></pre> <p>Connect to Project's Model Serving API.</p> Example for getting the Model Serving API of a project <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nms = project.get_model_serving()\n</code></pre> RETURNS DESCRIPTION <p>hsml.model_serving.ModelServing: The Model Serving API.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/project/#hopsworks_common.project.Project.get_kafka_api","title":"[source]  get_kafka_api","text":"<pre><code>get_kafka_api() -&gt; kafka_api.KafkaApi\n</code></pre> <p>Get the kafka api for the project.</p> RETURNS DESCRIPTION <code>kafka_api.KafkaApi</code> <p>The Kafka Api handle.</p>"},{"location":"python-api/hopsworks/project/#hopsworks_common.project.Project.get_opensearch_api","title":"[source]  get_opensearch_api","text":"<pre><code>get_opensearch_api() -&gt; opensearch_api.OpenSearchApi\n</code></pre> <p>Get the opensearch api for the project.</p> RETURNS DESCRIPTION <code>opensearch_api.OpenSearchApi</code> <p>The OpenSearch Api handle.</p>"},{"location":"python-api/hopsworks/project/#hopsworks_common.project.Project.get_job_api","title":"[source]  get_job_api","text":"<pre><code>get_job_api() -&gt; job_api.JobApi\n</code></pre> <p>Get the job API for the project.</p> RETURNS DESCRIPTION <code>job_api.JobApi</code> <p>The Job Api handle.</p>"},{"location":"python-api/hopsworks/project/#hopsworks_common.project.Project.get_flink_cluster_api","title":"[source]  get_flink_cluster_api","text":"<pre><code>get_flink_cluster_api() -&gt; (\n    flink_cluster_api.FlinkClusterApi\n)\n</code></pre> <p>Get the flink cluster API for the project.</p> RETURNS DESCRIPTION <code>flink_cluster_api.FlinkClusterApi</code> <p>The Flink Cluster Api handle.</p>"},{"location":"python-api/hopsworks/project/#hopsworks_common.project.Project.get_git_api","title":"[source]  get_git_api","text":"<pre><code>get_git_api() -&gt; git_api.GitApi\n</code></pre> <p>Get the git repository api for the project.</p> RETURNS DESCRIPTION <code>git_api.GitApi</code> <p>The Git Api handle.</p>"},{"location":"python-api/hopsworks/project/#hopsworks_common.project.Project.get_dataset_api","title":"[source]  get_dataset_api","text":"<pre><code>get_dataset_api() -&gt; dataset_api.DatasetApi\n</code></pre> <p>Get the dataset api for the project.</p> RETURNS DESCRIPTION <code>dataset_api.DatasetApi</code> <p>The Datasets Api handle.</p>"},{"location":"python-api/hopsworks/project/#hopsworks_common.project.Project.get_environment_api","title":"[source]  get_environment_api","text":"<pre><code>get_environment_api() -&gt; environment_api.EnvironmentApi\n</code></pre> <p>Get the Python environment API for the project.</p> RETURNS DESCRIPTION <code>environment_api.EnvironmentApi</code> <p>The Python Environment Api handle.</p>"},{"location":"python-api/hopsworks/project/#hopsworks_common.project.Project.get_alerts_api","title":"[source]  get_alerts_api","text":"<pre><code>get_alerts_api() -&gt; alerts_api.AlertsApi\n</code></pre> <p>Get the alerts api for the project.</p> RETURNS DESCRIPTION <code>alerts_api.AlertsApi</code> <p>The Alerts Api handle.</p>"},{"location":"python-api/hopsworks/project/#hopsworks_common.project.Project.get_search_api","title":"[source]  get_search_api","text":"<pre><code>get_search_api() -&gt; search_api.SearchApi\n</code></pre> <p>Get the search api for the project.</p> RETURNS DESCRIPTION <code>search_api.SearchApi</code> <p>The Search Api handle.</p>"},{"location":"python-api/hopsworks/project/#hopsworks_common.project.Project.get_alerts","title":"[source]  get_alerts","text":"<pre><code>get_alerts() -&gt; list[alert.ProjectAlert]\n</code></pre> <p>Get all alerts for the project.</p> RETURNS DESCRIPTION <code>list[alert.ProjectAlert]</code> <p>List of <code>ProjectAlert</code> objects.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/project/#hopsworks_common.project.Project.get_alert","title":"[source]  get_alert","text":"<pre><code>get_alert(alert_id: int) -&gt; alert.ProjectAlert | None\n</code></pre> <p>Get an alert for the project by ID.</p> PARAMETER DESCRIPTION <code>alert_id</code> <p>The ID of the alert.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>alert.ProjectAlert | None</code> <p>The ProjectAlert object.</p>"},{"location":"python-api/hopsworks/project/#hopsworks_common.project.Project.create_job_alert","title":"[source]  create_job_alert","text":"<pre><code>create_job_alert(\n    receiver: str,\n    status: Literal[\n        \"job_finished\",\n        \"job_failed\",\n        \"job_killed\",\n        \"job_long_running\",\n    ],\n    severity: Literal[\"critical\", \"warning\", \"info\"],\n) -&gt; alert.ProjectAlert\n</code></pre> <p>Create an alert for jobs in this project.</p> Example for creating a job alert <pre><code>import hopsworks\nproject = hopsworks.login()\nproject.create_job_alert(\"my_receiver\", \"long_running\", \"info\")\n</code></pre> PARAMETER DESCRIPTION <code>receiver</code> <p>The receiver of the alert.</p> <p> TYPE: <code>str</code> </p> <code>status</code> <p>The status of the alert.</p> <p> TYPE: <code>Literal['job_finished', 'job_failed', 'job_killed', 'job_long_running']</code> </p> <code>severity</code> <p>The severity of the alert.</p> <p> TYPE: <code>Literal['critical', 'warning', 'info']</code> </p> RETURNS DESCRIPTION <code>alert.ProjectAlert</code> <p>The created <code>ProjectAlert</code> object.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If <code>status</code> or <code>severity</code> is invalid, also if <code>receiver</code> is <code>None</code>.</p> <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/project/#hopsworks_common.project.Project.create_featurestore_alert","title":"[source]  create_featurestore_alert","text":"<pre><code>create_featurestore_alert(\n    receiver: str,\n    status: Literal[\n        \"feature_validation_success\",\n        \"feature_validation_warning\",\n        \"feature_validation_failure\",\n        \"feature_monitor_shift_undetected\",\n        \"feature_monitor_shift_detected\",\n    ],\n    severity: Literal[\"critical\", \"warning\", \"info\"],\n) -&gt; alert.ProjectAlert\n</code></pre> <p>Create an alert for feature validation and monitoring in this project.</p> Example for creating a featurestore alert <pre><code>import hopsworks\nproject = hopsworks.login()\nproject.create_featurestore_alert(\"my_receiver\", \"feature_validation_success\", \"info\")\n</code></pre> PARAMETER DESCRIPTION <code>receiver</code> <p>The receiver of the alert.</p> <p> TYPE: <code>str</code> </p> <code>status</code> <p>The status of the alert.</p> <p> TYPE: <code>Literal['feature_validation_success', 'feature_validation_warning', 'feature_validation_failure', 'feature_monitor_shift_undetected', 'feature_monitor_shift_detected']</code> </p> <code>severity</code> <p>The severity of the alert.</p> <p> TYPE: <code>Literal['critical', 'warning', 'info']</code> </p> RETURNS DESCRIPTION <code>alert.ProjectAlert</code> <p>The created <code>ProjectAlert</code> object.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If <code>status</code> or <code>severity</code> is invalid, also if <code>receiver</code> is <code>None</code>.</p> <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hopsworks/project/#hopsworks_common.project.Project.get_url","title":"[source]  get_url","text":"<pre><code>get_url()\n</code></pre> <p>Get url to the project in Hopsworks.</p>"},{"location":"python-api/hopsworks/secret/","title":"hopsworks.secret","text":""},{"location":"python-api/hopsworks/secret/#hopsworks.secret","title":"hopsworks.secret","text":""},{"location":"python-api/hopsworks/secret/#hopsworks_common.secret.Secret","title":"[source]  Secret","text":"<p>Represents a secret in Hopsworks.</p> <p>Use <code>SecretsApi</code> to manage secrets; namely you can create a secret with <code>SecretsApi.create</code> and get secrets with <code>SecretsApi.get_secret</code> and <code>SecretsApi.get_secrets</code>.</p> Returned by <ul> <li> <code><code></code>SecretsApi.<code></code>create_secret</code> </li><li> <code><code></code>SecretsApi.<code></code>get_secret</code> </li><li> <code><code></code>SecretsApi.<code></code>get_secrets</code> </li> </ul>"},{"location":"python-api/hopsworks/secret/#hopsworks_common.secret.Secret.name","title":"[source]  name  <code>property</code>","text":"<p>Name of the secret.</p>"},{"location":"python-api/hopsworks/secret/#hopsworks_common.secret.Secret.value","title":"[source]  value  <code>property</code>","text":"<p>Value of the secret.</p>"},{"location":"python-api/hopsworks/secret/#hopsworks_common.secret.Secret.created","title":"[source]  created  <code>property</code>","text":"<p>Date when secret was created.</p>"},{"location":"python-api/hopsworks/secret/#hopsworks_common.secret.Secret.visibility","title":"[source]  visibility  <code>property</code>","text":"<p>Visibility of the secret.</p>"},{"location":"python-api/hopsworks/secret/#hopsworks_common.secret.Secret.scope","title":"[source]  scope  <code>property</code>","text":"<p>Scope of the secret.</p>"},{"location":"python-api/hopsworks/secret/#hopsworks_common.secret.Secret.owner","title":"[source]  owner  <code>property</code>","text":"<p>Owner of the secret.</p>"},{"location":"python-api/hopsworks/secret/#hopsworks_common.secret.Secret.delete","title":"[source]  delete","text":"<pre><code>delete()\n</code></pre> <p>Delete the secret.</p> Potentially dangerous operation <p>This operation deletes the secret and may break applications using it.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request</p>"},{"location":"python-api/hopsworks/secret/#hopsworks_common.secret.Secret.get_url","title":"[source]  get_url","text":"<pre><code>get_url()\n</code></pre> <p>Get url to the secret in Hopsworks.</p>"},{"location":"python-api/hsfs/constructor/query/","title":"hsfs.constructor.query","text":""},{"location":"python-api/hsfs/constructor/query/#hsfs.constructor.query","title":"hsfs.constructor.query","text":""},{"location":"python-api/hsfs/constructor/query/#hsfs.constructor.query.Query","title":"[source]  Query","text":""},{"location":"python-api/hsfs/constructor/query/#hsfs.constructor.query.Query.left_feature_group_start_time","title":"[source]  left_feature_group_start_time  <code>property</code> <code>writable</code>","text":"<pre><code>left_feature_group_start_time: (\n    str | int | date | datetime | None\n)\n</code></pre> <p>Start time of time travel for the left feature group.</p>"},{"location":"python-api/hsfs/constructor/query/#hsfs.constructor.query.Query.left_feature_group_end_time","title":"[source]  left_feature_group_end_time  <code>property</code> <code>writable</code>","text":"<pre><code>left_feature_group_end_time: (\n    str | int | date | datetime | None\n)\n</code></pre> <p>End time of time travel for the left feature group.</p>"},{"location":"python-api/hsfs/constructor/query/#hsfs.constructor.query.Query.joins","title":"[source]  joins  <code>property</code>","text":"<pre><code>joins: list[join.Join]\n</code></pre> <p>List of joins in the query.</p>"},{"location":"python-api/hsfs/constructor/query/#hsfs.constructor.query.Query.featuregroups","title":"[source]  featuregroups  <code>property</code>","text":"<pre><code>featuregroups: list[\n    fg_mod.FeatureGroup\n    | fg_mod.ExternalFeatureGroup\n    | fg_mod.SpineGroup\n]\n</code></pre> <p>List of feature groups used in the query.</p>"},{"location":"python-api/hsfs/constructor/query/#hsfs.constructor.query.Query.filters","title":"[source]  filters  <code>property</code>","text":"<pre><code>filters: Logic | None\n</code></pre> <p>All filters used in the query.</p>"},{"location":"python-api/hsfs/constructor/query/#hsfs.constructor.query.Query.features","title":"[source]  features  <code>property</code>","text":"<pre><code>features: list[Feature]\n</code></pre> <p>List of all features in the query.</p>"},{"location":"python-api/hsfs/constructor/query/#hsfs.constructor.query.Query.check_and_warn_ambiguous_features","title":"[source]  check_and_warn_ambiguous_features","text":"<pre><code>check_and_warn_ambiguous_features() -&gt; None\n</code></pre> <p>Function that fetches ambiguous features from a query and displays a warning.</p>"},{"location":"python-api/hsfs/constructor/query/#hsfs.constructor.query.Query.get_ambiguous_features","title":"[source]  get_ambiguous_features","text":"<pre><code>get_ambiguous_features() -&gt; dict[str, set[str]]\n</code></pre> <p>Function to check ambiguous features in the query. The function will return a dictionary with feature name of the ambiguous features as key and list feature groups they are in as value.</p> RETURNS DESCRIPTION <code>dict[str, set[str]]</code> <p><code>Dict[str, List[str]]</code>: Dictionary with ambiguous feature name as key and corresponding set of feature group names and version as value.</p>"},{"location":"python-api/hsfs/constructor/query/#hsfs.constructor.query.Query.read","title":"[source]  read","text":"<pre><code>read(\n    online: bool = False,\n    dataframe_type: str = \"default\",\n    read_options: dict[str, Any] | None = None,\n) -&gt; (\n    pd.DataFrame\n    | np.ndarray\n    | list[list[Any]]\n    | TypeVar(\"pyspark.sql.DataFrame\")\n    | TypeVar(\"pyspark.RDD\")\n)\n</code></pre> <p>Read the specified query into a DataFrame.</p> <p>It is possible to specify the storage (online/offline) to read from and the type of the output DataFrame (Spark, Pandas, Numpy, Python Lists).</p> External Feature Group Engine Support <p>Spark only</p> <p>Reading a Query containing an External Feature Group directly into a Pandas Dataframe using Python/Pandas as Engine is not supported, however, you can use the Query API to create Feature Views/Training Data containing External Feature Groups.</p> PARAMETER DESCRIPTION <code>online</code> <p>Read from online storage. Defaults to <code>False</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>dataframe_type</code> <p>DataFrame type to return. Defaults to <code>\"default\"</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default'</code> </p> <code>read_options</code> <p>Dictionary of read options for Spark in spark engine. Only for python engine: * key <code>\"arrow_flight_config\"</code> to pass a dictionary of arrow flight configurations.   For example: <code>{\"arrow_flight_config\": {\"timeout\": 900}}</code> Defaults to <code>{}</code>.</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>pd.DataFrame | np.ndarray | list[list[Any]] | TypeVar('pyspark.sql.DataFrame') | TypeVar('pyspark.RDD')</code> <p><code>DataFrame</code>: DataFrame depending on the chosen type.</p>"},{"location":"python-api/hsfs/constructor/query/#hsfs.constructor.query.Query.show","title":"[source]  show","text":"<pre><code>show(n: int, online: bool = False) -&gt; list[list[Any]]\n</code></pre> <p>Show the first N rows of the Query.</p> Show the first 10 rows <pre><code>fg1 = fs.get_feature_group(\"...\")\nfg2 = fs.get_feature_group(\"...\")\n\nquery = fg1.select_all().join(fg2.select_all())\n\nquery.show(10)\n</code></pre> PARAMETER DESCRIPTION <code>n</code> <p>Number of rows to show.</p> <p> TYPE: <code>int</code> </p> <code>online</code> <p>Show from online storage. Defaults to <code>False</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"python-api/hsfs/constructor/query/#hsfs.constructor.query.Query.join","title":"[source]  join","text":"<pre><code>join(\n    sub_query: Query,\n    on: list[str] | None = None,\n    left_on: list[str] | None = None,\n    right_on: list[str] | None = None,\n    join_type: str | None = \"left\",\n    prefix: str | None = None,\n) -&gt; Query\n</code></pre> <p>Join Query with another Query.</p> <p>If no join keys are specified, Hopsworks will use the maximal matching subset of the primary keys of the feature groups you are joining.</p> Join two feature groups <pre><code>fg1 = fs.get_feature_group(\"...\")\nfg2 = fs.get_feature_group(\"...\")\n\nquery = fg1.select_all().join(fg2.select_all())\n</code></pre> More complex join <pre><code>fg1 = fs.get_feature_group(\"...\")\nfg2 = fs.get_feature_group(\"...\")\nfg3 = fs.get_feature_group(\"...\")\n\nquery = fg1.select_all()\n        .join(fg2.select_all(), on=[\"date\", \"location_id\"])\n        .join(fg3.select_all(), left_on=[\"location_id\"], right_on=[\"id\"], join_type=\"left\")\n</code></pre> PARAMETER DESCRIPTION <code>sub_query</code> <p>Right-hand side query to join.</p> <p> TYPE: <code>Query</code> </p> <code>on</code> <p>List of feature names to join on if they are available in both feature groups. Defaults to <code>[]</code>.</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>left_on</code> <p>List of feature names to join on from the left feature group of the join. Defaults to <code>[]</code>.</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>right_on</code> <p>List of feature names to join on from the right feature group of the join. Defaults to <code>[]</code>.</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>join_type</code> <p>Type of join to perform, can be <code>\"inner\"</code>, <code>\"outer\"</code>, <code>\"left\"</code> or <code>\"right\"</code>. Defaults to \"left\".</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>'left'</code> </p> <code>prefix</code> <p>User provided prefix to avoid feature name clash. If no prefix was provided and there is feature name clash then prefixes will be automatically generated and applied. Generated prefix is feature group alias in the query (e.g. fg1, fg2). Prefix is applied to the right feature group of the query. Defaults to <code>None</code>.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Query</code> <p><code>Query</code>: A new Query object representing the join.</p>"},{"location":"python-api/hsfs/constructor/query/#hsfs.constructor.query.Query.as_of","title":"[source]  as_of","text":"<pre><code>as_of(\n    wallclock_time: str\n    | int\n    | datetime\n    | date\n    | None = None,\n    exclude_until: str\n    | int\n    | datetime\n    | date\n    | None = None,\n) -&gt; Query\n</code></pre> <p>Perform time travel on the given Query.</p> Pyspark/Spark Only <p>Apache HUDI exclusively supports Time Travel and Incremental Query via Spark Context</p> <p>This method returns a new Query object at the specified point in time. Optionally, commits before a specified point in time can be excluded from the query. The Query can then either be read into a Dataframe or used further to perform joins or construct a training dataset.</p> Reading features at a specific point in time <pre><code>fs = connection.get_feature_store();\nquery = fs.get_feature_group(\"example_feature_group\", 1).select_all()\nquery.as_of(\"2020-10-20 07:34:11\").read().show()\n</code></pre> Reading commits incrementally between specified points in time <pre><code>fs = connection.get_feature_store();\nquery = fs.get_feature_group(\"example_feature_group\", 1).select_all()\nquery.as_of(\"2020-10-20 07:34:11\", exclude_until=\"2020-10-19 07:34:11\").read().show()\n</code></pre> <p>The first parameter is inclusive while the latter is exclusive. That means, in order to query a single commit, you need to query that commit time and exclude everything just before the commit.</p> Reading only the changes from a single commit <pre><code>fs = connection.get_feature_store();\nquery = fs.get_feature_group(\"example_feature_group\", 1).select_all()\nquery.as_of(\"2020-10-20 07:31:38\", exclude_until=\"2020-10-20 07:31:37\").read().show()\n</code></pre> <p>When no wallclock_time is given, the latest state of features is returned. Optionally, commits before a specified point in time can still be excluded.</p> Reading the latest state of features, excluding commits before a specified point in time <pre><code>fs = connection.get_feature_store();\nquery = fs.get_feature_group(\"example_feature_group\", 1).select_all()\nquery.as_of(None, exclude_until=\"2020-10-20 07:31:38\").read().show()\n</code></pre> <p>Note that the interval will be applied to all joins in the query. If you want to query different intervals for different feature groups in the query, you have to apply them in a nested fashion: <pre><code>query1.as_of(..., ...)\n    .join(query2.as_of(..., ...))\n</code></pre></p> <p>If instead you apply another <code>as_of</code> selection after the join, all joined feature groups will be queried with this interval: <pre><code>query1.as_of(..., ...)  # as_of is not applied\n    .join(query2.as_of(..., ...))  # as_of is not applied\n    .as_of(..., ...)\n</code></pre></p> Warning <p>This function only works for queries on feature groups with time_travel_format='HUDI'.</p> Warning <p>Excluding commits via exclude_until is only possible within the range of the Hudi active timeline. By default, Hudi keeps the last 20 to 30 commits in the active timeline. If you need to keep a longer active timeline, you can overwrite the options: <code>hoodie.keep.min.commits</code> and <code>hoodie.keep.max.commits</code> when calling the <code>insert()</code> method.</p> PARAMETER DESCRIPTION <code>wallclock_time</code> <p>Read data as of this point in time. Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, or <code>%Y-%m-%d %H:%M:%S</code>.</p> <p> TYPE: <code>str | int | datetime | date | None</code> DEFAULT: <code>None</code> </p> <code>exclude_until</code> <p>Exclude commits until this point in time. Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, or <code>%Y-%m-%d %H:%M:%S</code>.</p> <p> TYPE: <code>str | int | datetime | date | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Query</code> <p><code>Query</code>. The query object with the applied time travel condition.</p>"},{"location":"python-api/hsfs/constructor/query/#hsfs.constructor.query.Query.pull_changes","title":"[source]  pull_changes","text":"<pre><code>pull_changes(\n    wallclock_start_time: str | int | date | datetime,\n    wallclock_end_time: str | int | date | datetime,\n) -&gt; Query\n</code></pre> <p>Same as <code>as_of</code> method, kept for backward compatibility.</p> Deprecated <p><code>pull_changes</code> method is deprecated. Use `as_of(end_wallclock_time, exclude_until=start_wallclock_time) instead.</p>"},{"location":"python-api/hsfs/constructor/query/#hsfs.constructor.query.Query.filter","title":"[source]  filter","text":"<pre><code>filter(f: Filter | Logic) -&gt; Query\n</code></pre> <p>Apply filter to the feature group.</p> <p>Selects all features and returns the resulting <code>Query</code> with the applied filter.</p> Example <pre><code>from hsfs.feature import Feature\n\nquery.filter(Feature(\"weekly_sales\") &gt; 1000)\nquery.filter(Feature(\"name\").like(\"max%\"))\n</code></pre> <p>If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group: <pre><code>query.filter(fg.feature1 == 1).show(10)\n</code></pre></p> <p>Composite filters require parenthesis and symbols for logical operands (e.g. <code>&amp;</code>, <code>|</code>, ...): <pre><code>query.filter((fg.feature1 == 1) | (fg.feature2 &gt;= 2))\n</code></pre></p> Filters are fully compatible with joins <pre><code>fg1 = fs.get_feature_group(\"...\")\nfg2 = fs.get_feature_group(\"...\")\nfg3 = fs.get_feature_group(\"...\")\n\nquery = fg1.select_all()\n    .join(fg2.select_all(), on=[\"date\", \"location_id\"])\n    .join(fg3.select_all(), left_on=[\"location_id\"], right_on=[\"id\"], join_type=\"left\")\n    .filter((fg1.location_id == 10) | (fg1.location_id == 20))\n</code></pre> Filters can be applied at any point of the query <pre><code>fg1 = fs.get_feature_group(\"...\")\nfg2 = fs.get_feature_group(\"...\")\nfg3 = fs.get_feature_group(\"...\")\n\nquery = fg1.select_all()\n    .join(fg2.select_all().filter(fg2.avg_temp &gt;= 22), on=[\"date\", \"location_id\"])\n    .join(fg3.select_all(), left_on=[\"location_id\"], right_on=[\"id\"], join_type=\"left\")\n    .filter(fg1.location_id == 10)\n</code></pre> PARAMETER DESCRIPTION <code>f</code> <p>Filter object.</p> <p> TYPE: <code>Filter | Logic</code> </p> RETURNS DESCRIPTION <code>Query</code> <p><code>Query</code>. The query object with the applied filter.</p>"},{"location":"python-api/hsfs/constructor/query/#hsfs.constructor.query.Query.to_string","title":"[source]  to_string","text":"<pre><code>to_string(\n    online: bool = False, arrow_flight: bool = False\n) -&gt; str\n</code></pre> <p>Convert the Query to its string representation.</p> Example <pre><code>fg1 = fs.get_feature_group(\"...\")\nfg2 = fs.get_feature_group(\"...\").\n\nquery = fg1.select_all().join(fg2.select_all())\n\nquery.to_string()\n</code></pre>"},{"location":"python-api/hsfs/constructor/query/#hsfs.constructor.query.Query.append_feature","title":"[source]  append_feature","text":"<pre><code>append_feature(feature: str | Feature) -&gt; Query\n</code></pre> <p>Append a feature to the query.</p> PARAMETER DESCRIPTION <code>feature</code> <p><code>[str, Feature]</code>. Name of the feature to append to the query.</p> <p> TYPE: <code>str | Feature</code> </p>"},{"location":"python-api/hsfs/constructor/query/#hsfs.constructor.query.Query.is_time_travel","title":"[source]  is_time_travel","text":"<pre><code>is_time_travel() -&gt; bool\n</code></pre> <p>Query contains time travel.</p>"},{"location":"python-api/hsfs/constructor/query/#hsfs.constructor.query.Query.is_cache_feature_group_only","title":"[source]  is_cache_feature_group_only","text":"<pre><code>is_cache_feature_group_only() -&gt; bool\n</code></pre> <p>Query contains only cached feature groups.</p>"},{"location":"python-api/hsfs/constructor/query/#hsfs.constructor.query.Query.get_feature","title":"[source]  get_feature","text":"<pre><code>get_feature(feature_name: str) -&gt; Feature\n</code></pre> <p>Get a feature by name.</p> PARAMETER DESCRIPTION <code>feature_name</code> <p><code>str</code>. Name of the feature to get.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Feature</code> <p><code>Feature</code>. Feature object.</p>"},{"location":"python-api/hsfs/core/data_source/","title":"hsfs.core.data_source","text":""},{"location":"python-api/hsfs/core/data_source/#hsfs.core.data_source","title":"hsfs.core.data_source","text":""},{"location":"python-api/hsfs/core/data_source/#hsfs.core.data_source.DataSource","title":"[source]  DataSource","text":"<p>Metadata object used to provide data source information.</p> <p>You can obtain data sources using [<code>FeatureStore.get_data_source</code>][hsfs.feature_store.FeatureStore.get_data_source].</p> <p>The DataSource class encapsulates the details of a data source that can be used for reading or writing data. It supports various types of sources, such as SQL queries, database tables, file paths, and storage connectors.</p> ATTRIBUTE DESCRIPTION <code>_query</code> <p>SQL query string for the data source, if applicable.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>_database</code> <p>Name of the database containing the data source.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>_group</code> <p>Group or schema name for the data source.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>_table</code> <p>Table name for the data source.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>_path</code> <p>File system path for the data source.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>_storage_connector</code> <p>Storage connector object holds configuration for accessing the data source.</p> <p> TYPE: <code>Optional[StorageConnector]</code> </p> <code>_metrics</code> <p>List of metric column names for the data source.</p> <p> TYPE: <code>List[str]</code> </p> <code>_dimensions</code> <p>List of dimension column names for the data source.</p> <p> TYPE: <code>List[str]</code> </p> <code>_rest_endpoint</code> <p>REST endpoint configuration for the data source.</p> <p> TYPE: <code>Optional[RestEndpointConfig]</code> </p>"},{"location":"python-api/hsfs/core/data_source/#hsfs.core.data_source.DataSource.query","title":"[source]  query  <code>property</code> <code>writable</code>","text":"<pre><code>query: str | None\n</code></pre> <p>Get or set the SQL query string for the data source.</p> RETURNS DESCRIPTION <code>str | None</code> <p>The SQL query string.</p>"},{"location":"python-api/hsfs/core/data_source/#hsfs.core.data_source.DataSource.database","title":"[source]  database  <code>property</code> <code>writable</code>","text":"<pre><code>database: str | None\n</code></pre> <p>Get or set the database name for the data source.</p> RETURNS DESCRIPTION <code>str | None</code> <p>The database name.</p>"},{"location":"python-api/hsfs/core/data_source/#hsfs.core.data_source.DataSource.group","title":"[source]  group  <code>property</code> <code>writable</code>","text":"<pre><code>group: str | None\n</code></pre> <p>Get or set the group/schema name for the data source.</p> RETURNS DESCRIPTION <code>str | None</code> <p>The group or schema name.</p>"},{"location":"python-api/hsfs/core/data_source/#hsfs.core.data_source.DataSource.table","title":"[source]  table  <code>property</code> <code>writable</code>","text":"<pre><code>table: str | None\n</code></pre> <p>Get or set the table name for the data source.</p> RETURNS DESCRIPTION <code>str | None</code> <p>The table name.</p>"},{"location":"python-api/hsfs/core/data_source/#hsfs.core.data_source.DataSource.path","title":"[source]  path  <code>property</code> <code>writable</code>","text":"<pre><code>path: str | None\n</code></pre> <p>Get or set the file system path for the data source.</p> RETURNS DESCRIPTION <code>str | None</code> <p>The file system path.</p>"},{"location":"python-api/hsfs/core/data_source/#hsfs.core.data_source.DataSource.storage_connector","title":"[source]  storage_connector  <code>property</code> <code>writable</code>","text":"<pre><code>storage_connector: sc.StorageConnector | None\n</code></pre> <p>Get or set the storage connector for the data source.</p> RETURNS DESCRIPTION <code>sc.StorageConnector | None</code> <p>The storage connector object.</p>"},{"location":"python-api/hsfs/core/data_source/#hsfs.core.data_source.DataSource.get_databases","title":"[source]  get_databases","text":"<pre><code>get_databases() -&gt; list[str]\n</code></pre> <p>Retrieve the list of available databases.</p> Example <pre><code># connect to the Feature Store\nfs = ...\n\ndata_source = fs.get_data_source(\"test_data_source\")\n\ndatabases = data_source.get_databases()\n</code></pre> RETURNS DESCRIPTION <code>list[str]</code> <p>A list of database names available in the data source.</p>"},{"location":"python-api/hsfs/core/data_source/#hsfs.core.data_source.DataSource.get_tables","title":"[source]  get_tables","text":"<pre><code>get_tables(database: str | None = None) -&gt; list[DataSource]\n</code></pre> <p>Retrieve the list of tables from the specified database.</p> Example <pre><code># connect to the Feature Store\nfs = ...\n\ndata_source = fs.get_data_source(\"test_data_source\")\n\ntables = data_source.get_tables()\n</code></pre> PARAMETER DESCRIPTION <code>database</code> <p>The name of the database to list tables from. If not provided, the default database is used.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>list[DataSource]</code> <p>A list of DataSource objects representing the tables.</p>"},{"location":"python-api/hsfs/core/data_source/#hsfs.core.data_source.DataSource.get_data","title":"[source]  get_data","text":"<pre><code>get_data() -&gt; dsd.DataSourceData\n</code></pre> <p>Retrieve the data from the data source.</p> Example <pre><code># connect to the Feature Store\nfs = ...\n\ntable = fs.get_data_source(\"test_data_source\").get_tables()[0]\n\ndata = table.get_data()\n</code></pre> RETURNS DESCRIPTION <code>dsd.DataSourceData</code> <p>An object containing the data retrieved from the data source.</p>"},{"location":"python-api/hsfs/core/data_source/#hsfs.core.data_source.DataSource.get_metadata","title":"[source]  get_metadata","text":"<pre><code>get_metadata() -&gt; dict\n</code></pre> <p>Retrieve metadata information about the data source.</p> Example <pre><code># connect to the Feature Store\nfs = ...\n\ntable = fs.get_data_source(\"test_data_source\").get_tables()[0]\n\nmetadata = table.get_metadata()\n</code></pre> RETURNS DESCRIPTION <code>dict</code> <p>A dictionary containing metadata about the data source.</p>"},{"location":"python-api/hsfs/core/data_source/#hsfs.core.data_source.DataSource.get_feature_groups_provenance","title":"[source]  get_feature_groups_provenance","text":"<pre><code>get_feature_groups_provenance() -&gt; Links | None\n</code></pre> <p>Get the generated feature groups using this data source, based on explicit provenance.</p> <p>These feature groups can be accessible or inaccessible. Explicit provenance does not track deleted generated feature group links, so deleted will always be empty. For inaccessible feature groups, only a minimal information is returned.</p> RETURNS DESCRIPTION <code>Links | None</code> <p>The feature groups generated using this data source or <code>None</code> if none were created.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>In case the backend encounters an issue.</p>"},{"location":"python-api/hsfs/core/data_source/#hsfs.core.data_source.DataSource.get_feature_groups","title":"[source]  get_feature_groups","text":"<pre><code>get_feature_groups() -&gt; list[fg.FeatureGroup]\n</code></pre> <p>Get the feature groups using this data source, based on explicit provenance.</p> <p>Only the accessible feature groups are returned. For more items use the base method, <code>DataSource.get_feature_groups_provenance</code>.</p> RETURNS DESCRIPTION <code>list[fg.FeatureGroup]</code> <p>List of feature groups.</p>"},{"location":"python-api/hsfs/core/data_source/#hsfs.core.data_source.DataSource.get_training_datasets_provenance","title":"[source]  get_training_datasets_provenance","text":"<pre><code>get_training_datasets_provenance() -&gt; Links\n</code></pre> <p>Get the generated training datasets using this data source, based on explicit provenance.</p> <p>These training datasets can be accessible or inaccessible. Explicit provenance does not track deleted generated training dataset links, so deleted will always be empty. For inaccessible training datasets, only a minimal information is returned.</p> RETURNS DESCRIPTION <code>Links</code> <p>The training datasets generated using this data source or <code>None</code> if none were created.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>In case the backend encounters an issue.</p>"},{"location":"python-api/hsfs/core/data_source/#hsfs.core.data_source.DataSource.get_training_datasets","title":"[source]  get_training_datasets","text":"<pre><code>get_training_datasets() -&gt; list[TrainingDataset]\n</code></pre> <p>Get the training datasets using this data source, based on explicit provenance.</p> <p>Only the accessible training datasets are returned. For more items use the base method, <code>get_training_datasets_provenance</code>.</p> RETURNS DESCRIPTION <code>list[TrainingDataset]</code> <p>List of training datasets.</p>"},{"location":"python-api/hsfs/core/explicit_provenance/","title":"hsfs.core.explicit_provenance","text":""},{"location":"python-api/hsfs/core/explicit_provenance/#hsfs.core.explicit_provenance","title":"hsfs.core.explicit_provenance","text":""},{"location":"python-api/hsfs/core/explicit_provenance/#hsfs.core.explicit_provenance.Artifact","title":"[source]  Artifact","text":""},{"location":"python-api/hsfs/core/explicit_provenance/#hsfs.core.explicit_provenance.Artifact.feature_store_name","title":"[source]  feature_store_name  <code>property</code>","text":"<p>Name of the feature store in which the artifact is located.</p>"},{"location":"python-api/hsfs/core/explicit_provenance/#hsfs.core.explicit_provenance.Artifact.name","title":"[source]  name  <code>property</code>","text":"<p>Name of the artifact.</p>"},{"location":"python-api/hsfs/core/explicit_provenance/#hsfs.core.explicit_provenance.Artifact.version","title":"[source]  version  <code>property</code>","text":"<p>Version of the artifact.</p>"},{"location":"python-api/hsfs/core/explicit_provenance/#hsfs.core.explicit_provenance.Links","title":"[source]  Links","text":""},{"location":"python-api/hsfs/core/explicit_provenance/#hsfs.core.explicit_provenance.Links.deleted","title":"[source]  deleted  <code>property</code>","text":"<p>List of [Artifact objects] which contains minimal information (name, version) about the entities (storage connectors, feature groups, feature views, models) they represent.</p> <p>These entities have been removed from the feature store/model registry.</p>"},{"location":"python-api/hsfs/core/explicit_provenance/#hsfs.core.explicit_provenance.Links.inaccessible","title":"[source]  inaccessible  <code>property</code>","text":"<p>List of [Artifact objects] which contains minimal information (name, version) about the entities (storage connectors, feature groups, feature views, models) they represent.</p> <p>These entities exist in the feature store/model registry, however the user does not have access to them anymore.</p>"},{"location":"python-api/hsfs/core/explicit_provenance/#hsfs.core.explicit_provenance.Links.accessible","title":"[source]  accessible  <code>property</code>","text":"<p>List of [StorageConnectors|FeatureGroups|FeatureViews|Models] objects which are part of the provenance graph requested.</p> <p>These entities exist in the feature store/model registry and the user has access to them.</p>"},{"location":"python-api/hsfs/core/explicit_provenance/#hsfs.core.explicit_provenance.Links.faulty","title":"[source]  faulty  <code>property</code>","text":"<p>List of [Artifact objects] which contains minimal information (name, version) about the entities (storage connectors, feature groups, feature views, models) they represent.</p> <p>These entities exist in the feature store/model registry, however they are corrupted.</p>"},{"location":"python-api/hsfs/core/feature_descriptive_statistics/","title":"hsfs.core.feature_descriptive_statistics","text":""},{"location":"python-api/hsfs/core/feature_descriptive_statistics/#hsfs.core.feature_descriptive_statistics","title":"hsfs.core.feature_descriptive_statistics","text":""},{"location":"python-api/hsfs/core/feature_descriptive_statistics/#hsfs.core.feature_descriptive_statistics.FeatureDescriptiveStatistics","title":"[source]  FeatureDescriptiveStatistics","text":""},{"location":"python-api/hsfs/core/feature_descriptive_statistics/#hsfs.core.feature_descriptive_statistics.FeatureDescriptiveStatistics.id","title":"[source]  id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>ID of the feature descriptive statistics object.</p>"},{"location":"python-api/hsfs/core/feature_descriptive_statistics/#hsfs.core.feature_descriptive_statistics.FeatureDescriptiveStatistics.feature_type","title":"[source]  feature_type  <code>property</code>","text":"<pre><code>feature_type: str\n</code></pre> <p>Data type of the feature. It can be one of Boolean, Fractional, Integral, or String.</p>"},{"location":"python-api/hsfs/core/feature_descriptive_statistics/#hsfs.core.feature_descriptive_statistics.FeatureDescriptiveStatistics.feature_name","title":"[source]  feature_name  <code>property</code>","text":"<pre><code>feature_name: str\n</code></pre> <p>Name of the feature.</p>"},{"location":"python-api/hsfs/core/feature_descriptive_statistics/#hsfs.core.feature_descriptive_statistics.FeatureDescriptiveStatistics.count","title":"[source]  count  <code>property</code>","text":"<pre><code>count: int\n</code></pre> <p>Number of values.</p>"},{"location":"python-api/hsfs/core/feature_descriptive_statistics/#hsfs.core.feature_descriptive_statistics.FeatureDescriptiveStatistics.completeness","title":"[source]  completeness  <code>property</code>","text":"<pre><code>completeness: float | None\n</code></pre> <p>Fraction of non-null values in a column.</p>"},{"location":"python-api/hsfs/core/feature_descriptive_statistics/#hsfs.core.feature_descriptive_statistics.FeatureDescriptiveStatistics.num_non_null_values","title":"[source]  num_non_null_values  <code>property</code>","text":"<pre><code>num_non_null_values: int | None\n</code></pre> <p>Number of non-null values.</p>"},{"location":"python-api/hsfs/core/feature_descriptive_statistics/#hsfs.core.feature_descriptive_statistics.FeatureDescriptiveStatistics.num_null_values","title":"[source]  num_null_values  <code>property</code>","text":"<pre><code>num_null_values: int | None\n</code></pre> <p>Number of null values.</p>"},{"location":"python-api/hsfs/core/feature_descriptive_statistics/#hsfs.core.feature_descriptive_statistics.FeatureDescriptiveStatistics.approx_num_distinct_values","title":"[source]  approx_num_distinct_values  <code>property</code>","text":"<pre><code>approx_num_distinct_values: int | None\n</code></pre> <p>Approximate number of distinct values.</p>"},{"location":"python-api/hsfs/core/feature_descriptive_statistics/#hsfs.core.feature_descriptive_statistics.FeatureDescriptiveStatistics.min","title":"[source]  min  <code>property</code>","text":"<pre><code>min: float | None\n</code></pre> <p>Minimum value.</p>"},{"location":"python-api/hsfs/core/feature_descriptive_statistics/#hsfs.core.feature_descriptive_statistics.FeatureDescriptiveStatistics.max","title":"[source]  max  <code>property</code>","text":"<pre><code>max: float | None\n</code></pre> <p>Maximum value.</p>"},{"location":"python-api/hsfs/core/feature_descriptive_statistics/#hsfs.core.feature_descriptive_statistics.FeatureDescriptiveStatistics.sum","title":"[source]  sum  <code>property</code>","text":"<pre><code>sum: float | None\n</code></pre> <p>Sum of all feature values.</p>"},{"location":"python-api/hsfs/core/feature_descriptive_statistics/#hsfs.core.feature_descriptive_statistics.FeatureDescriptiveStatistics.mean","title":"[source]  mean  <code>property</code>","text":"<pre><code>mean: float | None\n</code></pre> <p>Mean value.</p>"},{"location":"python-api/hsfs/core/feature_descriptive_statistics/#hsfs.core.feature_descriptive_statistics.FeatureDescriptiveStatistics.stddev","title":"[source]  stddev  <code>property</code>","text":"<pre><code>stddev: float | None\n</code></pre> <p>Standard deviation of the feature values.</p>"},{"location":"python-api/hsfs/core/feature_descriptive_statistics/#hsfs.core.feature_descriptive_statistics.FeatureDescriptiveStatistics.percentiles","title":"[source]  percentiles  <code>property</code>","text":"<pre><code>percentiles: Mapping[str, float] | None\n</code></pre> <p>Percentiles.</p>"},{"location":"python-api/hsfs/core/feature_descriptive_statistics/#hsfs.core.feature_descriptive_statistics.FeatureDescriptiveStatistics.distinctness","title":"[source]  distinctness  <code>property</code>","text":"<pre><code>distinctness: float | None\n</code></pre> <p>Fraction of distinct values of a feature over the number of all its values. Distinct values occur at least once.</p> Example <p>[a, a, b] contains two distinct values a and b, so distinctness is 2/3.</p>"},{"location":"python-api/hsfs/core/feature_descriptive_statistics/#hsfs.core.feature_descriptive_statistics.FeatureDescriptiveStatistics.entropy","title":"[source]  entropy  <code>property</code>","text":"<pre><code>entropy: float | None\n</code></pre> <p>Entropy is a measure of the level of information contained in an event (feature value) when considering all possible events (all feature values).</p> <p>Entropy is estimated using observed value counts as the negative sum of (value_count/total_count) * log(value_count/total_count).</p> Example <p>[a, b, b, c, c] has three distinct values with counts [1, 2, 2].</p> <p>Entropy is then (-1/5*log(1/5)-2/5*log(2/5)-2/5*log(2/5)) = 1.055.</p>"},{"location":"python-api/hsfs/core/feature_descriptive_statistics/#hsfs.core.feature_descriptive_statistics.FeatureDescriptiveStatistics.uniqueness","title":"[source]  uniqueness  <code>property</code>","text":"<pre><code>uniqueness: float | None\n</code></pre> <p>Fraction of unique values over the number of all values of a column. Unique values occur exactly once.</p> Example <p>[a, a, b] contains one unique value b, so uniqueness is 1/3.</p>"},{"location":"python-api/hsfs/core/feature_descriptive_statistics/#hsfs.core.feature_descriptive_statistics.FeatureDescriptiveStatistics.exact_num_distinct_values","title":"[source]  exact_num_distinct_values  <code>property</code>","text":"<pre><code>exact_num_distinct_values: int | None\n</code></pre> <p>Exact number of distinct values.</p>"},{"location":"python-api/hsfs/core/feature_descriptive_statistics/#hsfs.core.feature_descriptive_statistics.FeatureDescriptiveStatistics.extended_statistics","title":"[source]  extended_statistics  <code>property</code>","text":"<pre><code>extended_statistics: dict | None\n</code></pre> <p>Additional statistics computed on the feature values such as histograms and correlations.</p>"},{"location":"python-api/hsfs/core/feature_monitoring_config/","title":"hsfs.core.feature_monitoring_config","text":""},{"location":"python-api/hsfs/core/feature_monitoring_config/#hsfs.core.feature_monitoring_config","title":"hsfs.core.feature_monitoring_config","text":""},{"location":"python-api/hsfs/core/feature_monitoring_config/#hsfs.core.feature_monitoring_config.FeatureMonitoringConfig","title":"[source]  FeatureMonitoringConfig","text":""},{"location":"python-api/hsfs/core/feature_monitoring_config/#hsfs.core.feature_monitoring_config.FeatureMonitoringConfig.id","title":"[source]  id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Id of the feature monitoring configuration.</p>"},{"location":"python-api/hsfs/core/feature_monitoring_config/#hsfs.core.feature_monitoring_config.FeatureMonitoringConfig.feature_store_id","title":"[source]  feature_store_id  <code>property</code>","text":"<pre><code>feature_store_id: int\n</code></pre> <p>Id of the Feature Store.</p>"},{"location":"python-api/hsfs/core/feature_monitoring_config/#hsfs.core.feature_monitoring_config.FeatureMonitoringConfig.feature_group_id","title":"[source]  feature_group_id  <code>property</code>","text":"<pre><code>feature_group_id: int | None\n</code></pre> <p>Id of the Feature Group to which this feature monitoring configuration is attached.</p>"},{"location":"python-api/hsfs/core/feature_monitoring_config/#hsfs.core.feature_monitoring_config.FeatureMonitoringConfig.feature_view_name","title":"[source]  feature_view_name  <code>property</code>","text":"<pre><code>feature_view_name: str | None\n</code></pre> <p>Name of the Feature View to which this feature monitoring configuration is attached.</p>"},{"location":"python-api/hsfs/core/feature_monitoring_config/#hsfs.core.feature_monitoring_config.FeatureMonitoringConfig.feature_view_version","title":"[source]  feature_view_version  <code>property</code>","text":"<pre><code>feature_view_version: int | None\n</code></pre> <p>Version of the Feature View to which this feature monitoring configuration is attached.</p>"},{"location":"python-api/hsfs/core/feature_monitoring_config/#hsfs.core.feature_monitoring_config.FeatureMonitoringConfig.feature_name","title":"[source]  feature_name  <code>property</code>","text":"<pre><code>feature_name: str | None\n</code></pre> <p>The name of the feature to monitor.</p> <p>If not set, all features of the Feature Group or Feature View are monitored, only available for scheduled statistics.</p> Info <p>This property is read-only</p>"},{"location":"python-api/hsfs/core/feature_monitoring_config/#hsfs.core.feature_monitoring_config.FeatureMonitoringConfig.name","title":"[source]  name  <code>property</code> <code>writable</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the feature monitoring config.</p> <p>A Feature Group or Feature View cannot have multiple feature monitoring configurations with the same name. The name of a feature monitoring configuration is limited to 63 characters.</p> Info <p>This property is read-only once the feature monitoring configuration has been saved.</p>"},{"location":"python-api/hsfs/core/feature_monitoring_config/#hsfs.core.feature_monitoring_config.FeatureMonitoringConfig.description","title":"[source]  description  <code>property</code> <code>writable</code>","text":"<pre><code>description: str | None\n</code></pre> <p>Description of the feature monitoring configuration.</p>"},{"location":"python-api/hsfs/core/feature_monitoring_config/#hsfs.core.feature_monitoring_config.FeatureMonitoringConfig.job_name","title":"[source]  job_name  <code>property</code>","text":"<pre><code>job_name: str | None\n</code></pre> <p>Name of the feature monitoring job.</p>"},{"location":"python-api/hsfs/core/feature_monitoring_config/#hsfs.core.feature_monitoring_config.FeatureMonitoringConfig.enabled","title":"[source]  enabled  <code>property</code> <code>writable</code>","text":"<pre><code>enabled: bool\n</code></pre> <p>Controls whether or not this config is spawning new feature monitoring jobs.</p> <p>This field belongs to the scheduler configuration but is made transparent to the user for convenience.</p>"},{"location":"python-api/hsfs/core/feature_monitoring_config/#hsfs.core.feature_monitoring_config.FeatureMonitoringConfig.feature_monitoring_type","title":"[source]  feature_monitoring_type  <code>property</code>","text":"<pre><code>feature_monitoring_type: str | None\n</code></pre> <p>The type of feature monitoring to perform. Used for internal validation.</p> Options are <ul> <li>STATISTICS_COMPUTATION if no reference window (and, therefore, comparison config) is provided</li> <li>STATISTICS_COMPARISON if a reference window (and, therefore, comparison config) is provided.</li> </ul> Info <p>This property is read-only.</p>"},{"location":"python-api/hsfs/core/feature_monitoring_config/#hsfs.core.feature_monitoring_config.FeatureMonitoringConfig.job_schedule","title":"[source]  job_schedule  <code>property</code> <code>writable</code>","text":"<pre><code>job_schedule: JobSchedule\n</code></pre> <p>Schedule of the feature monitoring job.</p> <p>This field belongs to the job configuration but is made transparent to the user for convenience.</p>"},{"location":"python-api/hsfs/core/feature_monitoring_config/#hsfs.core.feature_monitoring_config.FeatureMonitoringConfig.detection_window_config","title":"[source]  detection_window_config  <code>property</code> <code>writable</code>","text":"<pre><code>detection_window_config: mwc.MonitoringWindowConfig\n</code></pre> <p>Configuration for the detection window.</p>"},{"location":"python-api/hsfs/core/feature_monitoring_config/#hsfs.core.feature_monitoring_config.FeatureMonitoringConfig.reference_window_config","title":"[source]  reference_window_config  <code>property</code> <code>writable</code>","text":"<pre><code>reference_window_config: mwc.MonitoringWindowConfig\n</code></pre> <p>Configuration for the reference window.</p>"},{"location":"python-api/hsfs/core/feature_monitoring_config/#hsfs.core.feature_monitoring_config.FeatureMonitoringConfig.statistics_comparison_config","title":"[source]  statistics_comparison_config  <code>property</code> <code>writable</code>","text":"<pre><code>statistics_comparison_config: dict[str, Any] | None\n</code></pre> <p>Configuration for the comparison of detection and reference statistics.</p>"},{"location":"python-api/hsfs/core/feature_monitoring_config/#hsfs.core.feature_monitoring_config.FeatureMonitoringConfig.with_detection_window","title":"[source]  with_detection_window","text":"<pre><code>with_detection_window(\n    time_offset: str | None = None,\n    window_length: str | None = None,\n    row_percentage: float | None = None,\n) -&gt; FeatureMonitoringConfig\n</code></pre> <p>Sets the detection window of data to compute statistics on.</p> Example <pre><code># Fetch your feature group or feature view\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n# Compute statistics on a regular basis\nfg.create_statistics_monitoring(\n    name=\"regular_stats\",\n    cron_expression=\"0 0 12 ? * * *\",\n).with_detection_window(\n    time_offset=\"1d\",\n    window_length=\"1d\",\n    row_percentage=0.1,\n).save()\n# Compute and compare statistics\nfg.create_feature_monitoring(\n    name=\"regular_stats\",\n    feature_name=\"my_feature\",\n    cron_expression=\"0 0 12 ? * * *\",\n).with_detection_window(\n    time_offset=\"1d\",\n    window_length=\"1d\",\n    row_percentage=0.1,\n).with_reference_window(...).compare_on(...).save()\n</code></pre> PARAMETER DESCRIPTION <code>time_offset</code> <p>The time offset from the current time to the start of the time window.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>window_length</code> <p>The length of the time window.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>row_percentage</code> <p>The fraction of rows to use when computing the statistics [0, 1.0].</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>FeatureMonitoringConfig</code> <p><code>FeatureMonitoringConfig</code>. The updated FeatureMonitoringConfig object.</p>"},{"location":"python-api/hsfs/core/feature_monitoring_config/#hsfs.core.feature_monitoring_config.FeatureMonitoringConfig.with_reference_window","title":"[source]  with_reference_window","text":"<pre><code>with_reference_window(\n    time_offset: str | None = None,\n    window_length: str | None = None,\n    row_percentage: float | None = None,\n) -&gt; FeatureMonitoringConfig\n</code></pre> <p>Sets the reference window of data to compute statistics on.</p> <p>See also <code>with_reference_value(...)</code> and <code>with_reference_training_dataset(...)</code> for other reference options.</p> Example <pre><code># Fetch your feature group or feature view\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n# Setup feature monitoring and a detection window\nmy_monitoring_config = fg.create_feature_monitoring(...).with_detection_window(...)\n# Statistics computed on a rolling time window, e.g. same day last week\nmy_monitoring_config.with_reference_window(\n    time_offset=\"1w\",\n    window_length=\"1d\",\n).compare_on(...).save()\n</code></pre> Provide a comparison configuration <p>You must provide a comparison configuration via <code>compare_on()</code> before saving the feature monitoring config.</p> PARAMETER DESCRIPTION <code>time_offset</code> <p>The time offset from the current time to the start of the time window.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>window_length</code> <p>The length of the time window.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>row_percentage</code> <p>The percentage of rows to use when computing the statistics. Defaults to 20%.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>FeatureMonitoringConfig</code> <p><code>FeatureMonitoringConfig</code>. The updated FeatureMonitoringConfig object.</p>"},{"location":"python-api/hsfs/core/feature_monitoring_config/#hsfs.core.feature_monitoring_config.FeatureMonitoringConfig.with_reference_value","title":"[source]  with_reference_value","text":"<pre><code>with_reference_value(\n    value: float | None = None,\n) -&gt; FeatureMonitoringConfig\n</code></pre> <p>Sets the reference value to compare statistics with.</p> <p>See also <code>with_reference_window(...)</code> and <code>with_reference_training_dataset(...)</code> for other reference options.</p> Example <pre><code># Fetch your feature group or feature view\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n# Setup feature monitoring and a detection window\nmy_monitoring_config = fg.create_feature_monitoring(...).with_detection_window(...)\n# Simplest reference window is a specific value\nmy_monitoring_config.with_reference_value(\n    value=0.0,\n).compare_on(...).save()\n</code></pre> Provide a comparison configuration <p>You must provide a comparison configuration via <code>compare_on()</code> before saving the feature monitoring config.</p> PARAMETER DESCRIPTION <code>value</code> <p>A float value to use as reference.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>FeatureMonitoringConfig</code> <p><code>FeatureMonitoringConfig</code>. The updated FeatureMonitoringConfig object.</p>"},{"location":"python-api/hsfs/core/feature_monitoring_config/#hsfs.core.feature_monitoring_config.FeatureMonitoringConfig.with_reference_training_dataset","title":"[source]  with_reference_training_dataset","text":"<pre><code>with_reference_training_dataset(\n    training_dataset_version: int | None = None,\n) -&gt; FeatureMonitoringConfig\n</code></pre> <p>Sets the reference training dataset to compare statistics with.</p> <p>See also <code>with_reference_value(...)</code> and <code>with_reference_window(...)</code> for other reference options.</p> Example <pre><code># Fetch your feature group or feature view\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n# Setup feature monitoring and a detection window\nmy_monitoring_config = fg.create_feature_monitoring(...).with_detection_window(...)\n# Only for feature views: Compare to the statistics computed for one of your training datasets\n# particularly useful if it has been used to train a model currently in production\nmy_monitoring_config.with_reference_training_dataset(\n    training_dataset_version=3,\n).compare_on(...).save()\n</code></pre> Provide a comparison configuration <p>You must provide a comparison configuration via <code>compare_on()</code> before saving the feature monitoring config.</p> PARAMETER DESCRIPTION <code>training_dataset_version</code> <p>The version of the training dataset to use as reference.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>FeatureMonitoringConfig</code> <p><code>FeatureMonitoringConfig</code>. The updated FeatureMonitoringConfig object.</p>"},{"location":"python-api/hsfs/core/feature_monitoring_config/#hsfs.core.feature_monitoring_config.FeatureMonitoringConfig.compare_on","title":"[source]  compare_on","text":"<pre><code>compare_on(\n    metric: str | None,\n    threshold: float | None,\n    strict: bool | None = False,\n    relative: bool | None = False,\n) -&gt; FeatureMonitoringConfig\n</code></pre> <p>Sets the statistics comparison criteria for feature monitoring with a reference window.</p> Example <pre><code># Fetch your feature group or feature view\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n# Setup feature monitoring, a detection window and a reference window\nmy_monitoring_config = fg.create_feature_monitoring(\n    ...\n).with_detection_window(...).with_reference_window(...)\n# Choose a metric and set a threshold for the difference\n# e.g compare the relative mean of detection and reference window\nmy_monitoring_config.compare_on(\n    metric=\"mean\",\n    threshold=1.0,\n    relative=True,\n).save()\n</code></pre> Note <p>Detection window and reference window/value/training_dataset must be set prior to comparison configuration.</p> PARAMETER DESCRIPTION <code>metric</code> <p>The metric to use for comparison. Different metric are available for different feature type.</p> <p> TYPE: <code>str | None</code> </p> <code>threshold</code> <p>The threshold to apply to the difference to potentially trigger an alert.</p> <p> TYPE: <code>float | None</code> </p> <code>strict</code> <p>Whether to use a strict comparison (e.g. &gt; or &lt;) or a non-strict comparison (e.g. &gt;= or &lt;=).</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>False</code> </p> <code>relative</code> <p>Whether to use a relative comparison (e.g. relative mean) or an absolute comparison (e.g. absolute mean).</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>FeatureMonitoringConfig</code> <p><code>FeatureMonitoringConfig</code>. The updated FeatureMonitoringConfig object.</p>"},{"location":"python-api/hsfs/core/feature_monitoring_config/#hsfs.core.feature_monitoring_config.FeatureMonitoringConfig.save","title":"[source]  save","text":"<pre><code>save()\n</code></pre> <p>Saves the feature monitoring configuration.</p> Example <pre><code># Fetch your feature group or feature view\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n# Setup feature monitoring and a detection window\nmy_monitoring_config = fg.create_statistics_monitoring(\n    name=\"my_monitoring_config\",\n).save()\n</code></pre> RETURNS DESCRIPTION <p><code>FeatureMonitoringConfig</code>. The saved FeatureMonitoringConfig object.</p>"},{"location":"python-api/hsfs/core/feature_monitoring_config/#hsfs.core.feature_monitoring_config.FeatureMonitoringConfig.update","title":"[source]  update","text":"<pre><code>update()\n</code></pre> <p>Updates allowed fields of the saved feature monitoring configuration.</p> Example <pre><code># Fetch your feature group or feature view\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n# Fetch registered config by name\nmy_monitoring_config = fg.get_feature_monitoring_configs(name=\"my_monitoring_config\")\n# Update the percentage of rows to use when computing the statistics\nmy_monitoring_config.detection_window.row_percentage = 10\nmy_monitoring_config.update()\n</code></pre> RETURNS DESCRIPTION <p><code>FeatureMonitoringConfig</code>. The updated FeatureMonitoringConfig object.</p>"},{"location":"python-api/hsfs/core/feature_monitoring_config/#hsfs.core.feature_monitoring_config.FeatureMonitoringConfig.run_job","title":"[source]  run_job","text":"<pre><code>run_job()\n</code></pre> <p>Trigger the feature monitoring job which computes and compares statistics on the detection and reference windows.</p> Example <pre><code># Fetch your feature group or feature view\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n# Fetch registered config by name\nmy_monitoring_config = fg.get_feature_monitoring_configs(name=\"my_monitoring_config\")\n# Trigger the feature monitoring job once\nmy_monitoring_config.run_job()\n</code></pre> Info <p>The feature monitoring job will be triggered asynchronously and the method will return immediately. Calling this method does not affect the ongoing schedule.</p> RAISES DESCRIPTION <code>`hopsworks.client.exceptions.FeatureStoreException`</code> <p>If the feature monitoring config has not been saved.</p> RETURNS DESCRIPTION <p><code>Job</code>. A handle for the job computing the statistics.</p>"},{"location":"python-api/hsfs/core/feature_monitoring_config/#hsfs.core.feature_monitoring_config.FeatureMonitoringConfig.get_job","title":"[source]  get_job","text":"<pre><code>get_job()\n</code></pre> <p>Get the feature monitoring job which computes and compares statistics on the detection and reference windows.</p> Example <pre><code># Fetch registered config by name via feature group or feature view\nmy_monitoring_config = fg.get_feature_monitoring_configs(name=\"my_monitoring_config\")\n# Get the job which computes statistics on detection and reference window\njob = my_monitoring_config.get_job()\n# Print job history and ongoing executions\njob.executions\n</code></pre> <p>Raises:     <code>hopsworks.client.exceptions.FeatureStoreException</code>: If the feature monitoring config has not been saved.</p> RETURNS DESCRIPTION <p><code>Job</code>. A handle for the job computing the statistics.</p>"},{"location":"python-api/hsfs/core/feature_monitoring_config/#hsfs.core.feature_monitoring_config.FeatureMonitoringConfig.delete","title":"[source]  delete","text":"<pre><code>delete()\n</code></pre> <p>Deletes the feature monitoring configuration.</p> Example <pre><code># Fetch your feature group or feature view\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n# Fetch registered config by name\nmy_monitoring_config = fg.get_feature_monitoring_configs(name=\"my_monitoring_config\")\n# Delete the feature monitoring config\nmy_monitoring_config.delete()\n</code></pre> RAISES DESCRIPTION <code>`hopsworks.client.exceptions.FeatureStoreException`</code> <p>If the feature monitoring config has not been saved.</p>"},{"location":"python-api/hsfs/core/feature_monitoring_config/#hsfs.core.feature_monitoring_config.FeatureMonitoringConfig.disable","title":"[source]  disable","text":"<pre><code>disable()\n</code></pre> <p>Disables the schedule of the feature monitoring job.</p> Example <pre><code># Fetch your feature group or feature view\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n# Fetch registered config by name\nmy_monitoring_config = fg.get_feature_monitoring_configs(name=\"my_monitoring_config\")\n# Disable the feature monitoring config\nmy_monitoring_config.disable()\n</code></pre> RAISES DESCRIPTION <code>`hopsworks.client.exceptions.FeatureStoreException`</code> <p>If the feature monitoring config has not been saved.</p>"},{"location":"python-api/hsfs/core/feature_monitoring_config/#hsfs.core.feature_monitoring_config.FeatureMonitoringConfig.enable","title":"[source]  enable","text":"<pre><code>enable()\n</code></pre> <p>Enables the schedule of the feature monitoring job.</p> <p>The scheduler can be configured via the <code>job_schedule</code> property.</p> Example <pre><code># Fetch your feature group or feature view\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n# Fetch registered config by name\nmy_monitoring_config = fg.get_feature_monitoring_configs(name=\"my_monitoring_config\")\n# Enable the feature monitoring config\nmy_monitoring_config.enable()\n</code></pre> RAISES DESCRIPTION <code>`hopsworks.client.exceptions.FeatureStoreException`</code> <p>If the feature monitoring config has not been saved.</p>"},{"location":"python-api/hsfs/core/feature_monitoring_config/#hsfs.core.feature_monitoring_config.FeatureMonitoringConfig.get_history","title":"[source]  get_history","text":"<pre><code>get_history(\n    start_time: datetime | date | str | int | None = None,\n    end_time: datetime | date | str | int | None = None,\n    with_statistics: bool = True,\n) -&gt; list[FeatureMonitoringResult]\n</code></pre> <p>Fetch the history of the computed statistics and comparison results for this configuration.</p> Example <pre><code># Fetch your feature group or feature view\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n# Fetch registered config by name\nmy_monitoring_config = fg.get_feature_monitoring_configs(name=\"my_monitoring_config\")\n# Fetch the history of the computed statistics for this configuration\nhistory = my_monitoring_config.get_history(\n    start_time=\"2021-01-01\",\n    end_time=\"2021-01-31\",\n)\n</code></pre> PARAMETER DESCRIPTION <code>start_time</code> <p>The start time of the time range to fetch the history for.</p> <p> TYPE: <code>datetime | date | str | int | None</code> DEFAULT: <code>None</code> </p> <code>end_time</code> <p>The end time of the time range to fetch the history for.</p> <p> TYPE: <code>datetime | date | str | int | None</code> DEFAULT: <code>None</code> </p> <code>with_statistics</code> <p>Whether to include the computed statistics in the results.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RAISES DESCRIPTION <code>`hopsworks.client.exceptions.FeatureStoreException`</code> <p>If the feature monitoring config has not been saved.</p>"},{"location":"python-api/hsfs/core/feature_monitoring_result/","title":"hsfs.core.feature_monitoring_result","text":""},{"location":"python-api/hsfs/core/feature_monitoring_result/#hsfs.core.feature_monitoring_result","title":"hsfs.core.feature_monitoring_result","text":""},{"location":"python-api/hsfs/core/feature_monitoring_result/#hsfs.core.feature_monitoring_result.FeatureMonitoringResult","title":"[source]  FeatureMonitoringResult","text":""},{"location":"python-api/hsfs/core/feature_monitoring_result/#hsfs.core.feature_monitoring_result.FeatureMonitoringResult.id","title":"[source]  id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Id of the feature monitoring result.</p>"},{"location":"python-api/hsfs/core/feature_monitoring_result/#hsfs.core.feature_monitoring_result.FeatureMonitoringResult.config_id","title":"[source]  config_id  <code>property</code>","text":"<pre><code>config_id: int\n</code></pre> <p>Id of the feature monitoring configuration containing this result.</p>"},{"location":"python-api/hsfs/core/feature_monitoring_result/#hsfs.core.feature_monitoring_result.FeatureMonitoringResult.feature_store_id","title":"[source]  feature_store_id  <code>property</code>","text":"<pre><code>feature_store_id: int\n</code></pre> <p>Id of the Feature Store.</p>"},{"location":"python-api/hsfs/core/feature_monitoring_result/#hsfs.core.feature_monitoring_result.FeatureMonitoringResult.detection_statistics_id","title":"[source]  detection_statistics_id  <code>property</code>","text":"<pre><code>detection_statistics_id: int | None\n</code></pre> <p>Id of the feature descriptive statistics computed on the detection window.</p>"},{"location":"python-api/hsfs/core/feature_monitoring_result/#hsfs.core.feature_monitoring_result.FeatureMonitoringResult.reference_statistics_id","title":"[source]  reference_statistics_id  <code>property</code>","text":"<pre><code>reference_statistics_id: int | None\n</code></pre> <p>Id of the feature descriptive statistics computed on the reference window.</p>"},{"location":"python-api/hsfs/core/feature_monitoring_result/#hsfs.core.feature_monitoring_result.FeatureMonitoringResult.detection_statistics","title":"[source]  detection_statistics  <code>property</code>","text":"<pre><code>detection_statistics: FeatureDescriptiveStatistics | None\n</code></pre> <p>Feature descriptive statistics computed on the detection window.</p>"},{"location":"python-api/hsfs/core/feature_monitoring_result/#hsfs.core.feature_monitoring_result.FeatureMonitoringResult.reference_statistics","title":"[source]  reference_statistics  <code>property</code>","text":"<pre><code>reference_statistics: FeatureDescriptiveStatistics | None\n</code></pre> <p>Feature descriptive statistics computed on the reference window.</p>"},{"location":"python-api/hsfs/core/feature_monitoring_result/#hsfs.core.feature_monitoring_result.FeatureMonitoringResult.execution_id","title":"[source]  execution_id  <code>property</code>","text":"<pre><code>execution_id: int | None\n</code></pre> <p>Execution id of the feature monitoring job.</p>"},{"location":"python-api/hsfs/core/feature_monitoring_result/#hsfs.core.feature_monitoring_result.FeatureMonitoringResult.monitoring_time","title":"[source]  monitoring_time  <code>property</code>","text":"<pre><code>monitoring_time: int\n</code></pre> <p>Time at which this feature monitoring result was created.</p>"},{"location":"python-api/hsfs/core/feature_monitoring_result/#hsfs.core.feature_monitoring_result.FeatureMonitoringResult.difference","title":"[source]  difference  <code>property</code>","text":"<pre><code>difference: float | None\n</code></pre> <p>Difference between detection and reference values. It can be relative or absolute difference, depending on the statistics comparison configuration provided in <code>relative</code> parameter passed to <code>compare_on()</code> when enabling feature monitoring.</p>"},{"location":"python-api/hsfs/core/feature_monitoring_result/#hsfs.core.feature_monitoring_result.FeatureMonitoringResult.shift_detected","title":"[source]  shift_detected  <code>property</code>","text":"<pre><code>shift_detected: bool\n</code></pre> <p>Whether or not shift was detected in the detection window based on the computed statistics and the threshold provided in <code>compare_on()</code> when enabling feature monitoring.</p>"},{"location":"python-api/hsfs/core/feature_monitoring_result/#hsfs.core.feature_monitoring_result.FeatureMonitoringResult.feature_name","title":"[source]  feature_name  <code>property</code>","text":"<pre><code>feature_name: str\n</code></pre> <p>Name of the feature being monitored.</p>"},{"location":"python-api/hsfs/core/feature_monitoring_result/#hsfs.core.feature_monitoring_result.FeatureMonitoringResult.empty_detection_window","title":"[source]  empty_detection_window  <code>property</code>","text":"<pre><code>empty_detection_window: bool\n</code></pre> <p>Whether or not the detection window was empty in this feature monitoring run.</p>"},{"location":"python-api/hsfs/core/feature_monitoring_result/#hsfs.core.feature_monitoring_result.FeatureMonitoringResult.empty_reference_window","title":"[source]  empty_reference_window  <code>property</code>","text":"<pre><code>empty_reference_window: bool\n</code></pre> <p>Whether or not the reference window was empty in this feature monitoring run.</p>"},{"location":"python-api/hsfs/core/feature_monitoring_result/#hsfs.core.feature_monitoring_result.FeatureMonitoringResult.specific_value","title":"[source]  specific_value  <code>property</code>","text":"<pre><code>specific_value: float | None\n</code></pre> <p>Specific value used as reference in the statistics comparison.</p>"},{"location":"python-api/hsfs/core/monitoring_window_config/","title":"hsfs.core.monitoring_window_config","text":""},{"location":"python-api/hsfs/core/monitoring_window_config/#hsfs.core.monitoring_window_config","title":"hsfs.core.monitoring_window_config","text":""},{"location":"python-api/hsfs/core/monitoring_window_config/#hsfs.core.monitoring_window_config.MonitoringWindowConfig","title":"[source]  MonitoringWindowConfig","text":""},{"location":"python-api/hsfs/core/monitoring_window_config/#hsfs.core.monitoring_window_config.MonitoringWindowConfig.id","title":"[source]  id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Id of the window configuration.</p>"},{"location":"python-api/hsfs/core/monitoring_window_config/#hsfs.core.monitoring_window_config.MonitoringWindowConfig.window_config_type","title":"[source]  window_config_type  <code>property</code> <code>writable</code>","text":"<pre><code>window_config_type: WindowConfigType\n</code></pre> <p>Type of the window. It can be one of <code>ALL_TIME</code>, <code>ROLLING_TIME</code>, <code>TRAINING_DATASET</code> or <code>SPECIFIC_VALUE</code>.</p>"},{"location":"python-api/hsfs/core/monitoring_window_config/#hsfs.core.monitoring_window_config.MonitoringWindowConfig.time_offset","title":"[source]  time_offset  <code>property</code>","text":"<pre><code>time_offset: str | None\n</code></pre> <p>The time offset from the current time to the start of the time window. Only used for windows of type <code>ROLLING_TIME</code>.</p>"},{"location":"python-api/hsfs/core/monitoring_window_config/#hsfs.core.monitoring_window_config.MonitoringWindowConfig.window_length","title":"[source]  window_length  <code>property</code> <code>writable</code>","text":"<pre><code>window_length: str | None\n</code></pre> <p>The length of the time window. Only used for windows of type <code>ROLLING_TIME</code>.</p>"},{"location":"python-api/hsfs/core/monitoring_window_config/#hsfs.core.monitoring_window_config.MonitoringWindowConfig.training_dataset_version","title":"[source]  training_dataset_version  <code>property</code> <code>writable</code>","text":"<pre><code>training_dataset_version: int | None\n</code></pre> <p>The version of the training dataset to use as reference. Only used for windows of type <code>TRAINING_DATASET</code>.</p>"},{"location":"python-api/hsfs/core/monitoring_window_config/#hsfs.core.monitoring_window_config.MonitoringWindowConfig.specific_value","title":"[source]  specific_value  <code>property</code> <code>writable</code>","text":"<pre><code>specific_value: float | None\n</code></pre> <p>The specific value to use as reference. Only used for windows of type <code>SPECIFIC_VALUE</code>.</p>"},{"location":"python-api/hsfs/core/monitoring_window_config/#hsfs.core.monitoring_window_config.MonitoringWindowConfig.row_percentage","title":"[source]  row_percentage  <code>property</code> <code>writable</code>","text":"<pre><code>row_percentage: float | None\n</code></pre> <p>The percentage of rows to fetch and compute the statistics on. Only used for windows of type <code>ROLLING_TIME</code> and <code>ALL_TIME</code>.</p>"},{"location":"python-api/hsfs/core/online_ingestion/","title":"hsfs.core.online_ingestion","text":""},{"location":"python-api/hsfs/core/online_ingestion/#hsfs.core.online_ingestion","title":"hsfs.core.online_ingestion","text":""},{"location":"python-api/hsfs/core/online_ingestion/#hsfs.core.online_ingestion.OnlineIngestion","title":"[source]  OnlineIngestion","text":"<p>Metadata object used to provide Online Ingestion information for a feature group.</p> <p>This class encapsulates the state and results of an online ingestion operation, including progress tracking and log retrieval.</p>"},{"location":"python-api/hsfs/core/online_ingestion/#hsfs.core.online_ingestion.OnlineIngestion.id","title":"[source]  id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Get the unique identifier for the ingestion operation.</p> RETURNS DESCRIPTION <code>int | None</code> <p>Optional[int]: The ingestion ID.</p>"},{"location":"python-api/hsfs/core/online_ingestion/#hsfs.core.online_ingestion.OnlineIngestion.num_entries","title":"[source]  num_entries  <code>property</code> <code>writable</code>","text":"<pre><code>num_entries: int | None\n</code></pre> <p>Get the total number of entries to ingest.</p> RETURNS DESCRIPTION <code>int | None</code> <p>Optional[int]: The number of entries.</p>"},{"location":"python-api/hsfs/core/online_ingestion/#hsfs.core.online_ingestion.OnlineIngestion.results","title":"[source]  results  <code>property</code>","text":"<pre><code>results: list[online_ingestion_result.OnlineIngestionResult]\n</code></pre> <p>Get the list of ingestion results.</p> RETURNS DESCRIPTION <code>list[online_ingestion_result.OnlineIngestionResult]</code> <p>List[OnlineIngestionResult]: List of ingestion result objects.</p>"},{"location":"python-api/hsfs/core/online_ingestion/#hsfs.core.online_ingestion.OnlineIngestion.feature_group","title":"[source]  feature_group  <code>property</code>","text":"<pre><code>feature_group: fg_mod.FeatureGroup\n</code></pre> <p>Get the feature group associated with this ingestion.</p> RETURNS DESCRIPTION <code>FeatureGroup</code> <p>The associated feature group.</p> <p> TYPE: <code>fg_mod.FeatureGroup</code> </p>"},{"location":"python-api/hsfs/core/online_ingestion/#hsfs.core.online_ingestion.OnlineIngestion.refresh","title":"[source]  refresh","text":"<pre><code>refresh()\n</code></pre> <p>Refresh the state of this OnlineIngestion object from the backend.</p>"},{"location":"python-api/hsfs/core/online_ingestion/#hsfs.core.online_ingestion.OnlineIngestion.wait_for_completion","title":"[source]  wait_for_completion","text":"<pre><code>wait_for_completion(options: dict[str, Any] = None)\n</code></pre> <p>Wait for the online ingestion operation to complete, displaying a progress bar.</p> PARAMETER DESCRIPTION <code>options</code> <p>Options for waiting. - \"timeout\" (int): Maximum time to wait in seconds (default: 60). - \"period\" (int): Polling period in seconds (default: 1).</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>Warning</code> <p>If the timeout is exceeded before completion.</p>"},{"location":"python-api/hsfs/core/online_ingestion/#hsfs.core.online_ingestion.OnlineIngestion.print_logs","title":"[source]  print_logs","text":"<pre><code>print_logs(priority: str = 'error', size: int = 20)\n</code></pre> <p>Print logs related to the online ingestion operation from OpenSearch.</p> PARAMETER DESCRIPTION <code>priority</code> <p>Log priority to filter by (default: \"error\").</p> <p> TYPE: <code>str</code> DEFAULT: <code>'error'</code> </p> <code>size</code> <p>Number of log entries to retrieve (default: 20).</p> <p> TYPE: <code>int</code> DEFAULT: <code>20</code> </p>"},{"location":"python-api/hsfs/core/online_ingestion_result/","title":"hsfs.core.online_ingestion_result","text":""},{"location":"python-api/hsfs/core/online_ingestion_result/#hsfs.core.online_ingestion_result","title":"hsfs.core.online_ingestion_result","text":""},{"location":"python-api/hsfs/core/online_ingestion_result/#hsfs.core.online_ingestion_result.OnlineIngestionResult","title":"[source]  OnlineIngestionResult","text":"<p>Metadata object used to provide Online Ingestion Batch Result information.</p> <p>This class encapsulates the result of a single batch operation during online ingestion, including the ingestion ID, status, and number of rows processed.</p>"},{"location":"python-api/hsfs/core/online_ingestion_result/#hsfs.core.online_ingestion_result.OnlineIngestionResult.online_ingestion_id","title":"[source]  online_ingestion_id  <code>property</code>","text":"<pre><code>online_ingestion_id: int\n</code></pre> <p>Get the unique identifier for the online ingestion batch.</p> RETURNS DESCRIPTION <code>int</code> <p>The online ingestion batch ID.</p> <p> TYPE: <code>int</code> </p>"},{"location":"python-api/hsfs/core/online_ingestion_result/#hsfs.core.online_ingestion_result.OnlineIngestionResult.status","title":"[source]  status  <code>property</code>","text":"<pre><code>status: str\n</code></pre> <p>Get the status of the ingestion batch.</p> RETURNS DESCRIPTION <code>str</code> <p>The status of the batch (e.g., \"UPSERTED\", \"FAILED\").</p> <p> TYPE: <code>str</code> </p>"},{"location":"python-api/hsfs/core/online_ingestion_result/#hsfs.core.online_ingestion_result.OnlineIngestionResult.rows","title":"[source]  rows  <code>property</code>","text":"<pre><code>rows: int\n</code></pre> <p>Get the number of rows processed in this batch.</p> RETURNS DESCRIPTION <code>int</code> <p>The number of rows.</p> <p> TYPE: <code>int</code> </p>"},{"location":"python-api/hsfs/embedding/","title":"hsfs.embedding","text":""},{"location":"python-api/hsfs/embedding/#hsfs.embedding","title":"hsfs.embedding","text":""},{"location":"python-api/hsfs/embedding/#hsfs.embedding.EmbeddingFeature","title":"[source]  EmbeddingFeature","text":"<p>Represents an embedding feature.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the embedding feature.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>dimension</code> <p>The dimensionality of the embedding feature.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>similarity_function_type</code> <p>The type of similarity function used for the embedding feature. Available functions are <code>L2</code>, <code>COSINE</code>, and <code>DOT_PRODUCT</code>. Defaults to <code>SimilarityFunctionType.L2</code>.</p> <p> TYPE: <code>SimilarityFunctionType</code> DEFAULT: <code>SimilarityFunctionType.L2</code> </p> <code>model</code> <p><code>hsml.model.Model</code> A Model in hsml.</p> <p> DEFAULT: <code>None</code> </p> <code>feature_group</code> <p>The feature group object that contains the embedding feature.</p> <p> DEFAULT: <code>None</code> </p> <code>embedding_index</code> <p><code>EmbeddingIndex</code> The index for managing embedding features.</p> <p> DEFAULT: <code>None</code> </p>"},{"location":"python-api/hsfs/embedding/#hsfs.embedding.EmbeddingFeature.name","title":"[source]  name  <code>property</code>","text":"<p>str: The name of the embedding feature.</p>"},{"location":"python-api/hsfs/embedding/#hsfs.embedding.EmbeddingFeature.dimenstion","title":"[source]  dimenstion  <code>property</code>","text":"<p>The dimensionality of the embedding feature.</p> <p>This one is excluded from the docs as the name is misspelled but kept to avoid breaking the API.</p>"},{"location":"python-api/hsfs/embedding/#hsfs.embedding.EmbeddingFeature.dimension","title":"[source]  dimension  <code>property</code>","text":"<p>int: The dimensionality of the embedding feature.</p>"},{"location":"python-api/hsfs/embedding/#hsfs.embedding.EmbeddingFeature.similarity_function_type","title":"[source]  similarity_function_type  <code>property</code>","text":"<p>SimilarityFunctionType: The type of similarity function used for the embedding feature.</p>"},{"location":"python-api/hsfs/embedding/#hsfs.embedding.EmbeddingFeature.model","title":"[source]  model  <code>property</code>","text":"<p>hsml.model.Model: The Model in hsml.</p>"},{"location":"python-api/hsfs/embedding/#hsfs.embedding.EmbeddingFeature.feature_group","title":"[source]  feature_group  <code>property</code> <code>writable</code>","text":"<p>FeatureGroup: The feature group object that contains the embedding feature.</p>"},{"location":"python-api/hsfs/embedding/#hsfs.embedding.EmbeddingFeature.embedding_index","title":"[source]  embedding_index  <code>property</code> <code>writable</code>","text":"<p>EmbeddingIndex: The index for managing embedding features.</p>"},{"location":"python-api/hsfs/embedding/#hsfs.embedding.EmbeddingIndex","title":"[source]  EmbeddingIndex","text":"<p>Represents an index for managing embedding features.</p> PARAMETER DESCRIPTION <code>index_name</code> <p>The name of the embedding index. The name of the project index is used if not provided.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>features</code> <p>A list of <code>EmbeddingFeature</code> objects for the features that contain embeddings that should be indexed for similarity search.</p> <p> TYPE: <code>list[EmbeddingFeature] | None</code> DEFAULT: <code>None</code> </p> <code>col_prefix</code> <p>The prefix to be added to column names when using project index. It is managed by Hopsworks and should not be provided.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> Example <pre><code>embedding_index = EmbeddingIndex()\nembedding_index.add_embedding(name=\"user_vector\", dimension=256)\nembeddings = embedding_index.get_embeddings()\n</code></pre>"},{"location":"python-api/hsfs/embedding/#hsfs.embedding.EmbeddingIndex.feature_group","title":"[source]  feature_group  <code>property</code> <code>writable</code>","text":"<p>FeatureGroup: The feature group object that contains the embedding feature.</p>"},{"location":"python-api/hsfs/embedding/#hsfs.embedding.EmbeddingIndex.index_name","title":"[source]  index_name  <code>property</code>","text":"<p>str: The name of the embedding index.</p>"},{"location":"python-api/hsfs/embedding/#hsfs.embedding.EmbeddingIndex.col_prefix","title":"[source]  col_prefix  <code>property</code>","text":"<p>str: The prefix to be added to column names.</p>"},{"location":"python-api/hsfs/embedding/#hsfs.embedding.EmbeddingIndex.add_embedding","title":"[source]  add_embedding","text":"<pre><code>add_embedding(\n    name: str,\n    dimension: int,\n    similarity_function_type: SimilarityFunctionType\n    | None = SimilarityFunctionType.L2,\n    model=None,\n)\n</code></pre> <p>Adds a new embedding feature to the index.</p> Example <pre><code>embedding_index = EmbeddingIndex()\nembedding_index.add_embedding(name=\"user_vector\", dimension=256)\n\n# Attach a hsml model to the embedding feature\nembedding_index = EmbeddingIndex()\nembedding_index.add_embedding(name=\"user_vector\", dimension=256, model=hsml_model)\n</code></pre> PARAMETER DESCRIPTION <code>name</code> <p>The name of the embedding feature.</p> <p> TYPE: <code>str</code> </p> <code>dimension</code> <p>The dimensionality of the embedding feature.</p> <p> TYPE: <code>int</code> </p> <code>similarity_function_type</code> <p>The type of similarity function to be used.</p> <p> TYPE: <code>SimilarityFunctionType | None</code> DEFAULT: <code>SimilarityFunctionType.L2</code> </p> <code>model</code> <p><code>hsml.model.Model | None</code> The hsml model used to generate the embedding.</p> <p> DEFAULT: <code>None</code> </p>"},{"location":"python-api/hsfs/embedding/#hsfs.embedding.EmbeddingIndex.get_embedding","title":"[source]  get_embedding","text":"<pre><code>get_embedding(name: str) -&gt; EmbeddingFeature\n</code></pre> <p>Get <code>EmbeddingFeature</code> associated with the feature name.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the embedding feature.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>EmbeddingFeature</code> <p>The <code>EmbeddingFeature</code> associated with the name.</p>"},{"location":"python-api/hsfs/embedding/#hsfs.embedding.EmbeddingIndex.get_embeddings","title":"[source]  get_embeddings","text":"<pre><code>get_embeddings() -&gt; list[EmbeddingFeature]\n</code></pre> <p>Returns the list of <code>EmbeddingFeature</code> objects associated with the index.</p> RETURNS DESCRIPTION <code>list[EmbeddingFeature]</code> <p>All embedding features in the index.</p>"},{"location":"python-api/hsfs/embedding/#hsfs.embedding.EmbeddingIndex.count","title":"[source]  count","text":"<pre><code>count(options: map = None) -&gt; int\n</code></pre> <p>Count the number of records in the feature group.</p> PARAMETER DESCRIPTION <code>options</code> <p>The options used for the request to the vector database. The keys are attribute values of [<code>OpensearchRequestOption</code>][hopsworks_common.core.opensearch.OpensearchRequestOption].</p> <p> TYPE: <code>map</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>int</code> <p>The number of records in the feature group.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the feature group is not initialized.</p> <code>hopsworks.client.exceptions.FeatureStoreException</code> <p>If an error occurs during the count operation.</p>"},{"location":"python-api/hsfs/embedding/#hsfs.embedding.SimilarityFunctionType","title":"[source]  SimilarityFunctionType","text":"<p>Enumeration class representing different types of similarity functions.</p> PARAMETER DESCRIPTION <code>L2</code> <p>Represents L2 norm similarity function.</p> <p> </p> <code>COSINE</code> <p>Represents cosine similarity function.</p> <p> </p> <code>DOT_PRODUCT</code> <p>Represents dot product similarity function.</p> <p> </p>"},{"location":"python-api/hsfs/expectation_suite/","title":"hsfs.expectation_suite","text":""},{"location":"python-api/hsfs/expectation_suite/#hsfs.expectation_suite","title":"hsfs.expectation_suite","text":""},{"location":"python-api/hsfs/expectation_suite/#hsfs.expectation_suite.ExpectationSuite","title":"[source]  ExpectationSuite","text":"<p>Metadata object representing a feature validation expectation in the Feature Store.</p>"},{"location":"python-api/hsfs/expectation_suite/#hsfs.expectation_suite.ExpectationSuite.id","title":"[source]  id  <code>property</code> <code>writable</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Id of the expectation suite, set by backend.</p>"},{"location":"python-api/hsfs/expectation_suite/#hsfs.expectation_suite.ExpectationSuite.expectation_suite_name","title":"[source]  expectation_suite_name  <code>property</code> <code>writable</code>","text":"<pre><code>expectation_suite_name: str\n</code></pre> <p>Name of the expectation suite.</p>"},{"location":"python-api/hsfs/expectation_suite/#hsfs.expectation_suite.ExpectationSuite.data_asset_type","title":"[source]  data_asset_type  <code>property</code> <code>writable</code>","text":"<pre><code>data_asset_type: str | None\n</code></pre> <p>Data asset type of the expectation suite, not used by backend.</p>"},{"location":"python-api/hsfs/expectation_suite/#hsfs.expectation_suite.ExpectationSuite.ge_cloud_id","title":"[source]  ge_cloud_id  <code>property</code>","text":"<pre><code>ge_cloud_id: int | None\n</code></pre> <p>ge_cloud_id of the expectation suite, not used by backend.</p>"},{"location":"python-api/hsfs/expectation_suite/#hsfs.expectation_suite.ExpectationSuite.run_validation","title":"[source]  run_validation  <code>property</code> <code>writable</code>","text":"<pre><code>run_validation: bool\n</code></pre> <p>Boolean to determine whether or not the expectation suite shoudl run on ingestion.</p>"},{"location":"python-api/hsfs/expectation_suite/#hsfs.expectation_suite.ExpectationSuite.validation_ingestion_policy","title":"[source]  validation_ingestion_policy  <code>property</code> <code>writable</code>","text":"<pre><code>validation_ingestion_policy: Literal['always', 'strict']\n</code></pre> <p>Whether to ingest a df based on the validation result.</p> <ul> <li><code>\"strict\"</code>: ingest df only if all expectations succeed, or</li> <li><code>\"always\"</code>: always ingest df, even if one or more expectations fail.</li> </ul>"},{"location":"python-api/hsfs/expectation_suite/#hsfs.expectation_suite.ExpectationSuite.expectations","title":"[source]  expectations  <code>property</code> <code>writable</code>","text":"<pre><code>expectations: list[GeExpectation]\n</code></pre> <p>List of expectations to run at validation.</p>"},{"location":"python-api/hsfs/expectation_suite/#hsfs.expectation_suite.ExpectationSuite.meta","title":"[source]  meta  <code>property</code> <code>writable</code>","text":"<pre><code>meta: dict[str, Any]\n</code></pre> <p>Meta field of the expectation suite to store additional information.</p>"},{"location":"python-api/hsfs/expectation_suite/#hsfs.expectation_suite.ExpectationSuite.from_ge_type","title":"[source]  from_ge_type  <code>classmethod</code>","text":"<pre><code>from_ge_type(\n    ge_expectation_suite: great_expectations.core.ExpectationSuite,\n    run_validation: bool = True,\n    validation_ingestion_policy: Literal[\n        \"ALWAYS\", \"STRICT\"\n    ] = \"ALWAYS\",\n    id: int | None = None,\n    feature_store_id: int | None = None,\n    feature_group_id: int | None = None,\n) -&gt; ExpectationSuite\n</code></pre> <p>Used to create a Hopsworks Expectation Suite instance from a great_expectations instance.</p> PARAMETER DESCRIPTION <code>ge_expectation_suite</code> <p>The great_expectations ExpectationSuite instance to convert to a Hopsworks ExpectationSuite.</p> <p> TYPE: <code>great_expectations.core.ExpectationSuite</code> </p> <code>run_validation</code> <p>Whether to run validation on inserts when the expectation suite is attached.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>validation_ingestion_policy</code> <p>The validation ingestion policy to use when the expectation suite is attached.</p> <p> TYPE: <code>Literal['ALWAYS', 'STRICT']</code> DEFAULT: <code>'ALWAYS'</code> </p> <code>id</code> <p>The id of the expectation suite in Hopsworks. If not provided, a new expectation suite will be created.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>feature_store_id</code> <p>The id of the feature store of the feature group to which the expectation suite belongs.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>feature_group_id</code> <p>The id of the feature group to which the expectation suite belongs.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ExpectationSuite</code> <p>Hopsworks Expectation Suite instance.</p>"},{"location":"python-api/hsfs/expectation_suite/#hsfs.expectation_suite.ExpectationSuite.to_ge_type","title":"[source]  to_ge_type","text":"<pre><code>to_ge_type() -&gt; great_expectations.core.ExpectationSuite\n</code></pre> <p>Convert to Great Expectations ExpectationSuite type.</p>"},{"location":"python-api/hsfs/expectation_suite/#hsfs.expectation_suite.ExpectationSuite.get_expectation","title":"[source]  get_expectation","text":"<pre><code>get_expectation(\n    expectation_id: int,\n    ge_type: bool = HAS_GREAT_EXPECTATIONS,\n) -&gt; (\n    GeExpectation\n    | great_expectations.core.ExpectationConfiguration\n)\n</code></pre> <p>Fetch expectation with expectation_id from the backend.</p> Example <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nexpectation_suite = fg.get_expectation_suite()\nselected_expectation = expectation_suite.get_expectation(expectation_id=123)\n</code></pre> PARAMETER DESCRIPTION <code>expectation_id</code> <p>ID of the expectation to fetch from the backend.</p> <p> TYPE: <code>int</code> </p> <code>ge_type</code> <p>Whether to return native Great Expectations object or Hopsworks abstraction.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>HAS_GREAT_EXPECTATIONS</code> </p> RETURNS DESCRIPTION <code>GeExpectation | great_expectations.core.ExpectationConfiguration</code> <p>The expectation with <code>expectation_id</code> registered in the backend.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p> <code>hopsworks.client.exceptions.FeatureStoreException</code> <p>If the expectation suite is not registered yet.</p>"},{"location":"python-api/hsfs/expectation_suite/#hsfs.expectation_suite.ExpectationSuite.add_expectation","title":"[source]  add_expectation","text":"<pre><code>add_expectation(\n    expectation: GeExpectation\n    | great_expectations.core.ExpectationConfiguration,\n    ge_type: bool = HAS_GREAT_EXPECTATIONS,\n) -&gt; (\n    GeExpectation\n    | great_expectations.core.ExpectationConfiguration\n)\n</code></pre> <p>Append an expectation to the local suite or in the backend if attached to a Feature Group.</p> Example <pre><code># check if the minimum value of specific column is within a range of 0 and 1\nexpectation_suite.add_expectation(\n    ge.core.ExpectationConfiguration(\n        expectation_type=\"expect_column_min_to_be_between\",\n        kwargs={\n            \"column\": \"foo_id\",\n            \"min_value\": 0,\n            \"max_value\": 1\n        }\n    )\n)\n\n# check if the length of specific column value is within a range of 3 and 10\nexpectation_suite.add_expectation(\n    ge.core.ExpectationConfiguration(\n        expectation_type=\"expect_column_value_lengths_to_be_between\",\n        kwargs={\n            \"column\": \"bar_name\",\n            \"min_value\": 3,\n            \"max_value\": 10\n        }\n    )\n)\n</code></pre> PARAMETER DESCRIPTION <code>expectation</code> <p>The new expectation object.</p> <p> TYPE: <code>GeExpectation | great_expectations.core.ExpectationConfiguration</code> </p> <code>ge_type</code> <p>Whether to return native Great Expectations object or Hopsworks abstraction.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>HAS_GREAT_EXPECTATIONS</code> </p> RETURNS DESCRIPTION <code>GeExpectation | great_expectations.core.ExpectationConfiguration</code> <p>The new expectation attached to the Feature Group.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p> <code>hopsworks.client.exceptions.FeatureStoreException</code> <p>If the expectation suite is not registered yet.</p>"},{"location":"python-api/hsfs/expectation_suite/#hsfs.expectation_suite.ExpectationSuite.replace_expectation","title":"[source]  replace_expectation","text":"<pre><code>replace_expectation(\n    expectation: GeExpectation\n    | great_expectations.core.ExpectationConfiguration,\n    ge_type: bool = HAS_GREAT_EXPECTATIONS,\n) -&gt; (\n    GeExpectation\n    | great_expectations.core.ExpectationConfiguration\n)\n</code></pre> <p>Update an expectation from the suite locally or from the backend if attached to a Feature Group.</p> Example <pre><code>updated_expectation = expectation_suite.replace_expectation(new_expectation_object)\n</code></pre> PARAMETER DESCRIPTION <code>expectation</code> <p>The updated expectation object. The meta field should contain an expectationId field.</p> <p> TYPE: <code>GeExpectation | great_expectations.core.ExpectationConfiguration</code> </p> <code>ge_type</code> <p>Whether to return native Great Expectations object or Hopsworks abstraction.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>HAS_GREAT_EXPECTATIONS</code> </p> RETURNS DESCRIPTION <code>GeExpectation | great_expectations.core.ExpectationConfiguration</code> <p>The updated expectation attached to the Feature Group.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p> <code>hopsworks.client.exceptions.FeatureStoreException</code> <p>If the expectation suite is not registered yet.</p>"},{"location":"python-api/hsfs/expectation_suite/#hsfs.expectation_suite.ExpectationSuite.remove_expectation","title":"[source]  remove_expectation","text":"<pre><code>remove_expectation(\n    expectation_id: int | None = None,\n) -&gt; None\n</code></pre> <p>Remove an expectation from the suite locally and from the backend if attached to a Feature Group.</p> Example <pre><code>expectation_suite.remove_expectation(expectation_id=123)\n</code></pre> PARAMETER DESCRIPTION <code>expectation_id</code> <p>ID of the expectation to remove. The expectation will be deleted both locally and from the backend.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p> <code>hopsworks.client.exceptions.FeatureStoreException</code> <p>If the expectation suite is not registered yet.</p>"},{"location":"python-api/hsfs/feature/","title":"hsfs.feature","text":""},{"location":"python-api/hsfs/feature/#hsfs.feature","title":"hsfs.feature","text":""},{"location":"python-api/hsfs/feature/#hsfs.feature.Feature","title":"[source]  Feature","text":"<p>Metadata object representing a feature in a feature group in the Feature Store.</p> <p>See Training Dataset Feature for the feature representation of training dataset schemas.</p>"},{"location":"python-api/hsfs/feature/#hsfs.feature.Feature.use_fully_qualified_name","title":"[source]  use_fully_qualified_name  <code>property</code> <code>writable</code>","text":"<pre><code>use_fully_qualified_name: bool\n</code></pre> <p>Use fully qualified name for the feature when generating dataframes for training/batch data.</p>"},{"location":"python-api/hsfs/feature/#hsfs.feature.Feature.name","title":"[source]  name  <code>property</code> <code>writable</code>","text":"<pre><code>name: str\n</code></pre> <p>Name of the feature.</p>"},{"location":"python-api/hsfs/feature/#hsfs.feature.Feature.description","title":"[source]  description  <code>property</code> <code>writable</code>","text":"<pre><code>description: str | None\n</code></pre> <p>Description of the feature.</p>"},{"location":"python-api/hsfs/feature/#hsfs.feature.Feature.type","title":"[source]  type  <code>property</code> <code>writable</code>","text":"<pre><code>type: str | None\n</code></pre> <p>Data type of the feature in the offline feature store.</p> Not a Python type <p>This type property is not to be confused with Python types. The type property represents the actual data type of the feature in the feature store.</p>"},{"location":"python-api/hsfs/feature/#hsfs.feature.Feature.online_type","title":"[source]  online_type  <code>property</code> <code>writable</code>","text":"<pre><code>online_type: str | None\n</code></pre> <p>Data type of the feature in the online feature store.</p>"},{"location":"python-api/hsfs/feature/#hsfs.feature.Feature.primary","title":"[source]  primary  <code>property</code> <code>writable</code>","text":"<pre><code>primary: bool\n</code></pre> <p>Whether the feature is part of the primary key of the feature group.</p>"},{"location":"python-api/hsfs/feature/#hsfs.feature.Feature.foreign","title":"[source]  foreign  <code>property</code> <code>writable</code>","text":"<pre><code>foreign: bool\n</code></pre> <p>Whether the feature is part of the foreign key of the feature group.</p>"},{"location":"python-api/hsfs/feature/#hsfs.feature.Feature.partition","title":"[source]  partition  <code>property</code> <code>writable</code>","text":"<pre><code>partition: bool\n</code></pre> <p>Whether the feature is part of the partition key of the feature group.</p>"},{"location":"python-api/hsfs/feature/#hsfs.feature.Feature.hudi_precombine_key","title":"[source]  hudi_precombine_key  <code>property</code> <code>writable</code>","text":"<pre><code>hudi_precombine_key: bool\n</code></pre> <p>Whether the feature is part of the hudi precombine key of the feature group.</p>"},{"location":"python-api/hsfs/feature/#hsfs.feature.Feature.default_value","title":"[source]  default_value  <code>property</code> <code>writable</code>","text":"<pre><code>default_value: str | None\n</code></pre> <p>Default value of the feature as string, if the feature was appended to the feature group.</p>"},{"location":"python-api/hsfs/feature/#hsfs.feature.Feature.feature_group_id","title":"[source]  feature_group_id  <code>property</code>","text":"<pre><code>feature_group_id: int | None\n</code></pre> <p>ID of the feature group to which this feature belongs.</p>"},{"location":"python-api/hsfs/feature/#hsfs.feature.Feature.on_demand","title":"[source]  on_demand  <code>property</code> <code>writable</code>","text":"<pre><code>on_demand: bool\n</code></pre> <p>Whether the feature is a on-demand feature computed using on-demand transformation functions.</p>"},{"location":"python-api/hsfs/feature/#hsfs.feature.Feature.is_complex","title":"[source]  is_complex","text":"<pre><code>is_complex() -&gt; bool\n</code></pre> <p>Returns true if the feature has a complex type.</p> Example <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nselected_feature = fg.get_feature(\"min_temp\")\nselected_feature.is_complex()\n</code></pre>"},{"location":"python-api/hsfs/feature/#hsfs.feature.Feature.contains","title":"[source]  contains","text":"<pre><code>contains(other: str | list[Any]) -&gt; filter.Filter\n</code></pre> <p>Construct a filter similar to SQL's <code>IN</code> operator.</p> Deprecated <p><code>contains</code> method is deprecated. Use <code>Feature.isin</code> instead.</p> PARAMETER DESCRIPTION <code>other</code> <p>A single feature value or a list of feature values.</p> <p> TYPE: <code>str | list[Any]</code> </p> RETURNS DESCRIPTION <code>filter.Filter</code> <p>A filter that leaves only the feature values also contained in <code>other</code>.</p>"},{"location":"python-api/hsfs/feature/#hsfs.feature.Feature.isin","title":"[source]  isin","text":"<pre><code>isin(other: str | list[Any]) -&gt; filter.Filter\n</code></pre> <p>Returns <code>IN</code> filter for the feature; replicating the behavior of SQL <code>IN</code> clause.</p>"},{"location":"python-api/hsfs/feature/#hsfs.feature.Feature.like","title":"[source]  like","text":"<pre><code>like(other: Any) -&gt; filter.Filter\n</code></pre> <p>Returns <code>LIKE</code> filter for the feature; replicating the behavior of SQL <code>LIKE</code> clause.</p>"},{"location":"python-api/hsfs/feature_group/","title":"hsfs.feature_group","text":""},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group","title":"hsfs.feature_group","text":""},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.ExternalFeatureGroup","title":"[source]  ExternalFeatureGroup","text":"<p>               Bases: <code>FeatureGroupBase</code></p> <p>A feature group that references data stored outside Hopsworks.</p>"},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.ExternalFeatureGroup.id","title":"[source]  id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>ID of the feature group, set by backend.</p>"},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.ExternalFeatureGroup.description","title":"[source]  description  <code>property</code> <code>writable</code>","text":"<pre><code>description: str | None\n</code></pre> <p>Description of the feature group, as it appears in the UI.</p>"},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.ExternalFeatureGroup.creator","title":"[source]  creator  <code>property</code>","text":"<pre><code>creator: user.User | None\n</code></pre> <p>User who created the feature group.</p>"},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.ExternalFeatureGroup.feature_store_name","title":"[source]  feature_store_name  <code>property</code>","text":"<pre><code>feature_store_name: str | None\n</code></pre> <p>Name of the feature store in which the feature group is located.</p>"},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.ExternalFeatureGroup.save","title":"[source]  save","text":"<pre><code>save() -&gt; None\n</code></pre> <p>Persist the metadata for this external feature group.</p> <p>Without calling this method, your feature group will only exist in your Python Kernel, but not in Hopsworks.</p> <pre><code>query = \"SELECT * FROM sales\"\n\nfg = feature_store.create_external_feature_group(name=\"sales\",\n    version=1,\n    description=\"Physical shop sales features\",\n    data_source=ds,\n    primary_key=['ss_store_sk'],\n    event_time='sale_date'\n)\n\nfg.save()\n</code></pre>"},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.ExternalFeatureGroup.insert","title":"[source]  insert","text":"<pre><code>insert(\n    features: pd.DataFrame\n    | TypeVar(\"pyspark.sql.DataFrame\")\n    | TypeVar(\"pyspark.RDD\")\n    | np.ndarray\n    | list[list],\n    write_options: dict[str, Any] | None = None,\n    validation_options: dict[str, Any] | None = None,\n    wait: bool = False,\n) -&gt; tuple[\n    None,\n    great_expectations.core.ExpectationSuiteValidationResult\n    | None,\n]\n</code></pre> <p>Insert the dataframe feature values ONLY in the online feature store.</p> <p>External Feature Groups contains metadata about feature data in an external storage system. External storage system are usually offline, meaning feature values cannot be retrieved in real-time. In order to use the feature values for real-time use-cases, you can insert them in Hopsoworks Online Feature Store via this method.</p> <p>The Online Feature Store has a single-entry per primary key value, meaining that providing a new value with for a given primary key will overwrite the existing value. No record of the previous value is kept.</p> Example <pre><code># connect to the Feature Store\nfs = ...\n\n# get the External Feature Group instance\nfg = fs.get_feature_group(name=\"external_sales_records\", version=1)\n\n# get the feature values, e.g reading from csv files in a S3 bucket\nfeature_values = ...\n\n# insert the feature values in the online feature store\nfg.insert(feature_values)\n</code></pre> Note <p>Data Validation via Great Expectation is supported if you have attached an expectation suite to your External Feature Group. However, as opposed to regular Feature Groups, this can lead to discrepancies between the data in the external storage system and the online feature store.</p> PARAMETER DESCRIPTION <code>features</code> <p>Features to be saved.</p> <p> TYPE: <code>pd.DataFrame | TypeVar('pyspark.sql.DataFrame') | TypeVar('pyspark.RDD') | np.ndarray | list[list]</code> </p> <code>write_options</code> <p>Additional write options as key-value pairs.</p> <p>When using the <code>python</code> engine, write_options can contain the following entries:</p> <ul> <li>key <code>wait_for_job</code> and value <code>True</code> or <code>False</code> to configure whether or not to the insert call should return only after the Hopsworks Job has finished.   By default it waits.</li> <li>key <code>wait_for_online_ingestion</code> and value <code>True</code> or <code>False</code> to configure whether or not to the save call should return only after the Hopsworks online ingestion has finished.   By default it does not wait.</li> <li>key <code>online_ingestion_options</code> and value a dict to configure waiting on online ingestion.   Applied when <code>wait_for_online_ingestion</code> write option is <code>True</code> or the <code>wait</code> parameter is <code>True</code>.   Supported keys are <code>timeout</code> (seconds to wait, default <code>60</code>, set to <code>0</code> for indefinite) and <code>period</code> (polling interval in seconds, default <code>1</code>).</li> <li>key <code>kafka_producer_config</code> and value an object of type properties used to configure the Kafka client.   To optimize for throughput in high latency connection consider changing producer properties.</li> <li>key <code>internal_kafka</code> and value <code>True</code> or <code>False</code> in case you established connectivity from you Python environment to the internal advertised listeners of the Hopsworks Kafka Cluster.   Defaults to <code>False</code> and will use external listeners when connecting from outside of Hopsworks.</li> </ul> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>validation_options</code> <p>Additional validation options as key-value pairs.</p> <ul> <li>key <code>run_validation</code> boolean value, set to <code>False</code> to skip validation temporarily on ingestion.</li> <li>key <code>save_report</code> boolean value, set to <code>False</code> to skip upload of the validation report to Hopsworks.</li> <li>key <code>ge_validate_kwargs</code> a dictionary containing kwargs for the validate method of Great Expectations.</li> <li>key <code>fetch_expectation_suite</code> a boolean value, by default <code>True</code>, to control whether the expectation suite of the feature group should be fetched before every insert.</li> </ul> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>wait</code> <p>Wait for job and online ingestion to finish before returning. Shortcut for write_options <code>{\"wait_for_job\": False, \"wait_for_online_ingestion\": False}</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>tuple[None, great_expectations.core.ExpectationSuiteValidationResult | None]</code> <p>The validation report if validation is enabled.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>e.g., fail to create feature group, dataframe schema does not match existing feature group schema, etc.</p> <code>hsfs.client.exceptions.DataValidationException</code> <p>If data validation fails and the expectation suite <code>validation_ingestion_policy</code> is set to <code>STRICT</code>. Data is NOT ingested.</p>"},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.ExternalFeatureGroup.read","title":"[source]  read","text":"<pre><code>read(\n    dataframe_type: Literal[\n        \"default\",\n        \"spark\",\n        \"pandas\",\n        \"polars\",\n        \"numpy\",\n        \"python\",\n    ] = \"default\",\n    online: bool = False,\n    read_options: dict[str, Any] | None = None,\n) -&gt; (\n    TypeVar(\"pyspark.sql.DataFrame\")\n    | TypeVar(\"pyspark.RDD\")\n    | pd.DataFrame\n    | pl.DataFrame\n    | np.ndarray\n)\n</code></pre> <p>Get the feature group as a DataFrame.</p> Example <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\ndf = fg.read()\n</code></pre> Engine Support <p>Spark only</p> <p>Reading an External Feature Group directly into a Pandas Dataframe using Python/Pandas as Engine is not supported, however, you can use the Query API to create Feature Views/Training Data containing External Feature Groups.</p> PARAMETER DESCRIPTION <code>dataframe_type</code> <p>The type of the returned dataframe. By default, maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</p> <p> TYPE: <code>Literal['default', 'spark', 'pandas', 'polars', 'numpy', 'python']</code> DEFAULT: <code>'default'</code> </p> <code>online</code> <p>If <code>True</code> read from online feature store.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>read_options</code> <p>Additional options as key/value pairs to pass to the spark engine.</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>TypeVar('pyspark.sql.DataFrame') | TypeVar('pyspark.RDD') | pd.DataFrame | pl.DataFrame | np.ndarray</code> <p>One of:</p> <code>TypeVar('pyspark.sql.DataFrame') | TypeVar('pyspark.RDD') | pd.DataFrame | pl.DataFrame | np.ndarray</code> <ul> <li><code>DataFrame</code>: The spark dataframe containing the feature data.</li> </ul> <code>TypeVar('pyspark.sql.DataFrame') | TypeVar('pyspark.RDD') | pd.DataFrame | pl.DataFrame | np.ndarray</code> <ul> <li><code>pyspark.DataFrame</code>: A Spark DataFrame.</li> </ul> <code>TypeVar('pyspark.sql.DataFrame') | TypeVar('pyspark.RDD') | pd.DataFrame | pl.DataFrame | np.ndarray</code> <ul> <li><code>pandas.DataFrame</code>: A Pandas DataFrame.</li> </ul> <code>TypeVar('pyspark.sql.DataFrame') | TypeVar('pyspark.RDD') | pd.DataFrame | pl.DataFrame | np.ndarray</code> <ul> <li><code>numpy.ndarray</code>: A two-dimensional Numpy array.</li> </ul> <code>TypeVar('pyspark.sql.DataFrame') | TypeVar('pyspark.RDD') | pd.DataFrame | pl.DataFrame | np.ndarray</code> <ul> <li><code>list</code>: A two-dimensional Python list.</li> </ul> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p> <code>hopsworks.client.exceptions.FeatureStoreException</code> <p>If trying to read an external feature group directly in.</p>"},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.ExternalFeatureGroup.show","title":"[source]  show","text":"<pre><code>show(n: int, online: bool = False) -&gt; list[list[Any]]\n</code></pre> <p>Show the first <code>n</code> rows of the feature group.</p> Example <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\n# make a query and show top 5 rows\nfg.select(['date','weekly_sales','is_holiday']).show(5)\n</code></pre> PARAMETER DESCRIPTION <code>n</code> <p>Number of rows to show.</p> <p> TYPE: <code>int</code> </p> <code>online</code> <p>If <code>True</code> read from online feature store.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.ExternalFeatureGroup.find_neighbors","title":"[source]  find_neighbors","text":"<pre><code>find_neighbors(\n    embedding: list[int | float],\n    col: str | None = None,\n    k: int | None = 10,\n    filter: Filter | Logic | None = None,\n    options: dict | None = None,\n) -&gt; list[tuple[float, list[Any]]]\n</code></pre> <p>Finds the nearest neighbors for a given embedding in the vector database.</p> <p>If <code>filter</code> is specified, or if embedding feature is stored in default project index, the number of results returned may be less than k. Try using a large value of k and extract the top k items from the results if needed.</p> PARAMETER DESCRIPTION <code>embedding</code> <p>The target embedding for which neighbors are to be found.</p> <p> TYPE: <code>list[int | float]</code> </p> <code>col</code> <p>The column name used to compute similarity score. Required only if there are multiple embeddings.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>k</code> <p>The number of nearest neighbors to retrieve.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>10</code> </p> <code>filter</code> <p>A filter expression to restrict the search space.</p> <p> TYPE: <code>Filter | Logic | None</code> DEFAULT: <code>None</code> </p> <code>options</code> <p>The options used for the request to the vector database. The keys are attribute values of the <code>hsfs.core.opensearch.OpensearchRequestOption</code> class.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>list[tuple[float, list[Any]]]</code> <p>A list of tuples representing the nearest neighbors.</p> <code>list[tuple[float, list[Any]]]</code> <p>Each tuple contains: <code>(The similarity score, A list of feature values)</code>.</p> Example <pre><code>embedding_index = EmbeddingIndex()\nembedding_index.add_embedding(name=\"user_vector\", dimension=3)\nfg = fs.create_feature_group(\n            name='air_quality',\n            embedding_index = embedding_index,\n            version=1,\n            primary_key=['id1'],\n            online_enabled=True,\n        )\nfg.insert(data)\nfg.find_neighbors(\n    [0.1, 0.2, 0.3],\n    k=5,\n)\n\n# apply filter\nfg.find_neighbors(\n    [0.1, 0.2, 0.3],\n    k=5,\n    filter=(fg.id1 &gt; 10) &amp; (fg.id1 &lt; 30)\n)\n</code></pre>"},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.FeatureGroup","title":"[source]  FeatureGroup","text":"<p>               Bases: <code>FeatureGroupBase</code></p>"},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.FeatureGroup.id","title":"[source]  id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Feature group id.</p>"},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.FeatureGroup.description","title":"[source]  description  <code>property</code> <code>writable</code>","text":"<pre><code>description: str | None\n</code></pre> <p>Description of the feature group contents.</p>"},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.FeatureGroup.time_travel_format","title":"[source]  time_travel_format  <code>property</code> <code>writable</code>","text":"<pre><code>time_travel_format: str | None\n</code></pre> <p>Setting of the feature group time travel format.</p>"},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.FeatureGroup.partition_key","title":"[source]  partition_key  <code>property</code> <code>writable</code>","text":"<pre><code>partition_key: list[str]\n</code></pre> <p>List of features building the partition key.</p>"},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.FeatureGroup.hudi_precombine_key","title":"[source]  hudi_precombine_key  <code>property</code> <code>writable</code>","text":"<pre><code>hudi_precombine_key: str | None\n</code></pre> <p>Feature name that is the hudi precombine key.</p>"},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.FeatureGroup.feature_store_name","title":"[source]  feature_store_name  <code>property</code>","text":"<pre><code>feature_store_name: str | None\n</code></pre> <p>Name of the feature store in which the feature group is located.</p>"},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.FeatureGroup.creator","title":"[source]  creator  <code>property</code>","text":"<pre><code>creator: user.User | None\n</code></pre> <p>Username of the creator.</p>"},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.FeatureGroup.created","title":"[source]  created  <code>property</code>","text":"<pre><code>created: str | None\n</code></pre> <p>Timestamp when the feature group was created.</p>"},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.FeatureGroup.stream","title":"[source]  stream  <code>property</code> <code>writable</code>","text":"<pre><code>stream: bool\n</code></pre> <p>Whether to enable real time stream writing capabilities.</p>"},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.FeatureGroup.parents","title":"[source]  parents  <code>property</code> <code>writable</code>","text":"<pre><code>parents: list[explicit_provenance.Links]\n</code></pre> <p>Parent feature groups as origin of the data in the current feature group.</p> <p>This is part of explicit provenance.</p>"},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.FeatureGroup.materialization_job","title":"[source]  materialization_job  <code>property</code>","text":"<pre><code>materialization_job: Job | None\n</code></pre> <p>Get the Job object reference for the materialization job for this Feature Group.</p>"},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.FeatureGroup.statistics","title":"[source]  statistics  <code>property</code>","text":"<pre><code>statistics: Statistics\n</code></pre> <p>Get the latest computed statistics for the whole feature group.</p>"},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.FeatureGroup.transformation_functions","title":"[source]  transformation_functions  <code>property</code> <code>writable</code>","text":"<pre><code>transformation_functions: list[TransformationFunction]\n</code></pre> <p>Get transformation functions.</p>"},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.FeatureGroup.offline_backfill_every_hr","title":"[source]  offline_backfill_every_hr  <code>property</code> <code>writable</code>","text":"<pre><code>offline_backfill_every_hr: int | str | None\n</code></pre> <p>On Feature Group creation, used to set scheduled run of the materialisation job.</p>"},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.FeatureGroup.sink_enabled","title":"[source]  sink_enabled  <code>property</code>","text":"<pre><code>sink_enabled: bool\n</code></pre> <p>Get whether sink is enabled for this feature group.</p>"},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.FeatureGroup.sink_job","title":"[source]  sink_job  <code>property</code>","text":"<pre><code>sink_job: job.Job | None\n</code></pre> <p>Return the sink job created for this feature group, if any.</p>"},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.FeatureGroup.sink_job_conf","title":"[source]  sink_job_conf  <code>property</code> <code>writable</code>","text":"<pre><code>sink_job_conf: SinkJobConfiguration\n</code></pre> <p>Sink job configuration object defining the settings for sink job of the feature group.</p>"},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.FeatureGroup.read","title":"[source]  read","text":"<pre><code>read(\n    wallclock_time: str\n    | int\n    | datetime\n    | date\n    | None = None,\n    online: bool = False,\n    dataframe_type: Literal[\n        \"default\",\n        \"spark\",\n        \"pandas\",\n        \"polars\",\n        \"numpy\",\n        \"python\",\n    ] = \"default\",\n    read_options: dict | None = None,\n) -&gt; (\n    pd.DataFrame\n    | np.ndarray\n    | list[list[Any]]\n    | TypeVar(\"pyspark.sql.DataFrame\")\n    | TypeVar(\"pyspark.RDD\")\n    | pl.DataFrame\n)\n</code></pre> <p>Read the feature group into a dataframe.</p> <p>Reads the feature group by default from the offline storage as Spark DataFrame on Hopsworks and Databricks, and as Pandas dataframe on AWS Sagemaker and pure Python environments.</p> <p>Set <code>online</code> to <code>True</code> to read from the online storage, or change <code>dataframe_type</code> to read as a different format.</p> Reading feature group as of latest state <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\nfg.read()\n</code></pre> Reading feature group as of specific point in time: <pre><code>fg = fs.get_or_create_feature_group(...)\nfg.read(\"2020-10-20 07:34:11\")\n</code></pre> PARAMETER DESCRIPTION <code>wallclock_time</code> <p>If specified, retrieves feature group as of specific point in time. If not specified, returns as of most recent time. Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code>.</p> <p> TYPE: <code>str | int | datetime | date | None</code> DEFAULT: <code>None</code> </p> <code>online</code> <p>If <code>True</code>, read from online feature store.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>dataframe_type</code> <p>The type of the returned dataframe. By default, maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</p> <p> TYPE: <code>Literal['default', 'spark', 'pandas', 'polars', 'numpy', 'python']</code> DEFAULT: <code>'default'</code> </p> <code>read_options</code> <p>Additional options as key/value pairs to pass to the execution engine.</p> <p>For spark engine: Dictionary of read options for Spark.</p> <p>For python engine: - key <code>\"arrow_flight_config\"</code> to pass a dictionary of arrow flight configurations.   For example: <code>{\"arrow_flight_config\": {\"timeout\": 900}}</code>. - key <code>\"pandas_types\"</code> and value <code>True</code> to retrieve columns as Pandas nullable types rather than numpy/object(string) types (experimental).</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>pd.DataFrame | np.ndarray | list[list[Any]] | TypeVar('pyspark.sql.DataFrame') | TypeVar('pyspark.RDD') | pl.DataFrame</code> <p>One of the following:</p> <code>pd.DataFrame | np.ndarray | list[list[Any]] | TypeVar('pyspark.sql.DataFrame') | TypeVar('pyspark.RDD') | pl.DataFrame</code> <ul> <li><code>DataFrame</code>: The spark dataframe containing the feature data.</li> </ul> <code>pd.DataFrame | np.ndarray | list[list[Any]] | TypeVar('pyspark.sql.DataFrame') | TypeVar('pyspark.RDD') | pl.DataFrame</code> <ul> <li><code>pyspark.DataFrame</code>: A Spark DataFrame.</li> </ul> <code>pd.DataFrame | np.ndarray | list[list[Any]] | TypeVar('pyspark.sql.DataFrame') | TypeVar('pyspark.RDD') | pl.DataFrame</code> <ul> <li><code>pandas.DataFrame</code>: A Pandas DataFrame.</li> </ul> <code>pd.DataFrame | np.ndarray | list[list[Any]] | TypeVar('pyspark.sql.DataFrame') | TypeVar('pyspark.RDD') | pl.DataFrame</code> <ul> <li><code>polars.DataFrame</code>: A Polars DataFrame.</li> </ul> <code>pd.DataFrame | np.ndarray | list[list[Any]] | TypeVar('pyspark.sql.DataFrame') | TypeVar('pyspark.RDD') | pl.DataFrame</code> <ul> <li><code>numpy.ndarray</code>: A two-dimensional Numpy array.</li> </ul> <code>pd.DataFrame | np.ndarray | list[list[Any]] | TypeVar('pyspark.sql.DataFrame') | TypeVar('pyspark.RDD') | pl.DataFrame</code> <ul> <li><code>list</code>: A two-dimensional Python list.</li> </ul> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>No data is available for feature group with this commit date, if time travel enabled.</p>"},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.FeatureGroup.read_changes","title":"[source]  read_changes","text":"<pre><code>read_changes(\n    start_wallclock_time: str | int | datetime | date,\n    end_wallclock_time: str | int | datetime | date,\n    read_options: dict | None = None,\n) -&gt; (\n    pd.DataFrame\n    | np.ndarray\n    | list[list[Any]]\n    | TypeVar(\"pyspark.sql.DataFrame\")\n    | TypeVar(\"pyspark.RDD\")\n    | pl.DataFrame\n)\n</code></pre> <p>Reads updates of this feature that occurred between specified points in time.</p> Deprecated <p><code>read_changes</code> method is deprecated. Use <code>as_of(end_wallclock_time, exclude_until=start_wallclock_time).read(read_options=read_options)</code> instead.</p> Pyspark/Spark Only <p>Apache HUDI exclusively supports Time Travel and Incremental Query via Spark Context.</p> Warning <p>This function only works for feature groups with time_travel_format='HUDI'.</p> PARAMETER DESCRIPTION <code>start_wallclock_time</code> <p>Start time of the time travel query. Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code>.</p> <p> TYPE: <code>str | int | datetime | date</code> </p> <code>end_wallclock_time</code> <p>End time of the time travel query. Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code>.</p> <p> TYPE: <code>str | int | datetime | date</code> </p> <code>read_options</code> <p>Additional options as key/value pairs to pass to the execution engine. For spark engine, it is a dictionary of read options for Spark.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>pd.DataFrame | np.ndarray | list[list[Any]] | TypeVar('pyspark.sql.DataFrame') | TypeVar('pyspark.RDD') | pl.DataFrame</code> <p>The spark dataframe containing the incremental changes of feature data.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>No data is available for feature group with this commit date.</p> <code>hopsworks.client.exceptions.FeatureStoreException</code> <p>If the feature group does not have <code>HUDI</code> time travel format.</p>"},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.FeatureGroup.find_neighbors","title":"[source]  find_neighbors","text":"<pre><code>find_neighbors(\n    embedding: list[int | float],\n    col: str | None = None,\n    k: int | None = 10,\n    filter: Filter | Logic | None = None,\n    options: dict | None = None,\n) -&gt; list[tuple[float, list[Any]]]\n</code></pre> <p>Finds the nearest neighbors for a given embedding in the vector database.</p> <p>If <code>filter</code> is specified, or if embedding feature is stored in default project index, the number of results returned may be less than k. Try using a large value of k and extract the top k items from the results if needed.</p> PARAMETER DESCRIPTION <code>embedding</code> <p>The target embedding for which neighbors are to be found.</p> <p> TYPE: <code>list[int | float]</code> </p> <code>col</code> <p>The column name used to compute similarity score. Required only if there are multiple embeddings.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>k</code> <p>The number of nearest neighbors to retrieve.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>10</code> </p> <code>filter</code> <p>A filter expression to restrict the search space.</p> <p> TYPE: <code>Filter | Logic | None</code> DEFAULT: <code>None</code> </p> <code>options</code> <p>The options used for the request to the vector database. The keys are attribute values of the <code>hsfs.core.opensearch.OpensearchRequestOption</code> class.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>list[tuple[float, list[Any]]]</code> <p>A list of tuples representing the nearest neighbors.</p> <code>list[tuple[float, list[Any]]]</code> <p>Each tuple contains: <code>(The similarity score, A list of feature values)</code></p> Example <pre><code>embedding_index = EmbeddingIndex()\nembedding_index.add_embedding(name=\"user_vector\", dimension=3)\nfg = fs.create_feature_group(\n            name='air_quality',\n            embedding_index = embedding_index,\n            version=1,\n            primary_key=['id1'],\n            online_enabled=True,\n        )\nfg.insert(data)\nfg.find_neighbors(\n    [0.1, 0.2, 0.3],\n    k=5,\n)\n\n# apply filter\nfg.find_neighbors(\n    [0.1, 0.2, 0.3],\n    k=5,\n    filter=(fg.id1 &gt; 10) &amp; (fg.id1 &lt; 30)\n)\n</code></pre>"},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.FeatureGroup.show","title":"[source]  show","text":"<pre><code>show(n: int, online: bool = False) -&gt; list[list[Any]]\n</code></pre> <p>Show the first <code>n</code> rows of the feature group.</p> Example <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\n# make a query and show top 5 rows\nfg.select(['date','weekly_sales','is_holiday']).show(5)\n</code></pre> PARAMETER DESCRIPTION <code>n</code> <p>Number of rows to show.</p> <p> TYPE: <code>int</code> </p> <code>online</code> <p>If <code>True</code> read from online feature store.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.FeatureGroup.save","title":"[source]  save","text":"<pre><code>save(\n    features: pd.DataFrame\n    | pl.DataFrame\n    | TypeVar(\"pyspark.sql.DataFrame\")\n    | TypeVar(\"pyspark.RDD\")\n    | np.ndarray\n    | list[feature.Feature] = None,\n    write_options: dict[str, Any] | None = None,\n    validation_options: dict[str, Any] | None = None,\n    wait: bool = False,\n) -&gt; tuple[\n    Job | None,\n    great_expectations.core.ExpectationSuiteValidationResult\n    | None,\n]\n</code></pre> <p>Persist the metadata and materialize the feature group to the feature store.</p> Changed in 3.3.0 <p><code>insert</code> and <code>save</code> methods are now async by default in non-spark clients. To achieve the old behaviour, set <code>wait</code> argument to <code>True</code>.</p> <p>Calling <code>save</code> creates the metadata for the feature group in the feature store. If a Pandas DataFrame, Polars DatFrame, RDD or Ndarray is provided, the data is written to the online/offline feature store as specified. By default, this writes the feature group to the offline storage, and if <code>online_enabled</code> for the feature group, also to the online feature store. The <code>features</code> dataframe can be a Spark DataFrame or RDD, a Pandas DataFrame, or a two-dimensional Numpy array or a two-dimensional Python nested list.</p> PARAMETER DESCRIPTION <code>features</code> <p>Features to be saved. This argument is optional if the feature list is provided in the <code>create_feature_group</code> or in the <code>get_or_create_feature_group</code> method invokation.</p> <p> TYPE: <code>pd.DataFrame | pl.DataFrame | TypeVar('pyspark.sql.DataFrame') | TypeVar('pyspark.RDD') | np.ndarray | list[feature.Feature]</code> DEFAULT: <code>None</code> </p> <code>write_options</code> <p>Additional write options as key-value pairs.</p> <p>When using the <code>python</code> engine, write_options can contain the following entries:</p> <ul> <li>key <code>spark</code> and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to write data into the feature group.</li> <li>key <code>wait_for_job</code> and value <code>True</code> or <code>False</code> to configure whether or not to the save call should return only after the Hopsworks Job has finished.   By default it does not wait.</li> <li>key <code>wait_for_online_ingestion</code> and value <code>True</code> or <code>False</code> to configure whether or not to the save call should return only after the Hopsworks online ingestion has finished.   By default it does not wait.</li> <li>key <code>start_offline_backfill</code> and value <code>True</code> or <code>False</code> to configure whether or not to start the materialization job to write data to the offline storage. <code>start_offline_backfill</code> is deprecated.   Use <code>start_offline_materialization</code> instead.</li> <li>key <code>start_offline_materialization</code> and value <code>True</code> or <code>False</code> to configure whether or not to start the materialization job to write data to the offline storage.   By default the materialization job gets started immediately.</li> <li>key <code>kafka_producer_config</code> and value an object of type properties used to configure the Kafka client.   To optimize for throughput in high latency connection, consider changing the producer properties.</li> <li>key <code>internal_kafka</code> and value <code>True</code> or <code>False</code> in case you established connectivity from you Python environment to the internal advertised listeners of the Hopsworks Kafka Cluster.   Defaults to <code>False</code> and will use external listeners when connecting from outside of Hopsworks.</li> <li>key <code>delta.enableChangeDataFeed</code> set to a string value of true or false to enable or disable cdf operations on the feature group delta table.   Set to true by default on Feature Group creation.</li> </ul> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>validation_options</code> <p>Additional validation options as key-value pairs.</p> <ul> <li>key <code>run_validation</code> boolean value, set to <code>False</code> to skip validation temporarily on ingestion.</li> <li>key <code>save_report</code> boolean value, set to <code>False</code> to skip upload of the validation report to Hopsworks.</li> <li>key <code>ge_validate_kwargs</code> a dictionary containing kwargs for the validate method of Great Expectations.</li> <li>key <code>schema_validation</code> boolean value, set to <code>True</code> to validate the schema.</li> </ul> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>wait</code> <p>Wait for job and online ingestion to finish before returning. Shortcut for write_options <code>{\"wait_for_job\": False, \"wait_for_online_ingestion\": False}</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>tuple[Job | None, great_expectations.core.ExpectationSuiteValidationResult | None]</code> <p>When using the <code>python</code> engine, it returns the Hopsworks Job that was launched to ingest the feature group data.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.FeatureGroup.insert","title":"[source]  insert","text":"<pre><code>insert(\n    features: pd.DataFrame\n    | pl.DataFrame\n    | TypeVar(\"pyspark.sql.DataFrame\")\n    | TypeVar(\"pyspark.RDD\")\n    | np.ndarray\n    | list[list],\n    overwrite: bool = False,\n    operation: Literal[\"insert\", \"upsert\"] = \"upsert\",\n    storage: str | None = None,\n    write_options: dict[str, Any] | None = None,\n    validation_options: dict[str, Any] | None = None,\n    wait: bool = False,\n    transformation_context: dict[str, Any] = None,\n    transform: bool = True,\n) -&gt; tuple[Job | None, ValidationReport | None]\n</code></pre> <p>Persist the metadata and materialize the feature group to the feature store or insert data from a dataframe into the existing feature group.</p> <p>Incrementally insert data to a feature group or overwrite all data contained in the feature group. By default, the data is inserted into the offline storage as well as the online storage if the feature group is <code>online_enabled=True</code>.</p> <p>The <code>features</code> dataframe can be a Spark DataFrame or RDD, a Pandas DataFrame, a Polars DataFrame or a two-dimensional Numpy array or a two-dimensional Python nested list. If statistics are enabled, statistics are recomputed for the entire feature group. If feature group's time travel format is <code>HUDI</code> then <code>operation</code> argument can be either <code>insert</code> or <code>upsert</code>.</p> <p>If feature group doesn't exist the insert method will create the necessary metadata the first time it is invoked and writes the specified <code>features</code> dataframe as feature group to the online/offline feature store.</p> Changed in 3.3.0 <p><code>insert</code> and <code>save</code> methods are now async by default in non-spark clients. To achieve the old behaviour, set <code>wait</code> argument to <code>True</code>.</p> Upsert new feature data with time travel format <code>HUDI</code> <pre><code># connect to the Feature Store\nfs = ...\n\nfg = fs.get_or_create_feature_group(\n    name='bitcoin_price',\n    description='Bitcoin price aggregated for days',\n    version=1,\n    primary_key=['unix'],\n    online_enabled=True,\n    event_time='unix'\n)\n\nfg.insert(df_bitcoin_processed)\n</code></pre> Async insert <pre><code># connect to the Feature Store\nfs = ...\n\nfg1 = fs.get_or_create_feature_group(\n    name='feature_group_name1',\n    description='Description of the first FG',\n    version=1,\n    primary_key=['unix'],\n    online_enabled=True,\n    event_time='unix'\n)\n# async insertion in order not to wait till finish of the job\nfg.insert(df_for_fg1, write_options={\"wait_for_job\" : False})\n\nfg2 = fs.get_or_create_feature_group(\n    name='feature_group_name2',\n    description='Description of the second FG',\n    version=1,\n    primary_key=['unix'],\n    online_enabled=True,\n    event_time='unix'\n)\nfg.insert(df_for_fg2)\n</code></pre> PARAMETER DESCRIPTION <code>features</code> <p>Features to be saved.</p> <p> TYPE: <code>pd.DataFrame | pl.DataFrame | TypeVar('pyspark.sql.DataFrame') | TypeVar('pyspark.RDD') | np.ndarray | list[list]</code> </p> <code>overwrite</code> <p>Drop all data in the feature group before inserting new data. This does not affect metadata.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>operation</code> <p>Apache Hudi operation type <code>\"insert\"</code> or <code>\"upsert\"</code>.</p> <p> TYPE: <code>Literal['insert', 'upsert']</code> DEFAULT: <code>'upsert'</code> </p> <code>storage</code> <p>Overwrite default behaviour, write to offline storage only with <code>\"offline\"</code> or online only with <code>\"online\"</code>. If the streaming APIs are enabled, specifying the storage option is not supported.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>write_options</code> <p>Additional write options as key-value pairs.</p> <p>When using the <code>python</code> engine, write_options can contain the following entries:</p> <ul> <li>key <code>spark</code> and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to write data into the feature group.</li> <li>key <code>wait_for_job</code> and value <code>True</code> or <code>False</code> to configure whether or not to the insert call should return only after the Hopsworks Job has finished. By default it waits.</li> <li>key <code>wait_for_online_ingestion</code> and value <code>True</code> or <code>False</code> to configure whether or not to the save call should return only after the Hopsworks online ingestion has finished. By default it does not wait.</li> <li>key <code>online_ingestion_options</code> and value a dict to configure waiting on online ingestion.   Applied when <code>wait_for_online_ingestion</code> write option is <code>True</code> or the <code>wait</code> parameter is <code>True</code>.   Supported keys are <code>timeout</code> (seconds to wait, default <code>60</code>, set to <code>0</code> for indefinite) and <code>period</code> (polling interval in seconds, default <code>1</code>).</li> <li>key <code>start_offline_backfill</code> and value <code>True</code> or <code>False</code> to configure whether or not to start the materialization job to write data to the offline storage.   <code>start_offline_backfill</code> is deprecated.   Use <code>start_offline_materialization</code> instead.</li> <li>key <code>start_offline_materialization</code> and value <code>True</code> or <code>False</code> to configure whether or not to start the materialization job to write data to the offline storage.   By default the materialization job gets started immediately.</li> <li>key <code>kafka_producer_config</code> and value an object of type properties used to configure the Kafka client.   To optimize for throughput in high latency connection consider changing producer properties.</li> <li>key <code>internal_kafka</code> and value <code>True</code> or <code>False</code> in case you established connectivity from you Python environment to the internal advertised listeners of the Hopsworks Kafka Cluster.   Defaults to <code>False</code> and will use external listeners when connecting from outside of Hopsworks.</li> <li>key <code>delta.enableChangeDataFeed</code> set to a string value of true or false to enable or disable cdf operations on the feature group delta table.   Set to true by default on Feature Group creation.</li> </ul> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>validation_options</code> <p>Additional validation options as key-value pairs.</p> <ul> <li>key <code>run_validation</code> boolean value, set to <code>False</code> to skip validation temporarily on ingestion.</li> <li>key <code>save_report</code> boolean value, set to <code>False</code> to skip upload of the validation report to Hopsworks.</li> <li>key <code>ge_validate_kwargs</code> a dictionary containing kwargs for the validate method of Great Expectations.</li> <li>key <code>fetch_expectation_suite</code> a boolean value, by default <code>True</code>, to control whether the expectation suite of the feature group should be fetched before every insert.</li> <li>key <code>schema_validation</code> boolean value, set to <code>True</code> to validate the schema.</li> </ul> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>wait</code> <p>Wait for job and online ingestion to finish before returning. Shortcut for write_options <code>{\"wait_for_job\": False, \"wait_for_online_ingestion\": False}</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>transformation_context</code> <p>A dictionary mapping variable names to objects that will be provided as contextual information to the transformation function at runtime. The <code>context</code> variable must be explicitly defined as parameters in the transformation function for these to be accessible during execution.</p> <p> TYPE: <code>dict[str, Any]</code> DEFAULT: <code>None</code> </p> <code>transform</code> <p>When set to <code>False</code>, the dataframe is inserted without applying any on-demand transformations In this case, all required on-demand features must already exist in the provided dataframe.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Job</code> <p>The job information if python engine is used.</p> <p> TYPE: <code>Job | None</code> </p> <code>ValidationReport</code> <p>The validation report if validation is enabled.</p> <p> TYPE: <code>ValidationReport | None</code> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>e.g., fail to create feature group, dataframe schema does not match existing feature group schema, etc.</p> <code>hsfs.client.exceptions.DataValidationException</code> <p>If data validation fails and the expectation suite <code>validation_ingestion_policy</code> is set to <code>STRICT</code>. Data is NOT ingested.</p>"},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.FeatureGroup.multi_part_insert","title":"[source]  multi_part_insert","text":"<pre><code>multi_part_insert(\n    features: pd.DataFrame\n    | pl.DataFrame\n    | TypeVar(\"pyspark.sql.DataFrame\")\n    | TypeVar(\"pyspark.RDD\")\n    | np.ndarray\n    | list[list]\n    | None = None,\n    overwrite: bool = False,\n    operation: Literal[\"insert\", \"upsert\"] = \"upsert\",\n    storage: str | None = None,\n    write_options: dict[str, Any] | None = None,\n    validation_options: dict[str, Any] | None = None,\n    transformation_context: dict[str, Any] = None,\n    transform: bool = True,\n) -&gt; (\n    tuple[Job | None, ValidationReport | None]\n    | feature_group_writer.FeatureGroupWriter\n)\n</code></pre> <p>Get FeatureGroupWriter for optimized multi part inserts or call this method to start manual multi part optimized inserts.</p> <p>In use cases where very small batches (1 to 1000) rows per Dataframe need to be written to the feature store repeatedly, it might be inefficient to use the standard <code>feature_group.insert()</code> method as it performs some background actions to update the metadata of the feature group object first.</p> <p>For these cases, the feature group provides the <code>multi_part_insert</code> API, which is optimized for writing many small Dataframes after another.</p> <p>There are two ways to use this API:</p> Python Context Manager <p>Using the Python <code>with</code> syntax you can acquire a FeatureGroupWriter object that implements the same <code>multi_part_insert</code> API.</p> <pre><code>feature_group = fs.get_or_create_feature_group(\"fg_name\", version=1)\n\nwith feature_group.multi_part_insert() as writer:\n    # run inserts in a loop:\n    while loop:\n        small_batch_df = ...\n        writer.insert(small_batch_df)\n</code></pre> <p>The writer batches the small Dataframes and transmits them to Hopsworks efficiently. When exiting the context, the feature group writer is sure to exit only once all the rows have been transmitted.</p> Multi part insert with manual context management <p>Instead of letting Python handle the entering and exiting of the multi part insert context, you can start and finalize the context manually.</p> <pre><code>feature_group = fs.get_or_create_feature_group(\"fg_name\", version=1)\n\nwhile loop:\n    small_batch_df = ...\n    feature_group.multi_part_insert(small_batch_df)\n\n# IMPORTANT: finalize the multi part insert to make sure all rows\n# have been transmitted\nfeature_group.finalize_multi_part_insert()\n</code></pre> <p>Note that the first call to <code>multi_part_insert</code> initiates the context and be sure to finalize it. The <code>finalize_multi_part_insert</code> is a blocking call that returns once all rows have been transmitted.</p> <p>Once you are done with the multi part insert, it is good practice to start the materialization job in order to write the data to the offline storage:</p> <pre><code>feature_group.materialization_job.run(await_termination=True)\n</code></pre> PARAMETER DESCRIPTION <code>features</code> <p>Features to be saved.</p> <p> TYPE: <code>pd.DataFrame | pl.DataFrame | TypeVar('pyspark.sql.DataFrame') | TypeVar('pyspark.RDD') | np.ndarray | list[list] | None</code> DEFAULT: <code>None</code> </p> <code>overwrite</code> <p>Drop all data in the feature group before inserting new data. This does not affect metadata.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>operation</code> <p>Apache Hudi operation type <code>\"insert\"</code> or <code>\"upsert\"</code>.</p> <p> TYPE: <code>Literal['insert', 'upsert']</code> DEFAULT: <code>'upsert'</code> </p> <code>storage</code> <p>Overwrite default behaviour, write to offline storage only with <code>\"offline\"</code> or online only with <code>\"online\"</code>.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>write_options</code> <p>Additional write options as key-value pairs.</p> <p>When using the <code>python</code> engine, write_options can contain the following entries:</p> <ul> <li>key <code>spark</code> and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to write data into the feature group.</li> <li>key <code>wait_for_job</code> and value <code>True</code> or <code>False</code> to configure whether or not to the insert call should return only after the Hopsworks Job has finished.   By default it waits.</li> <li>key <code>start_offline_backfill</code> and value <code>True</code> or <code>False</code> to configure whether or not to start the materialization job to write data to the offline storage.   <code>start_offline_backfill</code> is deprecated.   Use <code>start_offline_materialization</code> instead.</li> <li>key <code>start_offline_materialization</code> and value <code>True</code> or <code>False</code> to configure whether or not to start the materialization job to write data to the offline storage.   By default the materialization job does not get started automatically for multi part inserts.</li> <li>key <code>kafka_producer_config</code> and value an object of type properties used to configure the Kafka client.   To optimize for throughput in high latency connection consider changing producer properties.</li> <li>key <code>internal_kafka</code> and value <code>True</code> or <code>False</code> in case you established connectivity from you Python environment to the internal advertised listeners of the Hopsworks Kafka Cluster.   Defaults to <code>False</code> and will use external listeners when connecting from outside of Hopsworks.</li> </ul> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>validation_options</code> <p>Additional validation options as key-value pairs.</p> <ul> <li>key <code>run_validation</code> boolean value, set to <code>False</code> to skip validation temporarily on ingestion.</li> <li>key <code>save_report</code> boolean value, set to <code>False</code> to skip upload of the validation report to Hopsworks.</li> <li>key <code>ge_validate_kwargs</code> a dictionary containing kwargs for the validate method of Great Expectations.</li> <li>key <code>fetch_expectation_suite</code> a boolean value, by default <code>False</code> for multi part inserts, to control whether the expectation suite of the feature group should be fetched before every insert.</li> </ul> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>transformation_context</code> <p>A dictionary mapping variable names to objects that will be provided as contextual information to the transformation function at runtime. The <code>context</code> variable must be explicitly defined as parameters in the transformation function for these to be accessible during execution.</p> <p> TYPE: <code>dict[str, Any]</code> DEFAULT: <code>None</code> </p> <code>transform</code> <p>When set to <code>False</code>, the dataframe is inserted without applying any on-demand transformations. In this case, all required on-demand features must already exist in the provided dataframe.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>tuple[Job | None, ValidationReport | None] | feature_group_writer.FeatureGroupWriter</code> <p>One of:</p> <code>tuple[Job | None, ValidationReport | None] | feature_group_writer.FeatureGroupWriter</code> <ul> <li>A tuple with job information if python engine is used and the validation report if validation is enabled, or</li> </ul> <code>tuple[Job | None, ValidationReport | None] | feature_group_writer.FeatureGroupWriter</code> <ul> <li><code>FeatureGroupWriter</code> when used as a context manager with Python <code>with</code> statement.</li> </ul>"},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.FeatureGroup.finalize_multi_part_insert","title":"[source]  finalize_multi_part_insert","text":"<pre><code>finalize_multi_part_insert() -&gt; None\n</code></pre> <p>Finalizes and exits the multi part insert context opened by <code>multi_part_insert</code> in a blocking fashion once all rows have been transmitted.</p> Multi part insert with manual context management <p>Instead of letting Python handle the entering and exiting of the multi part insert context, you can start and finalize the context manually.</p> <pre><code>feature_group = fs.get_or_create_feature_group(\"fg_name\", version=1)\n\nwhile loop:\n    small_batch_df = ...\n    feature_group.multi_part_insert(small_batch_df)\n\n# IMPORTANT: finalize the multi part insert to make sure all rows\n# have been transmitted\nfeature_group.finalize_multi_part_insert()\n</code></pre> <p>Note that the first call to <code>multi_part_insert</code> initiates the context and be sure to finalize it. The <code>finalize_multi_part_insert</code> is a blocking call that returns once all rows have been transmitted.</p>"},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.FeatureGroup.insert_stream","title":"[source]  insert_stream","text":"<pre><code>insert_stream(\n    features: TypeVar(\"pyspark.sql.DataFrame\"),\n    query_name: str | None = None,\n    output_mode: Literal[\n        \"append\", \"complete\", \"update\"\n    ] = \"append\",\n    await_termination: bool = False,\n    timeout: int | None = None,\n    checkpoint_dir: str | None = None,\n    write_options: dict[str, Any] | None = None,\n    transformation_context: dict[str, Any] = None,\n    transform: bool = True,\n) -&gt; TypeVar(\"StreamingQuery\")\n</code></pre> <p>Ingest a Spark Structured Streaming Dataframe to the online feature store.</p> <p>This method creates a long running Spark Streaming Query, you can control the termination of the query through the arguments.</p> <p>It is possible to stop the returned query with the <code>.stop()</code> and check its status with <code>.isActive</code>.</p> <p>To get a list of all active queries, use:</p> <pre><code>sqm = spark.streams\n\n# get the list of active streaming queries\n[q.name for q in sqm.active]\n</code></pre> Engine Support <p>Spark only</p> <p>Stream ingestion using Pandas/Python as engine is currently not supported. Python/Pandas has no notion of streaming.</p> Data Validation Support <p><code>insert_stream</code> does not perform any data validation using Great Expectations even when a expectation suite is attached.</p> PARAMETER DESCRIPTION <code>features</code> <p>Features in Streaming Dataframe to be saved.</p> <p> TYPE: <code>TypeVar('pyspark.sql.DataFrame')</code> </p> <code>query_name</code> <p>It is possible to optionally specify a name for the query to make it easier to recognise in the Spark UI.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>output_mode</code> <p>Specifies how data of a streaming DataFrame/Dataset is written to a streaming sink.</p> <ul> <li><code>\"append\"</code>: Only the new rows in the streaming DataFrame/Dataset will be written to the sink.</li> <li><code>\"complete\"</code>: All the rows in the streaming DataFrame/Dataset will be written to the sink every time there is some update.</li> <li><code>\"update\"</code>: Only the rows that were updated in the streaming DataFrame/Dataset will be written to the sink every time there are some updates.   If the query doesn't contain aggregations, it will be equivalent to append mode.</li> </ul> <p> TYPE: <code>Literal['append', 'complete', 'update']</code> DEFAULT: <code>'append'</code> </p> <code>await_termination</code> <p>Waits for the termination of this query, either by query.stop() or by an exception. If the query has terminated with an exception, then the exception will be thrown. If timeout is set, it returns whether the query has terminated or not within the timeout seconds.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>timeout</code> <p>Only relevant in combination with <code>await_termination=True</code>.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>checkpoint_dir</code> <p>Checkpoint directory location. This will be used to as a reference to from where to resume the streaming job. If <code>None</code> then hsfs will construct as \"insert_stream_\" + online_topic_name.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>write_options</code> <p>Additional write options for Spark as key-value pairs.</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>transformation_context</code> <p>A dictionary mapping variable names to objects that will be provided as contextual information to the transformation function at runtime. The <code>context</code> variable must be explicitly defined as parameters in the transformation function for these to be accessible during execution.</p> <p> TYPE: <code>dict[str, Any]</code> DEFAULT: <code>None</code> </p> <code>transform</code> <p>When set to <code>False</code>, the dataframe is inserted without applying any on-demand transformations. In this case, all required on-demand features must already exist in the provided dataframe.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>TypeVar('StreamingQuery')</code> <p>Spark Structured Streaming Query object.</p>"},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.FeatureGroup.commit_details","title":"[source]  commit_details","text":"<pre><code>commit_details(\n    wallclock_time: str\n    | int\n    | datetime\n    | date\n    | None = None,\n    limit: int | None = None,\n) -&gt; dict[str, dict[str, str]]\n</code></pre> <p>Retrieves commit timeline for this feature group.</p> <p>This method can only be used on time travel enabled feature groups</p> Example <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\ncommit_details = fg.commit_details()\n</code></pre> PARAMETER DESCRIPTION <code>wallclock_time</code> <p>Commit details as of specific point in time. Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code>.</p> <p> TYPE: <code>str | int | datetime | date | None</code> DEFAULT: <code>None</code> </p> <code>limit</code> <p>Number of commits to retrieve.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>dict[str, dict[str, str]]</code> <p>Dictionary object of commit metadata timeline, where Key is commit id and value is <code>Dict[str, str]</code> with key value pairs of date committed on, number of rows updated, inserted and deleted.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p> <code>hopsworks.client.exceptions.FeatureStoreException</code> <p>If the feature group does not have <code>HUDI</code> time travel format.</p>"},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.FeatureGroup.commit_delete_record","title":"[source]  commit_delete_record","text":"<pre><code>commit_delete_record(\n    delete_df: TypeVar(\"pyspark.sql.DataFrame\"),\n    write_options: dict[Any, Any] | None = None,\n) -&gt; None\n</code></pre> <p>Drops records present in the provided DataFrame and commits it as update to this Feature group.</p> <p>This method can only be used on feature groups stored as HUDI or DELTA.</p> PARAMETER DESCRIPTION <code>delete_df</code> <p>dataFrame containing records to be deleted.</p> <p> TYPE: <code>TypeVar('pyspark.sql.DataFrame')</code> </p> <code>write_options</code> <p>User provided write options.</p> <p> TYPE: <code>dict[Any, Any] | None</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.FeatureGroup.delta_vacuum","title":"[source]  delta_vacuum","text":"<pre><code>delta_vacuum(retention_hours: int = None) -&gt; None\n</code></pre> <p>Vacuum files that are no longer referenced by a Delta table and are older than the retention threshold.</p> <p>This method can only be used on feature groups stored as DELTA.</p> Example <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\ncommit_details = fg.delta_vacuum(retention_hours = 168)\n</code></pre> PARAMETER DESCRIPTION <code>retention_hours</code> <p>User provided retention period. The default retention threshold for the files is 7 days.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.FeatureGroup.as_of","title":"[source]  as_of","text":"<pre><code>as_of(\n    wallclock_time: str\n    | int\n    | datetime\n    | date\n    | None = None,\n    exclude_until: str\n    | int\n    | datetime\n    | date\n    | None = None,\n) -&gt; query.Query\n</code></pre> <p>Get Query object to retrieve all features of the group at a point in the past.</p> Pyspark/Spark Only <p>Apache HUDI exclusively supports Time Travel and Incremental Query via Spark Context</p> <p>This method selects all features in the feature group and returns a Query object at the specified point in time. Optionally, commits before a specified point in time can be excluded from the query. The Query can then either be read into a Dataframe or used further to perform joins or construct a training dataset.</p> Reading features at a specific point in time <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\n# get data at a specific point in time and show it\nfg.as_of(\"2020-10-20 07:34:11\").read().show()\n</code></pre> Reading commits incrementally between specified points in time <pre><code>fg.as_of(\"2020-10-20 07:34:11\", exclude_until=\"2020-10-19 07:34:11\").read().show()\n</code></pre> <p>The first parameter is inclusive while the latter is exclusive. That means, in order to query a single commit, you need to query that commit time and exclude everything just before the commit.</p> Reading only the changes from a single commit <pre><code>fg.as_of(\"2020-10-20 07:31:38\", exclude_until=\"2020-10-20 07:31:37\").read().show()\n</code></pre> <p>When no wallclock_time is given, the latest state of features is returned. Optionally, commits before a specified point in time can still be excluded.</p> Reading the latest state of features, excluding commits before a specified point in time <pre><code>fg.as_of(None, exclude_until=\"2020-10-20 07:31:38\").read().show()\n</code></pre> <p>Note that the interval will be applied to all joins in the query. If you want to query different intervals for different feature groups in the query, you have to apply them in a nested fashion:</p> Example <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg1 = fs.get_or_create_feature_group(...)\nfg2 = fs.get_or_create_feature_group(...)\n\nfg1.select_all().as_of(\"2020-10-20\", exclude_until=\"2020-10-19\")\n    .join(fg2.select_all().as_of(\"2020-10-20\", exclude_until=\"2020-10-19\"))\n</code></pre> <p>If instead you apply another <code>as_of</code> selection after the join, all joined feature groups will be queried with this interval:</p> Example <pre><code>fg1.select_all().as_of(\"2020-10-20\", exclude_until=\"2020-10-19\")  # as_of is not applied\n    .join(fg2.select_all().as_of(\"2020-10-20\", exclude_until=\"2020-10-15\"))  # as_of is not applied\n    .as_of(\"2020-10-20\", exclude_until=\"2020-10-19\")\n</code></pre> Warning <p>This function only works for feature groups with time_travel_format='HUDI'.</p> Warning <p>Excluding commits via exclude_until is only possible within the range of the Hudi active timeline. By default, Hudi keeps the last 20 to 30 commits in the active timeline. If you need to keep a longer active timeline, you can overwrite the options: <code>hoodie.keep.min.commits</code> and <code>hoodie.keep.max.commits</code> when calling the <code>insert()</code> method.</p> PARAMETER DESCRIPTION <code>wallclock_time</code> <p>Read data as of this point in time. Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, or <code>%Y-%m-%d %H:%M:%S</code>.</p> <p> TYPE: <code>str | int | datetime | date | None</code> DEFAULT: <code>None</code> </p> <code>exclude_until</code> <p>Exclude commits until this point in time. Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, or <code>%Y-%m-%d %H:%M:%S</code>.</p> <p> TYPE: <code>str | int | datetime | date | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>query.Query</code> <p>The query object with the applied time travel condition.</p>"},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.FeatureGroup.get_statistics_by_commit_window","title":"[source]  get_statistics_by_commit_window","text":"<pre><code>get_statistics_by_commit_window(\n    from_commit_time: str\n    | int\n    | datetime\n    | date\n    | None = None,\n    to_commit_time: str\n    | int\n    | datetime\n    | date\n    | None = None,\n    feature_names: list[str] | None = None,\n) -&gt; Statistics | list[Statistics] | None\n</code></pre> <p>Returns the statistics computed on a specific commit window for this feature group.</p> <p>If time travel is not enabled, it raises an exception.</p> <p>If <code>from_commit_time</code> is <code>None</code>, the commit window starts from the first commit. If <code>to_commit_time</code> is <code>None</code>, the commit window ends at the last commit.</p> Example <pre><code># connect to the Feature Store\nfs = ...\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\nfg_statistics = fg.get_statistics_by_commit_window(from_commit_time=None, to_commit_time=None)\n</code></pre> PARAMETER DESCRIPTION <code>to_commit_time</code> <p>Date and time of the last commit of the window. Defaults to <code>None</code>. Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code>.</p> <p> TYPE: <code>str | int | datetime | date | None</code> DEFAULT: <code>None</code> </p> <code>from_commit_time</code> <p>Date and time of the first commit of the window. Defaults to <code>None</code>. Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code>.</p> <p> TYPE: <code>str | int | datetime | date | None</code> DEFAULT: <code>None</code> </p> <code>feature_names</code> <p>List of feature names of which statistics are retrieved.</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Statistics | list[Statistics] | None</code> <p>Statistics object or <code>None</code> if it does not exist.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.FeatureGroup.compute_statistics","title":"[source]  compute_statistics","text":"<pre><code>compute_statistics(\n    wallclock_time: str\n    | int\n    | datetime\n    | date\n    | None = None,\n) -&gt; None\n</code></pre> <p>Recompute the statistics for the feature group and save them to the feature store.</p> <p>Statistics are only computed for data in the offline storage of the feature group.</p> PARAMETER DESCRIPTION <code>wallclock_time</code> <p>If specified will recompute statistics on feature group as of specific point in time. If not specified then will compute statistics as of most recent time of this feature group. Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code>.</p> <p> TYPE: <code>str | int | datetime | date | None</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.SpineGroup","title":"[source]  SpineGroup","text":"<p>               Bases: <code>FeatureGroupBase</code></p>"},{"location":"python-api/hsfs/feature_group/#hsfs.feature_group.SpineGroup.dataframe","title":"[source]  dataframe  <code>property</code> <code>writable</code>","text":"<pre><code>dataframe: (\n    pd.DataFrame | TypeVar(\"pyspark.sql.DataFrame\") | None\n)\n</code></pre> <p>Spine dataframe with primary key, event time and label column to use for point in time join when fetching features.</p>"},{"location":"python-api/hsfs/feature_store/","title":"hsfs.feature_store","text":""},{"location":"python-api/hsfs/feature_store/#hsfs.feature_store","title":"hsfs.feature_store","text":""},{"location":"python-api/hsfs/feature_store/#hsfs.feature_store.FeatureStore","title":"[source]  FeatureStore","text":"<p>Feature Store class used to manage feature store entities, like feature groups and feature views.</p>"},{"location":"python-api/hsfs/feature_store/#hsfs.feature_store.FeatureStore.id","title":"[source]  id  <code>property</code>","text":"<pre><code>id: int\n</code></pre> <p>Id of the feature store.</p>"},{"location":"python-api/hsfs/feature_store/#hsfs.feature_store.FeatureStore.name","title":"[source]  name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Name of the feature store.</p>"},{"location":"python-api/hsfs/feature_store/#hsfs.feature_store.FeatureStore.project_name","title":"[source]  project_name  <code>property</code>","text":"<pre><code>project_name: str\n</code></pre> <p>Name of the project in which the feature store is located.</p>"},{"location":"python-api/hsfs/feature_store/#hsfs.feature_store.FeatureStore.project_id","title":"[source]  project_id  <code>property</code>","text":"<pre><code>project_id: int\n</code></pre> <p>Id of the project in which the feature store is located.</p>"},{"location":"python-api/hsfs/feature_store/#hsfs.feature_store.FeatureStore.online_featurestore_name","title":"[source]  online_featurestore_name  <code>property</code>","text":"<pre><code>online_featurestore_name: str | None\n</code></pre> <p>Name of the online feature store database.</p>"},{"location":"python-api/hsfs/feature_store/#hsfs.feature_store.FeatureStore.online_enabled","title":"[source]  online_enabled  <code>property</code>","text":"<pre><code>online_enabled: bool\n</code></pre> <p>Indicator whether online feature store is enabled.</p>"},{"location":"python-api/hsfs/feature_store/#hsfs.feature_store.FeatureStore.offline_featurestore_name","title":"[source]  offline_featurestore_name  <code>property</code>","text":"<pre><code>offline_featurestore_name: str\n</code></pre> <p>Name of the offline feature store database.</p>"},{"location":"python-api/hsfs/feature_store/#hsfs.feature_store.FeatureStore.get_feature_group","title":"[source]  get_feature_group","text":"<pre><code>get_feature_group(\n    name: str, version: int = None\n) -&gt; (\n    feature_group.FeatureGroup\n    | feature_group.ExternalFeatureGroup\n    | feature_group.SpineGroup\n)\n</code></pre> <p>Get a feature group entity from the feature store.</p> <p>Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the <code>Query</code>-API to perform joins between feature groups.</p> Example <pre><code># connect to the Feature Store\nfs = ...\n\nfg = fs.get_feature_group(\n        name=\"electricity_prices\",\n        version=1,\n    )\n</code></pre> PARAMETER DESCRIPTION <code>name</code> <p>Name of the feature group to get.</p> <p> TYPE: <code>str</code> </p> <code>version</code> <p>Version of the feature group to retrieve, defaults to <code>None</code> and will return the <code>version=1</code>.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>feature_group.FeatureGroup | feature_group.ExternalFeatureGroup | feature_group.SpineGroup</code> <p>The feature group metadata object or <code>None</code> if it does not exist.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hsfs/feature_store/#hsfs.feature_store.FeatureStore.get_feature_groups","title":"[source]  get_feature_groups","text":"<pre><code>get_feature_groups(\n    name: str | None = None,\n) -&gt; list[\n    feature_group.FeatureGroup\n    | feature_group.ExternalFeatureGroup\n    | feature_group.SpineGroup\n]\n</code></pre> <p>Get all feature groups from the feature store, or all versions of a feature group specified by its name.</p> <p>Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the <code>Query</code>-API to perform joins between feature groups.</p> Example <pre><code># connect to the Feature Store\nfs = ...\n\n# retrieve all versions of electricity_prices feature group\nfgs_list = fs.get_feature_groups(\n        name=\"electricity_prices\"\n    )\n</code></pre> Example <pre><code># connect to the Feature Store\nfs = ...\n\n# retrieve all feature groups available in the feature store\nfgs_list = fs.get_feature_groups()\n</code></pre> PARAMETER DESCRIPTION <code>name</code> <p>Name of the feature group to get the versions of; by default it is <code>None</code> and all feature groups are returned.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>list[feature_group.FeatureGroup | feature_group.ExternalFeatureGroup | feature_group.SpineGroup]</code> <p>List of feature group metadata objects.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hsfs/feature_store/#hsfs.feature_store.FeatureStore.get_on_demand_feature_group","title":"[source]  get_on_demand_feature_group","text":"<pre><code>get_on_demand_feature_group(\n    name: str, version: int = None\n) -&gt; feature_group.ExternalFeatureGroup\n</code></pre> <p>Get an external feature group entity from the feature store.</p> Deprecated <p><code>get_on_demand_feature_group</code> method is deprecated. Use the <code>get_external_feature_group</code> method instead.</p> <p>Getting an external feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the <code>Query</code>-API to perform joins between feature groups.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the external feature group to get.</p> <p> TYPE: <code>str</code> </p> <code>version</code> <p>Version of the external feature group to retrieve, defaults to <code>None</code> and will return the <code>version=1</code>.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>feature_group.ExternalFeatureGroup</code> <p>The external feature group metadata object or <code>None</code> if it does not exist.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hsfs/feature_store/#hsfs.feature_store.FeatureStore.get_external_feature_group","title":"[source]  get_external_feature_group","text":"<pre><code>get_external_feature_group(\n    name: str, version: int = None\n) -&gt; feature_group.ExternalFeatureGroup\n</code></pre> <p>Get an external feature group entity from the feature store.</p> <p>Getting an external feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the <code>Query</code>-API to perform joins between feature groups.</p> Example <pre><code># connect to the Feature Store\nfs = ...\n\nexternal_fg = fs.get_external_feature_group(\"external_fg_test\")\n</code></pre> PARAMETER DESCRIPTION <code>name</code> <p>Name of the external feature group to get.</p> <p> TYPE: <code>str</code> </p> <code>version</code> <p>Version of the external feature group to retrieve, by defaults to <code>None</code> and will return the <code>version=1</code>.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>feature_group.ExternalFeatureGroup</code> <p>The external feature group metadata object or <code>None</code> if it does not exist.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hsfs/feature_store/#hsfs.feature_store.FeatureStore.get_on_demand_feature_groups","title":"[source]  get_on_demand_feature_groups","text":"<pre><code>get_on_demand_feature_groups(\n    name: str,\n) -&gt; list[feature_group.ExternalFeatureGroup]\n</code></pre> <p>Get a list of all versions of an external feature group entity from the feature store.</p> Deprecated <p><code>get_on_demand_feature_groups</code> method is deprecated. Use the <code>get_external_feature_groups</code> method instead.</p> <p>Getting an external feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the <code>Query</code>-API to perform joins between feature groups.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the external feature group to get.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>list[feature_group.ExternalFeatureGroup]</code> <p>List of external feature group metadata objects.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hsfs/feature_store/#hsfs.feature_store.FeatureStore.get_external_feature_groups","title":"[source]  get_external_feature_groups","text":"<pre><code>get_external_feature_groups(\n    name: str | None = None,\n) -&gt; list[feature_group.ExternalFeatureGroup]\n</code></pre> <p>Get a list of all external feature groups from the feature store, or all versions of an external feature group.</p> <p>Getting an external feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the <code>Query</code>-API to perform joins between feature groups.</p> Example <pre><code># connect to the Feature Store\nfs = ...\n\nexternal_fgs_list = fs.get_external_feature_groups(\"external_fg_test\")\n</code></pre> Example <pre><code># connect to the Feature Store\nfs = ...\n\n# retrieve all external feature groups available in the feature store\nexternal_fgs_list = fs.get_external_feature_groups()\n</code></pre> PARAMETER DESCRIPTION <code>name</code> <p>Name of the external feature group to get the versions of; by default it is <code>None</code> and all external feature groups are returned.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>list[feature_group.ExternalFeatureGroup]</code> <p>List of external feature group metadata objects.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hsfs/feature_store/#hsfs.feature_store.FeatureStore.get_training_dataset","title":"[source]  get_training_dataset","text":"<pre><code>get_training_dataset(\n    name: str, version: int = None\n) -&gt; training_dataset.TrainingDataset\n</code></pre> <p>Get a training dataset entity from the feature store.</p> Deprecated <p><code>TrainingDataset</code> is deprecated, use <code>FeatureView</code> instead. You can still retrieve old training datasets using this method, but after upgrading the old training datasets will also be available under a Feature View with the same name and version.</p> <p>It is recommended to use this method only for old training datasets that have been created directly from Dataframes and not with Query objects.</p> <p>Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the training dataset to get.</p> <p> TYPE: <code>str</code> </p> <code>version</code> <p>Version of the training dataset to retrieve, defaults to <code>None</code> and will return the <code>version=1</code>.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>training_dataset.TrainingDataset</code> <p>The training dataset metadata object.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hsfs/feature_store/#hsfs.feature_store.FeatureStore.get_training_datasets","title":"[source]  get_training_datasets","text":"<pre><code>get_training_datasets(\n    name: str,\n) -&gt; list[training_dataset.TrainingDataset]\n</code></pre> <p>Get a list of all versions of a training dataset entity from the feature store.</p> Deprecated <p><code>TrainingDataset</code> is deprecated, use <code>FeatureView</code> instead.</p> <p>Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the training dataset to get.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>list[training_dataset.TrainingDataset]</code> <p>List of training dataset metadata objects.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hsfs/feature_store/#hsfs.feature_store.FeatureStore.get_storage_connector","title":"[source]  get_storage_connector","text":"<pre><code>get_storage_connector(\n    name: str,\n) -&gt; storage_connector.StorageConnector\n</code></pre> <p>Get a previously created storage connector from the feature store.</p> <p>Storage connectors encapsulate all information needed for the execution engine to read and write to specific storage. This storage can be S3, a JDBC compliant database or the distributed filesystem HOPSFS.</p> <p>If you want to connect to the online feature store, see the <code>get_online_storage_connector</code> method to get the JDBC connector for the Online Feature Store.</p> <p>Deprecated</p> <p><code>get_storage_connector</code> method is deprecated. Use <code>get_data_source</code> instead.</p> Example <pre><code># connect to the Feature Store\nfs = ...\n\nsc = fs.get_storage_connector(\"demo_fs_meb10000_Training_Datasets\")\n</code></pre> PARAMETER DESCRIPTION <code>name</code> <p>Name of the storage connector to retrieve.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>storage_connector.StorageConnector</code> <p>Storage connector object.</p>"},{"location":"python-api/hsfs/feature_store/#hsfs.feature_store.FeatureStore.sql","title":"[source]  sql","text":"<pre><code>sql(\n    query: str,\n    dataframe_type: Literal[\n        \"default\",\n        \"spark\",\n        \"pandas\",\n        \"polars\",\n        \"numpy\",\n        \"python\",\n    ] = \"default\",\n    online: bool = False,\n    read_options: dict | None = None,\n) -&gt; pd.DataFrame | pd.Series | np.ndarray | pl.DataFrame\n</code></pre> <p>Execute SQL command on the offline or online feature store database.</p> Example <pre><code># connect to the Feature Store\nfs = ...\n\n# construct the query and show head rows\nquery_res_head = fs.sql(\"SELECT * FROM `fg_1`\").head()\n</code></pre> PARAMETER DESCRIPTION <code>query</code> <p>The SQL query to execute.</p> <p> TYPE: <code>str</code> </p> <code>dataframe_type</code> <p>The type of the returned dataframe. Defaults to <code>\"default\"</code>, which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</p> <p> TYPE: <code>Literal['default', 'spark', 'pandas', 'polars', 'numpy', 'python']</code> DEFAULT: <code>'default'</code> </p> <code>online</code> <p>Set to true to execute the query against the online feature store.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>read_options</code> <p>Additional options as key/value pairs to pass to the execution engine.</p> <p>For spark engine: Dictionary of read options for Spark.</p> <p>For python engine: If running queries on the online feature store, users can provide an entry <code>{'external': True}</code>, this instructs the library to use the <code>host</code> parameter in the <code>hopsworks.login</code> to establish the connection to the online feature store. If not set, or set to False, the online feature store storage connector is used which relies on the private ip.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>pd.DataFrame | pd.Series | np.ndarray | pl.DataFrame</code> <p>DataFrame depending on the chosen type.</p>"},{"location":"python-api/hsfs/feature_store/#hsfs.feature_store.FeatureStore.get_online_storage_connector","title":"[source]  get_online_storage_connector","text":"<pre><code>get_online_storage_connector() -&gt; (\n    storage_connector.StorageConnector\n)\n</code></pre> <p>Get the storage connector for the Online Feature Store of the respective project's feature store.</p> <p>The returned storage connector depends on the project that you are connected to.</p> <p>Deprecated</p> <p><code>get_online_storage_connector</code> method is deprecated. Use <code>get_online_data_source</code> instead.</p> Example <pre><code># connect to the Feature Store\nfs = ...\n\nonline_storage_connector = fs.get_online_storage_connector()\n</code></pre> RETURNS DESCRIPTION <code>storage_connector.StorageConnector</code> <p>JDBC storage connector to the Online Feature Store.</p>"},{"location":"python-api/hsfs/feature_store/#hsfs.feature_store.FeatureStore.create_feature_group","title":"[source]  create_feature_group","text":"<pre><code>create_feature_group(\n    name: str,\n    version: int | None = None,\n    description: str = \"\",\n    online_enabled: bool = False,\n    time_travel_format: str | None = None,\n    partition_key: list[str] | None = None,\n    primary_key: list[str] | None = None,\n    foreign_key: list[str] | None = None,\n    embedding_index: EmbeddingIndex | None = None,\n    hudi_precombine_key: str | None = None,\n    features: list[feature.Feature] | None = None,\n    statistics_config: StatisticsConfig\n    | bool\n    | dict\n    | None = None,\n    event_time: str | None = None,\n    stream: bool = False,\n    expectation_suite: expectation_suite.ExpectationSuite\n    | TypeVar(\"great_expectations.core.ExpectationSuite\")\n    | None = None,\n    parents: list[feature_group.FeatureGroup] | None = None,\n    topic_name: str | None = None,\n    notification_topic_name: str | None = None,\n    transformation_functions: list[\n        TransformationFunction | HopsworksUdf\n    ]\n    | None = None,\n    online_config: OnlineConfig\n    | dict[str, Any]\n    | None = None,\n    offline_backfill_every_hr: int | str | None = None,\n    storage_connector: storage_connector.StorageConnector\n    | dict[str, Any] = None,\n    path: str | None = None,\n    data_source: ds.DataSource\n    | dict[str, Any]\n    | None = None,\n    ttl: float | timedelta | None = None,\n    ttl_enabled: bool | None = None,\n    online_disk: bool | None = None,\n    sink_enabled: bool | None = False,\n    sink_job_conf: dict[str, Any] | None = None,\n    tags: tag.Tag\n    | dict[str, Any]\n    | list[tag.Tag | dict[str, Any]]\n    | None = None,\n) -&gt; feature_group.FeatureGroup\n</code></pre> <p>Create a feature group metadata object.</p> Example <pre><code># connect to the Feature Store\nfs = ...\n\n# define the on-demand transformation functions\n@udf(int)\ndef plus_one(value):\n    return value + 1\n\n@udf(int)\ndef plus_two(value):\n    return value + 2\n\n# construct list of \"transformation functions\" on features\ntransformation_functions = [plus_one(\"feature1\"), plus_two(\"feature2\")]\n\nfg = fs.create_feature_group(\n    name='air_quality',\n    description='Air Quality characteristics of each day',\n    version=1,\n    primary_key=['city','date'],\n    online_enabled=True,\n    event_time='date',\n    transformation_functions=transformation_functions,\n    online_config={'online_comments': ['NDB_TABLE=READ_BACKUP=1']},\n    online_disk=True,  # Online data will be stored on disk instead of in memory\n    ttl=timedelta(days=7)  # features will be deleted after 7 days\n)\n</code></pre> Lazy <p>This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the <code>save()</code> method with a DataFrame.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the feature group to create.</p> <p> TYPE: <code>str</code> </p> <code>version</code> <p>Version of the feature group to create, defaults to <code>None</code> and will create the feature group with incremented version from the last version in the feature store.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>description</code> <p>A string describing the contents of the feature group to improve discoverability for Data Scientists.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>online_enabled</code> <p>Define whether the feature group should be made available also in the online feature store for low latency access.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>time_travel_format</code> <p>Format used for time travel, defaults to <code>\"HUDI\"</code>.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>partition_key</code> <p>A list of feature names to be used as partition key when writing the feature data to the offline storage, defaults to empty list <code>[]</code>.</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>primary_key</code> <p>A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list <code>[]</code>, and the feature group won't have any primary key.</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>foreign_key</code> <p>A list of feature names to be used as foreign key for the feature group. Foreign key is referencing the primary key of another feature group and can be used as joining key. Defaults to empty list <code>[]</code>, and the feature group won't have any foreign key.</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>embedding_index</code> <p><code>EmbeddingIndex</code>. If an embedding index is provided, vector database is used as online feature store. This enables similarity search by using <code>FeatureGroup.find_neighbors</code>.</p> <p> TYPE: <code>EmbeddingIndex | None</code> DEFAULT: <code>None</code> </p> <code>hudi_precombine_key</code> <p>A feature name to be used as a precombine key for the <code>\"HUDI\"</code> feature group. If feature group has time travel format <code>\"HUDI\"</code> and hudi precombine key was not specified then the first primary key of the feature group will be used as hudi precombine key.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>features</code> <p>Optionally, define the schema of the feature group manually as a list of <code>Feature</code> objects. Defaults to empty list <code>[]</code> and will use the schema information of the DataFrame provided in the <code>save</code> method.</p> <p> TYPE: <code>list[feature.Feature] | None</code> DEFAULT: <code>None</code> </p> <code>statistics_config</code> <p>A configuration object, or a dictionary with keys:</p> <ul> <li><code>enabled</code> to generally enable descriptive statistics computation for this feature group,</li> <li><code>correlations</code> to turn on feature correlation computation,</li> <li><code>histograms</code> to compute feature value frequencies, and</li> <li><code>exact_uniqueness</code> to compute uniqueness, distinctness and entropy.</li> </ul> <p>The values should be booleans indicating the setting. To fully turn off statistics computation pass <code>statistics_config=False</code>. By default, it computes only descriptive statistics.</p> <p> TYPE: <code>StatisticsConfig | bool | dict | None</code> DEFAULT: <code>None</code> </p> <code>event_time</code> <p>Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins.</p> <p>Note: Event time data type restriction     The supported data types for the event time column are: <code>timestamp</code>, <code>date</code> and <code>bigint</code>.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>stream</code> <p>Optionally, define whether the feature group should support real time stream writing capabilities. Stream enabled Feature Groups have unified single API for writing streaming features transparently to both online and offline store.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>expectation_suite</code> <p>Optionally, attach an expectation suite to the feature group which dataframes should be validated against upon insertion.</p> <p> TYPE: <code>expectation_suite.ExpectationSuite | TypeVar('great_expectations.core.ExpectationSuite') | None</code> DEFAULT: <code>None</code> </p> <code>parents</code> <p>Optionally, define the parents of this feature group as the origin where the data is coming from.</p> <p> TYPE: <code>list[feature_group.FeatureGroup] | None</code> DEFAULT: <code>None</code> </p> <code>topic_name</code> <p>Optionally, define the name of the topic used for data ingestion. If left undefined it defaults to using project topic.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>notification_topic_name</code> <p>Optionally, define the name of the topic used for sending notifications when entries are inserted or updated on the online feature store. If left undefined no notifications are sent.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>transformation_functions</code> <p>On-Demand Transformation functions attached to the feature group. It can be a list of list of user defined functions defined using the hopsworks <code>@udf</code> decorator. Defaults to <code>None</code>, no transformations.</p> <p> TYPE: <code>list[TransformationFunction | HopsworksUdf] | None</code> DEFAULT: <code>None</code> </p> <code>online_config</code> <p>Optionally, define configuration which is used to configure online table.</p> <p> TYPE: <code>OnlineConfig | dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>offline_backfill_every_hr</code> <p>If specified, the materialization job will be scheduled to run periodically. The value can be either an integer representing the number of hours between each run or a string representing a cron expression. Set the value to None to avoid scheduling the materialization job. By default, no scheduling is done.</p> <p> TYPE: <code>int | str | None</code> DEFAULT: <code>None</code> </p> <code>storage_connector</code> <p>The storage connector used to establish connectivity with the data source. [DEPRECATED: Use <code>data_source</code> instead.]</p> <p> TYPE: <code>storage_connector.StorageConnector | dict[str, Any]</code> DEFAULT: <code>None</code> </p> <code>path</code> <p>The location within the scope of the storage connector, from where to read the data for the external feature group. [DEPRECATED: Use <code>data_source</code> instead.]</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>data_source</code> <p>The data source specifying the location of the data. Overrides the path and query arguments when specified.</p> <p> TYPE: <code>ds.DataSource | dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>ttl</code> <p>Optional time-to-live duration for features in this group. Can be specified as:</p> <ul> <li>An integer or float representing seconds</li> <li>A timedelta object</li> </ul> <p>This ttl value is added to the event time of the feature group and when the system time exceeds the event time + ttl, the entries will be automatically removed. The system time zone is in UTC.</p> <p>By default, no TTL is set.</p> <p> TYPE: <code>float | timedelta | None</code> DEFAULT: <code>None</code> </p> <code>ttl_enabled</code> <p>Optionally, enable TTL for this feature group. Defaults to True if ttl is set.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>online_disk</code> <p>Optionally, specify online data storage for this feature group. When set to True data will be stored on disk, instead of in memory. Overrides online_config.table_space. Defaults to using cluster wide configuration 'featurestore_online_tablespace' to identify tablespace for disk storage.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>sink_enabled</code> <p>Enable automatic ingestion from the configured data source using a sink job.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>False</code> </p> <code>sink_job_conf</code> <p>Optional configuration describing the sink job to create when <code>sink_enabled</code> is True. Accepts either a job configuration object or a dictionary.</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>tags</code> <p>Optionally, define tags for the feature group. Tags can be provided as: - A single Tag object - A dictionary with 'name' and 'value' keys (e.g., {\"name\": \"tag1\", \"value\": \"value1\"}) - A list of Tag objects - A list of dictionaries with 'name' and 'value' keys Tags will be attached to the feature group after it is saved. Defaults to None.</p> <p> TYPE: <code>tag.Tag | dict[str, Any] | list[tag.Tag | dict[str, Any]] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>feature_group.FeatureGroup</code> <p>The feature group metadata object.</p>"},{"location":"python-api/hsfs/feature_store/#hsfs.feature_store.FeatureStore.get_or_create_feature_group","title":"[source]  get_or_create_feature_group","text":"<pre><code>get_or_create_feature_group(\n    name: str,\n    version: int,\n    description: str | None = \"\",\n    online_enabled: bool | None = False,\n    time_travel_format: str | None = None,\n    partition_key: list[str] | None = None,\n    primary_key: list[str] | None = None,\n    foreign_key: list[str] | None = None,\n    embedding_index: EmbeddingIndex | None = None,\n    hudi_precombine_key: str | None = None,\n    features: list[feature.Feature] | None = None,\n    statistics_config: StatisticsConfig\n    | bool\n    | dict\n    | None = None,\n    expectation_suite: expectation_suite.ExpectationSuite\n    | TypeVar(\"great_expectations.core.ExpectationSuite\")\n    | None = None,\n    event_time: str | None = None,\n    stream: bool | None = False,\n    parents: list[feature_group.FeatureGroup] | None = None,\n    topic_name: str | None = None,\n    notification_topic_name: str | None = None,\n    transformation_functions: list[\n        TransformationFunction | HopsworksUdf\n    ]\n    | None = None,\n    online_config: OnlineConfig\n    | dict[str, Any]\n    | None = None,\n    offline_backfill_every_hr: int | str | None = None,\n    storage_connector: storage_connector.StorageConnector\n    | dict[str, Any] = None,\n    path: str | None = None,\n    data_source: ds.DataSource\n    | dict[str, Any]\n    | None = None,\n    ttl: float | timedelta | None = None,\n    ttl_enabled: bool | None = None,\n    online_disk: bool | None = None,\n    sink_enabled: bool | None = False,\n    sink_job_conf: dict[str, Any] | None = None,\n) -&gt; (\n    feature_group.FeatureGroup\n    | feature_group.ExternalFeatureGroup\n    | feature_group.SpineGroup\n)\n</code></pre> <p>Get feature group metadata object or create a new one if it doesn't exist.</p> <p>This method doesn't update existing feature group metadata object.</p> Example <pre><code># connect to the Feature Store\nfs = ...\n\nfg = fs.get_or_create_feature_group(\n    name=\"electricity_prices\",\n    version=1,\n    description=\"Electricity prices from NORD POOL\",\n    primary_key=[\"day\", \"area\"],\n    online_enabled=True,\n    event_time=\"timestamp\",\n    transformation_functions=transformation_functions,\n    online_config={'online_comments': ['NDB_TABLE=READ_BACKUP=1']},\n    online_disk=True, # Online data will be stored on disk instead of in memory\n    ttl=timedelta(days=30),\n)\n</code></pre> Lazy <p>This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the <code>insert()</code> method with a DataFrame.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the feature group to create.</p> <p> TYPE: <code>str</code> </p> <code>version</code> <p>Version of the feature group to retrieve or create.</p> <p> TYPE: <code>int</code> </p> <code>description</code> <p>A string describing the contents of the feature group to improve discoverability for Data Scientists.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>''</code> </p> <code>online_enabled</code> <p>Define whether the feature group should be made available also in the online feature store for low latency access.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>False</code> </p> <code>time_travel_format</code> <p>Format used for time travel, defaults to <code>\"HUDI\"</code>.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>partition_key</code> <p>A list of feature names to be used as partition key when writing the feature data to the offline storage, defaults to empty list <code>[]</code>.</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>primary_key</code> <p>A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list <code>[]</code>, and the feature group won't have any primary key.</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>foreign_key</code> <p>A list of feature names to be used as foreign key for the feature group. Foreign key is referencing the primary key of another feature group and can be used as joining key. Defaults to empty list <code>[]</code>, and the feature group won't have any foreign key.</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>embedding_index</code> <p><code>EmbeddingIndex</code>. If an embedding index is provided, vector database is used as online feature store. This enables similarity search by using <code>FeatureGroup.find_neighbors</code>.</p> <p> TYPE: <code>EmbeddingIndex | None</code> DEFAULT: <code>None</code> </p> <code>hudi_precombine_key</code> <p>A feature name to be used as a precombine key for the <code>\"HUDI\"</code> feature group. If feature group has time travel format <code>\"HUDI\"</code> and hudi precombine key was not specified then the first primary key of the feature group will be used as hudi precombine key.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>features</code> <p>Optionally, define the schema of the feature group manually as a list of <code>Feature</code> objects. Defaults to empty list <code>[]</code> and will use the schema information of the DataFrame provided in the <code>save</code> method.</p> <p> TYPE: <code>list[feature.Feature] | None</code> DEFAULT: <code>None</code> </p> <code>statistics_config</code> <p>A configuration object, or a dictionary with keys:</p> <ul> <li><code>enabled</code> to generally enable descriptive statistics computation for this feature group,</li> <li><code>correlations</code> to turn on feature correlation computation,</li> <li><code>histograms</code> to compute feature value frequencies, and</li> <li><code>exact_uniqueness</code> to compute uniqueness, distinctness and entropy.</li> </ul> <p>The values should be booleans indicating the setting. To fully turn off statistics computation pass <code>statistics_config=False</code>. By default, it computes only descriptive statistics.</p> <p> TYPE: <code>StatisticsConfig | bool | dict | None</code> DEFAULT: <code>None</code> </p> <code>event_time</code> <p>Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins.</p> <p>Note: Event time data type restriction     The supported data types for the event time column are: <code>timestamp</code>, <code>date</code> and <code>bigint</code>.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>stream</code> <p>Optionally, define whether the feature group should support real time stream writing capabilities. Stream enabled Feature Groups have unified single API for writing streaming features transparently to both online and offline store.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>False</code> </p> <code>expectation_suite</code> <p>Optionally, attach an expectation suite to the feature group which dataframes should be validated against upon insertion.</p> <p> TYPE: <code>expectation_suite.ExpectationSuite | TypeVar('great_expectations.core.ExpectationSuite') | None</code> DEFAULT: <code>None</code> </p> <code>parents</code> <p>Optionally, define the parents of this feature group as the origin where the data is coming from.</p> <p> TYPE: <code>list[feature_group.FeatureGroup] | None</code> DEFAULT: <code>None</code> </p> <code>topic_name</code> <p>Optionally, define the name of the topic used for data ingestion. If left undefined it defaults to using project topic.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>notification_topic_name</code> <p>Optionally, define the name of the topic used for sending notifications when entries are inserted or updated on the online feature store. If left undefined no notifications are sent.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>transformation_functions</code> <p>On-Demand Transformation functions attached to the feature group. It can be a list of list of user defined functions defined using the hopsworks <code>@udf</code> decorator. Defaults to <code>None</code>, no transformations.</p> <p> TYPE: <code>list[TransformationFunction | HopsworksUdf] | None</code> DEFAULT: <code>None</code> </p> <code>online_config</code> <p>Optionally, define configuration which is used to configure online table.</p> <p> TYPE: <code>OnlineConfig | dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>offline_backfill_every_hr</code> <p>If specified, the materialization job will be scheduled to run periodically. The value can be either an integer representing the number of hours between each run or a string representing a cron expression. Set the value to None to avoid scheduling the materialization job. By default, no scheduling is done.</p> <p> TYPE: <code>int | str | None</code> DEFAULT: <code>None</code> </p> <code>storage_connector</code> <p>The storage connector used to establish connectivity with the data source. [DEPRECATED: Use <code>data_source</code> instead.]</p> <p> TYPE: <code>storage_connector.StorageConnector | dict[str, Any]</code> DEFAULT: <code>None</code> </p> <code>path</code> <p>The location within the scope of the storage connector, from where to read the data for the external feature group. [DEPRECATED: Use <code>data_source</code> instead.]</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>data_source</code> <p>The data source specifying the location of the data. Overrides the path and query arguments when specified.</p> <p> TYPE: <code>ds.DataSource | dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>ttl</code> <p>Optional time-to-live duration for features in this group. Can be specified as:</p> <ul> <li>An integer or float representing seconds</li> <li>A timedelta object</li> </ul> <p>This ttl value is added to the event time of the feature group and when the system time exceeds the event time + ttl, the entries will be automatically removed. The system time zone is in UTC.</p> <p>By default, no TTL is set.</p> <p> TYPE: <code>float | timedelta | None</code> DEFAULT: <code>None</code> </p> <code>ttl_enabled</code> <p>Optionally, enable TTL for this feature group. Defaults to True if ttl is set.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>online_disk</code> <p>Optionally, specify online data storage for this feature group. When set to True data will be stored on disk, instead of in memory. Overrides online_config.table_space. Defaults to using cluster wide configuration 'featurestore_online_tablespace' to identify tablespace for disk storage.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>sink_enabled</code> <p>Enable copying data from the configured data source to the feature group.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>False</code> </p> <code>sink_job_conf</code> <p>Optional configuration describing the sink job to create when <code>sink_enabled</code> is True.</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>feature_group.FeatureGroup | feature_group.ExternalFeatureGroup | feature_group.SpineGroup</code> <p>The feature group metadata object.</p>"},{"location":"python-api/hsfs/feature_store/#hsfs.feature_store.FeatureStore.create_on_demand_feature_group","title":"[source]  create_on_demand_feature_group","text":"<pre><code>create_on_demand_feature_group(\n    name: str,\n    storage_connector: storage_connector.StorageConnector\n    | None = None,\n    query: str | None = None,\n    data_format: str | None = None,\n    path: str | None = \"\",\n    options: dict[str, str] | None = None,\n    version: int | None = None,\n    description: str | None = \"\",\n    primary_key: list[str] | None = None,\n    foreign_key: list[str] | None = None,\n    features: list[feature.Feature] | None = None,\n    statistics_config: StatisticsConfig\n    | bool\n    | dict\n    | None = None,\n    event_time: str | None = None,\n    expectation_suite: expectation_suite.ExpectationSuite\n    | TypeVar(\"great_expectations.core.ExpectationSuite\")\n    | None = None,\n    topic_name: str | None = None,\n    notification_topic_name: str | None = None,\n    data_source: ds.DataSource\n    | dict[str, Any]\n    | None = None,\n    online_enabled: bool = False,\n    ttl: float | timedelta | None = None,\n    ttl_enabled: bool | None = None,\n) -&gt; feature_group.ExternalFeatureGroup\n</code></pre> <p>Create an external feature group metadata object.</p> Deprecated <p><code>create_on_demand_feature_group</code> method is deprecated. Use the <code>create_external_feature_group</code> method instead.</p> Lazy <p>This method is lazy and does not persist any metadata in the feature store on its own. To persist the feature group metadata in the feature store, call the <code>save()</code> method.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the external feature group to create.</p> <p> TYPE: <code>str</code> </p> <code>storage_connector</code> <p>The storage connector used to establish connectivity with the data source. [DEPRECATED: Use <code>data_source</code> instead.]</p> <p> TYPE: <code>storage_connector.StorageConnector | None</code> DEFAULT: <code>None</code> </p> <code>query</code> <p>A string containing a SQL query valid for the target data source. The query will be used to pull data from the data sources when the feature group is used. [DEPRECATED: Use <code>data_source</code> instead.]</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>data_format</code> <p>If the external feature groups refers to a directory with data, the data format to use when reading it.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>path</code> <p>The location within the scope of the storage connector, from where to read the data for the external feature group. [DEPRECATED: Use <code>data_source</code> instead.]</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>''</code> </p> <code>options</code> <p>Additional options to be used by the engine when reading data from the specified storage connector. For example, <code>{\"header\": True}</code> when reading CSV files with column names in the first row.</p> <p> TYPE: <code>dict[str, str] | None</code> DEFAULT: <code>None</code> </p> <code>version</code> <p>Version of the external feature group to retrieve, defaults to <code>None</code> and will create the feature group with incremented version from the last version in the feature store.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>description</code> <p>A string describing the contents of the external feature group to improve discoverability for Data Scientists.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>''</code> </p> <code>primary_key</code> <p>A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list <code>[]</code>, and the feature group won't have any primary key.</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>foreign_key</code> <p>A list of feature names to be used as foreign key for the feature group. Foreign key is referencing the primary key of another feature group and can be used as joining key. Defaults to empty list <code>[]</code>, and the feature group won't have any foreign key.</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>features</code> <p>Optionally, define the schema of the external feature group manually as a list of <code>Feature</code> objects. Defaults to empty list <code>[]</code> and will use the schema information of the DataFrame resulting by executing the provided query against the data source.</p> <p> TYPE: <code>list[feature.Feature] | None</code> DEFAULT: <code>None</code> </p> <code>statistics_config</code> <p>A configuration object, or a dictionary with keys:</p> <ul> <li><code>\"enabled\"</code> to generally enable descriptive statistics computation for this external feature group,</li> <li><code>\"correlations\"</code> to turn on feature correlation computation,</li> <li><code>\"histograms\"</code> to compute feature value frequencies, and</li> <li><code>\"exact_uniqueness\"</code> to compute uniqueness, distinctness and entropy.</li> </ul> <p>The values should be booleans indicating the setting. To fully turn off statistics computation pass <code>statistics_config=False</code>. Defaults to <code>None</code> and will compute only descriptive statistics.</p> <p> TYPE: <code>StatisticsConfig | bool | dict | None</code> DEFAULT: <code>None</code> </p> <code>event_time</code> <p>Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins.</p> <p>Note: Event time data type restriction     The supported data types for the event time column are: <code>timestamp</code>, <code>date</code> and <code>bigint</code>.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>topic_name</code> <p>Optionally, define the name of the topic used for data ingestion. If left undefined it defaults to using project topic.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>notification_topic_name</code> <p>Optionally, define the name of the topic used for sending notifications when entries are inserted or updated on the online feature store. If left undefined no notifications are sent.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>expectation_suite</code> <p>Optionally, attach an expectation suite to the feature group which dataframes should be validated against upon insertion.</p> <p> TYPE: <code>expectation_suite.ExpectationSuite | TypeVar('great_expectations.core.ExpectationSuite') | None</code> DEFAULT: <code>None</code> </p> <code>data_source</code> <p>The data source specifying the location of the data. Overrides the storage_connector, path and query arguments when specified.</p> <p> TYPE: <code>ds.DataSource | dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>online_enabled</code> <p>Define whether it should be possible to sync the feature group to the online feature store for low latency access.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ttl</code> <p>Optional time-to-live duration for features in this group.</p> <p>Can be specified as:</p> <ul> <li>An integer or float representing seconds</li> <li>A timedelta object</li> </ul> <p>This ttl value is added to the event time of the feature group and when the system time exceeds the event time + ttl, the entries will be automatically removed. The system time zone is in UTC. By default no TTL is set.</p> <p> TYPE: <code>float | timedelta | None</code> DEFAULT: <code>None</code> </p> <code>ttl_enabled</code> <p>Optionally, enable TTL for this feature group. Defaults to True if ttl is set.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>feature_group.ExternalFeatureGroup</code> <p>The external feature group metadata object.</p>"},{"location":"python-api/hsfs/feature_store/#hsfs.feature_store.FeatureStore.create_external_feature_group","title":"[source]  create_external_feature_group","text":"<pre><code>create_external_feature_group(\n    name: str,\n    storage_connector: storage_connector.StorageConnector\n    | None = None,\n    query: str | None = None,\n    data_format: str | None = None,\n    path: str | None = \"\",\n    options: dict[str, str] | None = None,\n    version: int | None = None,\n    description: str | None = \"\",\n    primary_key: list[str] | None = None,\n    foreign_key: list[str] | None = None,\n    embedding_index: EmbeddingIndex | None = None,\n    features: list[feature.Feature] | None = None,\n    statistics_config: StatisticsConfig\n    | bool\n    | dict\n    | None = None,\n    event_time: str | None = None,\n    expectation_suite: expectation_suite.ExpectationSuite\n    | TypeVar(\"great_expectations.core.ExpectationSuite\")\n    | None = None,\n    online_enabled: bool = False,\n    topic_name: str | None = None,\n    notification_topic_name: str | None = None,\n    online_config: OnlineConfig\n    | dict[str, Any]\n    | None = None,\n    data_source: ds.DataSource\n    | dict[str, Any]\n    | None = None,\n    ttl: float | timedelta | None = None,\n    ttl_enabled: bool | None = None,\n    online_disk: bool | None = None,\n) -&gt; feature_group.ExternalFeatureGroup\n</code></pre> <p>Create an external feature group metadata object.</p> Example <pre><code># connect to the Feature Store\nfs = ...\n\nexternal_fg = fs.create_external_feature_group(\n    name=\"sales\",\n    version=1,\n    description=\"Physical shop sales features\",\n    data_source=data_source,\n    primary_key=['ss_store_sk'],\n    event_time='sale_date',\n    ttl=timedelta(days=30),\n)\n</code></pre> Lazy <p>This method is lazy and does not persist any metadata in the feature store on its own. To persist the feature group metadata in the feature store, call the <code>save()</code> method.</p> <p>You can enable online storage for external feature groups, however, the sync from the external storage to Hopsworks online storage needs to be done manually:</p> <pre><code>external_fg = fs.create_external_feature_group(\n    name=\"sales\",\n    version=1,\n    description=\"Physical shop sales features\",\n    data_source=data_source,\n    primary_key=['ss_store_sk'],\n    event_time='sale_date',\n    online_enabled=True,\n    online_config={'online_comments': ['NDB_TABLE=READ_BACKUP=1']},\n    online_disk=True, # Online data will be stored on disk instead of in memory\n    ttl=timedelta(days=30),\n)\nexternal_fg.save()\n\n# read from external storage and filter data to sync to online\ndf = external_fg.read().filter(external_fg.customer_status == \"active\")\n\n# insert to online storage\nexternal_fg.insert(df)\n</code></pre> PARAMETER DESCRIPTION <code>name</code> <p>Name of the external feature group to create.</p> <p> TYPE: <code>str</code> </p> <code>storage_connector</code> <p>The storage connector used to establish connectivity with the data source. [DEPRECATED: Use <code>data_source</code> instead.]</p> <p> TYPE: <code>storage_connector.StorageConnector | None</code> DEFAULT: <code>None</code> </p> <code>query</code> <p>A string containing a SQL query valid for the target data source. The query will be used to pull data from the data sources when the feature group is used. [DEPRECATED: Use <code>data_source</code> instead.]</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>data_format</code> <p>If the external feature groups refers to a directory with data, the data format to use when reading it.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>path</code> <p>The location within the scope of the storage connector, from where to read the data for the external feature group. [DEPRECATED: Use <code>data_source</code> instead.]</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>''</code> </p> <code>options</code> <p>Additional options to be used by the engine when reading data from the specified storage connector. For example, <code>{\"header\": True}</code> when reading CSV files with column names in the first row.</p> <p> TYPE: <code>dict[str, str] | None</code> DEFAULT: <code>None</code> </p> <code>version</code> <p>Version of the external feature group to retrieve, defaults to <code>None</code> and will create the feature group with incremented version from the last version in the feature store.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>description</code> <p>A string describing the contents of the external feature group to improve discoverability for Data Scientists.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>''</code> </p> <code>primary_key</code> <p>A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list <code>[]</code>, and the feature group won't have any primary key.</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>foreign_key</code> <p>A list of feature names to be used as foreign key for the feature group. Foreign key is referencing the primary key of another feature group and can be used as joining key. Defaults to empty list <code>[]</code>, and the feature group won't have any foreign key.</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>features</code> <p>Optionally, define the schema of the external feature group manually as a list of <code>Feature</code> objects. Defaults to empty list <code>[]</code> and will use the schema information of the DataFrame resulting by executing the provided query against the data source.</p> <p> TYPE: <code>list[feature.Feature] | None</code> DEFAULT: <code>None</code> </p> <code>statistics_config</code> <p>A configuration object, or a dictionary with keys:</p> <ul> <li><code>\"enabled\"</code> to generally enable descriptive statistics computation for this external feature group,</li> <li><code>\"correlations\"</code> to turn on feature correlation computation,</li> <li><code>\"histograms\"</code> to compute feature value frequencies, and</li> <li><code>\"exact_uniqueness\"</code> to compute uniqueness, distinctness and entropy.</li> </ul> <p>The values should be booleans indicating the setting. To fully turn off statistics computation pass <code>statistics_config=False</code>. Defaults to <code>None</code> and will compute only descriptive statistics.</p> <p> TYPE: <code>StatisticsConfig | bool | dict | None</code> DEFAULT: <code>None</code> </p> <code>event_time</code> <p>Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins.</p> <p>Note: Event time data type restriction     The supported data types for the event time column are: <code>timestamp</code>, <code>date</code> and <code>bigint</code>.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>online_enabled</code> <p>Define whether it should be possible to sync the feature group to the online feature store for low latency access.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>expectation_suite</code> <p>Optionally, attach an expectation suite to the feature group which dataframes should be validated against upon insertion.</p> <p> TYPE: <code>expectation_suite.ExpectationSuite | TypeVar('great_expectations.core.ExpectationSuite') | None</code> DEFAULT: <code>None</code> </p> <code>topic_name</code> <p>Optionally, define the name of the topic used for data ingestion. If left undefined it defaults to using project topic.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>notification_topic_name</code> <p>Optionally, define the name of the topic used for sending notifications when entries are inserted or updated on the online feature store. If left undefined no notifications are sent.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>online_config</code> <p>Optionally, define configuration which is used to configure online table.</p> <p> TYPE: <code>OnlineConfig | dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>data_source</code> <p>The data source specifying the location of the data. Overrides the storage_connector, path and query arguments when specified.</p> <p> TYPE: <code>ds.DataSource | dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>ttl</code> <p>Optional time-to-live duration for features in this group.</p> <p>Can be specified as:</p> <ul> <li>An integer or float representing seconds</li> <li>A timedelta object</li> </ul> <p>This ttl value is added to the event time of the feature group and when the system time exceeds the event time + ttl, the entries will be automatically removed. The system time zone is in UTC. By default no TTL is set.</p> <p> TYPE: <code>float | timedelta | None</code> DEFAULT: <code>None</code> </p> <code>ttl_enabled</code> <p>Optionally, enable TTL for this feature group. Defaults to True if ttl is set.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>online_disk</code> <p>Optionally, specify online data storage for this feature group. When set to True data will be stored on disk, instead of in memory. Overrides online_config.table_space. Defaults to using cluster wide configuration 'featurestore_online_tablespace' to identify tablespace for disk storage.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>feature_group.ExternalFeatureGroup</code> <p>The external feature group metadata object.</p>"},{"location":"python-api/hsfs/feature_store/#hsfs.feature_store.FeatureStore.get_or_create_spine_group","title":"[source]  get_or_create_spine_group","text":"<pre><code>get_or_create_spine_group(\n    name: str,\n    version: int | None = None,\n    description: str | None = \"\",\n    primary_key: list[str] | None = None,\n    foreign_key: list[str] | None = None,\n    event_time: str | None = None,\n    features: list[feature.Feature] | None = None,\n    dataframe: pd.DataFrame\n    | TypeVar(\"pyspark.sql.DataFrame\")\n    | TypeVar(\"pyspark.RDD\")\n    | np.ndarray\n    | list[list] = None,\n) -&gt; feature_group.SpineGroup\n</code></pre> <p>Create a spine group metadata object.</p> <p>Instead of using a feature group to save a label/prediction target, you can use a spine together with a dataframe containing the labels. A Spine is essentially a metadata object similar to a feature group, however, the data is not materialized in the feature store. It only containes the needed metadata such as the relevant event time column and primary key columns to perform point-in-time correct joins.</p> Example <pre><code># connect to the Feature Store\nfs = ...\n\nspine_df = pd.Dataframe()\n\nspine_group = fs.get_or_create_spine_group(\n    name=\"sales\",\n    version=1,\n    description=\"Physical shop sales features\",\n    primary_key=['ss_store_sk'],\n    event_time='sale_date',\n    dataframe=spine_df,\n)\n</code></pre> <p>Note that you can inspect the dataframe in the spine group, or replace the dataframe:</p> <pre><code>spine_group.dataframe.show()\n\nspine_group.dataframe = new_df\n</code></pre> <p>The spine can then be used to construct queries, with only one speciality:</p> Note <p>Spines can only be used on the left side of a feature join, as this is the base set of entities for which features are to be fetched and the left side of the join determines the event timestamps to compare against.</p> <p>If you want to use the query for a feature view to be used for online serving, you can only select the label or target feature from the spine. For the online lookup, the label is not required, therefore it is important to only select label from the left feature group, so that we don't need to provide a spine for online serving.</p> <p>These queries can then be used to create feature views. Since the dataframe contained in the spine is not being materialized, every time you use a feature view created with spine to read data you will have to provide a dataframe with the same structure again.</p> <p>For example, to generate training data:</p> <pre><code>X_train, X_test, y_train, y_test = feature_view_spine.train_test_split(0.2, spine=training_data_entities)\n</code></pre> <p>Or to get batches of fresh data for batch scoring:</p> <pre><code>feature_view_spine.get_batch_data(spine=scoring_entities_df).show()\n</code></pre> <p>Here you have the chance to pass a different set of entities to generate the training dataset.</p> <p>Sometimes it might be handy to create a feature view with a regular feature group containing the label, but then at serving time to use a spine in order to fetch features for example only for a small set of primary key values. To do this, you can pass the spine group instead of a dataframe. Just make sure it contains the needed primary key, event time and label column.</p> <pre><code>feature_view.get_batch_data(spine=spine_group)\n</code></pre> PARAMETER DESCRIPTION <code>name</code> <p>Name of the spine group to create.</p> <p> TYPE: <code>str</code> </p> <code>version</code> <p>Version of the spine group to retrieve, defaults to <code>None</code> and will create the spine group with incremented version from the last version in the feature store.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>description</code> <p>A string describing the contents of the spine group to improve discoverability for Data Scientists.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>''</code> </p> <code>primary_key</code> <p>A list of feature names to be used as primary key for the spine group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list <code>[]</code>, and the spine group won't have any primary key.</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>foreign_key</code> <p>A list of feature names to be used as foreign key for the feature group. Foreign key is referencing the primary key of another feature group and can be used as joining key. Defaults to empty list <code>[]</code>, and the feature group won't have any foreign key.</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>event_time</code> <p>Optionally, provide the name of the feature containing the event time for the features in this spine group. If event_time is set the spine group can be used for point-in-time joins.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>features</code> <p>Optionally, define the schema of the spine group manually as a list of <code>Feature</code> objects. Defaults to empty list <code>[]</code> and will use the schema information of the DataFrame resulting by executing the provided query against the data source.</p> <p>Note: Event time data type restriction     The supported data types for the event time column are: <code>timestamp</code>, <code>date</code> and <code>bigint</code>.</p> <p> TYPE: <code>list[feature.Feature] | None</code> DEFAULT: <code>None</code> </p> <code>dataframe</code> <p>Spine dataframe with primary key, event time and label column to use for point in time join when fetching features.</p> <p> TYPE: <code>pd.DataFrame | TypeVar('pyspark.sql.DataFrame') | TypeVar('pyspark.RDD') | np.ndarray | list[list]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>feature_group.SpineGroup</code> <p>The spine group metadata object.</p>"},{"location":"python-api/hsfs/feature_store/#hsfs.feature_store.FeatureStore.create_training_dataset","title":"[source]  create_training_dataset","text":"<pre><code>create_training_dataset(\n    name: str,\n    version: int | None = None,\n    description: str | None = \"\",\n    data_format: str | None = \"tfrecords\",\n    coalesce: bool | None = False,\n    storage_connector: storage_connector.StorageConnector\n    | None = None,\n    splits: dict[str, float] | None = None,\n    location: str | None = \"\",\n    seed: int | None = None,\n    statistics_config: StatisticsConfig\n    | bool\n    | dict\n    | None = None,\n    label: list[str] | None = None,\n    transformation_functions: dict[\n        str, TransformationFunction\n    ]\n    | None = None,\n    train_split: str = None,\n    data_source: ds.DataSource\n    | dict[str, Any]\n    | None = None,\n    tags: tag.Tag\n    | dict[str, Any]\n    | list[tag.Tag | dict[str, Any]]\n    | None = None,\n) -&gt; training_dataset.TrainingDataset\n</code></pre> <p>Create a training dataset metadata object.</p> Deprecated <p><code>TrainingDataset</code> is deprecated, use <code>FeatureView</code> instead. From version 3.0 training datasets created with this API are not visibile in the API anymore.</p> Lazy <p>This method is lazy and does not persist any metadata or feature data in the feature store on its own. To materialize the training dataset and save feature data along the metadata in the feature store, call the <code>save()</code> method with a <code>DataFrame</code> or <code>Query</code>.</p> Data Formats <p>The feature store currently supports the following data formats for training datasets:</p> <ol> <li>tfrecord</li> <li>csv</li> <li>tsv</li> <li>parquet</li> <li>avro</li> <li>orc</li> </ol> <p>Currently not supported petastorm, hdf5 and npy file formats.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the training dataset to create.</p> <p> TYPE: <code>str</code> </p> <code>version</code> <p>Version of the training dataset to retrieve, defaults to <code>None</code> and will create the training dataset with incremented version from the last version in the feature store.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>description</code> <p>A string describing the contents of the training dataset to improve discoverability for Data Scientists.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>''</code> </p> <code>data_format</code> <p>The data format used to save the training dataset.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>'tfrecords'</code> </p> <code>coalesce</code> <p>If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>False</code> </p> <code>storage_connector</code> <p>Storage connector defining the sink location for the training dataset, defaults to <code>None</code>, and materializes training dataset on HopsFS. [DEPRECATED: Use <code>data_source</code> instead.]</p> <p> TYPE: <code>storage_connector.StorageConnector | None</code> DEFAULT: <code>None</code> </p> <code>splits</code> <p>A dictionary defining training dataset splits to be created. Keys in the dictionary define the name of the split as <code>str</code>, values represent percentage of samples in the split as <code>float</code>. Currently, only random splits are supported. Defaults to empty dict<code>{}</code>, creating only a single training dataset without splits.</p> <p> TYPE: <code>dict[str, float] | None</code> DEFAULT: <code>None</code> </p> <code>location</code> <p>Path to complement the sink storage connector with, e.g., if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to <code>\"\"</code>, saving the training dataset at the root defined by the storage connector. [DEPRECATED: Use <code>data_source</code> instead.]</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>''</code> </p> <code>seed</code> <p>Optionally, define a seed to create the random splits with, in order to guarantee reproducability.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>statistics_config</code> <p>A configuration object, or a dictionary with keys: - <code>\"enabled\"</code> to generally enable descriptive statistics computation for this feature group, - <code>\"correlations\"</code> to turn on feature correlation computation, and - <code>\"histograms\"</code> to compute feature value frequencies.</p> <p>The values should be booleans indicating the setting. To fully turn off statistics computation pass <code>statistics_config=False</code>. Defaults to <code>None</code> and will compute only descriptive statistics.</p> <p> TYPE: <code>StatisticsConfig | bool | dict | None</code> DEFAULT: <code>None</code> </p> <code>label</code> <p>A list of feature names constituting the prediction label/feature of the training dataset. When replaying a <code>Query</code> during model inference, the label features can be omitted from the feature vector retrieval. Defaults to <code>[]</code>, no label.</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>transformation_functions</code> <p>A dictionary mapping transformation functions to the features they should be applied to before writing out the training data and at inference time. Defaults to <code>{}</code>, no transformations.</p> <p> TYPE: <code>dict[str, TransformationFunction] | None</code> DEFAULT: <code>None</code> </p> <code>train_split</code> <p>If <code>splits</code> is set, provide the name of the split that is going to be used for training. The statistics of this split will be used for transformation functions if necessary.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>data_source</code> <p>The data source specifying the location of the data. Overrides the storage_connector and location arguments when specified.</p> <p> TYPE: <code>ds.DataSource | dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>tags</code> <p>Optionally, define tags for the training dataset. Tags can be provided as: - A single Tag object - A dictionary with 'name' and 'value' keys (e.g., {\"name\": \"tag1\", \"value\": \"value1\"}) - A list of Tag objects - A list of dictionaries with 'name' and 'value' keys Tags will be attached to the training dataset after it is saved. Defaults to None.</p> <p> TYPE: <code>tag.Tag | dict[str, Any] | list[tag.Tag | dict[str, Any]] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>training_dataset.TrainingDataset</code> <p>The training dataset metadata object.</p>"},{"location":"python-api/hsfs/feature_store/#hsfs.feature_store.FeatureStore.create_transformation_function","title":"[source]  create_transformation_function","text":"<pre><code>create_transformation_function(\n    transformation_function: HopsworksUdf,\n    version: int | None = None,\n) -&gt; TransformationFunction\n</code></pre> <p>Create a transformation function metadata object.</p> Example <pre><code># define the transformation function as a Hopsworks's UDF\n@udf(int)\ndef plus_one(value):\n    return value + 1\n\n# create transformation function\nplus_one_meta = fs.create_transformation_function(\n        transformation_function=plus_one,\n        version=1\n    )\n\n# persist transformation function in backend\nplus_one_meta.save()\n</code></pre> Lazy <p>This method is lazy and does not persist the transformation function in the feature store on its own. To materialize the transformation function and save call the <code>save()</code> method of the transformation function metadata object.</p> PARAMETER DESCRIPTION <code>transformation_function</code> <p>Hopsworks UDF.</p> <p> TYPE: <code>HopsworksUdf</code> </p> RETURNS DESCRIPTION <code>TransformationFunction</code> <p>The TransformationFunction metadata object.</p>"},{"location":"python-api/hsfs/feature_store/#hsfs.feature_store.FeatureStore.get_transformation_function","title":"[source]  get_transformation_function","text":"<pre><code>get_transformation_function(\n    name: str, version: int | None = None\n) -&gt; TransformationFunction\n</code></pre> <p>Get  transformation function metadata object.</p> Get transformation function by name <p>This will default to version 1.</p> <pre><code># get feature store instance\nfs = ...\n\n# get transformation function metadata object\nplus_one_fn = fs.get_transformation_function(name=\"plus_one\")\n</code></pre> Get built-in transformation function min max scaler <pre><code># get feature store instance\nfs = ...\n\n# get transformation function metadata object\nmin_max_scaler_fn = fs.get_transformation_function(name=\"min_max_scaler\")\n</code></pre> Get transformation function by name and version <pre><code># get feature store instance\nfs = ...\n\n# get transformation function metadata object\nmin_max_scaler = fs.get_transformation_function(name=\"min_max_scaler\", version=2)\n</code></pre> <p>You can define in the feature view transformation functions as dict, where key is feature name and value is online transformation function instance. Then the transformation functions are applied when you read training data, get batch data, or get feature vector(s).</p> Attach transformation functions to the feature view <pre><code># get feature store instance\nfs = ...\n\n# define query object\nquery = ...\n\n# get transformation function metadata object\nmin_max_scaler = fs.get_transformation_function(name=\"min_max_scaler\", version=1)\n\n# attach transformation functions\nfeature_view = fs.create_feature_view(\n    name='feature_view_name',\n    query=query,\n    labels=[\"target_column\"],\n    transformation_functions=[min_max_scaler(\"feature1\")]\n)\n</code></pre> <p>Built-in transformation functions are attached in the same way. The only difference is that it will compute the necessary statistics for the specific function in the background. For example min and max values for <code>min_max_scaler</code>; mean and standard deviation for <code>standard_scaler</code> etc.</p> Attach built-in transformation functions to the feature view <pre><code># get feature store instance\nfs = ...\n\n# define query object\nquery = ...\n\n# retrieve transformation functions\nmin_max_scaler = fs.get_transformation_function(name=\"min_max_scaler\")\nstandard_scaler = fs.get_transformation_function(name=\"standard_scaler\")\nrobust_scaler = fs.get_transformation_function(name=\"robust_scaler\")\nlabel_encoder = fs.get_transformation_function(name=\"label_encoder\")\n\n# attach built-in transformation functions while creating feature view\nfeature_view = fs.create_feature_view(\n    name='transactions_view',\n    query=query,\n    labels=[\"fraud_label\"],\n    transformation_functions = [\n        label_encoder(\"category_column\"),\n        robust_scaler(\"weight\"),\n        min_max_scaler(\"age\"),\n        standard_scaler(\"salary\")\n    ]\n)\n</code></pre> PARAMETER DESCRIPTION <code>name</code> <p>Name of transformation function.</p> <p> TYPE: <code>str</code> </p> <code>version</code> <p>Version of transformation function. Optional, if not provided all functions that match to provided name will be retrieved.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>TransformationFunction</code> <p>The TransformationFunction metadata object.</p>"},{"location":"python-api/hsfs/feature_store/#hsfs.feature_store.FeatureStore.get_transformation_functions","title":"[source]  get_transformation_functions","text":"<pre><code>get_transformation_functions() -&gt; list[\n    TransformationFunction\n]\n</code></pre> <p>Get  all transformation functions metadata objects.</p> Get all transformation functions <pre><code># get feature store instance\nfs = ...\n\n# get all transformation functions\nlist_transformation_fns = fs.get_transformation_functions()\n</code></pre> RETURNS DESCRIPTION <code>list[TransformationFunction]</code> <p>List of transformation function instances.</p>"},{"location":"python-api/hsfs/feature_store/#hsfs.feature_store.FeatureStore.create_feature_view","title":"[source]  create_feature_view","text":"<pre><code>create_feature_view(\n    name: str,\n    query: Query,\n    version: int | None = None,\n    description: str | None = \"\",\n    labels: list[str] | None = None,\n    inference_helper_columns: list[str] | None = None,\n    training_helper_columns: list[str] | None = None,\n    transformation_functions: list[\n        TransformationFunction | HopsworksUdf\n    ]\n    | None = None,\n    logging_enabled: bool | None = False,\n    extra_log_columns: list[feature.Feature]\n    | list[dict[str, str]]\n    | None = None,\n    tags: tag.Tag\n    | dict[str, Any]\n    | list[tag.Tag | dict[str, Any]]\n    | None = None,\n) -&gt; feature_view.FeatureView\n</code></pre> <p>Create a feature view metadata object and saved it to hopsworks.</p> Example <pre><code># connect to the Feature Store\nfs = ...\n\n# get the feature group instances\nfg1 = fs.get_or_create_feature_group(...)\nfg2 = fs.get_or_create_feature_group(...)\n\n# construct the query\nquery = fg1.select_all().join(fg2.select_all())\n\n# define the transformation function as a Hopsworks's UDF\n@udf(int)\ndef plus_one(value):\n    return value + 1\n\n# construct list of \"transformation functions\" on features\ntransformation_functions = [plus_one(\"feature1\"), plus_one(\"feature1\"))]\n\nfeature_view = fs.create_feature_view(\n    name='air_quality_fv',\n    version=1,\n    transformation_functions=transformation_functions,\n    query=query\n)\n</code></pre> Example <pre><code># get feature store instance\nfs = ...\n\n# define query object\nquery = ...\n\n# define list of transformation functions\nmapping_transformers = ...\n\n# create feature view\nfeature_view = fs.create_feature_view(\n    name='feature_view_name',\n    version=1,\n    transformation_functions=mapping_transformers,\n    query=query\n)\n</code></pre> Warning <p><code>as_of</code> argument in the <code>Query</code> will be ignored because feature view does not support time travel query.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the feature view to create.</p> <p> TYPE: <code>str</code> </p> <code>query</code> <p>Feature store <code>Query</code>.</p> <p> TYPE: <code>Query</code> </p> <code>version</code> <p>Version of the feature view to create, defaults to <code>None</code> and will create the feature view with incremented version from the last version in the feature store.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>description</code> <p>A string describing the contents of the feature view to improve discoverability for Data Scientists.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>''</code> </p> <code>labels</code> <p>A list of feature names constituting the prediction label/feature of the feature view. When replaying a <code>Query</code> during model inference, the label features can be omitted from the feature vector retrieval. Defaults to <code>[]</code>, no label.</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>inference_helper_columns</code> <p>A list of feature names that are not used in training the model itself but can be used during batch or online inference for extra information. Inference helper column name(s) must be part of the <code>Query</code> object. If inference helper column name(s) belong to feature group that is part of a <code>Join</code> with <code>prefix</code> defined, then this prefix needs to be prepended to the original column name when defining <code>inference_helper_columns</code> list. When replaying a <code>Query</code> during model inference, the inference helper columns optionally can be omitted during batch (<code>get_batch_data</code>) and will be omitted during online  inference (<code>get_feature_vector(s)</code>). To get inference helper column(s) during online inference use <code>get_inference_helper(s)</code> method. Defaults to `[], no helper columns.</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>training_helper_columns</code> <p>A list of feature names that are not the part of the model schema itself but can be used during training as a helper for extra information. Training helper column name(s) must be part of the <code>Query</code> object. If training helper column name(s) belong to feature group that is part of a <code>Join</code> with <code>prefix</code> defined, then this prefix needs to prepended to the original column name when defining <code>training_helper_columns</code> list. When replaying a <code>Query</code> during model inference, the training helper columns will be omitted during both batch and online inference. Training helper columns can be optionally fetched with training data. For more details see documentation for feature view's get training data methods. Defaults to <code>[]</code>, no training helper columns.</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>transformation_functions</code> <p>Model Dependent Transformation functions attached to the feature view. It can be a list of list of user defined functions defined using the hopsworks <code>@udf</code> decorator. Defaults to <code>None</code>, no transformations.</p> <p> TYPE: <code>list[TransformationFunction | HopsworksUdf] | None</code> DEFAULT: <code>None</code> </p> <code>logging_enabled</code> <p>If true, enable feature logging for the feature view.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>False</code> </p> <code>extra_log_columns</code> <p>Extra columns to be logged in addition to the features used in the feature view. It can be a list of Feature objects or list a dictionaries that contains the the name and type of the columns as keys. Defaults to <code>None</code>, no extra log columns. Setting this argument implicitly enables feature logging.</p> <p> TYPE: <code>list[feature.Feature] | list[dict[str, str]] | None</code> DEFAULT: <code>None</code> </p> <code>tags</code> <p>Optionally, define tags for the feature view. Tags can be provided as: - A single Tag object - A dictionary with 'name' and 'value' keys (e.g., {\"name\": \"tag1\", \"value\": \"value1\"}) - A list of Tag objects - A list of dictionaries with 'name' and 'value' keys Tags will be attached to the feature view after it is saved. Defaults to None.</p> <p> TYPE: <code>tag.Tag | dict[str, Any] | list[tag.Tag | dict[str, Any]] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>feature_view.FeatureView</code> <p>The feature view metadata object.</p>"},{"location":"python-api/hsfs/feature_store/#hsfs.feature_store.FeatureStore.get_or_create_feature_view","title":"[source]  get_or_create_feature_view","text":"<pre><code>get_or_create_feature_view(\n    name: str,\n    query: Query,\n    version: int,\n    description: str | None = \"\",\n    labels: list[str] | None = None,\n    inference_helper_columns: list[str] | None = None,\n    training_helper_columns: list[str] | None = None,\n    transformation_functions: dict[\n        str, TransformationFunction\n    ]\n    | None = None,\n    logging_enabled: bool | None = False,\n    extra_log_columns: list[feature.Feature]\n    | list[dict[str, str]]\n    | None = None,\n) -&gt; feature_view.FeatureView\n</code></pre> <p>Get feature view metadata object or create a new one if it doesn't exist.</p> <p>This method doesn't update existing feature view metadata object.</p> Example <pre><code># connect to the Feature Store\nfs = ...\n\nfeature_view = fs.get_or_create_feature_view(\n    name='bitcoin_feature_view',\n    version=1,\n    transformation_functions=transformation_functions,\n    query=query\n)\n</code></pre> PARAMETER DESCRIPTION <code>name</code> <p>Name of the feature view to create.</p> <p> TYPE: <code>str</code> </p> <code>query</code> <p>Feature store <code>Query</code>.</p> <p> TYPE: <code>Query</code> </p> <code>version</code> <p>Version of the feature view to create.</p> <p> TYPE: <code>int</code> </p> <code>description</code> <p>A string describing the contents of the feature view to improve discoverability for Data Scientists.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>''</code> </p> <code>labels</code> <p>A list of feature names constituting the prediction label/feature of the feature view. When replaying a <code>Query</code> during model inference, the label features can be omitted from the feature vector retrieval. Defaults to <code>[]</code>, no label.</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>inference_helper_columns</code> <p>A list of feature names that are not used in training the model itself but can be used during batch or online inference for extra information. Inference helper column name(s) must be part of the <code>Query</code> object. If inference helper column name(s) belong to feature group that is part of a <code>Join</code> with <code>prefix</code> defined, then this prefix needs to be prepended to the original column name when defining <code>inference_helper_columns</code> list. When replaying a <code>Query</code> during model inference, the inference helper columns optionally can be omitted during batch (<code>get_batch_data</code>) and will be omitted during online  inference (<code>get_feature_vector(s)</code>). To get inference helper column(s) during online inference use <code>get_inference_helper(s)</code> method. Defaults to `[], no helper columns.</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>training_helper_columns</code> <p>A list of feature names that are not the part of the model schema itself but can be used during training as a helper for extra information. Training helper column name(s) must be part of the <code>Query</code> object. If training helper column name(s) belong to feature group that is part of a <code>Join</code> with <code>prefix</code> defined, then this prefix needs to prepended to the original column name when defining <code>training_helper_columns</code> list. When replaying a <code>Query</code> during model inference, the training helper columns will be omitted during both batch and online inference. Training helper columns can be optionally fetched with training data. For more details see documentation for feature view's get training data methods. Defaults to <code>[]</code>, no training helper columns.</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>transformation_functions</code> <p>Model Dependent Transformation functions attached to the feature view. It can be a list of list of user defined functions defined using the hopsworks <code>@udf</code> decorator. Defaults to <code>None</code>, no transformations.</p> <p> TYPE: <code>dict[str, TransformationFunction] | None</code> DEFAULT: <code>None</code> </p> <code>logging_enabled</code> <p>If true, enable feature logging for the feature view.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>False</code> </p> <code>extra_log_columns</code> <p>Extra columns to be logged in addition to the features used in the feature view. It can be a list of Feature objects or list a dictionaries that contains the the name and type of the columns as keys. Defaults to <code>None</code>, no extra log columns. Setting this argument implicitly enables feature logging.</p> <p> TYPE: <code>list[feature.Feature] | list[dict[str, str]] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>feature_view.FeatureView</code> <p>The feature view metadata object.</p>"},{"location":"python-api/hsfs/feature_store/#hsfs.feature_store.FeatureStore.get_feature_view","title":"[source]  get_feature_view","text":"<pre><code>get_feature_view(\n    name: str, version: int = None\n) -&gt; feature_view.FeatureView\n</code></pre> <p>Get a feature view entity from the feature store.</p> <p>Getting a feature view from the Feature Store means getting its metadata.</p> Example <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(\n    name='feature_view_name',\n    version=1\n)\n</code></pre> PARAMETER DESCRIPTION <code>name</code> <p>Name of the feature view to get.</p> <p> TYPE: <code>str</code> </p> <code>version</code> <p>Version of the feature view to retrieve, defaults to <code>None</code> and will return the <code>version=1</code>.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>feature_view.FeatureView</code> <p>The feature view metadata object or <code>None</code> if it does not exist.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hsfs/feature_store/#hsfs.feature_store.FeatureStore.get_feature_views","title":"[source]  get_feature_views","text":"<pre><code>get_feature_views(\n    name: str,\n) -&gt; list[feature_view.FeatureView]\n</code></pre> <p>Get a list of all versions of a feature view entity from the feature store.</p> <p>Getting a feature view from the Feature Store means getting its metadata.</p> Example <pre><code># get feature store instance\nfs = ...\n\n# get a list of all versions of a feature view\nfeature_view = fs.get_feature_views(\n    name='feature_view_name'\n)\n</code></pre> PARAMETER DESCRIPTION <code>name</code> <p>Name of the feature view to get.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>list[feature_view.FeatureView]</code> <p>List of feature view metadata objects.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hsfs/feature_view/","title":"hsfs.feature_view","text":""},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view","title":"hsfs.feature_view","text":""},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView","title":"[source]  FeatureView","text":"<p>Metadata class for Hopsworks feature views.</p> <p>Feature view is a logical grouping of features, defined by a query over feature groups.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.id","title":"[source]  id  <code>property</code> <code>writable</code>","text":"<pre><code>id: int\n</code></pre> <p>Feature view id.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.featurestore_id","title":"[source]  featurestore_id  <code>property</code> <code>writable</code>","text":"<pre><code>featurestore_id: int\n</code></pre> <p>Feature store id.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.feature_store_name","title":"[source]  feature_store_name  <code>property</code>","text":"<pre><code>feature_store_name: str | None\n</code></pre> <p>Name of the feature store in which the feature group is located.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.name","title":"[source]  name  <code>property</code> <code>writable</code>","text":"<pre><code>name: str\n</code></pre> <p>Name of the feature view.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.version","title":"[source]  version  <code>property</code> <code>writable</code>","text":"<pre><code>version: int\n</code></pre> <p>Version number of the feature view.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.missing_mandatory_tags","title":"[source]  missing_mandatory_tags  <code>property</code>","text":"<pre><code>missing_mandatory_tags: list[dict[str, Any]]\n</code></pre> <p>List of missing mandatory tags for the feature view.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.labels","title":"[source]  labels  <code>property</code> <code>writable</code>","text":"<pre><code>labels: list[str]\n</code></pre> <p>The labels/prediction feature of the feature view.</p> <p>Can be a composite of multiple features.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.inference_helper_columns","title":"[source]  inference_helper_columns  <code>property</code> <code>writable</code>","text":"<pre><code>inference_helper_columns: list[str]\n</code></pre> <p>The helper column sof the feature view.</p> <p>Can be a composite of multiple features.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.training_helper_columns","title":"[source]  training_helper_columns  <code>property</code> <code>writable</code>","text":"<pre><code>training_helper_columns: list[str]\n</code></pre> <p>The helper column sof the feature view.</p> <p>Can be a composite of multiple features.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.description","title":"[source]  description  <code>property</code> <code>writable</code>","text":"<pre><code>description: str | None\n</code></pre> <p>Description of the feature view.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.query","title":"[source]  query  <code>property</code> <code>writable</code>","text":"<pre><code>query: query.Query\n</code></pre> <p>Query of the feature view.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.transformation_functions","title":"[source]  transformation_functions  <code>property</code> <code>writable</code>","text":"<pre><code>transformation_functions: list[TransformationFunction]\n</code></pre> <p>Get transformation functions.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.model_dependent_transformations","title":"[source]  model_dependent_transformations  <code>property</code>","text":"<pre><code>model_dependent_transformations: dict[str, Callable]\n</code></pre> <p>Get Model-Dependent transformations as a dictionary mapping transformed feature names to transformation function.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.on_demand_transformations","title":"[source]  on_demand_transformations  <code>property</code>","text":"<pre><code>on_demand_transformations: dict[str, Callable]\n</code></pre> <p>Get On-Demand transformations as a dictionary mapping on-demand feature names to transformation function.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.request_parameters","title":"[source]  request_parameters  <code>property</code>","text":"<pre><code>request_parameters: list[str]\n</code></pre> <p>Get request parameters required for the for on-demand transformations atatched to the feature view.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.schema","title":"[source]  schema  <code>property</code> <code>writable</code>","text":"<pre><code>schema: list[\n    training_dataset_feature.TrainingDatasetFeature\n]\n</code></pre> <p>Schema of untransformed features in the Feature view.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.features","title":"[source]  features  <code>property</code>","text":"<pre><code>features: list[\n    training_dataset_feature.TrainingDatasetFeature\n]\n</code></pre> <p>Schema of untransformed features in the Feature view. (alias).</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.primary_keys","title":"[source]  primary_keys  <code>property</code>","text":"<pre><code>primary_keys: set[str]\n</code></pre> <p>Set of primary key names that is required as keys in input dict object for <code>FeatureView.get_feature_vector</code> method.</p> <p>When there are duplicated primary key names and prefix is not defined in the query, prefix is generated and prepended to the primary key name in this format \"fgId_{feature_group_id}_{join_index}\" where <code>join_index</code> is the order of the join.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.serving_keys","title":"[source]  serving_keys  <code>property</code> <code>writable</code>","text":"<pre><code>serving_keys: list[skm.ServingKey]\n</code></pre> <p>All primary keys of the feature groups included in the query.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.logging_enabled","title":"[source]  logging_enabled  <code>property</code> <code>writable</code>","text":"<pre><code>logging_enabled: bool\n</code></pre> <p>Whether feature logging is enabled for the feature view.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.feature_logging","title":"[source]  feature_logging  <code>property</code>","text":"<pre><code>feature_logging: FeatureLogging | None\n</code></pre> <p>Feature logging feature groups of this feature view.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.get_last_accessed_training_dataset","title":"[source]  get_last_accessed_training_dataset","text":"<pre><code>get_last_accessed_training_dataset()\n</code></pre> <p>Get the last accessed training dataset version used for this feature view.</p> Note <p>The value does not take into account other connections to Hopsworks. If multiple clients do training datasets operations, each will have its own view of the last accessed dataset. Also, the last accessed training dataset is not necessarily the newest one with the highest version.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.delete","title":"[source]  delete","text":"<pre><code>delete(force: bool = False) -&gt; None\n</code></pre> <p>Delete current feature view, all associated metadata and training data.</p> Example <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# delete a feature view\nfeature_view.delete()\n</code></pre> Potentially dangerous operation <p>This operation drops all metadata associated with this version of the feature view and related training dataset and materialized data in HopsFS.</p> PARAMETER DESCRIPTION <code>force</code> <p>If True, delete the feature view even if models or deployments are using it. Defaults to False, which will raise an error if the feature view is in use.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.clean","title":"[source]  clean  <code>staticmethod</code>","text":"<pre><code>clean(\n    feature_store_id: int,\n    feature_view_name: str,\n    feature_view_version: str,\n    force: bool = False,\n) -&gt; None\n</code></pre> <p>Delete the feature view and all associated metadata and training data.</p> <p>This can delete corrupted feature view which cannot be retrieved due to a corrupted query for example.</p> Example <pre><code># delete a feature view and all associated metadata\nfrom hsfs.feature_view import FeatureView\n\nFeatureView.clean(\n    feature_store_id=1,\n    feature_view_name='feature_view_name',\n    feature_view_version=1\n)\n</code></pre> Potentially dangerous operation <p>This operation drops all metadata associated with this version of the feature view and related training dataset and materialized data in HopsFS.</p> PARAMETER DESCRIPTION <code>feature_store_id</code> <p>ID of feature store.</p> <p> TYPE: <code>int</code> </p> <code>feature_view_name</code> <p>Name of feature view.</p> <p> TYPE: <code>str</code> </p> <code>feature_view_version</code> <p>Version of feature view.</p> <p> TYPE: <code>str</code> </p> <code>force</code> <p>If True, delete the feature view even if models or deployments are using it. Defaults to False, which will raise an error if the feature view is in use.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.update","title":"[source]  update","text":"<pre><code>update() -&gt; FeatureView\n</code></pre> <p>Update the description of the feature view.</p> Update the feature view with a new description <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\nfeature_view.description = \"new description\"\nfeature_view.update()\n\n# Description is updated in the metadata. Below should return \"new description\".\nfs.get_feature_view(\"feature_view_name\", 1).description\n</code></pre> RETURNS DESCRIPTION <code>FeatureView</code> <p>Updated feature view.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.init_serving","title":"[source]  init_serving","text":"<pre><code>init_serving(\n    training_dataset_version: int | None = None,\n    external: bool | None = None,\n    options: dict[str, Any] | None = None,\n    init_sql_client: bool | None = None,\n    init_rest_client: bool = False,\n    reset_rest_client: bool = False,\n    config_rest_client: dict[str, Any] | None = None,\n    default_client: Literal[\"sql\", \"rest\"] | None = None,\n    feature_logger: FeatureLogger | None = None,\n    **kwargs,\n) -&gt; None\n</code></pre> <p>Initialise feature view to retrieve feature vector from online and offline feature store.</p> Example <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# initialise feature view to retrieve a feature vector\nfeature_view.init_serving(training_dataset_version=1)\n</code></pre> PARAMETER DESCRIPTION <code>training_dataset_version</code> <p>Transformation statistics are fetched from training dataset and applied to the feature vector. Defaults to 1 for online feature store.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>external</code> <p>If set to <code>True</code>, the connection to the online feature store is established using the same host as for the <code>host</code> parameter in the <code>hopsworks.login</code> method. If set to <code>False</code>, the online feature store storage connector is used which relies on the private IP. Defaults to <code>True</code> if connection to Hopsworks is established from external environment (e.g AWS Sagemaker or Google Colab), otherwise to <code>False</code>.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>init_sql_client</code> <p>If set to <code>True</code>, this ensure the online store sql client is initialised, otherwise if init_rest_client is set to true it will skip initialising the sql client. By default the sql client is initialised if no client is specified to match legacy behaviour.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>init_rest_client</code> <p>If set to <code>True</code>, this ensure the online store rest client is initialised. Pass additional configuration options via the rest_config parameter. Set reset_rest_client to <code>True</code> to reset the rest client. By default the rest client is not initialised.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>default_client</code> <p>Which client to default to if both are initialised.</p> <p> TYPE: <code>Literal['sql', 'rest'] | None</code> DEFAULT: <code>None</code> </p> <code>options</code> <p>Additional options as key/value pairs for configuring online serving engine.</p> <ul> <li>key: kwargs of SqlAlchemy engine creation (See: https://docs.sqlalchemy.org/en/20/core/engines.html#sqlalchemy.create_engine).   For example: <code>{\"pool_size\": 10}</code>.</li> </ul> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>reset_rest_client</code> <p>If set to <code>True</code>, the rest client will be reset and reinitialised with provided configuration.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>config_rest_client</code> <p>Additional configuration options for the rest client. If the client is already initialised, this will be ignored. Options include:</p> <ul> <li><code>host</code>: string, optional.   The host of the online store.   Dynamically set if not provided.</li> <li><code>port</code>: int, optional.   The port of the online store.   Defaults to 4406.</li> <li><code>verify_certs</code>: boolean, optional.   Verify the certificates of the online store server.   Defaults to True.</li> <li><code>api_key</code>: string, optional.   The API key to authenticate with the online store.   The API key must be provided if initialising the rest client in an internal environment.</li> <li><code>timeout</code>: int, optional.   The timeout for the rest client in seconds.   Defaults to 2.</li> <li><code>use_ssl</code>: boolean, optional.   Use SSL to connect to the online store.   Defaults to True.</li> </ul> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>feature_logger</code> <p>Custom feature logger which <code>FeatureView.log</code> uses to log feature vectors. If provided, feature vectors will not be inserted to logging feature group automatically when <code>FeatureView.log</code> is called.</p> <p> TYPE: <code>FeatureLogger | None</code> DEFAULT: <code>None</code> </p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.init_batch_scoring","title":"[source]  init_batch_scoring","text":"<pre><code>init_batch_scoring(\n    training_dataset_version: int | None = None,\n) -&gt; None\n</code></pre> <p>Initialise feature view to retrieve feature vector from offline feature store.</p> Example <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# initialise feature view to retrieve feature vector from offline feature store\nfeature_view.init_batch_scoring(training_dataset_version=1)\n\n# get batch data\nbatch_data = feature_view.get_batch_data(...)\n</code></pre> PARAMETER DESCRIPTION <code>training_dataset_version</code> <p>Transformation statistics are fetched from training dataset and applied to the feature vector.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.get_batch_query","title":"[source]  get_batch_query","text":"<pre><code>get_batch_query(\n    start_time: str | int | datetime | date | None = None,\n    end_time: str | int | datetime | date | None = None,\n) -&gt; str\n</code></pre> <p>Get a query string of the batch query.</p> Batch query for the last 24 hours <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# set up dates\nimport datetime\nstart_date = (datetime.datetime.now() - datetime.timedelta(hours=24))\nend_date = (datetime.datetime.now())\n\n# get a query string of batch query\nquery_str = feature_view.get_batch_query(\n    start_time=start_date,\n    end_time=end_date\n)\n# print query string\nprint(query_str)\n</code></pre> PARAMETER DESCRIPTION <code>start_time</code> <p>Start event time for the batch query, inclusive. Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e., Unix Epoch should be in seconds.</p> <p> TYPE: <code>str | int | datetime | date | None</code> DEFAULT: <code>None</code> </p> <code>end_time</code> <p>End event time for the batch query, exclusive. Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e., Unix Epoch should be in seconds.</p> <p> TYPE: <code>str | int | datetime | date | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The batch query.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.get_feature_vector","title":"[source]  get_feature_vector","text":"<pre><code>get_feature_vector(\n    entry: dict[str, Any] | None = None,\n    passed_features: dict[str, Any] | None = None,\n    external: bool | None = None,\n    return_type: Literal[\n        \"list\", \"polars\", \"numpy\", \"pandas\"\n    ] = \"list\",\n    allow_missing: bool = False,\n    force_rest_client: bool = False,\n    force_sql_client: bool = False,\n    transform: bool | None = True,\n    on_demand_features: bool | None = True,\n    request_parameters: dict[str, Any] | None = None,\n    transformation_context: dict[str, Any] = None,\n    logging_data: bool = False,\n) -&gt; (\n    list[Any]\n    | pd.DataFrame\n    | np.ndarray\n    | pl.DataFrame\n    | HopsworksLoggingMetadataType\n)\n</code></pre> <p>Returns assembled feature vector from online feature store.</p> <p>Call <code>FeatureView.init_serving</code> before this method if the following configurations are needed:</p> <ol> <li>The training dataset version of the transformation statistics.</li> <li>Additional configurations of online serving engine.</li> </ol> Missing primary key entries <p>If the provided primary key <code>entry</code> can't be found in one or more of the feature groups used by this feature view the call to this method will raise an exception. Alternatively, setting <code>allow_missing</code> to <code>True</code> returns a feature vector with missing values.</p> Example <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get assembled serving vector as a python list\nfeature_view.get_feature_vector(\n    entry = {\"pk1\": 1, \"pk2\": 2}\n)\n\n# get assembled serving vector as a pandas dataframe\nfeature_view.get_feature_vector(\n    entry = {\"pk1\": 1, \"pk2\": 2},\n    return_type = \"pandas\"\n)\n\n# get assembled serving vector as a numpy array\nfeature_view.get_feature_vector(\n    entry = {\"pk1\": 1, \"pk2\": 2},\n    return_type = \"numpy\"\n)\n</code></pre> Get feature vector with user-supplied features <pre><code># get feature store instance\nfs = ...\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# the application provides a feature value 'app_attr'\napp_attr = ...\n\n# get a feature vector\nfeature_view.get_feature_vector(\n    entry = {\"pk1\": 1, \"pk2\": 2},\n    passed_features = { \"app_feature\" : app_attr }\n)\n</code></pre> Logging feature vector <pre><code># get feature store instance\nfs = ...\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# the application provides a feature value 'app_attr'\napp_attr = ...\n\n# get a feature vector\nfeature_vector = feature_view.get_feature_vector(\n    entry = {\"pk1\": 1, \"pk2\": 2},\n    passed_features = { \"app_feature\" : app_attr },\n    logging_data = True\n)\n\n# make predictions using the feature vector\npredictions = model.predict(feature_vector)\n\n# log the feature vector\nfeature_view.log(feature_vector, predictions=predictions)\n</code></pre> PARAMETER DESCRIPTION <code>entry</code> <p>Dictionary of feature group primary key and values provided by serving application. Set of required primary keys is <code>FeatureView.primary_keys</code>. If the required primary keys is not provided, it will look for name of the primary key in feature group in the entry.</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>passed_features</code> <p>Dictionary of feature values provided by the application at runtime. They can replace features values fetched from the feature store as well as providing feature values which are not available in the feature store. These values take priority over features retrieved from the online feature store but are overridden by <code>request_parameters</code> if the same key exists in both.</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>external</code> <p>If set to <code>True</code>, the connection to the online feature store is established using the same host as for the <code>host</code> parameter in the <code>hopsworks.login</code> method. If set to <code>False</code>, the online feature store storage connector is used which relies on the private IP. Defaults to <code>True</code> if connection to Hopsworks is established from external environment (e.g AWS Sagemaker or Google Colab), otherwise to <code>False</code>.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>return_type</code> <p>In which format to return the feature vector.</p> <p> TYPE: <code>Literal['list', 'polars', 'numpy', 'pandas']</code> DEFAULT: <code>'list'</code> </p> <code>force_rest_client</code> <p>If set to True, reads from online feature store using the REST client if initialised.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>force_sql_client</code> <p>If set to True, reads from online feature store using the SQL client if initialised.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>allow_missing</code> <p>Setting to <code>True</code> returns feature vectors with missing values.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>transform</code> <p>If set to <code>True</code>, model-dependent transformations are applied to the feature vector, and <code>on_demand_feature</code> is automatically set to <code>True</code>, ensuring the inclusion of on-demand features. If set to <code>False</code>, the function returns the feature vector without applying any model-dependent transformations.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>True</code> </p> <code>on_demand_features</code> <p>Setting this to <code>False</code> returns untransformed feature vectors without any on-demand features.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>True</code> </p> <code>request_parameters</code> <p>Request parameters required by on-demand transformation functions to compute on-demand features present in the feature view. These parameters take highest priority when resolving feature values - if a key exists in both <code>request_parameters</code> and <code>passed_features</code> or in the retrieved feature vector, the value from <code>request_parameters</code> is used.</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>transformation_context</code> <p>A dictionary mapping variable names to objects that will be provided as contextual information to the transformation function at runtime. The <code>context</code> variable must be explicitly defined as parameters in the transformation function for these to be accessible during execution. If no context variables are provided, this parameter defaults to <code>None</code>.</p> <p> TYPE: <code>dict[str, Any]</code> DEFAULT: <code>None</code> </p> <code>logging_data</code> <p>Setting this to <code>True</code> return feature vector with logging metadata. The feature vector will contain only the required features. The logging metadata is available as part of an additional attribute <code>hopsworks_logging_metadata</code> of the returned object. The logging metadata contains the untransformed features, transformed features, inference helpers, serving keys, request parameters and event time. The feature vector object returned can be passed to <code>feature_view.log()</code> to log the feature vector along with all the logging metadata.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>list[Any] | pd.DataFrame | np.ndarray | pl.DataFrame | HopsworksLoggingMetadataType</code> <p>Returned <code>list</code>, <code>pd.DataFrame</code>, <code>polars.DataFrame</code> or <code>np.ndarray</code> (the exact type dependends on <code>return_type</code>) contains feature values related to provided primary keys, ordered according to positions of this features in the feature view query.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.FeatureStoreException</code> <p>When primary key entry cannot be found in one or more of the feature groups used by this feature view.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.get_feature_vectors","title":"[source]  get_feature_vectors","text":"<pre><code>get_feature_vectors(\n    entry: list[dict[str, Any]] | None = None,\n    passed_features: list[dict[str, Any]] | None = None,\n    external: bool | None = None,\n    return_type: Literal[\n        \"list\", \"polars\", \"numpy\", \"pandas\"\n    ] = \"list\",\n    allow_missing: bool = False,\n    force_rest_client: bool = False,\n    force_sql_client: bool = False,\n    transform: bool | None = True,\n    on_demand_features: bool | None = True,\n    request_parameters: list[dict[str, Any]] | None = None,\n    transformation_context: dict[str, Any] = None,\n    logging_data: bool = False,\n) -&gt; (\n    list[list[Any]]\n    | pd.DataFrame\n    | np.ndarray\n    | pl.DataFrame\n    | HopsworksLoggingMetadataType\n)\n</code></pre> <p>Returns assembled feature vectors in batches from online feature store.</p> <p>Call <code>FeatureView.init_serving</code> before this method if the following configurations are needed.</p> <ol> <li>The training dataset version of the transformation statistics.</li> <li>Additional configurations of online serving engine.</li> </ol> Missing primary key entries <p>If any of the provided primary key elements in <code>entry</code> can't be found in any of the feature groups, no feature vector for that primary key value will be returned. If it can be found in at least one but not all feature groups used by this feature view the call to this method will raise an exception. Alternatively, setting <code>allow_missing</code> to <code>True</code> returns feature vectors with missing values.</p> Example <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get assembled serving vectors as a python list of lists\nfeature_view.get_feature_vectors(\n    entry = [\n        {\"pk1\": 1, \"pk2\": 2},\n        {\"pk1\": 3, \"pk2\": 4},\n        {\"pk1\": 5, \"pk2\": 6}\n    ]\n)\n\n# get assembled serving vectors as a pandas dataframe\nfeature_view.get_feature_vectors(\n    entry = [\n        {\"pk1\": 1, \"pk2\": 2},\n        {\"pk1\": 3, \"pk2\": 4},\n        {\"pk1\": 5, \"pk2\": 6}\n    ],\n    return_type = \"pandas\"\n)\n\n# get assembled serving vectors as a numpy array\nfeature_view.get_feature_vectors(\n    entry = [\n        {\"pk1\": 1, \"pk2\": 2},\n        {\"pk1\": 3, \"pk2\": 4},\n        {\"pk1\": 5, \"pk2\": 6}\n    ],\n    return_type = \"numpy\"\n)\n</code></pre> Logging feature vectors <pre><code># get feature store instance\nfs = ...\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# the application provides a feature value 'app_attr'\napp_attr = ...\n\n# get a feature vectors\nfeature_vectors = feature_view.get_feature_vectors(\n    entry = [\n        {\"pk1\": 1, \"pk2\": 2},\n        {\"pk1\": 3, \"pk2\": 4},\n        {\"pk1\": 5, \"pk2\": 6}\n    ],\n    logging_data = True\n)\n\n# make predictions using the feature vectors\npredictions = model.predict(feature_vectors)\n\n# log the feature vectors\nfeature_view.log(feature_vectors, predictions=predictions)\n</code></pre> PARAMETER DESCRIPTION <code>entry</code> <p>A list of dictionary of feature group primary key and values provided by serving application. Set of required primary keys is <code>FeatureView.primary_keys</code>. If the required primary keys is not provided, it will look for name of the primary key in feature group in the entry.</p> <p> TYPE: <code>list[dict[str, Any]] | None</code> DEFAULT: <code>None</code> </p> <code>passed_features</code> <p>A list of dictionary of feature values provided by the application at runtime. They can replace features values fetched from the feature store as well as providing feature values which are not available in the feature store. These values take priority over features retrieved from the online feature store but are overridden by <code>request_parameters</code> if the same key exists in both.</p> <p> TYPE: <code>list[dict[str, Any]] | None</code> DEFAULT: <code>None</code> </p> <code>external</code> <p>If set to <code>True</code>, the connection to the online feature store is established using the same host as for the <code>host</code> parameter in the <code>hopsworks.login</code> method. If set to <code>False</code>, the online feature store storage connector is used which relies on the private IP. Defaults to <code>True</code> if connection to Hopsworks is established from external environment (e.g AWS Sagemaker or Google Colab), otherwise to <code>False</code>.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>return_type</code> <p>The format in which to return the feature vectors.</p> <p> TYPE: <code>Literal['list', 'polars', 'numpy', 'pandas']</code> DEFAULT: <code>'list'</code> </p> <code>force_sql_client</code> <p>If set to <code>True</code>, reads from online feature store using the SQL client if initialised.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>force_rest_client</code> <p>If set to <code>True</code>, reads from online feature store using the REST client if initialised.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>allow_missing</code> <p>Setting to <code>True</code> returns feature vectors with missing values.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>transform</code> <p>If set to <code>True</code>, model-dependent transformations are applied to the feature vector, and <code>on_demand_feature</code> is automatically set to <code>True</code>, ensuring the inclusion of on-demand features. If set to <code>False</code>, the function returns the feature vector without applying any model-dependent transformations.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>True</code> </p> <code>on_demand_features</code> <p>Setting this to <code>False</code> returns untransformed feature vectors without any on-demand features.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>True</code> </p> <code>request_parameters</code> <p>Request parameters required by on-demand transformation functions to compute on-demand features present in the feature view. These parameters take highest priority when resolving feature values - if a key exists in both <code>request_parameters</code> and <code>passed_features</code> or in the retrieved feature vectors, the value from <code>request_parameters</code> is used.</p> <p> TYPE: <code>list[dict[str, Any]] | None</code> DEFAULT: <code>None</code> </p> <code>transformation_context</code> <p>A dictionary mapping variable names to objects that will be provided as contextual information to the transformation function at runtime. The <code>context</code> variable must be explicitly defined as parameters in the transformation function for these to be accessible during execution. If no context variables are provided, this parameter defaults to <code>None</code>.</p> <p> TYPE: <code>dict[str, Any]</code> DEFAULT: <code>None</code> </p> <code>logging_data</code> <p>Setting this to <code>True</code> return feature vector with logging metadata. The feature vectors will contain only contain the required features. The logging metadata is available as part of an additional attribute <code>hopsworks_logging_metadata</code> of the returned object. The logging metadata contains the untransformed features, transformed features, inference helpers, serving keys, request parameters and event time. The feature vector object returned can be passed to <code>feature_view.log()</code> to log the feature vectors along with all the logging metadata.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>list[list[Any]] | pd.DataFrame | np.ndarray | pl.DataFrame | HopsworksLoggingMetadataType</code> <p>Returned <code>List[list]</code>, <code>pd.DataFrame</code>, <code>polars.DataFrame</code> or <code>np.ndarray</code> (depending on the <code>return_type</code>) contains feature values related to provided primary keys, ordered according to positions of this features in the feature view query.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.FeatureStoreException</code> <p>When primary key entry cannot be found in one or more of the feature groups used by this feature view.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.get_inference_helper","title":"[source]  get_inference_helper","text":"<pre><code>get_inference_helper(\n    entry: dict[str, Any],\n    external: bool | None = None,\n    return_type: Literal[\n        \"pandas\", \"dict\", \"polars\"\n    ] = \"pandas\",\n    force_rest_client: bool = False,\n    force_sql_client: bool = False,\n) -&gt; pd.DataFrame | pl.DataFrame | dict[str, Any]\n</code></pre> <p>Returns assembled inference helper column vectors from online feature store.</p> Example <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get assembled inference helper column vector\nfeature_view.get_inference_helper(\n    entry = {\"pk1\": 1, \"pk2\": 2}\n)\n</code></pre> PARAMETER DESCRIPTION <code>entry</code> <p>Dictionary of feature group primary key and values provided by serving application. Set of required primary keys is <code>FeatureView.primary_keys</code>.</p> <p> TYPE: <code>dict[str, Any]</code> </p> <code>external</code> <p>If set to <code>True</code>, the connection to the online feature store is established using the same host as for the <code>host</code> parameter in the <code>hopsworks.login</code> method. If set to <code>False</code>, the online feature store storage connector is used which relies on the private IP. Defaults to <code>True</code> if connection to Hopsworks is established from external environment (e.g AWS Sagemaker or Google Colab), otherwise to <code>False</code>.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>return_type</code> <p>The format in which to return the dataframe.</p> <p> TYPE: <code>Literal['pandas', 'dict', 'polars']</code> DEFAULT: <code>'pandas'</code> </p> RETURNS DESCRIPTION <code>pd.DataFrame | pl.DataFrame | dict[str, Any]</code> <p>The dataframe.</p> RAISES DESCRIPTION <code>`Exception`</code> <p>When primary key entry cannot be found in one or more of the feature groups used by this feature view.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.get_inference_helpers","title":"[source]  get_inference_helpers","text":"<pre><code>get_inference_helpers(\n    entry: list[dict[str, Any]],\n    external: bool | None = None,\n    return_type: Literal[\n        \"pandas\", \"dict\", \"polars\"\n    ] = \"pandas\",\n    force_sql_client: bool = False,\n    force_rest_client: bool = False,\n) -&gt; list[dict[str, Any]] | pd.DataFrame | pl.DataFrame\n</code></pre> <p>Returns assembled inference helper column vectors in batches from online feature store.</p> Missing primary key entries <p>If any of the provided primary key elements in <code>entry</code> can't be found in any of the feature groups, no inference helper column vectors for that primary key value will be returned. If it can be found in at least one but not all feature groups used by this feature view the call to this method will raise an exception.</p> Example <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get assembled inference helper column vectors\nfeature_view.get_inference_helpers(\n    entry = [\n        {\"pk1\": 1, \"pk2\": 2},\n        {\"pk1\": 3, \"pk2\": 4},\n        {\"pk1\": 5, \"pk2\": 6}\n    ]\n)\n</code></pre> PARAMETER DESCRIPTION <code>entry</code> <p>A list of dictionary of feature group primary key and values provided by serving application. Set of required primary keys is <code>FeatureView.primary_keys</code>.</p> <p> TYPE: <code>list[dict[str, Any]]</code> </p> <code>external</code> <p>If set to <code>True</code>, the connection to the online feature store is established using the same host as for the <code>host</code> parameter in the <code>hopsworks.login</code> method. If set to <code>False</code>, the online feature store storage connector is used which relies on the private IP. Defaults to <code>True</code> if connection to Hopsworks is established from external environment (e.g AWS Sagemaker or Google Colab), otherwise to <code>False</code>.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>return_type</code> <p>The format in which to return the dataframes.</p> <p> TYPE: <code>Literal['pandas', 'dict', 'polars']</code> DEFAULT: <code>'pandas'</code> </p> RETURNS DESCRIPTION <code>list[dict[str, Any]] | pd.DataFrame | pl.DataFrame</code> <p>Returned <code>pd.DataFrame</code>, <code>polars.DataFrame</code> or <code>List[dict]</code> (depending on <code>return_type</code>) contains feature values related to provided primary keys, ordered according to positions of this features in the feature view query.</p> RAISES DESCRIPTION <code>Exception</code> <p>When primary key entry cannot be found in one or more of the feature groups used by this feature view.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.find_neighbors","title":"[source]  find_neighbors","text":"<pre><code>find_neighbors(\n    embedding: list[int | float],\n    feature: Feature | None = None,\n    k: int | None = 10,\n    filter: Filter | Logic | None = None,\n    external: bool | None = None,\n    return_type: Literal[\n        \"list\", \"polars\", \"pandas\"\n    ] = \"list\",\n) -&gt; list[list[Any]]\n</code></pre> <p>Finds the nearest neighbors for a given embedding in the vector database.</p> <p>If <code>filter</code> is specified, or if embedding feature is stored in default project index, the number of results returned may be less than k. Try using a large value of k and extract the top k items from the results if needed.</p> Duplicate column error in Polars <p>If the feature view has duplicate column names, attempting to create a polars DataFrame will raise an error. To avoid this, set <code>return_type</code> to <code>\"list\"</code> or <code>\"pandas\"</code>.</p> PARAMETER DESCRIPTION <code>embedding</code> <p>The target embedding for which neighbors are to be found.</p> <p> TYPE: <code>list[int | float]</code> </p> <code>feature</code> <p>The feature used to compute similarity score. Required only if there are multiple embeddings.</p> <p> TYPE: <code>Feature | None</code> DEFAULT: <code>None</code> </p> <code>k</code> <p>The number of nearest neighbors to retrieve.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>10</code> </p> <code>filter</code> <p>A filter expression to restrict the search space.</p> <p> TYPE: <code>Filter | Logic | None</code> DEFAULT: <code>None</code> </p> <code>external</code> <p>If set to <code>True</code>, the connection to the online feature store is established using the same host as for the <code>host</code> parameter in the <code>hopsworks.login</code> method. If set to <code>False</code>, the online feature store storage connector is used which relies on the private IP. Defaults to <code>True</code> if connection to Hopsworks is established from external environment (e.g AWS Sagemaker or Google Colab), otherwise to <code>False</code>.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>return_type</code> <p>The format in which to return the neighbors.</p> <p> TYPE: <code>Literal['list', 'polars', 'pandas']</code> DEFAULT: <code>'list'</code> </p> RETURNS DESCRIPTION <code>list[list[Any]]</code> <p>The nearest neighbor feature vectors.</p> Example <pre><code>embedding_index = EmbeddingIndex()\nembedding_index.add_embedding(name=\"user_vector\", dimension=3)\nfg = fs.create_feature_group(\n            name='air_quality',\n            embedding_index=embedding_index,\n            version=1,\n            primary_key=['id1'],\n            online_enabled=True,\n        )\nfg.insert(data)\nfv = fs.create_feature_view(\"air_quality\", fg.select_all())\nfv.find_neighbors(\n    [0.1, 0.2, 0.3],\n    k=5,\n)\n\n# apply filter\nfg.find_neighbors(\n    [0.1, 0.2, 0.3],\n    k=5,\n    feature=fg.user_vector,  # optional\n    filter=(fg.id1 &gt; 10) &amp; (fg.id1 &lt; 30)\n)\n</code></pre>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.get_batch_data","title":"[source]  get_batch_data","text":"<pre><code>get_batch_data(\n    start_time: str | int | datetime | date | None = None,\n    end_time: str | int | datetime | date | None = None,\n    read_options: dict[str, Any] | None = None,\n    spine: SplineDataFrameTypes | None = None,\n    primary_key: bool = False,\n    event_time: bool = False,\n    inference_helper_columns: bool = False,\n    dataframe_type: Literal[\n        \"default\",\n        \"spark\",\n        \"pandas\",\n        \"polars\",\n        \"numpy\",\n        \"python\",\n    ] = \"default\",\n    transformed: bool | None = True,\n    transformation_context: dict[str, Any] = None,\n    logging_data: bool = False,\n    **kwargs,\n) -&gt; (\n    TrainingDatasetDataFrameTypes\n    | HopsworksLoggingMetadataType\n)\n</code></pre> <p>Get a batch of data from an event time interval from the offline feature store.</p> Batch data for the last 24 hours <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# set up dates\nimport datetime\nstart_date = (datetime.datetime.now() - datetime.timedelta(hours=24))\nend_date = (datetime.datetime.now())\n\n# get a batch of data\ndf = feature_view.get_batch_data(\n    start_time=start_date,\n    end_time=end_date\n)\n</code></pre> Log Batch data for the last 24 hours <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# set up dates\nimport datetime\nstart_date = (datetime.datetime.now() - datetime.timedelta(hours=24))\nend_date = (datetime.datetime.now())\n\n# get a batch of data\ndf = feature_view.get_batch_data(\n    start_time=start_date,\n    end_time=end_date,\n    logging_data=True\n)\n\n# make predictions using the batch data\npredictions = model.predict(df)\n\n# log the batch data\nfeature_view.log(df, predictions=predictions)\n</code></pre> Spine Groups/Dataframes <p>Spine groups and dataframes are currently only supported with the Spark engine and Spark dataframes.</p> PARAMETER DESCRIPTION <code>start_time</code> <p>Start event time for the batch query, inclusive. Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e., Unix Epoch should be in seconds.</p> <p> TYPE: <code>str | int | datetime | date | None</code> DEFAULT: <code>None</code> </p> <code>end_time</code> <p>End event time for the batch query, exclusive. Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e., Unix Epoch should be in seconds.</p> <p> TYPE: <code>str | int | datetime | date | None</code> DEFAULT: <code>None</code> </p> <code>read_options</code> <p>User provided read options for python engine, defaults to <code>{}</code>:</p> <ul> <li>key <code>\"arrow_flight_config\"</code> to pass a dictionary of arrow flight configurations.   For example: <code>{\"arrow_flight_config\": {\"timeout\": 900}}</code>.</li> </ul> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>spine</code> <p>Spine dataframe with primary key, event time and label column to use for point in time join when fetching features. Defaults to <code>None</code> and is only required when feature view was created with spine group in the feature query. It is possible to directly pass a spine group instead of a dataframe to overwrite the left side of the feature join, however, the same features as in the original feature group that is being replaced need to be available in the spine group.</p> <p> TYPE: <code>SplineDataFrameTypes | None</code> DEFAULT: <code>None</code> </p> <code>primary_key</code> <p>Whether to include primary key features or not. Defaults to <code>False</code>, no primary key features.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>event_time</code> <p>Whether to include event time feature or not. Defaults to <code>False</code>, no event time feature.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>inference_helper_columns</code> <p>Whether to include inference helper columns or not. Inference helper columns are a list of feature names in the feature view, defined during its creation, that may not be used in training the model itself but can be used during batch or online inference for extra information. If inference helper columns were not defined in the feature view <code>inference_helper_columns=True</code> will not any effect. Defaults to <code>False</code>, no helper columns.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>dataframe_type</code> <p>The type of the returned dataframe. Defaults to <code>\"default\"</code>, which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</p> <p> TYPE: <code>Literal['default', 'spark', 'pandas', 'polars', 'numpy', 'python']</code> DEFAULT: <code>'default'</code> </p> <code>transformed</code> <p>Setting to <code>False</code> returns the untransformed feature vectors.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>True</code> </p> <code>transformation_context</code> <p>A dictionary mapping variable names to objects that will be provided as contextual information to the transformation function at runtime. The <code>context</code> variable must be explicitly defined as parameters in the transformation function for these to be accessible during execution. If no context variables are provided, this parameter defaults to <code>None</code>.</p> <p> TYPE: <code>dict[str, Any]</code> DEFAULT: <code>None</code> </p> <code>logging_data</code> <p>Setting this to <code>True</code> return batch data with logging metadata. The batch data will contain only contain the required features. The logging metadata is available as part of an additional attribute <code>hopsworks_logging_metadata</code> of the returned object. The logging metadata contains the untransformed features, transformed features, inference helpers, serving keys, request parameters and event time. The batch data object returned can be passed to <code>feature_view.log()</code> to log the feature vectors along with all the logging metadata.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>The spark dataframe containing the feature data.</p> <p> TYPE: <code>TrainingDatasetDataFrameTypes | HopsworksLoggingMetadataType</code> </p> <code>TrainingDatasetDataFrameTypes | HopsworksLoggingMetadataType</code> <p>pyspark.DataFrame: A Spark DataFrame.</p> <code>TrainingDatasetDataFrameTypes | HopsworksLoggingMetadataType</code> <p>pandas.DataFrame: A Pandas DataFrame.</p> <code>TrainingDatasetDataFrameTypes | HopsworksLoggingMetadataType</code> <p>polars.DataFrame: A Polars DataFrame.</p> <code>TrainingDatasetDataFrameTypes | HopsworksLoggingMetadataType</code> <p>numpy.ndarray: A two-dimensional Numpy array.</p> <code>list</code> <p>A two-dimensional Python list.</p> <p> TYPE: <code>TrainingDatasetDataFrameTypes | HopsworksLoggingMetadataType</code> </p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.add_tag","title":"[source]  add_tag","text":"<pre><code>add_tag(name: str, value: Any) -&gt; None\n</code></pre> <p>Attach a tag to a feature view.</p> <p>A tag consists of a name and value pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects.</p> Example <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# attach a tag to a feature view\nfeature_view.add_tag(name=\"tag_schema\", value={\"key\", \"value\"})\n</code></pre> PARAMETER DESCRIPTION <code>name</code> <p>Name of the tag to be added.</p> <p> TYPE: <code>str</code> </p> <code>value</code> <p>Value of the tag to be added.</p> <p> TYPE: <code>Any</code> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.get_tag","title":"[source]  get_tag","text":"<pre><code>get_tag(name: str) -&gt; tag.Tag | None\n</code></pre> <p>Get the tags of a feature view.</p> Example <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get a tag of a feature view\nname = feature_view.get_tag('tag_name')\n</code></pre> PARAMETER DESCRIPTION <code>name</code> <p>Name of the tag to get.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>tag.Tag | None</code> <p>Tag value or <code>None</code> if it does not exist.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.get_tags","title":"[source]  get_tags","text":"<pre><code>get_tags() -&gt; dict[str, tag.Tag]\n</code></pre> <p>Returns all tags attached to a feature view.</p> Example <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get tags\nlist_tags = feature_view.get_tags()\n</code></pre> RETURNS DESCRIPTION <code>dict[str, tag.Tag]</code> <p>The dictionary of tags.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.get_parent_feature_groups","title":"[source]  get_parent_feature_groups","text":"<pre><code>get_parent_feature_groups() -&gt; (\n    explicit_provenance.Links | None\n)\n</code></pre> <p>Get the parents of this feature view, based on explicit provenance.</p> <p>Parents are feature groups or external feature groups. These feature groups can be accessible, deleted or inaccessible. For deleted and inaccessible feature groups, only minimal information is returned.</p> RETURNS DESCRIPTION <code>explicit_provenance.Links | None</code> <p><code>Links</code>: Object containing the section of provenance graph requested or <code>None</code> if it does not exist.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.get_newest_model","title":"[source]  get_newest_model","text":"<pre><code>get_newest_model(\n    training_dataset_version: int | None = None,\n) -&gt; Model | None\n</code></pre> <p>Get the latest generated model using this feature view, based on explicit provenance.</p> <p>Search only through the accessible models. For more items use the base method, see get_models_provenance.</p> PARAMETER DESCRIPTION <code>training_dataset_version</code> <p>Filter generated models based on the used training dataset version.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Model | None</code> <p>Newest Generated Model or <code>None</code> if it does not exist.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.get_models","title":"[source]  get_models","text":"<pre><code>get_models(\n    training_dataset_version: int | None = None,\n) -&gt; list[Model]\n</code></pre> <p>Get the generated models using this feature view, based on explicit provenance.</p> <p>Only the accessible models are returned. For more items use the base method, see get_models_provenance.</p> PARAMETER DESCRIPTION <code>training_dataset_version</code> <p>Filter generated models based on the used training dataset version.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>list[Model]</code> <p>List of models.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.get_models_provenance","title":"[source]  get_models_provenance","text":"<pre><code>get_models_provenance(\n    training_dataset_version: int | None = None,\n) -&gt; explicit_provenance.Links\n</code></pre> <p>Get the generated models using this feature view, based on explicit provenance.</p> <p>These models can be accessible or inaccessible. Explicit provenance does not track deleted generated model links, so deleted will always be empty. For inaccessible models, only a minimal information is returned.</p> PARAMETER DESCRIPTION <code>training_dataset_version</code> <p>Filter generated models based on the used training dataset version.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>explicit_provenance.Links</code> <p>Object containing the section of provenance graph requested or <code>None</code> if it does not exist.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.delete_tag","title":"[source]  delete_tag","text":"<pre><code>delete_tag(name: str) -&gt; None\n</code></pre> <p>Delete a tag attached to a feature view.</p> Example <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# delete a tag\nfeature_view.delete_tag('name_of_tag')\n</code></pre> PARAMETER DESCRIPTION <code>name</code> <p>Name of the tag to be removed.</p> <p> TYPE: <code>str</code> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.update_last_accessed_training_dataset","title":"[source]  update_last_accessed_training_dataset","text":"<pre><code>update_last_accessed_training_dataset(version)\n</code></pre> <p>Update the cached last accessed training dataset version.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.create_training_data","title":"[source]  create_training_data","text":"<pre><code>create_training_data(\n    start_time: str | int | datetime | date | None = \"\",\n    end_time: str | int | datetime | date | None = \"\",\n    storage_connector: storage_connector.StorageConnector\n    | None = None,\n    location: str | None = \"\",\n    description: str | None = \"\",\n    extra_filter: filter.Filter\n    | filter.Logic\n    | None = None,\n    data_format: str | None = \"parquet\",\n    coalesce: bool | None = False,\n    seed: int | None = None,\n    statistics_config: StatisticsConfig\n    | bool\n    | dict\n    | None = None,\n    write_options: dict[Any, Any] | None = None,\n    spine: SplineDataFrameTypes | None = None,\n    transformation_context: dict[str, Any] = None,\n    data_source: ds.DataSource\n    | dict[str, Any]\n    | None = None,\n    tags: tag.Tag\n    | dict[str, Any]\n    | list[tag.Tag | dict[str, Any]]\n    | None = None,\n    **kwargs,\n) -&gt; tuple[int, job.Job]\n</code></pre> <p>Create the metadata for a training dataset and save the corresponding training data into <code>location</code>.</p> <p>The training data can be retrieved by calling <code>feature_view.get_training_data</code>.</p> Create training dataset <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# create a training dataset\nversion, job = feature_view.create_training_data(\n    description='Description of a dataset',\n    data_format='csv',\n    # async creation in order not to wait till finish of the job\n    write_options={\"wait_for_job\": False}\n)\n</code></pre> Create training data specifying date range with dates as strings <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# set up dates\nstart_time = \"2022-01-01 00:00:00\"\nend_time = \"2022-06-06 23:59:59\"\n\n# create a training dataset\nversion, job = feature_view.create_training_data(\n    start_time=start_time,\n    end_time=end_time,\n    description='Description of a dataset',\n    # you can have different data formats such as csv, tsv, tfrecord, parquet and others\n    data_format='csv'\n)\n\n# When we want to read the training data, we need to supply the training data version returned by the create_training_data method:\nX_train, X_test, y_train, y_test = feature_view.get_training_data(version)\n</code></pre> Create training data specifying date range with dates as datetime objects <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# set up dates\nfrom datetime import datetime\ndate_format = \"%Y-%m-%d %H:%M:%S\"\n\nstart_time = datetime.strptime(\"2022-01-01 00:00:00\", date_format)\nend_time = datetime.strptime(\"2022-06-06 23:59:59\", date_format)\n\n# create a training dataset\nversion, job = feature_view.create_training_data(\n    start_time=start_time,\n    end_time=end_time,\n    description='Description of a dataset',\n    # you can have different data formats such as csv, tsv, tfrecord, parquet and others\n    data_format='csv'\n)\n</code></pre> Write training dataset to external storage <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get storage connector instance\ndata_source = fs.get_data_source(\"test_data_source\")\n\n# create a train-test split dataset\nversion, job = feature_view.create_training_data(\n    start_time=...,\n    end_time=...,\n    data_source=data_source,\n    description=...,\n    # you can have different data formats such as csv, tsv, tfrecord, parquet and others\n    data_format=...\n)\n</code></pre> Data Formats <p>The feature store currently supports the following data formats for training datasets:</p> <ol> <li>tfrecord</li> <li>csv</li> <li>tsv</li> <li>parquet</li> <li>avro</li> <li>orc</li> <li>json</li> </ol> <p>Currently not supported petastorm, hdf5 and npy file formats.</p> Spine Groups/Dataframes <p>Spine groups and dataframes are currently only supported with the Spark engine and Spark dataframes.</p> PARAMETER DESCRIPTION <code>start_time</code> <p>Start event time for the training dataset query, inclusive. Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e., Unix Epoch should be in seconds.</p> <p> TYPE: <code>str | int | datetime | date | None</code> DEFAULT: <code>''</code> </p> <code>end_time</code> <p>End event time for the training dataset query, exclusive. Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e., Unix Epoch should be in seconds.</p> <p> TYPE: <code>str | int | datetime | date | None</code> DEFAULT: <code>''</code> </p> <code>storage_connector</code> <p>Storage connector defining the sink location for the training dataset, defaults to <code>None</code>, and materializes training dataset on HopsFS. [DEPRECATED: Use <code>data_source</code> instead.]</p> <p> TYPE: <code>storage_connector.StorageConnector | None</code> DEFAULT: <code>None</code> </p> <code>location</code> <p>Path to complement the sink storage connector with, e.g., if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to <code>\"\"</code>, saving the training dataset at the root defined by the storage connector. [DEPRECATED: Use <code>data_source</code> instead.]</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>''</code> </p> <code>description</code> <p>A string describing the contents of the training dataset to improve discoverability for Data Scientists.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>''</code> </p> <code>extra_filter</code> <p>Additional filters to be attached to the training dataset. The filters will be also applied in <code>get_batch_data</code>.</p> <p> TYPE: <code>filter.Filter | filter.Logic | None</code> DEFAULT: <code>None</code> </p> <code>data_format</code> <p>The data format used to save the training dataset.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>'parquet'</code> </p> <code>coalesce</code> <p>If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>False</code> </p> <code>seed</code> <p>Optionally, define a seed to create the random splits with, in order to guarantee reproducability.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>statistics_config</code> <p>A configuration object, or a dictionary with keys:</p> <ul> <li><code>\"enabled\"</code> to generally enable descriptive statistics computation for this feature group,</li> <li><code>\"correlations\"</code> to turn on feature correlation computation, and</li> <li><code>\"histograms\"</code> to compute feature value frequencies.</li> </ul> <p>The values should be booleans indicating the setting. To fully turn off statistics computation pass <code>statistics_config=False</code>. Defaults to <code>None</code> and will compute only descriptive statistics.</p> <p> TYPE: <code>StatisticsConfig | bool | dict | None</code> DEFAULT: <code>None</code> </p> <code>write_options</code> <p>Additional options as key/value pairs to pass to the execution engine.</p> <p>For <code>spark</code> engine: Dictionary of read options for Spark.</p> <p>When using the <code>python</code> engine, write_options can contain the following entries:</p> <ul> <li>key <code>use_spark</code> and value <code>True</code> to materialize training dataset with Spark instead of Hopsworks Feature Query Service.</li> <li>key <code>spark</code> and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset.</li> <li>key <code>wait_for_job</code> and value <code>True</code> or <code>False</code> to configure whether or not to the save call should return only after the Hopsworks Job has finished.   By default it waits.</li> </ul> <p> TYPE: <code>dict[Any, Any] | None</code> DEFAULT: <code>None</code> </p> <code>spine</code> <p>Spine dataframe with primary key, event time and label column to use for point in time join when fetching features. Defaults to <code>None</code> and is only required when feature view was created with spine group in the feature query. It is possible to directly pass a spine group instead of a dataframe to overwrite the left side of the feature join, however, the same features as in the original feature group that is being replaced need to be available in the spine group.</p> <p> TYPE: <code>SplineDataFrameTypes | None</code> DEFAULT: <code>None</code> </p> <code>transformation_context</code> <p>A dictionary mapping variable names to objects that will be provided as contextual information to the transformation function at runtime. The <code>context</code> variable must be explicitly defined as parameters in the transformation function for these to be accessible during execution.</p> <p> TYPE: <code>dict[str, Any]</code> DEFAULT: <code>None</code> </p> <code>data_source</code> <p>The data source specifying the location of the data. Overrides the storage_connector and location arguments when specified.</p> <p> TYPE: <code>ds.DataSource | dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>td_version</code> <p>training dataset version</p> <p> TYPE: <code>int</code> </p> <code>Job</code> <p>When using the <code>python</code> engine, the Hopsworks Job that was launched to create the training dataset.</p> <p> TYPE: <code>job.Job</code> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.create_train_test_split","title":"[source]  create_train_test_split","text":"<pre><code>create_train_test_split(\n    test_size: float | None = None,\n    train_start: str | int | datetime | date | None = \"\",\n    train_end: str | int | datetime | date | None = \"\",\n    test_start: str | int | datetime | date | None = \"\",\n    test_end: str | int | datetime | date | None = \"\",\n    storage_connector: storage_connector.StorageConnector\n    | None = None,\n    location: str | None = \"\",\n    description: str | None = \"\",\n    extra_filter: filter.Filter\n    | filter.Logic\n    | None = None,\n    data_format: str | None = \"parquet\",\n    coalesce: bool | None = False,\n    seed: int | None = None,\n    statistics_config: StatisticsConfig\n    | bool\n    | dict\n    | None = None,\n    write_options: dict[Any, Any] | None = None,\n    spine: SplineDataFrameTypes | None = None,\n    transformation_context: dict[str, Any] = None,\n    data_source: ds.DataSource\n    | dict[str, Any]\n    | None = None,\n    tags: tag.Tag\n    | dict[str, Any]\n    | list[tag.Tag | dict[str, Any]]\n    | None = None,\n    **kwargs,\n) -&gt; tuple[int, job.Job]\n</code></pre> <p>Create the metadata for a training dataset and save the corresponding training data into <code>location</code>.</p> <p>The training data is split into train and test set at random or according to time ranges. The training data can be retrieved by calling <code>feature_view.get_train_test_split</code>.</p> Create random splits <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# create a train-test split dataset\nversion, job = feature_view.create_train_test_split(\n    test_size=0.2,\n    description='Description of a dataset',\n    # you can have different data formats such as csv, tsv, tfrecord, parquet and others\n    data_format='csv'\n)\n</code></pre> Create time series splits by specifying date as string <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# set up dates\ntrain_start = \"2022-01-01 00:00:00\"\ntrain_end = \"2022-06-06 23:59:59\"\ntest_start = \"2022-06-07 00:00:00\"\ntest_end = \"2022-12-25 23:59:59\"\n\n# create a train-test split dataset\nversion, job = feature_view.create_train_test_split(\n    train_start=train_start,\n    train_end=train_end,\n    test_start=test_start,\n    test_end=test_end,\n    description='Description of a dataset',\n    # you can have different data formats such as csv, tsv, tfrecord, parquet and others\n    data_format='csv'\n)\n</code></pre> Create time series splits by specifying date as datetime object <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# set up dates\nfrom datetime import datetime\ndate_format = \"%Y-%m-%d %H:%M:%S\"\n\ntrain_start = datetime.strptime(\"2022-01-01 00:00:00\", date_format)\ntrain_end = datetime.strptime(\"2022-06-06 23:59:59\", date_format)\ntest_start = datetime.strptime(\"2022-06-07 00:00:00\", date_format)\ntest_end = datetime.strptime(\"2022-12-25 23:59:59\" , date_format)\n\n# create a train-test split dataset\nversion, job = feature_view.create_train_test_split(\n    train_start=train_start,\n    train_end=train_end,\n    test_start=test_start,\n    test_end=test_end,\n    description='Description of a dataset',\n    # you can have different data formats such as csv, tsv, tfrecord, parquet and others\n    data_format='csv'\n)\n</code></pre> Write training dataset to external storage <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get storage connector instance\ndata_source = fs.get_data_source(\"test_data_source\")\n\n# create a train-test split dataset\nversion, job = feature_view.create_train_test_split(\n    train_start=...,\n    train_end=...,\n    test_start=...,\n    test_end=...,\n    data_source=data_source,\n    description=...,\n    # you can have different data formats such as csv, tsv, tfrecord, parquet and others\n    data_format=...\n)\n</code></pre> Data Formats <p>The feature store currently supports the following data formats for training datasets:</p> <ol> <li>tfrecord</li> <li>csv</li> <li>tsv</li> <li>parquet</li> <li>avro</li> <li>orc</li> <li>json</li> </ol> <p>Currently not supported petastorm, hdf5 and npy file formats.</p> Warning, the following code will fail because category column contains sparse values and training dataset may not have all values available in test split. <pre><code>import pandas as pd\n\ndf = pd.DataFrame({\n    'category_col':['category_a','category_b','category_c','category_d'],\n    'numeric_col': [40,10,60,40]\n})\n\nfeature_group = fs.get_or_create_feature_group(\n    name='feature_group_name',\n    version=1,\n    primary_key=['category_col']\n)\n\nfeature_group.insert(df)\n\nlabel_encoder = fs.get_transformation_function(name='label_encoder')\n\nfeature_view = fs.create_feature_view(\n    name='feature_view_name',\n    query=feature_group.select_all(),\n    transformation_functions={'category_col':label_encoder}\n)\n\nfeature_view.create_train_test_split(\n    test_size=0.5\n)\n# Output: KeyError: 'category_c'\n</code></pre> Spine Groups/Dataframes <p>Spine groups and dataframes are currently only supported with the Spark engine and Spark dataframes.</p> PARAMETER DESCRIPTION <code>test_size</code> <p>size of test set.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>train_start</code> <p>Start event time for the train split query, inclusive. Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</p> <p> TYPE: <code>str | int | datetime | date | None</code> DEFAULT: <code>''</code> </p> <code>train_end</code> <p>End event time for the train split query, exclusive. Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</p> <p> TYPE: <code>str | int | datetime | date | None</code> DEFAULT: <code>''</code> </p> <code>test_start</code> <p>Start event time for the test split query, inclusive. Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</p> <p> TYPE: <code>str | int | datetime | date | None</code> DEFAULT: <code>''</code> </p> <code>test_end</code> <p>End event time for the test split query, exclusive. Strings should be  formatted in one of the following ormats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</p> <p> TYPE: <code>str | int | datetime | date | None</code> DEFAULT: <code>''</code> </p> <code>storage_connector</code> <p>Storage connector defining the sink location for the training dataset, defaults to <code>None</code>, and materializes training dataset on HopsFS. [DEPRECATED: Use <code>data_source</code> instead.]</p> <p> TYPE: <code>storage_connector.StorageConnector | None</code> DEFAULT: <code>None</code> </p> <code>location</code> <p>Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to <code>\"\"</code>, saving the training dataset at the root defined by the storage connector. [DEPRECATED: Use <code>data_source</code> instead.]</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>''</code> </p> <code>description</code> <p>A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string <code>\"\"</code>.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>''</code> </p> <code>extra_filter</code> <p>Additional filters to be attached to the training dataset. The filters will be also applied in <code>get_batch_data</code>.</p> <p> TYPE: <code>filter.Filter | filter.Logic | None</code> DEFAULT: <code>None</code> </p> <code>data_format</code> <p>The data format used to save the training dataset, defaults to <code>\"parquet\"</code>-format.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>'parquet'</code> </p> <code>coalesce</code> <p>If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split. Default False.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>False</code> </p> <code>seed</code> <p>Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to <code>None</code>.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>statistics_config</code> <p>A configuration object, or a dictionary with keys \"<code>enabled</code>\" to generally enable descriptive statistics computation for this feature group, <code>\"correlations</code>\" to turn on feature correlation computation and <code>\"histograms\"</code> to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass <code>statistics_config=False</code>. Defaults to <code>None</code> and will compute only descriptive statistics.</p> <p> TYPE: <code>StatisticsConfig | bool | dict | None</code> DEFAULT: <code>None</code> </p> <code>write_options</code> <p>Additional options as key/value pairs to pass to the execution engine. For spark engine: Dictionary of read options for Spark. When using the <code>python</code> engine, write_options can contain the following entries: * key <code>use_spark</code> and value <code>True</code> to materialize training dataset   with Spark instead of Hopsworks Feature Query Service. * key <code>spark</code> and value an object of type hsfs.core.job_configuration.JobConfiguration   to configure the Hopsworks Job used to compute the training dataset. * key <code>wait_for_job</code> and value <code>True</code> or <code>False</code> to configure   whether or not to the save call should return only   after the Hopsworks Job has finished. By default it waits. Defaults to <code>{}</code>.</p> <p> TYPE: <code>dict[Any, Any] | None</code> DEFAULT: <code>None</code> </p> <code>spine</code> <p>Spine dataframe with primary key, event time and label column to use for point in time join when fetching features. Defaults to <code>None</code> and is only required when feature view was created with spine group in the feature query. It is possible to directly pass a spine group instead of a dataframe to overwrite the left side of the feature join, however, the same features as in the original feature group that is being replaced need to be available in the spine group.</p> <p> TYPE: <code>SplineDataFrameTypes | None</code> DEFAULT: <code>None</code> </p> <code>transformation_context</code> <p><code>Dict[str, Any]</code> A dictionary mapping variable names to objects that will be provided as contextual information to the transformation function at runtime. The <code>context</code> variable must be explicitly defined as parameters in the transformation function for these to be accessible during execution. If no context variables are provided, this parameter defaults to <code>None</code>.</p> <p> TYPE: <code>dict[str, Any]</code> DEFAULT: <code>None</code> </p> <code>data_source</code> <p>The data source specifying the location of the data. Overrides the storage_connector and location arguments when specified.</p> <p> TYPE: <code>ds.DataSource | dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>td_version, `Job`</code> <p>Tuple of training dataset version and job. When using the <code>python</code> engine, it returns the Hopsworks Job that was launched to create the training dataset.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.create_train_validation_test_split","title":"[source]  create_train_validation_test_split","text":"<pre><code>create_train_validation_test_split(\n    validation_size: float | None = None,\n    test_size: float | None = None,\n    train_start: str | int | datetime | date | None = \"\",\n    train_end: str | int | datetime | date | None = \"\",\n    validation_start: str\n    | int\n    | datetime\n    | date\n    | None = \"\",\n    validation_end: str | int | datetime | date | None = \"\",\n    test_start: str | int | datetime | date | None = \"\",\n    test_end: str | int | datetime | date | None = \"\",\n    storage_connector: storage_connector.StorageConnector\n    | None = None,\n    location: str | None = \"\",\n    description: str | None = \"\",\n    extra_filter: filter.Filter\n    | filter.Logic\n    | None = None,\n    data_format: str | None = \"parquet\",\n    coalesce: bool | None = False,\n    seed: int | None = None,\n    statistics_config: StatisticsConfig\n    | bool\n    | dict\n    | None = None,\n    write_options: dict[Any, Any] | None = None,\n    spine: SplineDataFrameTypes | None = None,\n    transformation_context: dict[str, Any] = None,\n    data_source: ds.DataSource\n    | dict[str, Any]\n    | None = None,\n    tags: tag.Tag\n    | dict[str, Any]\n    | list[tag.Tag | dict[str, Any]]\n    | None = None,\n    **kwargs,\n) -&gt; tuple[int, job.Job]\n</code></pre> <p>Create the metadata for a training dataset and save the corresponding training data into <code>location</code>.</p> <p>The training data is split into train, validation, and test set at random or according to time range. The training data can be retrieved by calling <code>feature_view.get_train_validation_test_split</code>.</p> Create random splits <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# create a train-validation-test split dataset\nversion, job = feature_view.create_train_validation_test_split(\n    validation_size=0.3,\n    test_size=0.2,\n    description='Description of a dataset',\n    data_format='csv'\n)\n</code></pre> Create time series splits by specifying date as string <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# set up dates\ntrain_start = \"2022-01-01 00:00:00\"\ntrain_end = \"2022-06-01 23:59:59\"\nvalidation_start = \"2022-06-02 00:00:00\"\nvalidation_end = \"2022-07-01 23:59:59\"\ntest_start = \"2022-07-02 00:00:00\"\ntest_end = \"2022-08-01 23:59:59\"\n\n# create a train-validation-test split dataset\nversion, job = feature_view.create_train_validation_test_split(\n    train_start=train_start,\n    train_end=train_end,\n    validation_start=validation_start,\n    validation_end=validation_end,\n    test_start=test_start,\n    test_end=test_end,\n    description='Description of a dataset',\n    # you can have different data formats such as csv, tsv, tfrecord, parquet and others\n    data_format='csv'\n)\n</code></pre> Create time series splits by specifying date as datetime object <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# set up dates\nfrom datetime import datetime\ndate_format = \"%Y-%m-%d %H:%M:%S\"\n\ntrain_start = datetime.strptime(\"2022-01-01 00:00:00\", date_format)\ntrain_end = datetime.strptime(\"2022-06-06 23:59:59\", date_format)\nvalidation_start = datetime.strptime(\"2022-06-02 00:00:00\", date_format)\nvalidation_end = datetime.strptime(\"2022-07-01 23:59:59\", date_format)\ntest_start = datetime.strptime(\"2022-06-07 00:00:00\", date_format)\ntest_end = datetime.strptime(\"2022-12-25 23:59:59\", date_format)\n\n# create a train-validation-test split dataset\nversion, job = feature_view.create_train_validation_test_split(\n    train_start=train_start,\n    train_end=train_end,\n    validation_start=validation_start,\n    validation_end=validation_end,\n    test_start=test_start,\n    test_end=test_end,\n    description='Description of a dataset',\n    # you can have different data formats such as csv, tsv, tfrecord, parquet and others\n    data_format='csv'\n)\n</code></pre> Write training dataset to external storage <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get storage connector instance\ndata_source = fs.get_data_source(\"test_data_source\")\n\n# create a train-validation-test split dataset\nversion, job = feature_view.create_train_validation_test_split(\n    train_start=...,\n    train_end=...,\n    validation_start=...,\n    validation_end=...,\n    test_start=...,\n    test_end=...,\n    description=...,\n    data_source=data_source,\n    # you can have different data formats such as csv, tsv, tfrecord, parquet and others\n    data_format=...\n)\n</code></pre> Data Formats <p>The feature store currently supports the following data formats for training datasets:</p> <ol> <li>tfrecord</li> <li>csv</li> <li>tsv</li> <li>parquet</li> <li>avro</li> <li>orc</li> <li>json</li> </ol> <p>Currently not supported petastorm, hdf5 and npy file formats.</p> Spine Groups/Dataframes <p>Spine groups and dataframes are currently only supported with the Spark engine and Spark dataframes.</p> PARAMETER DESCRIPTION <code>validation_size</code> <p>size of validation set.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>test_size</code> <p>size of test set.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>train_start</code> <p>Start event time for the train split query, inclusive. Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</p> <p> TYPE: <code>str | int | datetime | date | None</code> DEFAULT: <code>''</code> </p> <code>train_end</code> <p>End event time for the train split query, exclusive. Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</p> <p> TYPE: <code>str | int | datetime | date | None</code> DEFAULT: <code>''</code> </p> <code>validation_start</code> <p>Start event time for the validation split query, inclusive. Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</p> <p> TYPE: <code>str | int | datetime | date | None</code> DEFAULT: <code>''</code> </p> <code>validation_end</code> <p>End event time for the validation split query, exclusive. Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</p> <p> TYPE: <code>str | int | datetime | date | None</code> DEFAULT: <code>''</code> </p> <code>test_start</code> <p>Start event time for the test split query, inclusive. Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</p> <p> TYPE: <code>str | int | datetime | date | None</code> DEFAULT: <code>''</code> </p> <code>test_end</code> <p>End event time for the test split query, exclusive. Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</p> <p> TYPE: <code>str | int | datetime | date | None</code> DEFAULT: <code>''</code> </p> <code>storage_connector</code> <p>Storage connector defining the sink location for the training dataset, defaults to <code>None</code>, and materializes training dataset on HopsFS. [DEPRECATED: Use <code>data_source</code> instead.]</p> <p> TYPE: <code>storage_connector.StorageConnector | None</code> DEFAULT: <code>None</code> </p> <code>location</code> <p>Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to <code>\"\"</code>, saving the training dataset at the root defined by the storage connector. [DEPRECATED: Use <code>data_source</code> instead.]</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>''</code> </p> <code>description</code> <p>A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string <code>\"\"</code>.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>''</code> </p> <code>extra_filter</code> <p>Additional filters to be attached to the training dataset. The filters will be also applied in <code>get_batch_data</code>.</p> <p> TYPE: <code>filter.Filter | filter.Logic | None</code> DEFAULT: <code>None</code> </p> <code>data_format</code> <p>The data format used to save the training dataset, defaults to <code>\"parquet\"</code>-format.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>'parquet'</code> </p> <code>coalesce</code> <p>If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split. Default False.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>False</code> </p> <code>seed</code> <p>Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to <code>None</code>.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>statistics_config</code> <p>A configuration object, or a dictionary with keys \"<code>enabled</code>\" to generally enable descriptive statistics computation for this feature group, <code>\"correlations</code>\" to turn on feature correlation computation and <code>\"histograms\"</code> to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass <code>statistics_config=False</code>. Defaults to <code>None</code> and will compute only descriptive statistics.</p> <p> TYPE: <code>StatisticsConfig | bool | dict | None</code> DEFAULT: <code>None</code> </p> <code>write_options</code> <p>Additional options as key/value pairs to pass to the execution engine. For spark engine: Dictionary of read options for Spark. When using the <code>python</code> engine, write_options can contain the following entries: * key <code>use_spark</code> and value <code>True</code> to materialize training dataset   with Spark instead of Hopsworks Feature Query Service. * key <code>spark</code> and value an object of type hsfs.core.job_configuration.JobConfiguration   to configure the Hopsworks Job used to compute the training dataset. * key <code>wait_for_job</code> and value <code>True</code> or <code>False</code> to configure   whether or not to the save call should return only   after the Hopsworks Job has finished. By default it waits. Defaults to <code>{}</code>.</p> <p> TYPE: <code>dict[Any, Any] | None</code> DEFAULT: <code>None</code> </p> <code>spine</code> <p>Spine dataframe with primary key, event time and label column to use for point in time join when fetching features. Defaults to <code>None</code> and is only required when feature view was created with spine group in the feature query. It is possible to directly pass a spine group instead of a dataframe to overwrite the left side of the feature join, however, the same features as in the original feature group that is being replaced need to be available in the spine group.</p> <p> TYPE: <code>SplineDataFrameTypes | None</code> DEFAULT: <code>None</code> </p> <code>transformation_context</code> <p><code>Dict[str, Any]</code> A dictionary mapping variable names to objects that will be provided as contextual information to the transformation function at runtime. The <code>context</code> variable must be explicitly defined as parameters in the transformation function for these to be accessible during execution. If no context variables are provided, this parameter defaults to <code>None</code>.</p> <p> TYPE: <code>dict[str, Any]</code> DEFAULT: <code>None</code> </p> <code>data_source</code> <p>The data source specifying the location of the data. Overrides the storage_connector and location arguments when specified.</p> <p> TYPE: <code>ds.DataSource | dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>td_version, `Job`</code> <p>Tuple of training dataset version and job. When using the <code>python</code> engine, it returns the Hopsworks Job that was launched to create the training dataset.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.recreate_training_dataset","title":"[source]  recreate_training_dataset","text":"<pre><code>recreate_training_dataset(\n    training_dataset_version: int,\n    statistics_config: StatisticsConfig\n    | bool\n    | dict\n    | None = None,\n    write_options: dict[Any, Any] | None = None,\n    spine: SplineDataFrameTypes | None = None,\n    transformation_context: dict[str, Any] = None,\n) -&gt; job.Job\n</code></pre> <p>Recreate a training dataset.</p> Example <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# recreate a training dataset that has been deleted\nfeature_view.recreate_training_dataset(training_dataset_version=1)\n</code></pre> Info <p>If a materialised training data has deleted. Use <code>recreate_training_dataset()</code> to recreate the training data.</p> Spine Groups/Dataframes <p>Spine groups and dataframes are currently only supported with the Spark engine and Spark dataframes.</p> PARAMETER DESCRIPTION <code>training_dataset_version</code> <p>training dataset version</p> <p> TYPE: <code>int</code> </p> <code>statistics_config</code> <p>A configuration object, or a dictionary with keys \"<code>enabled</code>\" to generally enable descriptive statistics computation for this feature group, <code>\"correlations</code>\" to turn on feature correlation computation and <code>\"histograms\"</code> to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass <code>statistics_config=False</code>. Defaults to <code>None</code> and will compute only descriptive statistics.</p> <p> TYPE: <code>StatisticsConfig | bool | dict | None</code> DEFAULT: <code>None</code> </p> <code>write_options</code> <p>Additional options as key/value pairs to pass to the execution engine. For spark engine: Dictionary of read options for Spark. When using the <code>python</code> engine, write_options can contain the following entries: * key <code>use_spark</code> and value <code>True</code> to materialize training dataset   with Spark instead of Hopsworks Feature Query Service. * key <code>spark</code> and value an object of type hsfs.core.job_configuration.JobConfiguration   to configure the Hopsworks Job used to compute the training dataset. * key <code>wait_for_job</code> and value <code>True</code> or <code>False</code> to configure   whether or not to the save call should return only   after the Hopsworks Job has finished. By default it waits. Defaults to <code>{}</code>.</p> <p> TYPE: <code>dict[Any, Any] | None</code> DEFAULT: <code>None</code> </p> <code>spine</code> <p>Spine dataframe with primary key, event time and label column to use for point in time join when fetching features. Defaults to <code>None</code> and is only required when feature view was created with spine group in the feature query. It is possible to directly pass a spine group instead of a dataframe to overwrite the left side of the feature join, however, the same features as in the original feature group that is being replaced need to be available in the spine group.</p> <p> TYPE: <code>SplineDataFrameTypes | None</code> DEFAULT: <code>None</code> </p> <code>transformation_context</code> <p><code>Dict[str, Any]</code> A dictionary mapping variable names to objects that will be provided as contextual information to the transformation function at runtime. The <code>context</code> variable must be explicitly defined as parameters in the transformation function for these to be accessible during execution. If no context variables are provided, this parameter defaults to <code>None</code>.</p> <p> TYPE: <code>dict[str, Any]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>job.Job</code> <p><code>Job</code>: When using the <code>python</code> engine, it returns the Hopsworks Job that was launched to create the training dataset.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.training_data","title":"[source]  training_data","text":"<pre><code>training_data(\n    start_time: str | int | datetime | date | None = None,\n    end_time: str | int | datetime | date | None = None,\n    description: str | None = \"\",\n    extra_filter: filter.Filter\n    | filter.Logic\n    | None = None,\n    statistics_config: StatisticsConfig\n    | bool\n    | dict\n    | None = None,\n    read_options: dict[Any, Any] | None = None,\n    spine: SplineDataFrameTypes | None = None,\n    primary_key: bool = False,\n    event_time: bool = False,\n    training_helper_columns: bool = False,\n    dataframe_type: str | None = \"default\",\n    transformation_context: dict[str, Any] = None,\n    **kwargs,\n) -&gt; tuple[\n    TrainingDatasetDataFrameTypes,\n    TrainingDatasetDataFrameTypes | None,\n]\n</code></pre> <p>Create the metadata for a training dataset and get the corresponding training data from the offline feature store.</p> <p>This returns the training data in memory and does not materialise data in storage. The training data can be recreated by calling <code>feature_view.get_training_data</code> with the metadata created.</p> Create random splits <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get training data\nfeatures_df, labels_df  = feature_view.training_data(\n    description='Descriprion of a dataset',\n)\n</code></pre> Create time-series based splits <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# set up a date\nstart_time = \"2022-05-01 00:00:00\"\nend_time = \"2022-06-04 23:59:59\"\n# you can also pass dates as datetime objects\n\n# get training data\nfeatures_df, labels_df = feature_view.training_data(\n    start_time=start_time,\n    end_time=end_time,\n    description='Description of a dataset'\n)\n</code></pre> Spine Groups/Dataframes <p>Spine groups and dataframes are currently only supported with the Spark engine and Spark dataframes.</p> PARAMETER DESCRIPTION <code>start_time</code> <p>Start event time for the training dataset query, inclusive. Strings should</p> <p> TYPE: <code>str | int | datetime | date | None</code> DEFAULT: <code>None</code> </p> <code>end_time</code> <p>End event time for the training dataset query, exclusive. Strings should be</p> <p> TYPE: <code>str | int | datetime | date | None</code> DEFAULT: <code>None</code> </p> <code>description</code> <p>A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string <code>\"\"</code>.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>''</code> </p> <code>extra_filter</code> <p>Additional filters to be attached to the training dataset. The filters will be also applied in <code>get_batch_data</code>.</p> <p> TYPE: <code>filter.Filter | filter.Logic | None</code> DEFAULT: <code>None</code> </p> <code>statistics_config</code> <p>A configuration object, or a dictionary with keys \"<code>enabled</code>\" to generally enable descriptive statistics computation for this feature group, <code>\"correlations</code>\" to turn on feature correlation computation and <code>\"histograms\"</code> to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass <code>statistics_config=False</code>. Defaults to <code>None</code> and will compute only descriptive statistics.</p> <p> TYPE: <code>StatisticsConfig | bool | dict | None</code> DEFAULT: <code>None</code> </p> <code>read_options</code> <p>Additional options as key/value pairs to pass to the execution engine. For spark engine: Dictionary of read options for Spark. When using the <code>python</code> engine, read_options can contain the following entries: * key <code>\"arrow_flight_config\"</code> to pass a dictionary of arrow flight configurations.   For example: <code>{\"arrow_flight_config\": {\"timeout\": 900}}</code>. * key <code>spark</code> and value an object of type   hsfs.core.job_configuration.JobConfiguration   to configure the Hopsworks Job used to compute the training dataset. Defaults to <code>{}</code>.</p> <p> TYPE: <code>dict[Any, Any] | None</code> DEFAULT: <code>None</code> </p> <code>spine</code> <p>Spine dataframe with primary key, event time and label column to use for point in time join when fetching features. Defaults to <code>None</code> and is only required when feature view was created with spine group in the feature query. It is possible to directly pass a spine group instead of a dataframe to overwrite the left side of the feature join, however, the same features as in the original feature group that is being replaced need to be available in the spine group.</p> <p> TYPE: <code>SplineDataFrameTypes | None</code> DEFAULT: <code>None</code> </p> <code>primary_key</code> <p>whether to include primary key features or not.  Defaults to <code>False</code>, no primary key features.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>event_time</code> <p>whether to include event time feature or not.  Defaults to <code>False</code>, no event time feature.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>training_helper_columns</code> <p>whether to include training helper columns or not. Training helper columns are a list of feature names in the feature view, defined during its creation, that are not the part of the model schema itself but can be used during training as a helper for extra information. If training helper columns were not defined in the feature view then<code>training_helper_columns=True</code> will not have any effect. Defaults to <code>False</code>, no training helper columns.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>dataframe_type</code> <p>str, optional. The type of the returned dataframe. Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>. Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>'default'</code> </p> <code>transformation_context</code> <p><code>Dict[str, Any]</code> A dictionary mapping variable names to objects that will be provided as contextual information to the transformation function at runtime. The <code>context</code> variable must be explicitly defined as parameters in the transformation function for these to be accessible during execution. If no context variables are provided, this parameter defaults to <code>None</code>.</p> <p> TYPE: <code>dict[str, Any]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>(X, y)</code> <p>Tuple of dataframe of features and labels. If there are no labels, y returns <code>None</code>.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.train_test_split","title":"[source]  train_test_split","text":"<pre><code>train_test_split(\n    test_size: float | None = None,\n    train_start: str | int | datetime | date | None = \"\",\n    train_end: str | int | datetime | date | None = \"\",\n    test_start: str | int | datetime | date | None = \"\",\n    test_end: str | int | datetime | date | None = \"\",\n    description: str | None = \"\",\n    extra_filter: filter.Filter\n    | filter.Logic\n    | None = None,\n    statistics_config: StatisticsConfig\n    | bool\n    | dict\n    | None = None,\n    read_options: dict[Any, Any] | None = None,\n    spine: SplineDataFrameTypes | None = None,\n    primary_key: bool = False,\n    event_time: bool = False,\n    training_helper_columns: bool = False,\n    dataframe_type: str | None = \"default\",\n    transformation_context: dict[str, Any] = None,\n    **kwargs,\n) -&gt; tuple[\n    TrainingDatasetDataFrameTypes,\n    TrainingDatasetDataFrameTypes,\n    TrainingDatasetDataFrameTypes | None,\n    TrainingDatasetDataFrameTypes | None,\n]\n</code></pre> <p>Create the metadata for a training dataset and get the corresponding training data from the offline feature store.</p> <p>This returns the training data in memory and does not materialise data in storage. The training data is split into train and test set at random or according to time ranges. The training data can be recreated by calling <code>feature_view.get_train_test_split</code> with the metadata created.</p> Create random train/test splits <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get training data\nX_train, X_test, y_train, y_test = feature_view.train_test_split(\n    test_size=0.2\n)\n</code></pre> Create time-series train/test splits <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# set up dates\ntrain_start = \"2022-05-01 00:00:00\"\ntrain_end = \"2022-06-04 23:59:59\"\ntest_start = \"2022-07-01 00:00:00\"\ntest_end= \"2022-08-04 23:59:59\"\n# you can also pass dates as datetime objects\n\n# get training data\nX_train, X_test, y_train, y_test = feature_view.train_test_split(\n    train_start=train_start,\n    train_end=train_end,\n    test_start=test_start,\n    test_end=test_end,\n    description='Description of a dataset'\n)\n</code></pre> Spine Groups/Dataframes <p>Spine groups and dataframes are currently only supported with the Spark engine and Spark dataframes.</p> PARAMETER DESCRIPTION <code>test_size</code> <p>size of test set. Should be between 0 and 1.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>train_start</code> <p>Start event time for the train split query, inclusive. Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code>.</p> <p> TYPE: <code>str | int | datetime | date | None</code> DEFAULT: <code>''</code> </p> <code>train_end</code> <p>End event time for the train split query, exclusive. Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</p> <p> TYPE: <code>str | int | datetime | date | None</code> DEFAULT: <code>''</code> </p> <code>test_start</code> <p>Start event time for the test split query, inclusive. Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</p> <p> TYPE: <code>str | int | datetime | date | None</code> DEFAULT: <code>''</code> </p> <code>test_end</code> <p>End event time for the test split query, exclusive. Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</p> <p> TYPE: <code>str | int | datetime | date | None</code> DEFAULT: <code>''</code> </p> <code>description</code> <p>A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string <code>\"\"</code>.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>''</code> </p> <code>extra_filter</code> <p>Additional filters to be attached to the training dataset. The filters will be also applied in <code>get_batch_data</code>.</p> <p> TYPE: <code>filter.Filter | filter.Logic | None</code> DEFAULT: <code>None</code> </p> <code>statistics_config</code> <p>A configuration object, or a dictionary with keys \"<code>enabled</code>\" to generally enable descriptive statistics computation for this feature group, <code>\"correlations</code>\" to turn on feature correlation computation and <code>\"histograms\"</code> to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass <code>statistics_config=False</code>. Defaults to <code>None</code> and will compute only descriptive statistics.</p> <p> TYPE: <code>StatisticsConfig | bool | dict | None</code> DEFAULT: <code>None</code> </p> <code>read_options</code> <p>Additional options as key/value pairs to pass to the execution engine. For spark engine: Dictionary of read options for Spark. When using the <code>python</code> engine, read_options can contain the following entries: * key <code>\"arrow_flight_config\"</code> to pass a dictionary of arrow flight configurations.   For example: <code>{\"arrow_flight_config\": {\"timeout\": 900}}</code> * key <code>spark</code> and value an object of type   hsfs.core.job_configuration.JobConfiguration   to configure the Hopsworks Job used to compute the training dataset. Defaults to <code>{}</code>.</p> <p> TYPE: <code>dict[Any, Any] | None</code> DEFAULT: <code>None</code> </p> <code>spine</code> <p>Spine dataframe with primary key, event time and label column to use for point in time join when fetching features. Defaults to <code>None</code> and is only required when feature view was created with spine group in the feature query. It is possible to directly pass a spine group instead of a dataframe to overwrite the left side of the feature join, however, the same features as in the original feature group that is being replaced need to be available in the spine group.</p> <p> TYPE: <code>SplineDataFrameTypes | None</code> DEFAULT: <code>None</code> </p> <code>primary_key</code> <p>whether to include primary key features or not.  Defaults to <code>False</code>, no primary key features.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>event_time</code> <p>whether to include event time feature or not.  Defaults to <code>False</code>, no event time feature.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>training_helper_columns</code> <p>whether to include training helper columns or not. Training helper columns are a list of feature names in the feature view, defined during its creation, that are not the part of the model schema itself but can be used during training as a helper for extra information. If training helper columns were not defined in the feature view then<code>training_helper_columns=True</code> will not have any effect. Defaults to <code>False</code>, no training helper columns.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>dataframe_type</code> <p>str, optional. The type of the returned dataframe. Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>. Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>'default'</code> </p> <code>transformation_context</code> <p><code>Dict[str, Any]</code> A dictionary mapping variable names to objects that will be provided as contextual information to the transformation function at runtime. The <code>context</code> variable must be explicitly defined as parameters in the transformation function for these to be accessible during execution. If no context variables are provided, this parameter defaults to <code>None</code>.</p> <p> TYPE: <code>dict[str, Any]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>(X_train, X_test, y_train, y_test)</code> <p>Tuple of dataframe of features and labels</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.train_validation_test_split","title":"[source]  train_validation_test_split","text":"<pre><code>train_validation_test_split(\n    validation_size: float | None = None,\n    test_size: float | None = None,\n    train_start: str | int | datetime | date | None = \"\",\n    train_end: str | int | datetime | date | None = \"\",\n    validation_start: str\n    | int\n    | datetime\n    | date\n    | None = \"\",\n    validation_end: str | int | datetime | date | None = \"\",\n    test_start: str | int | datetime | date | None = \"\",\n    test_end: str | int | datetime | date | None = \"\",\n    description: str | None = \"\",\n    extra_filter: filter.Filter\n    | filter.Logic\n    | None = None,\n    statistics_config: StatisticsConfig\n    | bool\n    | dict\n    | None = None,\n    read_options: dict[Any, Any] | None = None,\n    spine: SplineDataFrameTypes | None = None,\n    primary_key: bool = False,\n    event_time: bool = False,\n    training_helper_columns: bool = False,\n    dataframe_type: str | None = \"default\",\n    transformation_context: dict[str, Any] = None,\n    **kwargs,\n) -&gt; tuple[\n    TrainingDatasetDataFrameTypes,\n    TrainingDatasetDataFrameTypes,\n    TrainingDatasetDataFrameTypes,\n    TrainingDatasetDataFrameTypes | None,\n    TrainingDatasetDataFrameTypes | None,\n    TrainingDatasetDataFrameTypes | None,\n]\n</code></pre> <p>Create the metadata for a training dataset and get the corresponding training data from the offline feature store.</p> <p>This returns the training data in memory and does not materialise data in storage. The training data is split into train, validation, and test set at random or according to time ranges. The training data can be recreated by calling <code>feature_view.get_train_validation_test_split</code> with the metadata created.</p> Example <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get training data\nX_train, X_val, X_test, y_train, y_val, y_test = feature_view.train_validation_test_split(\n    validation_size=0.3,\n    test_size=0.2\n)\n</code></pre> Time Series split <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# set up dates\nstart_time_train = '2017-01-01 00:00:01'\nend_time_train = '2018-02-01 23:59:59'\n\nstart_time_val = '2018-02-02 23:59:59'\nend_time_val = '2019-02-01 23:59:59'\n\nstart_time_test = '2019-02-02 23:59:59'\nend_time_test = '2020-02-01 23:59:59'\n# you can also pass dates as datetime objects\n\n# get training data\nX_train, X_val, X_test, y_train, y_val, y_test = feature_view.train_validation_test_split(\n    train_start=start_time_train,\n    train_end=end_time_train,\n    validation_start=start_time_val,\n    validation_end=end_time_val,\n    test_start=start_time_test,\n    test_end=end_time_test\n)\n</code></pre> Spine Groups/Dataframes <p>Spine groups and dataframes are currently only supported with the Spark engine and Spark dataframes.</p> PARAMETER DESCRIPTION <code>validation_size</code> <p>size of validation set. Should be between 0 and 1.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>test_size</code> <p>size of test set. Should be between 0 and 1.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>train_start</code> <p>Start event time for the train split query, inclusive. Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</p> <p> TYPE: <code>str | int | datetime | date | None</code> DEFAULT: <code>''</code> </p> <code>train_end</code> <p>End event time for the train split query, exclusive. Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</p> <p> TYPE: <code>str | int | datetime | date | None</code> DEFAULT: <code>''</code> </p> <code>validation_start</code> <p>Start event time for the validation split query, inclusive. Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</p> <p> TYPE: <code>str | int | datetime | date | None</code> DEFAULT: <code>''</code> </p> <code>validation_end</code> <p>End event time for the validation split query, exclusive. Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</p> <p> TYPE: <code>str | int | datetime | date | None</code> DEFAULT: <code>''</code> </p> <code>test_start</code> <p>Start event time for the test split query, inclusive. Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</p> <p> TYPE: <code>str | int | datetime | date | None</code> DEFAULT: <code>''</code> </p> <code>test_end</code> <p>End event time for the test split query, exclusive. Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</p> <p> TYPE: <code>str | int | datetime | date | None</code> DEFAULT: <code>''</code> </p> <code>description</code> <p>A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string <code>\"\"</code>.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>''</code> </p> <code>extra_filter</code> <p>Additional filters to be attached to the training dataset. The filters will be also applied in <code>get_batch_data</code>.</p> <p> TYPE: <code>filter.Filter | filter.Logic | None</code> DEFAULT: <code>None</code> </p> <code>statistics_config</code> <p>A configuration object, or a dictionary with keys \"<code>enabled</code>\" to generally enable descriptive statistics computation for this feature group, <code>\"correlations</code>\" to turn on feature correlation computation and <code>\"histograms\"</code> to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass <code>statistics_config=False</code>. Defaults to <code>None</code> and will compute only descriptive statistics.</p> <p> TYPE: <code>StatisticsConfig | bool | dict | None</code> DEFAULT: <code>None</code> </p> <code>read_options</code> <p>Additional options as key/value pairs to pass to the execution engine. For spark engine: Dictionary of read options for Spark. When using the <code>python</code> engine, read_options can contain the following entries: * key <code>\"arrow_flight_config\"</code> to pass a dictionary of arrow flight configurations.   For example: <code>{\"arrow_flight_config\": {\"timeout\": 900}}</code> * key <code>spark</code> and value an object of type   hsfs.core.job_configuration.JobConfiguration   to configure the Hopsworks Job used to compute the training dataset. Defaults to <code>{}</code>.</p> <p> TYPE: <code>dict[Any, Any] | None</code> DEFAULT: <code>None</code> </p> <code>spine</code> <p>Spine dataframe with primary key, event time and label column to use for point in time join when fetching features. Defaults to <code>None</code> and is only required when feature view was created with spine group in the feature query. It is possible to directly pass a spine group instead of a dataframe to overwrite the left side of the feature join, however, the same features as in the original feature group that is being replaced need to be available in the spine group.</p> <p> TYPE: <code>SplineDataFrameTypes | None</code> DEFAULT: <code>None</code> </p> <code>primary_key</code> <p>whether to include primary key features or not.  Defaults to <code>False</code>, no primary key features.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>event_time</code> <p>whether to include event time feature or not.  Defaults to <code>False</code>, no event time feature.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>training_helper_columns</code> <p>whether to include training helper columns or not. Training helper columns are a list of feature names in the feature view, defined during its creation, that are not the part of the model schema itself but can be used during training as a helper for extra information. If training helper columns were not defined in the feature view then<code>training_helper_columns=True</code> will not have any effect. Defaults to <code>False</code>, no training helper columns.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>dataframe_type</code> <p>str, optional. The type of the returned dataframe. Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>. Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>'default'</code> </p> <code>transformation_context</code> <p><code>Dict[str, Any]</code> A dictionary mapping variable names to objects that will be provided as contextual information to the transformation function at runtime. The <code>context</code> variable must be explicitly defined as parameters in the transformation function for these to be accessible during execution. If no context variables are provided, this parameter defaults to <code>None</code>.</p> <p> TYPE: <code>dict[str, Any]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>(X_train, X_val, X_test, y_train, y_val, y_test)</code> <p>Tuple of dataframe of features and labels</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.get_training_data","title":"[source]  get_training_data","text":"<pre><code>get_training_data(\n    training_dataset_version: int,\n    read_options: dict[str, Any] | None = None,\n    primary_key: bool = False,\n    event_time: bool = False,\n    training_helper_columns: bool = False,\n    dataframe_type: str | None = \"default\",\n    transformation_context: dict[str, Any] = None,\n    **kwargs,\n) -&gt; tuple[\n    TrainingDatasetDataFrameTypes,\n    TrainingDatasetDataFrameTypes | None,\n]\n</code></pre> <p>Get training data created by <code>feature_view.create_training_data</code> or <code>feature_view.training_data</code>.</p> Example <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get training data\nfeatures_df, labels_df = feature_view.get_training_data(training_dataset_version=1)\n</code></pre> External Storage Support <p>Reading training data that was written to external storage using a Storage Connector other than S3 can currently not be read using HSFS APIs with Python as Engine, instead you will have to use the storage's native client.</p> PARAMETER DESCRIPTION <code>training_dataset_version</code> <p>training dataset version</p> <p> TYPE: <code>int</code> </p> <code>read_options</code> <p>Additional options as key/value pairs to pass to the execution engine. For spark engine: Dictionary of read options for Spark. For python engine: * key <code>\"arrow_flight_config\"</code> to pass a dictionary of arrow flight configurations.   For example: <code>{\"arrow_flight_config\": {\"timeout\": 900}}</code> Defaults to <code>{}</code>.</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>primary_key</code> <p>whether to include primary key features or not.  Defaults to <code>False</code>, no primary key features.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>event_time</code> <p>whether to include event time feature or not.  Defaults to <code>False</code>, no event time feature.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>training_helper_columns</code> <p>whether to include training helper columns or not. Training helper columns are a list of feature names in the feature view, defined during its creation, that are not the part of the model schema itself but can be used during training as a helper for extra information. If training helper columns were not defined in the feature view or during materializing training dataset in the file system then<code>training_helper_columns=True</code> will not have any effect. Defaults to <code>False</code>, no training helper columns.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>dataframe_type</code> <p>str, optional. The type of the returned dataframe. Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>. Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>'default'</code> </p> RETURNS DESCRIPTION <code>(X, y)</code> <p>Tuple of dataframe of features and labels</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.get_train_test_split","title":"[source]  get_train_test_split","text":"<pre><code>get_train_test_split(\n    training_dataset_version: int,\n    read_options: dict[Any, Any] | None = None,\n    primary_key: bool = False,\n    event_time: bool = False,\n    training_helper_columns: bool = False,\n    dataframe_type: str | None = \"default\",\n    transformation_context: dict[str, Any] = None,\n    **kwargs,\n) -&gt; tuple[\n    TrainingDatasetDataFrameTypes,\n    TrainingDatasetDataFrameTypes,\n    TrainingDatasetDataFrameTypes | None,\n    TrainingDatasetDataFrameTypes | None,\n]\n</code></pre> <p>Get training data created by <code>feature_view.create_train_test_split</code> or <code>feature_view.train_test_split</code>.</p> Example <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get training data\nX_train, X_test, y_train, y_test = feature_view.get_train_test_split(training_dataset_version=1)\n</code></pre> PARAMETER DESCRIPTION <code>training_dataset_version</code> <p>training dataset version</p> <p> TYPE: <code>int</code> </p> <code>read_options</code> <p>Additional options as key/value pairs to pass to the execution engine. For spark engine: Dictionary of read options for Spark. For python engine: * key <code>\"arrow_flight_config\"</code> to pass a dictionary of arrow flight configurations.   For example: <code>{\"arrow_flight_config\": {\"timeout\": 900}}</code> Defaults to <code>{}</code>.</p> <p> TYPE: <code>dict[Any, Any] | None</code> DEFAULT: <code>None</code> </p> <code>primary_key</code> <p>whether to include primary key features or not.  Defaults to <code>False</code>, no primary key features.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>event_time</code> <p>whether to include event time feature or not.  Defaults to <code>False</code>, no event time feature.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>training_helper_columns</code> <p>whether to include training helper columns or not. Training helper columns are a list of feature names in the feature view, defined during its creation, that are not the part of the model schema itself but can be used during training as a helper for extra information. If training helper columns were not defined in the feature view or during materializing training dataset in the file system then<code>training_helper_columns=True</code> will not have any effect. Defaults to <code>False</code>, no training helper columns.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>dataframe_type</code> <p>str, optional. The type of the returned dataframe. Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>. Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>'default'</code> </p> <code>transformation_context</code> <p><code>Dict[str, Any]</code> A dictionary mapping variable names to objects that will be provided as contextual information to the transformation function at runtime. The <code>context</code> variable must be explicitly defined as parameters in the transformation function for these to be accessible during execution. If no context variables are provided, this parameter defaults to <code>None</code>.</p> <p> TYPE: <code>dict[str, Any]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>(X_train, X_test, y_train, y_test)</code> <p>Tuple of dataframe of features and labels</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.get_train_validation_test_split","title":"[source]  get_train_validation_test_split","text":"<pre><code>get_train_validation_test_split(\n    training_dataset_version: int,\n    read_options: dict[str, Any] | None = None,\n    primary_key: bool = False,\n    event_time: bool = False,\n    training_helper_columns: bool = False,\n    dataframe_type: str = \"default\",\n    transformation_context: dict[str, Any] = None,\n    **kwargs,\n) -&gt; tuple[\n    TrainingDatasetDataFrameTypes,\n    TrainingDatasetDataFrameTypes,\n    TrainingDatasetDataFrameTypes,\n    TrainingDatasetDataFrameTypes | None,\n    TrainingDatasetDataFrameTypes | None,\n    TrainingDatasetDataFrameTypes | None,\n]\n</code></pre> <p>Get training data created by <code>feature_view.create_train_validation_test_split</code> or <code>feature_view.train_validation_test_split</code>.</p> Example <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get training data\nX_train, X_val, X_test, y_train, y_val, y_test = feature_view.get_train_validation_test_splits(training_dataset_version=1)\n</code></pre> PARAMETER DESCRIPTION <code>training_dataset_version</code> <p>training dataset version</p> <p> TYPE: <code>int</code> </p> <code>read_options</code> <p>Additional options as key/value pairs to pass to the execution engine. For spark engine: Dictionary of read options for Spark. For python engine: * key <code>\"arrow_flight_config\"</code> to pass a dictionary of arrow flight configurations.   For example: <code>{\"arrow_flight_config\": {\"timeout\": 900}}</code> Defaults to <code>{}</code>.</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>primary_key</code> <p>whether to include primary key features or not.  Defaults to <code>False</code>, no primary key features.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>event_time</code> <p>whether to include event time feature or not.  Defaults to <code>False</code>, no event time feature.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>training_helper_columns</code> <p>whether to include training helper columns or not. Training helper columns are a list of feature names in the feature view, defined during its creation, that are not the part of the model schema itself but can be used during training as a helper for extra information. If training helper columns were not defined in the feature view or during materializing training dataset in the file system then<code>training_helper_columns=True</code> will not have any effect. Defaults to <code>False</code>, no training helper columns.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>dataframe_type</code> <p>str, optional. The type of the returned dataframe. Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>. Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default'</code> </p> <code>transformation_context</code> <p><code>Dict[str, Any]</code> A dictionary mapping variable names to objects that will be provided as contextual information to the transformation function at runtime. The <code>context</code> variable must be explicitly defined as parameters in the transformation function for these to be accessible during execution. If no context variables are provided, this parameter defaults to <code>None</code>.</p> <p> TYPE: <code>dict[str, Any]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>(X_train, X_val, X_test, y_train, y_val, y_test)</code> <p>Tuple of dataframe of features and labels</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.get_training_datasets","title":"[source]  get_training_datasets","text":"<pre><code>get_training_datasets() -&gt; list[\n    training_dataset.TrainingDatasetBase\n]\n</code></pre> <p>Returns the metadata of all training datasets created with this feature view.</p> Example <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get all training dataset metadata\nlist_tds_meta = feature_view.get_training_datasets()\n</code></pre> RETURNS DESCRIPTION <code>list[training_dataset.TrainingDatasetBase]</code> <p><code>List[TrainingDatasetBase]</code> List of training datasets metadata.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.get_training_dataset_statistics","title":"[source]  get_training_dataset_statistics","text":"<pre><code>get_training_dataset_statistics(\n    training_dataset_version: int,\n    before_transformation: bool = False,\n    feature_names: list[str] | None = None,\n) -&gt; Statistics\n</code></pre> <p>Get statistics of a training dataset.</p> Example <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get training dataset statistics\nstatistics = feature_view.get_training_dataset_statistics(training_dataset_version=1)\n</code></pre> PARAMETER DESCRIPTION <code>training_dataset_version</code> <p>Training dataset version</p> <p> TYPE: <code>int</code> </p> <code>before_transformation</code> <p>Whether the statistics were computed before transformation functions or not.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>feature_names</code> <p>List of feature names of which statistics are retrieved.</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Statistics</code> <p><code>Statistics</code></p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.add_training_dataset_tag","title":"[source]  add_training_dataset_tag","text":"<pre><code>add_training_dataset_tag(\n    training_dataset_version: int,\n    name: str,\n    value: dict[str, Any] | tag.Tag,\n) -&gt; None\n</code></pre> <p>Attach a tag to a training dataset.</p> Example <pre><code># get feature store instance\nfs = ...\n\n# get feature feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# attach a tag to a training dataset\nfeature_view.add_training_dataset_tag(\n    training_dataset_version=1,\n    name=\"tag_schema\",\n    value={\"key\", \"value\"}\n)\n</code></pre> PARAMETER DESCRIPTION <code>training_dataset_version</code> <p>training dataset version</p> <p> TYPE: <code>int</code> </p> <code>name</code> <p>Name of the tag to be added.</p> <p> TYPE: <code>str</code> </p> <code>value</code> <p>Value of the tag to be added.</p> <p> TYPE: <code>dict[str, Any] | tag.Tag</code> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.get_training_dataset_tag","title":"[source]  get_training_dataset_tag","text":"<pre><code>get_training_dataset_tag(\n    training_dataset_version: int, name: str\n) -&gt; tag.Tag | None\n</code></pre> <p>Get the tags of a training dataset.</p> Example <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get a training dataset tag\ntag_str = feature_view.get_training_dataset_tag(\n    training_dataset_version=1,\n     name=\"tag_schema\"\n)\n</code></pre> PARAMETER DESCRIPTION <code>training_dataset_version</code> <p>training dataset version</p> <p> TYPE: <code>int</code> </p> <code>name</code> <p>Name of the tag to get.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>tag.Tag | None</code> <p>tag value or <code>None</code> if it does not exist.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.get_training_dataset_tags","title":"[source]  get_training_dataset_tags","text":"<pre><code>get_training_dataset_tags(\n    training_dataset_version: int,\n) -&gt; dict[str, tag.Tag]\n</code></pre> <p>Returns all tags attached to a training dataset.</p> Example <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get a training dataset tags\nlist_tags = feature_view.get_training_dataset_tags(\n    training_dataset_version=1\n)\n</code></pre> RETURNS DESCRIPTION <code>dict[str, tag.Tag]</code> <p><code>Dict[str, obj]</code> of tags.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.delete_training_dataset_tag","title":"[source]  delete_training_dataset_tag","text":"<pre><code>delete_training_dataset_tag(\n    training_dataset_version: int, name: str\n) -&gt; None\n</code></pre> <p>Delete a tag attached to a training dataset.</p> Example <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# delete training dataset tag\nfeature_view.delete_training_dataset_tag(\n    training_dataset_version=1,\n    name='name_of_dataset'\n)\n</code></pre> PARAMETER DESCRIPTION <code>training_dataset_version</code> <p>training dataset version</p> <p> TYPE: <code>int</code> </p> <code>name</code> <p>Name of the tag to be removed.</p> <p> TYPE: <code>str</code> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.purge_training_data","title":"[source]  purge_training_data","text":"<pre><code>purge_training_data(training_dataset_version: int) -&gt; None\n</code></pre> <p>Delete a training dataset (data only).</p> Example <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# purge training data\nfeature_view.purge_training_data(training_dataset_version=1)\n</code></pre> PARAMETER DESCRIPTION <code>training_dataset_version</code> <p>Version of the training dataset to be removed.</p> <p> TYPE: <code>int</code> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.purge_all_training_data","title":"[source]  purge_all_training_data","text":"<pre><code>purge_all_training_data() -&gt; None\n</code></pre> <p>Delete all training datasets (data only).</p> Example <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# purge all training data\nfeature_view.purge_all_training_data()\n</code></pre> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.delete_training_dataset","title":"[source]  delete_training_dataset","text":"<pre><code>delete_training_dataset(\n    training_dataset_version: int,\n) -&gt; None\n</code></pre> <p>Delete a training dataset. This will delete both metadata and training data.</p> Example <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# delete a training dataset\nfeature_view.delete_training_dataset(\n    training_dataset_version=1\n)\n</code></pre> PARAMETER DESCRIPTION <code>training_dataset_version</code> <p>Version of the training dataset to be removed.</p> <p> TYPE: <code>int</code> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.delete_all_training_datasets","title":"[source]  delete_all_training_datasets","text":"<pre><code>delete_all_training_datasets() -&gt; None\n</code></pre> <p>Delete all training datasets. This will delete both metadata and training data.</p> Example <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# delete all training datasets\nfeature_view.delete_all_training_datasets()\n</code></pre> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.get_feature_monitoring_configs","title":"[source]  get_feature_monitoring_configs","text":"<pre><code>get_feature_monitoring_configs(\n    name: str | None = None,\n    feature_name: str | None = None,\n    config_id: int | None = None,\n) -&gt; (\n    fmc.FeatureMonitoringConfig\n    | list[fmc.FeatureMonitoringConfig]\n    | None\n)\n</code></pre> <p>Fetch feature monitoring configs attached to the feature view.</p> <p>If no arguments is provided the method will return all feature monitoring configs attached to the feature view, meaning all feature monitoring configs that are attach to a feature in the feature view. If you wish to fetch a single config, provide the its name. If you wish to fetch all configs attached to a particular feature, provide the feature name.</p> Example <pre><code># fetch your feature view\nfv = fs.get_feature_view(name=\"my_feature_view\", version=1)\n# fetch all feature monitoring configs attached to the feature view\nfm_configs = fv.get_feature_monitoring_configs()\n# fetch a single feature monitoring config by name\nfm_config = fv.get_feature_monitoring_configs(name=\"my_config\")\n# fetch all feature monitoring configs attached to a particular feature\nfm_configs = fv.get_feature_monitoring_configs(feature_name=\"my_feature\")\n# fetch a single feature monitoring config with a particular id\nfm_config = fv.get_feature_monitoring_configs(config_id=1)\n</code></pre> PARAMETER DESCRIPTION <code>name</code> <p>If provided fetch only the feature monitoring config with the given name. Defaults to None.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>feature_name</code> <p>If provided, fetch only configs attached to a particular feature. Defaults to None.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>config_id</code> <p>If provided, fetch only the feature monitoring config with the given id. Defaults to None.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request</p> <code>`hopsworks.client.exceptions.FeatureStoreException`</code> <p>If the feature view is not registered in Hopsworks</p> <code>ValueError</code> <p>if both name and feature_name are provided.</p> <code>`TypeError`</code> <p>if name or feature_name are not string or None.</p> RETURNS DESCRIPTION <code>fmc.FeatureMonitoringConfig | list[fmc.FeatureMonitoringConfig] | None</code> <p>Union[<code>FeatureMonitoringConfig</code>, List[<code>FeatureMonitoringConfig</code>], None] A list of feature monitoring configs. If name provided, returns either a single config or None if not found.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.get_feature_monitoring_history","title":"[source]  get_feature_monitoring_history","text":"<pre><code>get_feature_monitoring_history(\n    config_name: str | None = None,\n    config_id: int | None = None,\n    start_time: int | str | datetime | date | None = None,\n    end_time: int | str | datetime | date | None = None,\n    with_statistics: bool | None = True,\n) -&gt; list[fmr.FeatureMonitoringResult]\n</code></pre> <p>Fetch feature monitoring history for a given feature monitoring config.</p> Example <pre><code># fetch your feature view\nfv = fs.get_feature_view(name=\"my_feature_group\", version=1)\n# fetch feature monitoring history for a given feature monitoring config\nfm_history = fv.get_feature_monitoring_history(\n    config_name=\"my_config\",\n    start_time=\"2020-01-01\",\n)\n# or use the config id\nfm_history = fv.get_feature_monitoring_history(\n    config_id=1,\n    start_time=datetime.now() - timedelta(weeks=2),\n    end_time=datetime.now() - timedelta(weeks=1),\n    with_statistics=False,\n)\n</code></pre> PARAMETER DESCRIPTION <code>config_name</code> <p>The name of the feature monitoring config to fetch history for. Defaults to None.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>config_id</code> <p>The id of the feature monitoring config to fetch history for. Defaults to None.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>start_date</code> <p>The start date of the feature monitoring history to fetch. Defaults to None.</p> <p> </p> <code>end_date</code> <p>The end date of the feature monitoring history to fetch. Defaults to None.</p> <p> </p> <code>with_statistics</code> <p>Whether to include statistics in the feature monitoring history. Defaults to True. If False, only metadata about the monitoring will be fetched.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>True</code> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>In case the backend encounters an issue</p> <code>`hopsworks.client.exceptions.FeatureStoreException`</code> <p>If the feature view is not registered in Hopsworks</p> <code>ValueError</code> <p>if both config_name and config_id are provided.</p> <code>`TypeError`</code> <p>if config_name or config_id are not respectively string, int or None.</p> RETURNS DESCRIPTION <code>list[fmr.FeatureMonitoringResult]</code> <p>List[<code>FeatureMonitoringResult</code>] A list of feature monitoring results containing the monitoring metadata as well as the computed statistics for the detection and reference window if requested.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.create_statistics_monitoring","title":"[source]  create_statistics_monitoring","text":"<pre><code>create_statistics_monitoring(\n    name: str,\n    feature_name: str | None = None,\n    description: str | None = None,\n    start_date_time: int\n    | str\n    | datetime\n    | date\n    | pd.Timestamp\n    | None = None,\n    end_date_time: int\n    | str\n    | datetime\n    | date\n    | pd.Timestamp\n    | None = None,\n    cron_expression: str | None = \"0 0 12 ? * * *\",\n) -&gt; fmc.FeatureMonitoringConfig\n</code></pre> <p>Run a job to compute statistics on snapshot of feature data on a schedule.</p> Experimental <p>Public API is subject to change, this feature is not suitable for production use-cases.</p> Example <pre><code># fetch feature view\nfv = fs.get_feature_view(name=\"my_feature_view\", version=1)\n# enable statistics monitoring\nmy_config = fv._create_statistics_monitoring(\n    name=\"my_config\",\n    start_date_time=\"2021-01-01 00:00:00\",\n    description=\"my description\",\n    cron_expression=\"0 0 12 ? * * *\",\n).with_detection_window(\n    # Statistics computed on 10% of the last week of data\n    time_offset=\"1w\",\n    row_percentage=0.1,\n).save()\n</code></pre> PARAMETER DESCRIPTION <code>name</code> <p>Name of the feature monitoring configuration. name must be unique for all configurations attached to the feature view.</p> <p> TYPE: <code>str</code> </p> <code>feature_name</code> <p>Name of the feature to monitor. If not specified, statistics will be computed for all features.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>description</code> <p>Description of the feature monitoring configuration.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>start_date_time</code> <p>Start date and time from which to start computing statistics.</p> <p> TYPE: <code>int | str | datetime | date | pd.Timestamp | None</code> DEFAULT: <code>None</code> </p> <code>end_date_time</code> <p>End date and time at which to stop computing statistics.</p> <p> TYPE: <code>int | str | datetime | date | pd.Timestamp | None</code> DEFAULT: <code>None</code> </p> <code>cron_expression</code> <p>Cron expression to use to schedule the job. The cron expression must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * * *', every day at 12pm UTC.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>'0 0 12 ? * * *'</code> </p> RAISES DESCRIPTION <code>`hopsworks.client.exceptions.FeatureStoreException`</code> <p>If the feature view is not registered in Hopsworks</p> RETURNS DESCRIPTION <code>fmc.FeatureMonitoringConfig</code> <p><code>FeatureMonitoringConfig</code> Configuration with minimal information about the feature monitoring. Additional information are required before feature monitoring is enabled.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.create_feature_monitoring","title":"[source]  create_feature_monitoring","text":"<pre><code>create_feature_monitoring(\n    name: str,\n    feature_name: str,\n    description: str | None = None,\n    start_date_time: int\n    | str\n    | datetime\n    | date\n    | pd.Timestamp\n    | None = None,\n    end_date_time: int\n    | str\n    | datetime\n    | date\n    | pd.Timestamp\n    | None = None,\n    cron_expression: str | None = \"0 0 12 ? * * *\",\n) -&gt; fmc.FeatureMonitoringConfig\n</code></pre> <p>Enable feature monitoring to compare statistics on snapshots of feature data over time.</p> Experimental <p>Public API is subject to change, this feature is not suitable for production use-cases.</p> Example <pre><code># fetch feature view\nfg = fs.get_feature_view(name=\"my_feature_view\", version=1)\n# enable feature monitoring\nmy_config = fg.create_feature_monitoring(\n    name=\"my_monitoring_config\",\n    feature_name=\"my_feature\",\n    description=\"my monitoring config description\",\n    cron_expression=\"0 0 12 ? * * *\",\n).with_detection_window(\n    # Data inserted in the last day\n    time_offset=\"1d\",\n    window_length=\"1d\",\n).with_reference_window(\n    # compare to a given value\n    specific_value=0.5,\n).compare_on(\n    metric=\"mean\",\n    threshold=0.5,\n).save()\n</code></pre> PARAMETER DESCRIPTION <code>name</code> <p>Name of the feature monitoring configuration. name must be unique for all configurations attached to the feature group.</p> <p> TYPE: <code>str</code> </p> <code>feature_name</code> <p>Name of the feature to monitor.</p> <p> TYPE: <code>str</code> </p> <code>description</code> <p>Description of the feature monitoring configuration.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>start_date_time</code> <p>Start date and time from which to start computing statistics.</p> <p> TYPE: <code>int | str | datetime | date | pd.Timestamp | None</code> DEFAULT: <code>None</code> </p> <code>end_date_time</code> <p>End date and time at which to stop computing statistics.</p> <p> TYPE: <code>int | str | datetime | date | pd.Timestamp | None</code> DEFAULT: <code>None</code> </p> <code>cron_expression</code> <p>Cron expression to use to schedule the job. The cron expression must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * * *', every day at 12pm UTC.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>'0 0 12 ? * * *'</code> </p> RAISES DESCRIPTION <code>`hopsworks.client.exceptions.FeatureStoreException`</code> <p>If the feature view is not registered in Hopsworks</p> RETURNS DESCRIPTION <code>fmc.FeatureMonitoringConfig</code> <p><code>FeatureMonitoringConfig</code> Configuration with minimal information about the feature monitoring. Additional information are required before feature monitoring is enabled.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.get_alerts","title":"[source]  get_alerts","text":"<pre><code>get_alerts()\n</code></pre> <p>Get all alerts for this feature view.</p> RETURNS DESCRIPTION <p>List[FeatureViewAlert] or Alert.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.get_alert","title":"[source]  get_alert","text":"<pre><code>get_alert(alert_id: int)\n</code></pre> <p>Get an alert for this feature view by ID.</p> PARAMETER DESCRIPTION <code>alert_id</code> <p>The id of the alert to get.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <p>A single FeatureViewAlert object is returned.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.create_alert","title":"[source]  create_alert","text":"<pre><code>create_alert(receiver: str, status: str, severity: str)\n</code></pre> <p>Create an alert for this feature view.</p> Example <pre><code># get feature store instance\nfs = ...\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n# create an alert\nalert = feature_view.create_alert(\n    receiver=\"email\",\n    status=\"feature_monitor_shift_undetected\",\n    severity=\"info\",\n)\n</code></pre> PARAMETER DESCRIPTION <code>receiver</code> <p>str. The receiver of the alert.</p> <p> TYPE: <code>str</code> </p> <code>status</code> <p>str. The status that will trigger the alert. Can be \"feature_monitor_shift_undetected\" or \"feature_monitor_shift_detected\".</p> <p> TYPE: <code>str</code> </p> <code>severity</code> <p>str. The severity of the alert. Can be \"info\", \"warning\" or \"critical\".</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <p>The created FeatureViewAlert object.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the status is not valid.</p> <code>ValueError</code> <p>If the severity is not valid.</p> <code>hopsworks.client.exceptions.RestAPIError</code> <p>If the backend encounters an error when handling the request</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.compute_on_demand_features","title":"[source]  compute_on_demand_features","text":"<pre><code>compute_on_demand_features(\n    feature_vector: list[Any]\n    | list[list[Any]]\n    | pd.DataFrame\n    | pl.DataFrame\n    | None = None,\n    request_parameters: list[dict[str, Any]]\n    | dict[str, Any]\n    | None = None,\n    transformation_context: dict[str, Any] = None,\n    return_type: Literal[\n        \"list\", \"numpy\", \"pandas\", \"polars\"\n    ] = None,\n)\n</code></pre> <p>Function computes on-demand features present in the feature view.</p> PARAMETER DESCRIPTION <code>feature_vector</code> <p><code>Union[List[Any], List[List[Any]], pd.DataFrame, pl.DataFrame]</code>. The feature vector to be transformed.</p> <p> TYPE: <code>list[Any] | list[list[Any]] | pd.DataFrame | pl.DataFrame | None</code> DEFAULT: <code>None</code> </p> <code>request_parameters</code> <p>Request parameters required by on-demand transformation functions to compute on-demand features present in the feature view. These parameters take higheshighestt priority when resolving feature values - if a key exists in both <code>request_parameters</code> and the feature vector, the value from <code>request_parameters</code> is used.</p> <p> TYPE: <code>list[dict[str, Any]] | dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>transformation_context</code> <p><code>Dict[str, Any]</code> A dictionary mapping variable names to objects that will be provided as contextual information to the transformation function at runtime. The <code>context</code> variable must be explicitly defined as parameters in the transformation function for these to be accessible during execution. If no context variables are provided, this parameter defaults to <code>None</code>.</p> <p> TYPE: <code>dict[str, Any]</code> DEFAULT: <code>None</code> </p> <code>return_type</code> <p><code>\"list\"</code>, <code>\"pandas\"</code>, <code>\"polars\"</code> or <code>\"numpy\"</code>. Defaults to the same type as the input feature vector.</p> <p> TYPE: <code>Literal['list', 'numpy', 'pandas', 'polars']</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <p><code>Union[List[Any], List[List[Any]], pd.DataFrame, pl.DataFrame]</code>: The feature vector that contains all on-demand features in the feature view.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.transform","title":"[source]  transform","text":"<pre><code>transform(\n    feature_vector: list[Any]\n    | list[list[Any]]\n    | pd.DataFrame\n    | pl.DataFrame,\n    external: bool | None = None,\n    transformation_context: dict[str, Any] = None,\n    return_type: Literal[\n        \"list\", \"numpy\", \"pandas\", \"polars\"\n    ] = None,\n)\n</code></pre> <p>Transform the input feature vector by applying Model-dependent transformations attached to the feature view.</p> List input must match the schema of the feature view <p>If features are provided as a List to the transform function. Make sure that the input are ordered to match the schema in the feature view.</p> <p>Parameters:     feature_vector: <code>Union[List[Any], List[List[Any]], pd.DataFrame, pl.DataFrame]</code>. The feature vector to be transformed.     external: boolean, optional. If set to True, the connection to the         online feature store is established using the same host as         for the <code>host</code> parameter in the <code>hopsworks.login</code> method.         If set to False, the online feature store storage connector is used         which relies on the private IP. Defaults to True if connection to Hopsworks is established from         external environment (e.g AWS Sagemaker or Google Colab), otherwise to False.     transformation_context: <code>Dict[str, Any]</code> A dictionary mapping variable names to objects that will be provided as contextual information to the transformation function at runtime.         The <code>context</code> variable must be explicitly defined as parameters in the transformation function for these to be accessible during execution. If no context variables are provided, this parameter defaults to <code>None</code>.     return_type: <code>\"list\"</code>, <code>\"pandas\"</code>, <code>\"polars\"</code> or <code>\"numpy\"</code>. Defaults to the same type as the input feature vector.</p> RETURNS DESCRIPTION <p><code>Union[List[Any], List[List[Any]], pd.DataFrame, pl.DataFrame]</code>: The transformed feature vector obtained by applying Model-Dependent Transformations.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.enable_logging","title":"[source]  enable_logging","text":"<pre><code>enable_logging(\n    extra_log_columns: Feature | dict[str, str] = None,\n) -&gt; None\n</code></pre> <p>Enable feature logging for the current feature view.</p> <p>This method activates logging of features.</p> PARAMETER DESCRIPTION <code>extra_log_columns</code> <p><code>Union[Feature, List[Dict[str, str]]]</code> Additional columns to be logged. Any duplicate columns will be ignored.</p> <p> TYPE: <code>Feature | dict[str, str]</code> DEFAULT: <code>None</code> </p> Enable feature logging <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# enable logging\nfeature_view.enable_logging()\n</code></pre> Enable feature logging and add extra log columns <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# enable logging with two extra log columns\nfeature_view.enable_logging(extra_log_columns=[{\"name\": \"logging_col_1\", \"type\": \"string\"},\n                                               {\"name\": \"logging_col_2\", \"type\": \"int\"}])\n</code></pre> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>In case the backend encounters an issue</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.init_feature_logger","title":"[source]  init_feature_logger","text":"<pre><code>init_feature_logger(feature_logger: FeatureLogger) -&gt; None\n</code></pre> <p>Initialize the feature logger.</p> PARAMETER DESCRIPTION <code>feature_logger</code> <p>The logger to be used for logging features.</p> <p> TYPE: <code>FeatureLogger</code> </p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.log","title":"[source]  log","text":"<pre><code>log(\n    logging_data: pd.DataFrame\n    | pl.DataFrame\n    | list[list[Any]]\n    | list[dict[str, Any]]\n    | np.ndarray\n    | TypeVar(\"pyspark.sql.DataFrame\") = None,\n    untransformed_features: pd.DataFrame\n    | pl.DataFrame\n    | list[list[Any]]\n    | list[dict[str, Any]]\n    | np.ndarray\n    | TypeVar(\"pyspark.sql.DataFrame\")\n    | None = None,\n    predictions: pd.DataFrame\n    | pl.DataFrame\n    | list[list[Any]]\n    | list[dict[str, Any]]\n    | np.ndarray\n    | None = None,\n    transformed_features: pd.DataFrame\n    | pl.DataFrame\n    | list[list[Any]]\n    | list[dict[str, Any]]\n    | np.ndarray\n    | TypeVar(\"pyspark.sql.DataFrame\") = None,\n    inference_helper_columns: pd.DataFrame\n    | pl.DataFrame\n    | list[list[Any]]\n    | list[dict[str, Any]]\n    | np.ndarray\n    | TypeVar(\"pyspark.sql.DataFrame\") = None,\n    request_parameters: pd.DataFrame\n    | pl.DataFrame\n    | list[list[Any]]\n    | list[dict[str, Any]]\n    | np.ndarray\n    | TypeVar(\"pyspark.sql.DataFrame\") = None,\n    event_time: pd.DataFrame\n    | pl.DataFrame\n    | list[list[Any]]\n    | list[dict[str, Any]]\n    | np.ndarray\n    | TypeVar(\"pyspark.sql.DataFrame\") = None,\n    serving_keys: pd.DataFrame\n    | pl.DataFrame\n    | list[list[Any]]\n    | list[dict[str, Any]]\n    | np.ndarray\n    | TypeVar(\"pyspark.sql.DataFrame\") = None,\n    extra_logging_features: pd.DataFrame\n    | pl.DataFrame\n    | list[list[Any]]\n    | list[dict[str, Any]]\n    | np.ndarray\n    | TypeVar(\"pyspark.sql.DataFrame\") = None,\n    request_id: str | list[str] = None,\n    write_options: dict[str, Any] | None = None,\n    training_dataset_version: int | None = None,\n    model: Model = None,\n    model_name: str | None = None,\n    model_version: int | None = None,\n) -&gt; list[Job] | None\n</code></pre> <p>Log features and optionally predictions for the current feature view. The logged features are written periodically to the offline store. If you need it to be available immediately, call <code>materialize_log</code>.</p> If features is a <code>pyspark.Dataframe</code>, prediction needs to be provided as columns in the dataframe, <p>values in <code>predictions</code> will be ignored.</p> PARAMETER DESCRIPTION <code>logging_dataframe</code> <p>The features to be logged, this can contain both transformed features, untransfored features and predictions. Can be a pandas DataFrame, polar DataFrame, or spark DataFrame, a list of lists, a list of dictionaries or a numpy ndarray.</p> <p> </p> <code>untransformed_features</code> <p>The untransformed features to be logged. Can be a pandas DataFrame, polar DataFrame, or spark DataFrame, a list of lists, a list of dictionaries or a numpy ndarray.</p> <p> TYPE: <code>pd.DataFrame | pl.DataFrame | list[list[Any]] | list[dict[str, Any]] | np.ndarray | TypeVar('pyspark.sql.DataFrame') | None</code> DEFAULT: <code>None</code> </p> <code>prediction</code> <p>The predictions to be logged.  Can be a pandas DataFrame, polar DataFrame, or spark DataFrame, a list, a list of lists, or a numpy ndarray.</p> <p> </p> <code>transformed_features</code> <p>The transformed features to be logged. Can be a pandas DataFrame, polar DataFrame, or spark DataFrame, a list of lists, a list of dictionaries or a numpy ndarray.</p> <p> TYPE: <code>pd.DataFrame | pl.DataFrame | list[list[Any]] | list[dict[str, Any]] | np.ndarray | TypeVar('pyspark.sql.DataFrame')</code> DEFAULT: <code>None</code> </p> <code>inference_helper_columns</code> <p>The inference helper columns to be logged. Can be a pandas DataFrame, polar DataFrame, or spark DataFrame, a list of lists, a list of dictionaries or a numpy ndarray.</p> <p> TYPE: <code>pd.DataFrame | pl.DataFrame | list[list[Any]] | list[dict[str, Any]] | np.ndarray | TypeVar('pyspark.sql.DataFrame')</code> DEFAULT: <code>None</code> </p> <code>request_parameters</code> <p>The request parameters to be logged. Can be a pandas DataFrame, polar DataFrame, or spark DataFrame, a list of lists, a list of dictionaries or a numpy ndarray.</p> <p> TYPE: <code>pd.DataFrame | pl.DataFrame | list[list[Any]] | list[dict[str, Any]] | np.ndarray | TypeVar('pyspark.sql.DataFrame')</code> DEFAULT: <code>None</code> </p> <code>event_time</code> <p>The event time to be logged. Can be a pandas DataFrame, polar DataFrame, or spark DataFrame, a list of lists, a list of dictionaries or a numpy ndarray.</p> <p> TYPE: <code>pd.DataFrame | pl.DataFrame | list[list[Any]] | list[dict[str, Any]] | np.ndarray | TypeVar('pyspark.sql.DataFrame')</code> DEFAULT: <code>None</code> </p> <code>serving_keys</code> <p>The serving keys to be logged. Can be a pandas DataFrame, polar DataFrame, or spark DataFrame, a list of lists, a list of dictionaries or a numpy ndarray.</p> <p> TYPE: <code>pd.DataFrame | pl.DataFrame | list[list[Any]] | list[dict[str, Any]] | np.ndarray | TypeVar('pyspark.sql.DataFrame')</code> DEFAULT: <code>None</code> </p> <code>extra_logging_features</code> <p>Extra features to be logged. The features must be specified when enabled logging or while creating the feature view. Can be a pandas DataFrame, polar DataFrame, or spark DataFrame, a list of lists, a list of dictionaries or a numpy ndarray.</p> <p> TYPE: <code>pd.DataFrame | pl.DataFrame | list[list[Any]] | list[dict[str, Any]] | np.ndarray | TypeVar('pyspark.sql.DataFrame')</code> DEFAULT: <code>None</code> </p> <code>request_id</code> <p>The request ID that can be used to identify an online inference request.</p> <p> TYPE: <code>str | list[str]</code> DEFAULT: <code>None</code> </p> <code>write_options</code> <p>Options for writing the log. Defaults to None.</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>training_dataset_version</code> <p>Version of the training dataset. If training dataset version is definied in <code>init_serving</code> or <code>init_batch_scoring</code>, or model has training dataset version, or training dataset version was cached, then the version will be used, otherwise defaults to None.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>model</code> <p><code>Union[str, hsml.model.Model]</code> Hopsworks model associated with the log. Defaults to None.</p> <p> TYPE: <code>Model</code> DEFAULT: <code>None</code> </p> <code>model_name</code> <p><code>Optional[str]</code>. Name of the model to be associated with the log. If <code>model</code> is provided, this parameter will be ignored.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>model_version</code> <p><code>Optional[int]</code>. Version of the model to be associated with the log. If <code>model</code> is provided, this parameter will be ignored.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>list[Job] | None</code> <p><code>list[Job]</code> job information for feature insertion if python engine is used</p> Implicitly Logging Batch Data and Predictions with all Logging metadata <pre><code>df = fv.get_batch_data(logging_data=True)\npredictions = model.predict(df)\n\n# log passed features\nfeature_view.log(df, predictions=predictions)\n</code></pre> Implicitly Logging Feature Vectors and Predictions with all Logging metadata <pre><code>feature_vector = fv.get_feature_vector({\"pk\": 1}, logging_data=True)\npredictions = model.predict(feature_vector)\n\n# log passed features\nfeature_view.log(feature_vector, predictions=predictions)\n</code></pre> Logging DataFrames with Predictions <pre><code>df = fv.get_batch_data()\npredictions = model.predict(df)\n\n# log passed features\nfeature_view.log(df, predictions=predictions)\n</code></pre> Explicit Logging of untransformed and transformed Features <pre><code>serving_keys = [{\"pk\": 1}]\nuntransformed_feature_vector = fv.get_feature_vectors({\"pk\": 1})\ntransformed_feature_vector = fv.transform(untransformed_feature_vector)\npredictions = model.predict(transformed_feature_vector)\n\n# log both untransformed and transformed features\nfeature_view.log(\n    untransformed_features=untransformed_feature_vector,\n    transformed_features=transformed_feature_vector,\n    servings_keys=serving_keys,\n    predictions=predictions\n)\n</code></pre> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>in case the backend fails to log features.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.get_log_timeline","title":"[source]  get_log_timeline","text":"<pre><code>get_log_timeline(\n    wallclock_time: str\n    | int\n    | datetime\n    | datetime.date\n    | None = None,\n    limit: int | None = None,\n    transformed: bool | None = False,\n) -&gt; dict[str, dict[str, str]]\n</code></pre> <p>Retrieve the log timeline for the current feature view.</p> PARAMETER DESCRIPTION <code>wallclock_time</code> <p>Specific time to get the log timeline for. Can be a string, integer, datetime, or date. Defaults to None.</p> <p> TYPE: <code>str | int | datetime | datetime.date | None</code> DEFAULT: <code>None</code> </p> <code>limit</code> <p>Maximum number of entries to retrieve. Defaults to None.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>transformed</code> <p>Whether to include transformed logs. Defaults to False.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>False</code> </p> Example <pre><code># get log timeline\nlog_timeline = feature_view.get_log_timeline(limit=10)\n</code></pre> RETURNS DESCRIPTION <code>dict[str, dict[str, str]]</code> <p><code>Dict[str, Dict[str, str]]</code>. Dictionary object of commit metadata timeline, where Key is commit id and value</p> <code>dict[str, dict[str, str]]</code> <p>is <code>Dict[str, str]</code> with key value pairs of date committed on, number of rows updated, inserted and deleted.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>in case the backend fails to retrieve the log timeline.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.read_log","title":"[source]  read_log","text":"<pre><code>read_log(\n    start_time: str\n    | int\n    | datetime\n    | datetime.date\n    | None = None,\n    end_time: str\n    | int\n    | datetime\n    | datetime.date\n    | None = None,\n    filter: Filter | Logic | None = None,\n    transformed: bool | None = False,\n    training_dataset_version: int | None = None,\n    model: Model = None,\n    model_name: str | None = None,\n    model_version: int | None = None,\n) -&gt; (\n    TypeVar(\"pyspark.sql.DataFrame\")\n    | pd.DataFrame\n    | pl.DataFrame\n)\n</code></pre> <p>Read the log entries for the current feature view.</p> <p>Optionally, filter can be applied to start/end time, training dataset version, hsml model, and custom filter.</p> PARAMETER DESCRIPTION <code>start_time</code> <p>Start time for the log entries. Can be a string, integer, datetime, or date. Defaults to None.</p> <p> TYPE: <code>str | int | datetime | datetime.date | None</code> DEFAULT: <code>None</code> </p> <code>end_time</code> <p>End time for the log entries. Can be a string, integer, datetime, or date. Defaults to None.</p> <p> TYPE: <code>str | int | datetime | datetime.date | None</code> DEFAULT: <code>None</code> </p> <code>filter</code> <p>Filter to apply on the log entries. Can be a Filter or Logic object. Defaults to None.</p> <p> TYPE: <code>Filter | Logic | None</code> DEFAULT: <code>None</code> </p> <code>transformed</code> <p>Whether to include transformed logs. Defaults to False.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>False</code> </p> <code>training_dataset_version</code> <p>Version of the training dataset. Defaults to None.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>model</code> <p>HSML model associated with the log. Defaults to None.</p> <p> TYPE: <code>Model</code> DEFAULT: <code>None</code> </p> <code>model_name</code> <p><code>Optional[str]</code>. Name of the model to filter the log entries. If <code>model</code> is provided, this parameter will be ignored.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>model_version</code> <p><code>Optional[int]</code>. Version of the model to filter the log entries. If <code>model</code> is provided, this parameter will be ignored.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> Example <pre><code># read all log entries\nlog_entries = feature_view.read_log()\n# read log entries within time ranges\nlog_entries = feature_view.read_log(start_time=\"2022-01-01\", end_time=\"2022-01-31\")\n# read log entries of a specific training dataset version\nlog_entries = feature_view.read_log(training_dataset_version=1)\n# read log entries of a specific hopsworks model\nlog_entries = feature_view.read_log(model=Model(1, \"dummy\", version=1))\n# read log entries by applying filter on features of feature group `fg` in the feature view\nlog_entries = feature_view.read_log(filter=fg.feature1 &gt; 10)\n</code></pre> RETURNS DESCRIPTION <code>TypeVar('pyspark.sql.DataFrame') | pd.DataFrame | pl.DataFrame</code> <p><code>DataFrame</code>: The spark dataframe containing the feature data.</p> <code>TypeVar('pyspark.sql.DataFrame') | pd.DataFrame | pl.DataFrame</code> <p><code>pyspark.DataFrame</code>. A Spark DataFrame.</p> <code>TypeVar('pyspark.sql.DataFrame') | pd.DataFrame | pl.DataFrame</code> <p><code>pandas.DataFrame</code>. A Pandas DataFrame.</p> <code>TypeVar('pyspark.sql.DataFrame') | pd.DataFrame | pl.DataFrame</code> <p><code>polars.DataFrame</code>. A Polars DataFrame.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>in case the backend fails to read the log entries.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.pause_logging","title":"[source]  pause_logging","text":"<pre><code>pause_logging() -&gt; None\n</code></pre> <p>Pause scheduled materialization job for the current feature view.</p> Example <pre><code># pause logging\nfeature_view.pause_logging()\n</code></pre> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>in case the backend fails to pause feature logging.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.resume_logging","title":"[source]  resume_logging","text":"<pre><code>resume_logging() -&gt; None\n</code></pre> <p>Resume scheduled materialization job for the current feature view.</p> Example <pre><code># resume logging\nfeature_view.resume_logging()\n</code></pre> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>in case the backend fails to pause feature logging.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.materialize_log","title":"[source]  materialize_log","text":"<pre><code>materialize_log(\n    wait: bool = False, transformed: bool | None = None\n) -&gt; list[Job]\n</code></pre> <p>Materialize the log for the current feature view.</p> PARAMETER DESCRIPTION <code>wait</code> <p>Whether to wait for the materialization to complete. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>transformed</code> <p>Whether to materialize transformed or untrasformed logs. Defaults to None, in which case the returned list contains a job for materialization of transformed features and then a job for untransformed features. Otherwise the list contains only transformed jobs if transformed is True and untransformed jobs if it is False.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> Example <pre><code># materialize log\nmaterialization_result = feature_view.materialize_log(wait=True)\n</code></pre> RETURNS DESCRIPTION <code>list[Job]</code> <p>List[<code>Job</code>] Job information for the materialization jobs of transformed and untransformed features.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>in case the backend fails to materialize the log.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.delete_log","title":"[source]  delete_log","text":"<pre><code>delete_log(transformed: bool | None = None) -&gt; None\n</code></pre> <p>Delete the logged feature data for the current feature view.</p> PARAMETER DESCRIPTION <code>transformed</code> <p>Whether to delete transformed logs. Defaults to None. Delete both transformed and untransformed logs.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> Example <pre><code># delete log\nfeature_view.delete_log()\n</code></pre> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>in case the backend fails to delete the log.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.create_feature_logger","title":"[source]  create_feature_logger","text":"<pre><code>create_feature_logger()\n</code></pre> <p>Create an asynchronous feature logger for logging features in Hopsworks serving deployments.</p> Example <pre><code># get feature logger\nfeature_logger = feature_view.create_feature_logger()\n\n# initialize feature view for serving with feature logger\nfeature_view.init_serving(1, feature_logger=feature_logger)\n\n# log features\nfeature_view.log(...)\n</code></pre> RAISES DESCRIPTION <code>`hopsworks.client.exceptions.FeatureStoreException`</code> <p>If not running in a Hopsworks serving deployment.</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.execute_odts","title":"[source]  execute_odts","text":"<pre><code>execute_odts(\n    data: pd.DataFrame | pl.DataFrame | dict[str, Any],\n    online: bool | None = None,\n    transformation_context: dict[str, Any]\n    | list[dict[str, Any]] = None,\n    request_parameters: dict[str, Any]\n    | list[dict[str, Any]] = None,\n) -&gt; dict[str, Any] | pd.DataFrame\n</code></pre> <p>Apply on-demand transformations attached to the feature view on the provided data.</p> <p>This method allows you to test on-demand transformation functions locally. It executes all on-demand transformations (ODTs) attached to the feature view on the input data.</p> Testing on-demand transformations <pre><code>@udf(return_type=float)\ndef compute_ratio(amount, quantity):\n    return amount / quantity\n\nfv = fs.get_or_create_feature_view(name=\"transactions_fv\",\n                            version=1,\n                            query=fg.select_features(),\n                            transformation_functions=[compute_ratio(\"amount\", \"quantity\")])\n\n# Test with a DataFrame (offline mode)\ntest_df = pd.DataFrame({\n    \"amount\": [100.0, 200.0, 300.0],\n    \"quantity\": [2, 4, 5]\n})\nresult_df = fv.execute_odts(test_df)\n\n# Test with a dictionary (single record, online inference simulation)\ntest_dict = {\"amount\": 100.0, \"quantity\": 2}\nresult_dict = fv.execute_odts(test_dict, online=True)\n</code></pre> PARAMETER DESCRIPTION <code>data</code> <p>Input data to apply transformations to. This can a dataframe or a dictionary.</p> <p> TYPE: <code>pd.DataFrame | pl.DataFrame | dict[str, Any]</code> </p> <code>online</code> <p>Whether to apply transformations in online mode (single values) or offline mode (batch/vectorized). Defaults to offline mode</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>transformation_context</code> <p>A dictionary mapping variable names to objects that provide contextual information to the transformation function at runtime. The <code>context</code> variables must be defined as parameters in the transformation function for these to be accessible during execution. For batch processing with different contexts per row, provide a list of dictionaries.</p> <p> TYPE: <code>dict[str, Any] | list[dict[str, Any]]</code> DEFAULT: <code>None</code> </p> <code>request_parameters</code> <p>Request parameters passed to the transformation functions. For batch processing with different parameters per row, provide a list of dictionaries. These parameters take highest priority when resolving feature values -- if a key exists in both <code>request_parameters</code> and the input data, the value from <code>request_parameters</code> is used.</p> <p> TYPE: <code>dict[str, Any] | list[dict[str, Any]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>dict[str, Any] | pd.DataFrame</code> <p>The transformed data in the same format as the input: - <code>pd.DataFrame</code> if input was a DataFrame - <code>dict[str, Any]</code> if input was a dictionary</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.execute_mdts","title":"[source]  execute_mdts","text":"<pre><code>execute_mdts(\n    data: pd.DataFrame | pl.DataFrame | dict[str, Any],\n    online: bool | None = None,\n    transformation_context: dict[str, Any]\n    | list[dict[str, Any]] = None,\n    request_parameters: dict[str, Any]\n    | list[dict[str, Any]] = None,\n) -&gt; dict[str, Any] | pd.DataFrame\n</code></pre> <p>Apply model-dependent transformations attached to the feature view on the provided data.</p> <p>This method allows you to test model-dependent transformation functions locally. It executes all model-dependent transformations (MDTs) attached to the feature view, using the statistics computed from training data.</p> Testing model-dependent transformations with statistics <pre><code>from hsfs.transformation_statistics import TransformationStatistics\n\n@udf(return_type=float)\ndef normalize(amount, statistics=TransformationStatistics(\"amount\")):\n    return (amount - statistics.amount.mean) / statistics.amount.std_dev\n\nfv = fs.get_or_create_feature_view(name=\"transactions_fv\",\n                            version=1,\n                            query=fg.select_features(),\n                            transformation_functions=[normalize(\"amount\")])\n\n# Create training data for training dataset statistics\n# Alternatively you can initialize the feature view with statistics of previously created training data using `init_batch_scoring` or `init_serving`.\nfeatures, labels = fv.create_training_data()\n\n# Testing with a DataFrame\ntest_df = pd.DataFrame({\"amount\": [100.0, 200.0, 300.0]})\nresult_df = fv.execute_mdts(test_df)\n\n# Testing with a dictionary (online inference simulation)\ntest_dict = {\"amount\": 100.0}\nresult_dict = fv.execute_mdts(test_dict, online=True)\n</code></pre> PARAMETER DESCRIPTION <code>data</code> <p>Input data to apply transformations to. This can a dataframe or a dictionary.</p> <p> TYPE: <code>pd.DataFrame | pl.DataFrame | dict[str, Any]</code> </p> <code>online</code> <p>Whether to apply transformations in online mode (single values) or offline mode (batch/vectorized). Defaults to offline mode.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>transformation_context</code> <p>A dictionary mapping variable names to objects that provide contextual information to the transformation function at runtime. The <code>context</code> variables must be defined as parameters in the transformation function for these to be accessible during execution. For batch processing with different contexts per row, provide a list of dictionaries.</p> <p> TYPE: <code>dict[str, Any] | list[dict[str, Any]]</code> DEFAULT: <code>None</code> </p> <code>request_parameters</code> <p>Request parameters passed to the transformation functions. For batch processing with different parameters per row, provide a list of dictionaries. These parameters take highest priority when resolving feature values -- if a key exists in both <code>request_parameters</code> and the input data, the value from <code>request_parameters</code> is used.</p> <p> TYPE: <code>dict[str, Any] | list[dict[str, Any]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>dict[str, Any] | pd.DataFrame</code> <p>The transformed data in the same format as the input: - <code>pd.DataFrame</code> if input was a DataFrame - <code>dict[str, Any]</code> if input was a dictionary</p>"},{"location":"python-api/hsfs/feature_view/#hsfs.feature_view.FeatureView.get_training_dataset_schema","title":"[source]  get_training_dataset_schema","text":"<pre><code>get_training_dataset_schema(\n    training_dataset_version: int | None = None,\n) -&gt; list[training_dataset_feature.TrainingDatasetFeature]\n</code></pre> <p>Function that returns the schema of the training dataset that is generated from a feature view.</p> <p>It provides the schema of the features after all transformation functions have been applied.</p> PARAMETER DESCRIPTION <code>training_dataset_version</code> <p>Specifies the version of the training dataset for which the schema should be generated. By default, this is set to None. However, if the <code>one_hot_encoder</code> transformation function is used, the training dataset version must be provided. This is because the schema will then depend on the statistics of the training data used.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> Example <pre><code>schema = feature_view.get_training_dataset_schema(training_dataset_version=1)\n</code></pre> RETURNS DESCRIPTION <code>list[training_dataset_feature.TrainingDatasetFeature]</code> <p><code>List[training_dataset_feature.TrainingDatasetFeature]</code>: List of training dataset features objects.</p>"},{"location":"python-api/hsfs/hopsworks_udf/","title":"hsfs.hopsworks_udf","text":""},{"location":"python-api/hsfs/hopsworks_udf/#hsfs.hopsworks_udf","title":"hsfs.hopsworks_udf","text":""},{"location":"python-api/hsfs/hopsworks_udf/#hsfs.hopsworks_udf.HopsworksUdf","title":"[source]  HopsworksUdf","text":"<p>Meta data for user defined functions.</p> <p>Stores meta data required to execute the user defined function in both spark and python engine. The class generates uses the metadata to dynamically generate user defined functions based on the engine it is executed in.</p> PARAMETER DESCRIPTION <code>func</code> <p>The transformation function object or the source code of the transformation function.</p> <p> TYPE: <code>Callable | str</code> </p> <code>return_types</code> <p>A python type or a list of python types that denotes the data types of the columns output from the transformation functions.</p> <p> TYPE: <code>list[type] | type | list[str] | str</code> </p> <code>name</code> <p>Name of the transformation function.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>transformation_features</code> <p>A list of objects of <code>TransformationFeature</code> that maps the feature used for transformation to their corresponding statistics argument names if any.</p> <p> TYPE: <code>list[TransformationFeature] | None</code> DEFAULT: <code>None</code> </p> <code>transformation_function_argument_names</code> <p>The argument names of the transformation function.</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>dropped_argument_names</code> <p>The arguments to be dropped from the finial DataFrame after the transformation functions are applied.</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>dropped_feature_names</code> <p>The feature name corresponding to the arguments names that are dropped.</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>feature_name_prefix</code> <p>Prefixes if any used in the feature view.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>output_column_names</code> <p>The names of the output columns returned from the transformation function.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>generate_output_col_names</code> <p>Generate default output column names for the transformation function.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p>"},{"location":"python-api/hsfs/hopsworks_udf/#hsfs.hopsworks_udf.HopsworksUdf.return_types","title":"[source]  return_types  <code>property</code>","text":"<pre><code>return_types: list[str]\n</code></pre> <p>Get the output types of the UDF.</p>"},{"location":"python-api/hsfs/hopsworks_udf/#hsfs.hopsworks_udf.HopsworksUdf.function_name","title":"[source]  function_name  <code>property</code>","text":"<pre><code>function_name: str\n</code></pre> <p>Get the function name of the UDF.</p>"},{"location":"python-api/hsfs/hopsworks_udf/#hsfs.hopsworks_udf.HopsworksUdf.statistics_required","title":"[source]  statistics_required  <code>property</code>","text":"<pre><code>statistics_required: bool\n</code></pre> <p>Get if statistics for any feature is required by the UDF.</p>"},{"location":"python-api/hsfs/hopsworks_udf/#hsfs.hopsworks_udf.HopsworksUdf.transformation_statistics","title":"[source]  transformation_statistics  <code>property</code> <code>writable</code>","text":"<pre><code>transformation_statistics: TransformationStatistics | None\n</code></pre> <p>Feature statistics required for the defined UDF.</p>"},{"location":"python-api/hsfs/hopsworks_udf/#hsfs.hopsworks_udf.HopsworksUdf.output_column_names","title":"[source]  output_column_names  <code>property</code> <code>writable</code>","text":"<pre><code>output_column_names: list[str]\n</code></pre> <p>Output columns names of the transformation function.</p>"},{"location":"python-api/hsfs/hopsworks_udf/#hsfs.hopsworks_udf.HopsworksUdf.transformation_features","title":"[source]  transformation_features  <code>property</code>","text":"<pre><code>transformation_features: list[str]\n</code></pre> <p>List of feature names to be used in the User Defined Function.</p>"},{"location":"python-api/hsfs/hopsworks_udf/#hsfs.hopsworks_udf.HopsworksUdf.unprefixed_transformation_features","title":"[source]  unprefixed_transformation_features  <code>property</code>","text":"<pre><code>unprefixed_transformation_features: list[str]\n</code></pre> <p>List of feature name used in the transformation function without the feature name prefix.</p>"},{"location":"python-api/hsfs/hopsworks_udf/#hsfs.hopsworks_udf.HopsworksUdf.feature_name_prefix","title":"[source]  feature_name_prefix  <code>property</code>","text":"<pre><code>feature_name_prefix: str | None\n</code></pre> <p>The feature name prefix that needs to be added to the feature names.</p>"},{"location":"python-api/hsfs/hopsworks_udf/#hsfs.hopsworks_udf.HopsworksUdf.statistics_features","title":"[source]  statistics_features  <code>property</code>","text":"<pre><code>statistics_features: list[str]\n</code></pre> <p>List of feature names that require statistics.</p>"},{"location":"python-api/hsfs/hopsworks_udf/#hsfs.hopsworks_udf.HopsworksUdf.dropped_features","title":"[source]  dropped_features  <code>property</code> <code>writable</code>","text":"<pre><code>dropped_features: list[str]\n</code></pre> <p>List of features that will be dropped after the UDF is applied.</p>"},{"location":"python-api/hsfs/hopsworks_udf/#hsfs.hopsworks_udf.HopsworksUdf.transformation_context","title":"[source]  transformation_context  <code>property</code> <code>writable</code>","text":"<pre><code>transformation_context: dict[str, Any]\n</code></pre> <p>Dictionary that contains the context variables required for the UDF.</p> <p>These context variables passed to the UDF during execution.</p>"},{"location":"python-api/hsfs/hopsworks_udf/#hsfs.hopsworks_udf.HopsworksUdf.alias","title":"[source]  alias","text":"<pre><code>alias(*args: str)\n</code></pre> <p>Set the names of the transformed features output by the UDF.</p>"},{"location":"python-api/hsfs/hopsworks_udf/#hsfs.hopsworks_udf.HopsworksUdf.execute","title":"[source]  execute","text":"<pre><code>execute(*args) -&gt; Any\n</code></pre> <p>Execute the UDF directly with the provided arguments.</p> <p>This is a convenience method for quick testing of simple UDFs that don't require statistics or transformation context. It executes the UDF in offline mode (batch processing).</p> <p>Quick UDF testing</p> <pre><code>@udf(return_type=float)\ndef add_one(value):\n    return value + 1\n\n# Direct execution for simple tests\nresult = add_one.execute(pd.Series([1.0, 2.0, 3.0]))\nassert result.tolist() == [2.0, 3.0, 4.0]\n</code></pre> <p>Note</p> <p>For UDFs that require statistics or transformation context or need to be executed in online mode, use [<code>executor()</code>][hsfs.hopsworks_udf.HopsworksUdf.executor] instead: <pre><code>result = my_udf.executor(statistics=stats, context=ctx).execute(data)\n</code></pre></p> PARAMETER DESCRIPTION <code>*args</code> <p>Input arguments matching the UDF's parameter signature. For batch processing, pass pandas Series or DataFrames.</p> <p> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>The transformed values.</p> <code>Any</code> <ul> <li>pd.Series - Single output Pandas UDFs.</li> </ul> <code>Any</code> <ul> <li>pd.DataFrame - Multi-output Pandas UDFs.</li> </ul> <code>Any</code> <ul> <li>int | float | str | bool | datetime | time | date - Single output Python UDFs.</li> </ul> <code>Any</code> <ul> <li>tuple[int | float | str | bool | datetime | time | date] - Multi-output Python UDFs.</li> </ul>"},{"location":"python-api/hsfs/hopsworks_udf/#hsfs.hopsworks_udf.HopsworksUdf.json","title":"[source]  json","text":"<pre><code>json() -&gt; str\n</code></pre> <p>Convert class into its json serialized form.</p> RETURNS DESCRIPTION <code>str</code> <p>JSON serialized object.</p>"},{"location":"python-api/hsfs/hopsworks_udf/#hsfs.hopsworks_udf.HopsworksUdf.from_response_json","title":"[source]  from_response_json  <code>classmethod</code>","text":"<pre><code>from_response_json(\n    json_dict: dict[str, Any],\n) -&gt; HopsworksUdf\n</code></pre> <p>Function that constructs the class object from its json serialization.</p> PARAMETER DESCRIPTION <code>json_dict</code> <p>JSON serialized dictionary for the class.</p> <p> TYPE: <code>dict[str, Any]</code> </p> RETURNS DESCRIPTION <code>HopsworksUdf</code> <p>JSON deserialized class object.</p>"},{"location":"python-api/hsfs/hopsworks_udf/#hsfs.hopsworks_udf.TransformationFeature","title":"[source]  TransformationFeature  <code>dataclass</code>","text":"<p>Mapping of feature names to their corresponding statistics argument names in the code.</p> <p>The statistic_argument_name for a feature name would be None if the feature does not need statistics.</p> PARAMETER DESCRIPTION <code>feature_name</code> <p>Name of the feature.</p> <p> TYPE: <code>str</code> </p> <code>statistic_argument_name</code> <p>Name of the statistics argument in the code for the feature specified in the feature name.</p> <p> TYPE: <code>str | None</code> </p>"},{"location":"python-api/hsfs/split_statistics/","title":"hsfs.split_statistics","text":""},{"location":"python-api/hsfs/split_statistics/#hsfs.split_statistics","title":"hsfs.split_statistics","text":""},{"location":"python-api/hsfs/split_statistics/#hsfs.split_statistics.SplitStatistics","title":"[source]  SplitStatistics","text":""},{"location":"python-api/hsfs/split_statistics/#hsfs.split_statistics.SplitStatistics.name","title":"[source]  name  <code>property</code>","text":"<p>Name of the training dataset split.</p>"},{"location":"python-api/hsfs/split_statistics/#hsfs.split_statistics.SplitStatistics.feature_descriptive_statistics","title":"[source]  feature_descriptive_statistics  <code>property</code>","text":"<p>List of feature descriptive statistics.</p>"},{"location":"python-api/hsfs/statistics/","title":"hsfs.statistics","text":""},{"location":"python-api/hsfs/statistics/#hsfs.statistics","title":"hsfs.statistics","text":""},{"location":"python-api/hsfs/statistics/#hsfs.statistics.Statistics","title":"[source]  Statistics","text":""},{"location":"python-api/hsfs/statistics/#hsfs.statistics.Statistics.computation_time","title":"[source]  computation_time  <code>property</code>","text":"<pre><code>computation_time: int\n</code></pre> <p>Time at which the statistics were computed.</p>"},{"location":"python-api/hsfs/statistics/#hsfs.statistics.Statistics.row_percentage","title":"[source]  row_percentage  <code>property</code> <code>writable</code>","text":"<pre><code>row_percentage: float\n</code></pre> <p>Percentage of data on which statistics were computed.</p>"},{"location":"python-api/hsfs/statistics/#hsfs.statistics.Statistics.feature_descriptive_statistics","title":"[source]  feature_descriptive_statistics  <code>property</code>","text":"<pre><code>feature_descriptive_statistics: (\n    list[FeatureDescriptiveStatistics] | None\n)\n</code></pre> <p>List of feature descriptive statistics.</p>"},{"location":"python-api/hsfs/statistics/#hsfs.statistics.Statistics.feature_group_id","title":"[source]  feature_group_id  <code>property</code>","text":"<pre><code>feature_group_id: int | None\n</code></pre> <p>Id of the feature group on whose data the statistics were computed.</p>"},{"location":"python-api/hsfs/statistics/#hsfs.statistics.Statistics.feature_view_name","title":"[source]  feature_view_name  <code>property</code>","text":"<pre><code>feature_view_name: str | None\n</code></pre> <p>Name of the feature view whose query was used to retrieve the data on which the statistics were computed.</p>"},{"location":"python-api/hsfs/statistics/#hsfs.statistics.Statistics.feature_view_version","title":"[source]  feature_view_version  <code>property</code>","text":"<pre><code>feature_view_version: int | None\n</code></pre> <p>Id of the feature view whose query was used to retrieve the data on which the statistics were computed.</p>"},{"location":"python-api/hsfs/statistics/#hsfs.statistics.Statistics.window_start_commit_time","title":"[source]  window_start_commit_time  <code>property</code>","text":"<pre><code>window_start_commit_time: int | None\n</code></pre> <p>Start time of the window of data on which statistics were computed.</p>"},{"location":"python-api/hsfs/statistics/#hsfs.statistics.Statistics.window_end_commit_time","title":"[source]  window_end_commit_time  <code>property</code>","text":"<pre><code>window_end_commit_time: int | None\n</code></pre> <p>End time of the window of data on which statistics were computed.</p>"},{"location":"python-api/hsfs/statistics/#hsfs.statistics.Statistics.training_dataset_version","title":"[source]  training_dataset_version  <code>property</code>","text":"<pre><code>training_dataset_version: int | None\n</code></pre> <p>Version of the training dataset on which statistics were computed.</p>"},{"location":"python-api/hsfs/statistics/#hsfs.statistics.Statistics.split_statistics","title":"[source]  split_statistics  <code>property</code>","text":"<pre><code>split_statistics: list[SplitStatistics] | None\n</code></pre> <p>List of statistics computed on each split of a training dataset.</p>"},{"location":"python-api/hsfs/statistics/#hsfs.statistics.Statistics.before_transformation","title":"[source]  before_transformation  <code>property</code>","text":"<pre><code>before_transformation: bool\n</code></pre> <p>Whether or not the statistics were computed on feature values before applying model-dependent transformations.</p>"},{"location":"python-api/hsfs/statistics_config/","title":"hsfs.statistics_config","text":""},{"location":"python-api/hsfs/statistics_config/#hsfs.statistics_config","title":"hsfs.statistics_config","text":""},{"location":"python-api/hsfs/statistics_config/#hsfs.statistics_config.StatisticsConfig","title":"[source]  StatisticsConfig","text":""},{"location":"python-api/hsfs/statistics_config/#hsfs.statistics_config.StatisticsConfig.enabled","title":"[source]  enabled  <code>property</code> <code>writable</code>","text":"<p>Enable statistics, by default this computes only descriptive statistics.</p>"},{"location":"python-api/hsfs/statistics_config/#hsfs.statistics_config.StatisticsConfig.correlations","title":"[source]  correlations  <code>property</code> <code>writable</code>","text":"<p>Enable correlations as an additional statistic to be computed for each feature pair.</p>"},{"location":"python-api/hsfs/statistics_config/#hsfs.statistics_config.StatisticsConfig.histograms","title":"[source]  histograms  <code>property</code> <code>writable</code>","text":"<p>Enable histograms as an additional statistic to be computed for each feature.</p>"},{"location":"python-api/hsfs/statistics_config/#hsfs.statistics_config.StatisticsConfig.exact_uniqueness","title":"[source]  exact_uniqueness  <code>property</code> <code>writable</code>","text":"<p>Enable exact uniqueness as an additional statistic to be computed for each feature.</p>"},{"location":"python-api/hsfs/statistics_config/#hsfs.statistics_config.StatisticsConfig.columns","title":"[source]  columns  <code>property</code> <code>writable</code>","text":"<p>Specify a subset of columns to compute statistics for.</p>"},{"location":"python-api/hsfs/storage_connector/","title":"hsfs.storage_connector","text":""},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector","title":"hsfs.storage_connector","text":""},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.AdlsConnector","title":"[source]  AdlsConnector","text":"<p>               Bases: <code>StorageConnector</code></p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.AdlsConnector.generation","title":"[source]  generation  <code>property</code>","text":"<pre><code>generation: str | None\n</code></pre> <p>Generation of the ADLS storage connector.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.AdlsConnector.directory_id","title":"[source]  directory_id  <code>property</code>","text":"<pre><code>directory_id: str | None\n</code></pre> <p>Directory ID of the ADLS storage connector.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.AdlsConnector.application_id","title":"[source]  application_id  <code>property</code>","text":"<pre><code>application_id: str | None\n</code></pre> <p>Application ID of the ADLS storage connector.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.AdlsConnector.account_name","title":"[source]  account_name  <code>property</code>","text":"<pre><code>account_name: str | None\n</code></pre> <p>Account name of the ADLS storage connector.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.AdlsConnector.container_name","title":"[source]  container_name  <code>property</code>","text":"<pre><code>container_name: str | None\n</code></pre> <p>Container name of the ADLS storage connector.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.AdlsConnector.service_credential","title":"[source]  service_credential  <code>property</code>","text":"<pre><code>service_credential: str | None\n</code></pre> <p>Service credential of the ADLS storage connector.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.AdlsConnector.path","title":"[source]  path  <code>property</code>","text":"<pre><code>path: str | None\n</code></pre> <p>If the connector refers to a path (e.g. ADLS) - return the path of the connector.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.AdlsConnector.spark_options","title":"[source]  spark_options","text":"<pre><code>spark_options() -&gt; dict[str, Any]\n</code></pre> <p>Return prepared options to be passed to Spark, based on the additional arguments.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.AdlsConnector.prepare_spark","title":"[source]  prepare_spark","text":"<pre><code>prepare_spark(path: str | None = None) -&gt; str | None\n</code></pre> <p>Prepare Spark to use this Storage Connector.</p> <pre><code>conn.prepare_spark()\n\nspark.read.format(\"json\").load(\"abfss://[container-name]@[account_name].dfs.core.windows.net/[path]\")\n\n# or\nspark.read.format(\"json\").load(conn.prepare_spark(\"abfss://[container-name]@[account_name].dfs.core.windows.net/[path]\"))\n</code></pre> PARAMETER DESCRIPTION <code>path</code> <p>Path to prepare for reading from cloud storage. Defaults to <code>None</code>.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.AdlsConnector.read","title":"[source]  read","text":"<pre><code>read(\n    query: str | None = None,\n    data_format: str | None = None,\n    options: dict[str, Any] | None = None,\n    path: str = \"\",\n    dataframe_type: str = \"default\",\n) -&gt; (\n    TypeVar(\"pyspark.sql.DataFrame\")\n    | TypeVar(\"pyspark.RDD\")\n    | pd.DataFrame\n    | np.ndarray\n    | pl.DataFrame\n)\n</code></pre> <p>Reads a path into a dataframe using the storage connector.</p> PARAMETER DESCRIPTION <code>query</code> <p>Not relevant for ADLS connectors.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>data_format</code> <p>The file format of the files to be read, e.g. <code>csv</code>, <code>parquet</code>.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>options</code> <p>Any additional key/value options to be passed to the ADLS connector.</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>path</code> <p>Path within the bucket to be read. For example, path=<code>path</code> will read directly from the container specified on connector by constructing the URI as 'abfss://[container-name]@[account_name].dfs.core.windows.net/[path]'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>dataframe_type</code> <p>str, optional. The type of the returned dataframe. Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>. Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default'</code> </p> RETURNS DESCRIPTION <code>TypeVar('pyspark.sql.DataFrame') | TypeVar('pyspark.RDD') | pd.DataFrame | np.ndarray | pl.DataFrame</code> <p><code>DataFrame</code>.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.BigQueryConnector","title":"[source]  BigQueryConnector","text":"<p>               Bases: <code>StorageConnector</code></p> <p>The BigQuery storage connector provides integration to Google Cloud BigQuery.</p> <p>You can use it to run bigquery on your GCP cluster and load results into spark dataframe by calling the <code>read</code> API.</p> <p>Authentication to GCP is handled by uploading the <code>JSON keyfile for service account</code> to the Hopsworks Project. For more information on service accounts and creating keyfile in GCP, read Google Cloud documentation.</p> <p>The storage connector uses the Google <code>spark-bigquery-connector</code> behind the scenes. To read more about the spark connector, like the spark options or usage, check Apache Spark SQL connector for Google BigQuery.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.BigQueryConnector.key_path","title":"[source]  key_path  <code>property</code>","text":"<pre><code>key_path: str | None\n</code></pre> <p>JSON keyfile for service account.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.BigQueryConnector.parent_project","title":"[source]  parent_project  <code>property</code>","text":"<pre><code>parent_project: str | None\n</code></pre> <p>BigQuery parent project (Google Cloud Project ID of the table to bill for the export).</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.BigQueryConnector.dataset","title":"[source]  dataset  <code>property</code>","text":"<pre><code>dataset: str | None\n</code></pre> <p>BigQuery dataset (The dataset containing the table).</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.BigQueryConnector.query_table","title":"[source]  query_table  <code>property</code>","text":"<pre><code>query_table: str | None\n</code></pre> <p>BigQuery table name.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.BigQueryConnector.query_project","title":"[source]  query_project  <code>property</code>","text":"<pre><code>query_project: str | None\n</code></pre> <p>BigQuery project (The Google Cloud Project ID of the table).</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.BigQueryConnector.materialization_dataset","title":"[source]  materialization_dataset  <code>property</code>","text":"<pre><code>materialization_dataset: str | None\n</code></pre> <p>BigQuery materialization dataset (The dataset where the materialized view is going to be created, used in case of query).</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.BigQueryConnector.arguments","title":"[source]  arguments  <code>property</code>","text":"<pre><code>arguments: dict[str, Any]\n</code></pre> <p>Additional spark options.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.BigQueryConnector.connector_options","title":"[source]  connector_options","text":"<pre><code>connector_options() -&gt; dict[str, Any]\n</code></pre> <p>Return options to be passed to an external BigQuery connector library.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.BigQueryConnector.spark_options","title":"[source]  spark_options","text":"<pre><code>spark_options() -&gt; dict[str, Any]\n</code></pre> <p>Return spark options to be set for BigQuery spark connector.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.BigQueryConnector.read","title":"[source]  read","text":"<pre><code>read(\n    query: str | None = None,\n    data_format: str | None = None,\n    options: dict[str, Any] | None = None,\n    path: str | None = None,\n    dataframe_type: str = \"default\",\n) -&gt; (\n    TypeVar(\"pyspark.sql.DataFrame\")\n    | TypeVar(\"pyspark.RDD\")\n    | pd.DataFrame\n    | np.ndarray\n    | pl.DataFrame\n)\n</code></pre> <p>Reads results from BigQuery into a spark dataframe using the storage connector.</p> <p>Reading from bigquery is done via either specifying the BigQuery table or BigQuery query.   For example, to read from a BigQuery table, set the BigQuery project, dataset and table on storage connector   and read directly from the corresponding path.     <pre><code>conn.read()\n</code></pre>   OR, to read results from a BigQuery query, set <code>Materialization Dataset</code> on storage connector,    and pass your SQL to <code>query</code> argument.     <pre><code>conn.read(query='SQL')\n</code></pre>   Optionally, passing <code>query</code> argument will take priority at runtime if the table options were also set   on the storage connector. This allows user to run from both a query or table with same connector, assuming   all fields were set.   Also, user can set the <code>path</code> argument to a bigquery table path to read at runtime,    if table options were not set initially while creating the connector.     <pre><code>conn.read(path='project.dataset.table')\n</code></pre></p> PARAMETER DESCRIPTION <code>query</code> <p>BigQuery query. Defaults to <code>None</code>.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>data_format</code> <p>Spark data format. Defaults to <code>None</code>.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>options</code> <p>Spark options. Defaults to <code>None</code>.</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>path</code> <p>BigQuery table path. Defaults to <code>None</code>.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>dataframe_type</code> <p>str, optional. The type of the returned dataframe. Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>. Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default'</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>Malformed arguments.</p> RETURNS DESCRIPTION <code>TypeVar('pyspark.sql.DataFrame') | TypeVar('pyspark.RDD') | pd.DataFrame | np.ndarray | pl.DataFrame</code> <p><code>Dataframe</code>: A Spark dataframe.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.GcsConnector","title":"[source]  GcsConnector","text":"<p>               Bases: <code>StorageConnector</code></p> <p>This storage connector provides integration to Google Cloud Storage (GCS).</p> <p>Once you create a connector in FeatureStore, you can transact data from a GCS bucket into a spark dataframe by calling the <code>read</code> API.</p> <p>Authentication to GCP is handled by uploading the <code>JSON keyfile for service account</code> to the Hopsworks Project. For more information on service accounts and creating keyfile in GCP, read Google Cloud documentation.</p> <p>The connector also supports the optional encryption method <code>Customer Supplied Encryption Key</code> by Google. The encryption details are stored as <code>Secrets</code> in the FeatureStore for keeping it secure. Read more about encryption on Google Documentation.</p> <p>The storage connector uses the Google <code>gcs-connector-hadoop</code> behind the scenes. For more information, check out Google Cloud Storage Connector for Spark and Hadoop.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.GcsConnector.key_path","title":"[source]  key_path  <code>property</code>","text":"<pre><code>key_path: str | None\n</code></pre> <p>JSON keyfile for service account.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.GcsConnector.algorithm","title":"[source]  algorithm  <code>property</code>","text":"<pre><code>algorithm: str | None\n</code></pre> <p>Encryption Algorithm.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.GcsConnector.encryption_key","title":"[source]  encryption_key  <code>property</code>","text":"<pre><code>encryption_key: str | None\n</code></pre> <p>Encryption Key.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.GcsConnector.encryption_key_hash","title":"[source]  encryption_key_hash  <code>property</code>","text":"<pre><code>encryption_key_hash: str | None\n</code></pre> <p>Encryption Key Hash.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.GcsConnector.path","title":"[source]  path  <code>property</code>","text":"<pre><code>path: str | None\n</code></pre> <p>The path of the connector along with gs file system prefixed.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.GcsConnector.bucket","title":"[source]  bucket  <code>property</code>","text":"<pre><code>bucket: str | None\n</code></pre> <p>GCS Bucket.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.GcsConnector.spark_options","title":"[source]  spark_options","text":"<pre><code>spark_options() -&gt; dict[str, Any]\n</code></pre> <p>Return prepared options to be passed to Spark, based on the additional arguments.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.GcsConnector.read","title":"[source]  read","text":"<pre><code>read(\n    query: str | None = None,\n    data_format: str | None = None,\n    options: dict[str, Any] | None = None,\n    path: str = \"\",\n    dataframe_type: str = \"default\",\n) -&gt; (\n    TypeVar(\"pyspark.sql.DataFrame\")\n    | TypeVar(\"pyspark.RDD\")\n    | pd.DataFrame\n    | np.ndarray\n    | pl.DataFrame\n)\n</code></pre> <p>Reads GCS path into a dataframe using the storage connector.</p> <p>To read directly from the default bucket, you can omit the path argument: <pre><code>conn.read(data_format='spark_formats')\n</code></pre> Or to read objects from default bucket provide the object path without gsUtil URI schema. For example, following will read from a path gs://bucket_on_connector/Path/object : <pre><code>conn.read(data_format='spark_formats', paths='Path/object')\n</code></pre> Or to read with full gsUtil URI path, <pre><code>conn.read(data_format='spark_formats',path='gs://BUCKET/DATA')\n</code></pre> Parameters:     query: Not relevant for GCS connectors.     data_format: Spark data format. Defaults to <code>None</code>.     options: Spark options. Defaults to <code>None</code>.     path: GCS path. Defaults to <code>None</code>.     dataframe_type: str, optional. The type of the returned dataframe.         Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>.         Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</p> RAISES DESCRIPTION <code>ValueError</code> <p>Malformed arguments.</p> RETURNS DESCRIPTION <code>TypeVar('pyspark.sql.DataFrame') | TypeVar('pyspark.RDD') | pd.DataFrame | np.ndarray | pl.DataFrame</code> <p><code>Dataframe</code>: A Spark dataframe.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.GcsConnector.prepare_spark","title":"[source]  prepare_spark","text":"<pre><code>prepare_spark(path: str | None = None) -&gt; str | None\n</code></pre> <p>Prepare Spark to use this Storage Connector.</p> <pre><code>conn.prepare_spark()\nspark.read.format(\"json\").load(\"gs://bucket/path\")\n# or\nspark.read.format(\"json\").load(conn.prepare_spark(\"gs://bucket/path\"))\n</code></pre> PARAMETER DESCRIPTION <code>path</code> <p>Path to prepare for reading from Google cloud storage. Defaults to <code>None</code>.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.HopsFSConnector","title":"[source]  HopsFSConnector","text":"<p>               Bases: <code>StorageConnector</code></p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.HopsFSConnector.spark_options","title":"[source]  spark_options","text":"<pre><code>spark_options() -&gt; dict[str, Any]\n</code></pre> <p>Return prepared options to be passed to Spark, based on the additional arguments.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.JdbcConnector","title":"[source]  JdbcConnector","text":"<p>               Bases: <code>StorageConnector</code></p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.JdbcConnector.connection_string","title":"[source]  connection_string  <code>property</code>","text":"<pre><code>connection_string: str | None\n</code></pre> <p>JDBC connection string.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.JdbcConnector.arguments","title":"[source]  arguments  <code>property</code>","text":"<pre><code>arguments: dict[str, Any] | None\n</code></pre> <p>Additional JDBC arguments.</p> <p>When running hsfs with PySpark/Spark in Hopsworks, the driver is automatically provided in the classpath but you need to set the <code>driver</code> argument to <code>com.mysql.cj.jdbc.Driver</code> when creating the Storage Connector.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.JdbcConnector.spark_options","title":"[source]  spark_options","text":"<pre><code>spark_options() -&gt; dict[str, Any]\n</code></pre> <p>Return prepared options to be passed to Spark, based on the additional arguments.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.JdbcConnector.read","title":"[source]  read","text":"<pre><code>read(\n    query: str,\n    data_format: str | None = None,\n    options: dict[str, Any] | None = None,\n    path: str | None = None,\n    dataframe_type: str = \"default\",\n) -&gt; (\n    TypeVar(\"pyspark.sql.DataFrame\")\n    | TypeVar(\"pyspark.RDD\")\n    | pd.DataFrame\n    | np.ndarray\n    | pl.DataFrame\n)\n</code></pre> <p>Reads a query into a dataframe using the storage connector.</p> PARAMETER DESCRIPTION <code>query</code> <p>A SQL query to be read.</p> <p> TYPE: <code>str</code> </p> <code>data_format</code> <p>Not relevant for JDBC based connectors.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>options</code> <p>Any additional key/value options to be passed to the JDBC connector.</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>path</code> <p>Not relevant for JDBC based connectors.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>dataframe_type</code> <p>str, optional. The type of the returned dataframe. Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>. Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default'</code> </p> RETURNS DESCRIPTION <code>TypeVar('pyspark.sql.DataFrame') | TypeVar('pyspark.RDD') | pd.DataFrame | np.ndarray | pl.DataFrame</code> <p><code>DataFrame</code>.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.KafkaConnector","title":"[source]  KafkaConnector","text":"<p>               Bases: <code>StorageConnector</code></p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.KafkaConnector.bootstrap_servers","title":"[source]  bootstrap_servers  <code>property</code>","text":"<pre><code>bootstrap_servers: list[str] | None\n</code></pre> <p>Bootstrap servers string.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.KafkaConnector.security_protocol","title":"[source]  security_protocol  <code>property</code>","text":"<pre><code>security_protocol: str | None\n</code></pre> <p>Bootstrap servers string.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.KafkaConnector.ssl_truststore_location","title":"[source]  ssl_truststore_location  <code>property</code>","text":"<pre><code>ssl_truststore_location: str | None\n</code></pre> <p>Bootstrap servers string.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.KafkaConnector.ssl_keystore_location","title":"[source]  ssl_keystore_location  <code>property</code>","text":"<pre><code>ssl_keystore_location: str | None\n</code></pre> <p>Bootstrap servers string.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.KafkaConnector.ssl_endpoint_identification_algorithm","title":"[source]  ssl_endpoint_identification_algorithm  <code>property</code>","text":"<pre><code>ssl_endpoint_identification_algorithm: str | None\n</code></pre> <p>Bootstrap servers string.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.KafkaConnector.options","title":"[source]  options  <code>property</code>","text":"<pre><code>options: dict[str, Any]\n</code></pre> <p>Bootstrap servers string.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.KafkaConnector.create_pem_files","title":"[source]  create_pem_files","text":"<pre><code>create_pem_files(kafka_options: dict[str, Any]) -&gt; None\n</code></pre> <p>Create PEM (Privacy Enhanced Mail) files for Kafka SSL authentication.</p> <p>This method writes the necessary PEM files for SSL authentication with Kafka, using the provided keystore and truststore locations and passwords. The generated file paths are stored as the following instance variables:</p> <pre><code>- self.ca_chain_path: Path to the generated CA chain PEM file.\n- self.client_cert_path: Path to the generated client certificate PEM file.\n- self.client_key_path: Path to the generated client key PEM file.\n</code></pre> <p>These files are used for configuring secure Kafka connections (e.g., with Spark or confluent_kafka). The method is idempotent and will only create the files once per connector instance.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.KafkaConnector.kafka_options","title":"[source]  kafka_options","text":"<pre><code>kafka_options(distribute=True) -&gt; dict[str, Any]\n</code></pre> <p>Return prepared options to be passed to kafka, based on the additional arguments.</p> <p>See https://kafka.apache.org/documentation/.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.KafkaConnector.confluent_options","title":"[source]  confluent_options","text":"<pre><code>confluent_options() -&gt; dict[str, Any]\n</code></pre> <p>Return prepared options to be passed to confluent_kafka, based on the provided apache spark configuration.</p> <p>Right now only producer values with Importance &gt;= medium are implemented.</p> <p>See https://docs.confluent.io/platform/current/clients/librdkafka/html/md_CONFIGURATION.html.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.KafkaConnector.spark_options","title":"[source]  spark_options","text":"<pre><code>spark_options() -&gt; dict[str, Any]\n</code></pre> <p>Return prepared options to be passed to Spark, based on the additional arguments.</p> <p>This is done by just adding 'kafka.' prefix to kafka_options.</p> <p>See https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html#kafka-specific-configurations.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.KafkaConnector.read","title":"[source]  read","text":"<pre><code>read(\n    query: str | None = None,\n    data_format: str | None = None,\n    options: dict[str, Any] | None = None,\n    path: str | None = None,\n    dataframe_type: str = \"default\",\n) -&gt; None\n</code></pre> <p>NOT SUPPORTED.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.KafkaConnector.read_stream","title":"[source]  read_stream","text":"<pre><code>read_stream(\n    topic: str,\n    topic_pattern: bool = False,\n    message_format: str = \"avro\",\n    schema: str | None = None,\n    options: dict[str, Any] | None = None,\n    include_metadata: bool = False,\n) -&gt; TypeVar(\"pyspark.sql.DataFrame\") | TypeVar(\n    \"pyspark.sql.streaming.StreamingQuery\"\n)\n</code></pre> <p>Reads a Kafka stream from a topic or multiple topics into a Dataframe.</p> Engine Support <p>Spark only</p> <p>Reading from data streams using Pandas/Python as engine is currently not supported. Python/Pandas has no notion of streaming.</p> PARAMETER DESCRIPTION <code>topic</code> <p>Name or pattern of the topic(s) to subscribe to.</p> <p> TYPE: <code>str</code> </p> <code>topic_pattern</code> <p>Flag to indicate if <code>topic</code> string is a pattern. Defaults to <code>False</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>message_format</code> <p>The format of the messages to use for decoding. Can be <code>\"avro\"</code> or <code>\"json\"</code>. Defaults to <code>\"avro\"</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'avro'</code> </p> <code>schema</code> <p>Optional schema, to use for decoding, can be an Avro schema string for <code>\"avro\"</code> message format, or for JSON encoding a Spark StructType schema, or a DDL formatted string. Defaults to <code>None</code>.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>options</code> <p>Additional options as key/value string pairs to be passed to Spark. Defaults to <code>{}</code>.</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>include_metadata</code> <p>Indicate whether to return additional metadata fields from messages in the stream. Otherwise, only the decoded value fields are returned. Defaults to <code>False</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>Malformed arguments.</p> RETURNS DESCRIPTION <code>TypeVar('pyspark.sql.DataFrame') | TypeVar('pyspark.sql.streaming.StreamingQuery')</code> <p><code>StreamingDataframe</code>: A Spark streaming dataframe.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.RedshiftConnector","title":"[source]  RedshiftConnector","text":"<p>               Bases: <code>StorageConnector</code></p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.RedshiftConnector.cluster_identifier","title":"[source]  cluster_identifier  <code>property</code>","text":"<pre><code>cluster_identifier: str | None\n</code></pre> <p>Cluster identifier for redshift cluster.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.RedshiftConnector.database_driver","title":"[source]  database_driver  <code>property</code>","text":"<pre><code>database_driver: str | None\n</code></pre> <p>Database endpoint for redshift cluster.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.RedshiftConnector.database_endpoint","title":"[source]  database_endpoint  <code>property</code>","text":"<pre><code>database_endpoint: str | None\n</code></pre> <p>Database endpoint for redshift cluster.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.RedshiftConnector.database_name","title":"[source]  database_name  <code>property</code>","text":"<pre><code>database_name: str | None\n</code></pre> <p>Database name for redshift cluster.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.RedshiftConnector.database_port","title":"[source]  database_port  <code>property</code>","text":"<pre><code>database_port: int | str | None\n</code></pre> <p>Database port for redshift cluster.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.RedshiftConnector.table_name","title":"[source]  table_name  <code>property</code>","text":"<pre><code>table_name: str | None\n</code></pre> <p>Table name for redshift cluster.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.RedshiftConnector.database_user_name","title":"[source]  database_user_name  <code>property</code>","text":"<pre><code>database_user_name: str | None\n</code></pre> <p>Database username for redshift cluster.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.RedshiftConnector.auto_create","title":"[source]  auto_create  <code>property</code>","text":"<pre><code>auto_create: bool | None\n</code></pre> <p>Database username for redshift cluster.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.RedshiftConnector.database_group","title":"[source]  database_group  <code>property</code>","text":"<pre><code>database_group: str | None\n</code></pre> <p>Database username for redshift cluster.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.RedshiftConnector.database_password","title":"[source]  database_password  <code>property</code>","text":"<pre><code>database_password: str | None\n</code></pre> <p>Database password for redshift cluster.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.RedshiftConnector.iam_role","title":"[source]  iam_role  <code>property</code>","text":"<pre><code>iam_role: Any | None\n</code></pre> <p>IAM role.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.RedshiftConnector.expiration","title":"[source]  expiration  <code>property</code>","text":"<pre><code>expiration: int | str | None\n</code></pre> <p>Cluster temporary credential expiration time.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.RedshiftConnector.arguments","title":"[source]  arguments  <code>property</code>","text":"<pre><code>arguments: str | None\n</code></pre> <p>Additional JDBC, REDSHIFT, or Snowflake arguments.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.RedshiftConnector.connector_options","title":"[source]  connector_options","text":"<pre><code>connector_options() -&gt; dict[str, Any]\n</code></pre> <p>Return options to be passed to an external Redshift connector library.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.RedshiftConnector.spark_options","title":"[source]  spark_options","text":"<pre><code>spark_options() -&gt; dict[str, Any]\n</code></pre> <p>Return prepared options to be passed to Spark, based on the additional arguments.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.RedshiftConnector.read","title":"[source]  read","text":"<pre><code>read(\n    query: str | None = None,\n    data_format: str | None = None,\n    options: dict[str, Any] | None = None,\n    path: str | None = None,\n    dataframe_type: str = \"default\",\n) -&gt; (\n    TypeVar(\"pyspark.sql.DataFrame\")\n    | TypeVar(\"pyspark.RDD\")\n    | pd.DataFrame\n    | np.ndarray\n    | pl.DataFrame\n)\n</code></pre> <p>Reads a table or query into a dataframe using the storage connector.</p> PARAMETER DESCRIPTION <code>query</code> <p>By default, the storage connector will read the table configured together with the connector, if any. It's possible to overwrite this by passing a SQL query here. Defaults to <code>None</code>.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>data_format</code> <p>Not relevant for JDBC based connectors such as Redshift.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>options</code> <p>Any additional key/value options to be passed to the JDBC connector.</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>path</code> <p>Not relevant for JDBC based connectors such as Redshift.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>dataframe_type</code> <p>str, optional. The type of the returned dataframe. Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>. Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default'</code> </p> RETURNS DESCRIPTION <code>TypeVar('pyspark.sql.DataFrame') | TypeVar('pyspark.RDD') | pd.DataFrame | np.ndarray | pl.DataFrame</code> <p><code>DataFrame</code>.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.RedshiftConnector.refetch","title":"[source]  refetch","text":"<pre><code>refetch() -&gt; None\n</code></pre> <p>Refetch storage connector in order to retrieve updated temporary credentials.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.S3Connector","title":"[source]  S3Connector","text":"<p>               Bases: <code>StorageConnector</code></p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.S3Connector.access_key","title":"[source]  access_key  <code>property</code>","text":"<pre><code>access_key: str | None\n</code></pre> <p>Access key.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.S3Connector.secret_key","title":"[source]  secret_key  <code>property</code>","text":"<pre><code>secret_key: str | None\n</code></pre> <p>Secret key.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.S3Connector.server_encryption_algorithm","title":"[source]  server_encryption_algorithm  <code>property</code>","text":"<pre><code>server_encryption_algorithm: str | None\n</code></pre> <p>Encryption algorithm if server-side S3 bucket encryption is enabled.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.S3Connector.server_encryption_key","title":"[source]  server_encryption_key  <code>property</code>","text":"<pre><code>server_encryption_key: str | None\n</code></pre> <p>Encryption key if server-side S3 bucket encryption is enabled.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.S3Connector.bucket","title":"[source]  bucket  <code>property</code>","text":"<pre><code>bucket: str | None\n</code></pre> <p>Return the bucket for S3 connectors.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.S3Connector.region","title":"[source]  region  <code>property</code>","text":"<pre><code>region: str | None\n</code></pre> <p>Return the region for S3 connectors.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.S3Connector.session_token","title":"[source]  session_token  <code>property</code>","text":"<pre><code>session_token: str | None\n</code></pre> <p>Session token.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.S3Connector.iam_role","title":"[source]  iam_role  <code>property</code>","text":"<pre><code>iam_role: str | None\n</code></pre> <p>IAM role.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.S3Connector.path","title":"[source]  path  <code>property</code>","text":"<pre><code>path: str | None\n</code></pre> <p>If the connector refers to a path (e.g. S3) - return the path of the connector.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.S3Connector.arguments","title":"[source]  arguments  <code>property</code>","text":"<pre><code>arguments: dict[str, Any] | None\n</code></pre> <p>Additional spark options for the S3 connector, passed as a dictionary.</p> <p>These are set using the <code>Spark Options</code> field in the UI when creating the connector. Example: <code>{\"fs.s3a.endpoint\": \"s3.eu-west-1.amazonaws.com\", \"fs.s3a.path.style.access\": \"true\"}</code>.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.S3Connector.spark_options","title":"[source]  spark_options","text":"<pre><code>spark_options() -&gt; dict[str, str]\n</code></pre> <p>Return prepared options to be passed to Spark, based on the additional arguments.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.S3Connector.prepare_spark","title":"[source]  prepare_spark","text":"<pre><code>prepare_spark(path: str | None = None) -&gt; str | None\n</code></pre> <p>Prepare Spark to use this Storage Connector.</p> <pre><code>conn.prepare_spark()\n\nspark.read.format(\"json\").load(\"s3a://[bucket]/path\")\n\n# or\nspark.read.format(\"json\").load(conn.prepare_spark(\"s3a://[bucket]/path\"))\n</code></pre> PARAMETER DESCRIPTION <code>path</code> <p>Path to prepare for reading from cloud storage.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.S3Connector.connector_options","title":"[source]  connector_options","text":"<pre><code>connector_options() -&gt; dict[str, Any]\n</code></pre> <p>Return options to be passed to an external S3 connector library.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.S3Connector.read","title":"[source]  read","text":"<pre><code>read(\n    query: str | None = None,\n    data_format: str | None = None,\n    options: dict[str, Any] | None = None,\n    path: str = \"\",\n    dataframe_type: Literal[\n        \"default\",\n        \"spark\",\n        \"pandas\",\n        \"polars\",\n        \"numpy\",\n        \"python\",\n    ] = \"default\",\n) -&gt; (\n    TypeVar(\"pyspark.sql.DataFrame\")\n    | TypeVar(\"pyspark.RDD\")\n    | pd.DataFrame\n    | np.ndarray\n    | pl.DataFrame\n)\n</code></pre> <p>Reads a query or a path into a dataframe using the storage connector.</p> <p>Note, paths are only supported for object stores like S3, HopsFS and ADLS, while queries are meant for JDBC or databases like Redshift and Snowflake.</p> PARAMETER DESCRIPTION <code>query</code> <p>Not relevant for S3 connectors.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>data_format</code> <p>The file format of the files to be read, e.g. <code>csv</code>, <code>parquet</code>.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>options</code> <p>Any additional key/value options to be passed to the S3 connector.</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>path</code> <p>Path within the bucket to be read.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>dataframe_type</code> <p>The type of the returned dataframe. Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</p> <p> TYPE: <code>Literal['default', 'spark', 'pandas', 'polars', 'numpy', 'python']</code> DEFAULT: <code>'default'</code> </p> RETURNS DESCRIPTION <code>TypeVar('pyspark.sql.DataFrame') | TypeVar('pyspark.RDD') | pd.DataFrame | np.ndarray | pl.DataFrame</code> <p><code>DataFrame</code>.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.SnowflakeConnector","title":"[source]  SnowflakeConnector","text":"<p>               Bases: <code>StorageConnector</code></p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.SnowflakeConnector.url","title":"[source]  url  <code>property</code>","text":"<pre><code>url: str | None\n</code></pre> <p>URL of the Snowflake storage connector.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.SnowflakeConnector.warehouse","title":"[source]  warehouse  <code>property</code>","text":"<pre><code>warehouse: str | None\n</code></pre> <p>Warehouse of the Snowflake storage connector.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.SnowflakeConnector.database","title":"[source]  database  <code>property</code>","text":"<pre><code>database: str | None\n</code></pre> <p>Database of the Snowflake storage connector.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.SnowflakeConnector.user","title":"[source]  user  <code>property</code>","text":"<pre><code>user: Any | None\n</code></pre> <p>User of the Snowflake storage connector.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.SnowflakeConnector.password","title":"[source]  password  <code>property</code>","text":"<pre><code>password: str | None\n</code></pre> <p>Password of the Snowflake storage connector.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.SnowflakeConnector.token","title":"[source]  token  <code>property</code>","text":"<pre><code>token: str | None\n</code></pre> <p>OAuth token of the Snowflake storage connector.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.SnowflakeConnector.schema","title":"[source]  schema  <code>property</code>","text":"<pre><code>schema: str | None\n</code></pre> <p>Schema of the Snowflake storage connector.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.SnowflakeConnector.table","title":"[source]  table  <code>property</code>","text":"<pre><code>table: str | None\n</code></pre> <p>Table of the Snowflake storage connector.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.SnowflakeConnector.role","title":"[source]  role  <code>property</code>","text":"<pre><code>role: Any | None\n</code></pre> <p>Role of the Snowflake storage connector.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.SnowflakeConnector.account","title":"[source]  account  <code>property</code>","text":"<pre><code>account: str | None\n</code></pre> <p>Account of the Snowflake storage connector.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.SnowflakeConnector.application","title":"[source]  application  <code>property</code>","text":"<pre><code>application: Any\n</code></pre> <p>Application of the Snowflake storage connector.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.SnowflakeConnector.options","title":"[source]  options  <code>property</code>","text":"<pre><code>options: dict[str, Any] | None\n</code></pre> <p>Additional options for the Snowflake storage connector.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.SnowflakeConnector.private_key","title":"[source]  private_key  <code>property</code>","text":"<pre><code>private_key: str | None\n</code></pre> <p>Path to the private key file for key pair authentication.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.SnowflakeConnector.passphrase","title":"[source]  passphrase  <code>property</code>","text":"<pre><code>passphrase: str | None\n</code></pre> <p>Passphrase for the private key file.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.SnowflakeConnector.snowflake_connector_options","title":"[source]  snowflake_connector_options","text":"<pre><code>snowflake_connector_options() -&gt; dict[str, Any] | None\n</code></pre> <p>Alias for <code>connector_options</code>.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.SnowflakeConnector.connector_options","title":"[source]  connector_options","text":"<pre><code>connector_options() -&gt; dict[str, Any] | None\n</code></pre> <p>Prepare a Python dictionary with the needed arguments for you to connect to a Snowflake database.</p> <p>It is useful for the <code>snowflake.connector</code> Python library.</p> <pre><code>import snowflake.connector\n\nsc = fs.get_storage_connector(\"snowflake_conn\")\nctx = snowflake.connector.connect(**sc.connector_options())\n</code></pre>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.SnowflakeConnector.spark_options","title":"[source]  spark_options","text":"<pre><code>spark_options() -&gt; dict[str, Any]\n</code></pre> <p>Return prepared options to be passed to Spark, based on the additional arguments.</p>"},{"location":"python-api/hsfs/storage_connector/#hsfs.storage_connector.SnowflakeConnector.read","title":"[source]  read","text":"<pre><code>read(\n    query: str | None = None,\n    data_format: str | None = None,\n    options: dict[str, Any] | None = None,\n    path: str | None = None,\n    dataframe_type: str = \"default\",\n) -&gt; (\n    TypeVar(\"pyspark.sql.DataFrame\")\n    | TypeVar(\"pyspark.RDD\")\n    | pd.DataFrame\n    | np.ndarray\n    | pl.DataFrame\n)\n</code></pre> <p>Reads a table or query into a dataframe using the storage connector.</p> PARAMETER DESCRIPTION <code>query</code> <p>By default, the storage connector will read the table configured together with the connector, if any. It's possible to overwrite this by passing a SQL query here. Defaults to <code>None</code>.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>data_format</code> <p>Not relevant for Snowflake connectors.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>options</code> <p>Any additional key/value options to be passed to the engine.</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>path</code> <p>Not relevant for Snowflake connectors.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>dataframe_type</code> <p>str, optional. The type of the returned dataframe. Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>. Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default'</code> </p> RETURNS DESCRIPTION <code>TypeVar('pyspark.sql.DataFrame') | TypeVar('pyspark.RDD') | pd.DataFrame | np.ndarray | pl.DataFrame</code> <p><code>DataFrame</code>.</p>"},{"location":"python-api/hsfs/training_dataset/","title":"hsfs.training_dataset","text":""},{"location":"python-api/hsfs/training_dataset/#hsfs.training_dataset","title":"hsfs.training_dataset","text":""},{"location":"python-api/hsfs/training_dataset/#hsfs.training_dataset.TrainingDataset","title":"[source]  TrainingDataset","text":"<p>               Bases: <code>TrainingDatasetBase</code></p>"},{"location":"python-api/hsfs/training_dataset/#hsfs.training_dataset.TrainingDataset.id","title":"[source]  id  <code>property</code> <code>writable</code>","text":"<p>Training dataset id.</p>"},{"location":"python-api/hsfs/training_dataset/#hsfs.training_dataset.TrainingDataset.write_options","title":"[source]  write_options  <code>property</code> <code>writable</code>","text":"<p>User provided options to write training dataset.</p>"},{"location":"python-api/hsfs/training_dataset/#hsfs.training_dataset.TrainingDataset.schema","title":"[source]  schema  <code>property</code> <code>writable</code>","text":"<p>Training dataset schema.</p>"},{"location":"python-api/hsfs/training_dataset/#hsfs.training_dataset.TrainingDataset.statistics","title":"[source]  statistics  <code>property</code>","text":"<p>Get computed statistics for the training dataset.</p> RETURNS DESCRIPTION <p><code>Statistics</code>. Object with statistics information.</p>"},{"location":"python-api/hsfs/training_dataset/#hsfs.training_dataset.TrainingDataset.query","title":"[source]  query  <code>property</code>","text":"<p>Query to generate this training dataset from online feature store.</p>"},{"location":"python-api/hsfs/training_dataset/#hsfs.training_dataset.TrainingDataset.label","title":"[source]  label  <code>property</code> <code>writable</code>","text":"<pre><code>label: str | list[str]\n</code></pre> <p>The label/prediction feature of the training dataset.</p> <p>Can be a composite of multiple features.</p>"},{"location":"python-api/hsfs/training_dataset/#hsfs.training_dataset.TrainingDataset.feature_store_id","title":"[source]  feature_store_id  <code>property</code>","text":"<pre><code>feature_store_id: int\n</code></pre> <p>ID of the feature store to which this training dataset belongs.</p>"},{"location":"python-api/hsfs/training_dataset/#hsfs.training_dataset.TrainingDataset.feature_store_name","title":"[source]  feature_store_name  <code>property</code>","text":"<pre><code>feature_store_name: str\n</code></pre> <p>Name of the feature store in which the feature group is located.</p>"},{"location":"python-api/hsfs/training_dataset/#hsfs.training_dataset.TrainingDataset.serving_keys","title":"[source]  serving_keys  <code>property</code>","text":"<pre><code>serving_keys: set[str]\n</code></pre> <p>Set of primary key names that is used as keys in input dict object for <code>get_serving_vector</code> method.</p>"},{"location":"python-api/hsfs/training_dataset/#hsfs.training_dataset.TrainingDataset.save","title":"[source]  save","text":"<pre><code>save(\n    features: query.Query\n    | pd.DataFrame\n    | TypeVar(\"pyspark.sql.DataFrame\")\n    | TypeVar(\"pyspark.RDD\")\n    | np.ndarray\n    | list[list],\n    write_options: dict[Any, Any] | None = None,\n)\n</code></pre> <p>Materialize the training dataset to storage.</p> <p>This method materializes the training dataset either from a Feature Store <code>Query</code>, a Spark or Pandas <code>DataFrame</code>, a Spark RDD, two-dimensional Python lists or Numpy ndarrays. From v2.5 onward, filters are saved along with the <code>Query</code>.</p> Engine Support <p>Creating Training Datasets from Dataframes is only supported using Spark as Engine.</p> PARAMETER DESCRIPTION <code>features</code> <p>Feature data to be materialized.</p> <p> TYPE: <code>query.Query | pd.DataFrame | TypeVar('pyspark.sql.DataFrame') | TypeVar('pyspark.RDD') | np.ndarray | list[list]</code> </p> <code>write_options</code> <p>Additional write options as key-value pairs, defaults to <code>{}</code>. When using the <code>python</code> engine, write_options can contain the following entries: * key <code>spark</code> and value an object of type hsfs.core.job_configuration.JobConfiguration   to configure the Hopsworks Job used to compute the training dataset. * key <code>wait_for_job</code> and value <code>True</code> or <code>False</code> to configure   whether or not to the save call should return only   after the Hopsworks Job has finished. By default it waits.</p> <p> TYPE: <code>dict[Any, Any] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <p><code>Job</code>: When using the <code>python</code> engine, it returns the Hopsworks Job that was launched to create the training dataset.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>Unable to create training dataset metadata.</p>"},{"location":"python-api/hsfs/training_dataset/#hsfs.training_dataset.TrainingDataset.insert","title":"[source]  insert","text":"<pre><code>insert(\n    features: query.Query\n    | pd.DataFrame\n    | TypeVar(\"pyspark.sql.DataFrame\")\n    | TypeVar(\"pyspark.RDD\")\n    | np.ndarray\n    | list[list],\n    overwrite: bool,\n    write_options: dict[Any, Any] | None = None,\n)\n</code></pre> <p>Insert additional feature data into the training dataset.</p> Deprecated <p><code>insert</code> method is deprecated.</p> <p>This method appends data to the training dataset either from a Feature Store <code>Query</code>, a Spark or Pandas <code>DataFrame</code>, a Spark RDD, two-dimensional Python lists or Numpy ndarrays. The schemas must match for this operation.</p> <p>This can also be used to overwrite all data in an existing training dataset.</p> PARAMETER DESCRIPTION <code>features</code> <p>Feature data to be materialized.</p> <p> TYPE: <code>query.Query | pd.DataFrame | TypeVar('pyspark.sql.DataFrame') | TypeVar('pyspark.RDD') | np.ndarray | list[list]</code> </p> <code>overwrite</code> <p>Whether to overwrite the entire data in the training dataset.</p> <p> TYPE: <code>bool</code> </p> <code>write_options</code> <p>Additional write options as key-value pairs, defaults to <code>{}</code>. When using the <code>python</code> engine, write_options can contain the following entries: * key <code>spark</code> and value an object of type   hsfs.core.job_configuration.JobConfiguration   to configure the Hopsworks Job used to compute the training dataset. * key <code>wait_for_job</code> and value <code>True</code> or <code>False</code> to configure   whether or not to the insert call should return only   after the Hopsworks Job has finished. By default it waits.</p> <p> TYPE: <code>dict[Any, Any] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <p><code>Job</code>: When using the <code>python</code> engine, it returns the Hopsworks Job that was launched to create the training dataset.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>Unable to create training dataset metadata.</p>"},{"location":"python-api/hsfs/training_dataset/#hsfs.training_dataset.TrainingDataset.read","title":"[source]  read","text":"<pre><code>read(split=None, read_options=None)\n</code></pre> <p>Read the training dataset into a dataframe.</p> <p>It is also possible to read only a specific split.</p> PARAMETER DESCRIPTION <code>split</code> <p>Name of the split to read, defaults to <code>None</code>, reading the entire training dataset. If the training dataset has split, the <code>split</code> parameter is mandatory.</p> <p> DEFAULT: <code>None</code> </p> <code>read_options</code> <p>Additional read options as key/value pairs, defaults to <code>{}</code>.</p> <p> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <p><code>DataFrame</code>: The spark dataframe containing the feature data of the training dataset.</p>"},{"location":"python-api/hsfs/training_dataset/#hsfs.training_dataset.TrainingDataset.compute_statistics","title":"[source]  compute_statistics","text":"<pre><code>compute_statistics()\n</code></pre> <p>Compute the statistics for the training dataset and save them to the feature store.</p>"},{"location":"python-api/hsfs/training_dataset/#hsfs.training_dataset.TrainingDataset.show","title":"[source]  show","text":"<pre><code>show(n: int, split: str = None)\n</code></pre> <p>Show the first <code>n</code> rows of the training dataset.</p> <p>You can specify a split from which to retrieve the rows.</p> PARAMETER DESCRIPTION <code>n</code> <p>Number of rows to show.</p> <p> TYPE: <code>int</code> </p> <code>split</code> <p>Name of the split to show, defaults to <code>None</code>, showing the first rows when taking all splits together.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p>"},{"location":"python-api/hsfs/training_dataset/#hsfs.training_dataset.TrainingDataset.add_tag","title":"[source]  add_tag","text":"<pre><code>add_tag(name: str, value)\n</code></pre> <p>Attach a tag to a training dataset.</p> <p>A tag consists of a  pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. PARAMETER DESCRIPTION <code>name</code> <p>Name of the tag to be added.</p> <p> TYPE: <code>str</code> </p> <code>value</code> <p>Value of the tag to be added.</p> <p> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>in case the backend fails to add the tag.</p>"},{"location":"python-api/hsfs/training_dataset/#hsfs.training_dataset.TrainingDataset.delete_tag","title":"[source]  delete_tag","text":"<pre><code>delete_tag(name: str)\n</code></pre> <p>Delete a tag attached to a training dataset.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the tag to be removed.</p> <p> TYPE: <code>str</code> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>in case the backend fails to delete the tag.</p>"},{"location":"python-api/hsfs/training_dataset/#hsfs.training_dataset.TrainingDataset.get_tag","title":"[source]  get_tag","text":"<pre><code>get_tag(name)\n</code></pre> <p>Get the tags of a training dataset.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the tag to get.</p> <p> </p> RETURNS DESCRIPTION <p>tag value</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>in case the backend fails to retrieve the tag.</p>"},{"location":"python-api/hsfs/training_dataset/#hsfs.training_dataset.TrainingDataset.get_tags","title":"[source]  get_tags","text":"<pre><code>get_tags()\n</code></pre> <p>Returns all tags attached to a training dataset.</p> RETURNS DESCRIPTION <p><code>Dict[str, obj]</code> of tags.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>in case the backend fails to retrieve the tags.</p>"},{"location":"python-api/hsfs/training_dataset/#hsfs.training_dataset.TrainingDataset.update_statistics_config","title":"[source]  update_statistics_config","text":"<pre><code>update_statistics_config()\n</code></pre> <p>Update the statistics configuration of the training dataset.</p> <p>Change the <code>statistics_config</code> object and persist the changes by calling this method.</p> RETURNS DESCRIPTION <p><code>TrainingDataset</code>. The updated metadata object of the training dataset.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>in case the backend encounters an issue</p>"},{"location":"python-api/hsfs/training_dataset/#hsfs.training_dataset.TrainingDataset.delete","title":"[source]  delete","text":"<pre><code>delete()\n</code></pre> <p>Delete training dataset and all associated metadata.</p> Drops only HopsFS data <p>Note that this operation drops only files which were materialized in HopsFS. If you used a Storage Connector for a cloud storage such as S3, the data will not be deleted, but you will not be able to track it anymore from the Feature Store.</p> Potentially dangerous operation <p>This operation drops all metadata associated with this version of the training dataset and and the materialized data in HopsFS.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>In case of a server error.</p>"},{"location":"python-api/hsfs/training_dataset/#hsfs.training_dataset.TrainingDataset.get_query","title":"[source]  get_query","text":"<pre><code>get_query(online: bool = True, with_label: bool = False)\n</code></pre> <p>Returns the query used to generate this training dataset.</p> PARAMETER DESCRIPTION <code>online</code> <p>boolean, optional. Return the query for the online storage, else for offline storage, defaults to <code>True</code> - for online storage.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>with_label</code> <p>Indicator whether the query should contain features which were marked as prediction label/feature when the training dataset was created, defaults to <code>False</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <p><code>str</code>. Query string for the chosen storage used to generate this training dataset.</p>"},{"location":"python-api/hsfs/training_dataset/#hsfs.training_dataset.TrainingDataset.init_prepared_statement","title":"[source]  init_prepared_statement","text":"<pre><code>init_prepared_statement(\n    batch: bool | None = None, external: bool | None = None\n)\n</code></pre> <p>Initialise and cache parametrized prepared statement to retrieve feature vector from online feature store.</p> PARAMETER DESCRIPTION <code>batch</code> <p>boolean, optional. If set to True, prepared statements will be initialised for retrieving serving vectors as a batch.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>external</code> <p>boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the <code>host</code> parameter in the <code>hopsworks.login</code> method. If set to False, the online feature store storage connector is used which relies on the private IP. Defaults to True if connection to Hopsworks is established from external environment (e.g AWS Sagemaker or Google Colab), otherwise to False.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p>"},{"location":"python-api/hsfs/training_dataset/#hsfs.training_dataset.TrainingDataset.get_serving_vector","title":"[source]  get_serving_vector","text":"<pre><code>get_serving_vector(\n    entry: dict[str, Any], external: bool | None = None\n)\n</code></pre> <p>Returns assembled serving vector from online feature store.</p> PARAMETER DESCRIPTION <code>entry</code> <p>dictionary of training dataset feature group primary key names as keys and values provided by serving application.</p> <p> TYPE: <code>dict[str, Any]</code> </p> <code>external</code> <p>boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the <code>host</code> parameter in the <code>hopsworks.login</code> method. If set to False, the online feature store storage connector is used which relies on the private IP. Defaults to True if connection to Hopsworks is established from external environment (e.g AWS Sagemaker or Google Colab), otherwise to False.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <p><code>list</code> List of feature values related to provided primary keys, ordered according to positions of this</p> <p>features in training dataset query.</p>"},{"location":"python-api/hsfs/training_dataset/#hsfs.training_dataset.TrainingDataset.get_serving_vectors","title":"[source]  get_serving_vectors","text":"<pre><code>get_serving_vectors(\n    entry: dict[str, list[Any]],\n    external: bool | None = None,\n)\n</code></pre> <p>Returns assembled serving vectors in batches from online feature store.</p> PARAMETER DESCRIPTION <code>entry</code> <p>dict of feature group primary key names as keys and value as list of primary keys provided by serving application.</p> <p> TYPE: <code>dict[str, list[Any]]</code> </p> <code>external</code> <p>boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the <code>host</code> parameter in the <code>hopsworks.login</code> method. If set to False, the online feature store storage connector is used which relies on the private IP. Defaults to True if connection to Hopsworks is established from external environment (e.g AWS Sagemaker or Google Colab), otherwise to False.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <p><code>List[list]</code> List of lists of feature values related to provided primary keys, ordered according to</p> <p>positions of this features in training dataset query.</p>"},{"location":"python-api/hsfs/transformation_function/","title":"hsfs.transformation_function","text":""},{"location":"python-api/hsfs/transformation_function/#hsfs.transformation_function","title":"hsfs.transformation_function","text":""},{"location":"python-api/hsfs/transformation_function/#hsfs.transformation_function.TransformationFunction","title":"[source]  TransformationFunction","text":""},{"location":"python-api/hsfs/transformation_function/#hsfs.transformation_function.TransformationFunction.NOT_FOUND_ERROR_CODE","title":"[source]  NOT_FOUND_ERROR_CODE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>NOT_FOUND_ERROR_CODE = 270160\n</code></pre> <p>DTO class for transformation functions.</p> PARAMETER DESCRIPTION <code>featurestore_id </code> <p><code>int</code>. Id of the feature store in which the transformation function is saved.</p> <p> </p> <code>hopsworks_udf </code> <p><code>HopsworksUDF</code>. The meta data object for UDF in Hopsworks, which can be created using the <code>@udf</code> decorator.</p> <p> </p> <code>version </code> <p><code>int</code>. The version of the transformation function.</p> <p> </p> <code>id </code> <p><code>int</code>. The id of the transformation function in the feature store.</p> <p> </p> <code>transformation_type </code> <p><code>UDFType</code>. The type of the transformation function. Can be \"on-demand\" or \"model-dependent\"</p> <p> </p>"},{"location":"python-api/hsfs/transformation_function/#hsfs.transformation_function.TransformationFunction.id","title":"[source]  id  <code>property</code> <code>writable</code>","text":"<pre><code>id: id\n</code></pre> <p>Transformation function id.</p>"},{"location":"python-api/hsfs/transformation_function/#hsfs.transformation_function.TransformationFunction.version","title":"[source]  version  <code>property</code> <code>writable</code>","text":"<pre><code>version: int\n</code></pre> <p>Version of the transformation function.</p>"},{"location":"python-api/hsfs/transformation_function/#hsfs.transformation_function.TransformationFunction.hopsworks_udf","title":"[source]  hopsworks_udf  <code>property</code>","text":"<pre><code>hopsworks_udf: HopsworksUdf\n</code></pre> <p>Meta data class for the user defined transformation function.</p>"},{"location":"python-api/hsfs/transformation_function/#hsfs.transformation_function.TransformationFunction.transformation_type","title":"[source]  transformation_type  <code>property</code> <code>writable</code>","text":"<pre><code>transformation_type: TransformationType\n</code></pre> <p>Type of the Transformation: can be <code>model dependent</code> or <code>on-demand</code>.</p>"},{"location":"python-api/hsfs/transformation_function/#hsfs.transformation_function.TransformationFunction.transformation_statistics","title":"[source]  transformation_statistics  <code>property</code> <code>writable</code>","text":"<pre><code>transformation_statistics: TransformationStatistics | None\n</code></pre> <p>Feature statistics required for the defined UDF.</p>"},{"location":"python-api/hsfs/transformation_function/#hsfs.transformation_function.TransformationFunction.output_column_names","title":"[source]  output_column_names  <code>property</code>","text":"<pre><code>output_column_names: list[str]\n</code></pre> <p>Names of the output columns generated by the transformation functions.</p>"},{"location":"python-api/hsfs/transformation_function/#hsfs.transformation_function.TransformationFunction.save","title":"[source]  save","text":"<pre><code>save() -&gt; None\n</code></pre> <p>Save a transformation function into the backend.</p> Example <pre><code># import hopsworks udf decorator\nfrom hopworks import udf\n\n# define function\n@udf(int)\ndef plus_one(value):\n    return value + 1\n\n# create transformation function\nplus_one_meta = fs.create_transformation_function(\n        transformation_function=plus_one,\n        version=1\n    )\n\n# persist transformation function in backend\nplus_one_meta.save()\n</code></pre>"},{"location":"python-api/hsfs/transformation_function/#hsfs.transformation_function.TransformationFunction.delete","title":"[source]  delete","text":"<pre><code>delete() -&gt; None\n</code></pre> <p>Delete transformation function from backend.</p> Example <pre><code># import hopsworks udf decorator\nfrom hopworks import udf\n\n# define function\n@udf(int)\ndef plus_one(value):\n    return value + 1\n\n# create transformation function\nplus_one_meta = fs.create_transformation_function(\n        transformation_function=plus_one,\n        version=1\n    )\n# persist transformation function in backend\nplus_one_meta.save()\n\n# retrieve transformation function\nplus_one_fn = fs.get_transformation_function(name=\"plus_one\")\n\n# delete transformation function from backend\nplus_one_fn.delete()\n</code></pre>"},{"location":"python-api/hsfs/transformation_function/#hsfs.transformation_function.TransformationFunction.alias","title":"[source]  alias","text":"<pre><code>alias(*args: str)\n</code></pre> <p>Set the names of the transformed features output by the transformation function.</p>"},{"location":"python-api/hsfs/transformation_statistics/","title":"hsfs.transformation_statistics","text":""},{"location":"python-api/hsfs/transformation_statistics/#hsfs.transformation_statistics","title":"hsfs.transformation_statistics","text":""},{"location":"python-api/hsfs/transformation_statistics/#hsfs.transformation_statistics.FeatureTransformationStatistics","title":"[source]  FeatureTransformationStatistics","text":"<p>Data class that contains all the statistics parameters that can be used for transformations inside a custom transformation function.</p>"},{"location":"python-api/hsfs/transformation_statistics/#hsfs.transformation_statistics.FeatureTransformationStatistics.feature_name","title":"[source]  feature_name  <code>property</code>","text":"<pre><code>feature_name: str\n</code></pre> <p>Name of the feature.</p>"},{"location":"python-api/hsfs/transformation_statistics/#hsfs.transformation_statistics.FeatureTransformationStatistics.count","title":"[source]  count  <code>property</code>","text":"<pre><code>count: int | None\n</code></pre> <p>Number of values.</p>"},{"location":"python-api/hsfs/transformation_statistics/#hsfs.transformation_statistics.FeatureTransformationStatistics.completeness","title":"[source]  completeness  <code>property</code>","text":"<pre><code>completeness: float | None\n</code></pre> <p>Fraction of non-null values in a column.</p>"},{"location":"python-api/hsfs/transformation_statistics/#hsfs.transformation_statistics.FeatureTransformationStatistics.num_non_null_values","title":"[source]  num_non_null_values  <code>property</code>","text":"<pre><code>num_non_null_values: int | None\n</code></pre> <p>Number of non-null values.</p>"},{"location":"python-api/hsfs/transformation_statistics/#hsfs.transformation_statistics.FeatureTransformationStatistics.num_null_values","title":"[source]  num_null_values  <code>property</code>","text":"<pre><code>num_null_values: int | None\n</code></pre> <p>Number of null values.</p>"},{"location":"python-api/hsfs/transformation_statistics/#hsfs.transformation_statistics.FeatureTransformationStatistics.approx_num_distinct_values","title":"[source]  approx_num_distinct_values  <code>property</code>","text":"<pre><code>approx_num_distinct_values: int | None\n</code></pre> <p>Approximate number of distinct values.</p>"},{"location":"python-api/hsfs/transformation_statistics/#hsfs.transformation_statistics.FeatureTransformationStatistics.min","title":"[source]  min  <code>property</code>","text":"<pre><code>min: float | None\n</code></pre> <p>Minimum value.</p>"},{"location":"python-api/hsfs/transformation_statistics/#hsfs.transformation_statistics.FeatureTransformationStatistics.max","title":"[source]  max  <code>property</code>","text":"<pre><code>max: float | None\n</code></pre> <p>Maximum value.</p>"},{"location":"python-api/hsfs/transformation_statistics/#hsfs.transformation_statistics.FeatureTransformationStatistics.sum","title":"[source]  sum  <code>property</code>","text":"<pre><code>sum: float | None\n</code></pre> <p>Sum of all feature values.</p>"},{"location":"python-api/hsfs/transformation_statistics/#hsfs.transformation_statistics.FeatureTransformationStatistics.mean","title":"[source]  mean  <code>property</code>","text":"<pre><code>mean: float | None\n</code></pre> <p>Mean value.</p>"},{"location":"python-api/hsfs/transformation_statistics/#hsfs.transformation_statistics.FeatureTransformationStatistics.stddev","title":"[source]  stddev  <code>property</code>","text":"<pre><code>stddev: float | None\n</code></pre> <p>Standard deviation of the feature values.</p>"},{"location":"python-api/hsfs/transformation_statistics/#hsfs.transformation_statistics.FeatureTransformationStatistics.percentiles","title":"[source]  percentiles  <code>property</code>","text":"<pre><code>percentiles: Mapping[str, float] | None\n</code></pre> <p>Percentiles.</p>"},{"location":"python-api/hsfs/transformation_statistics/#hsfs.transformation_statistics.FeatureTransformationStatistics.distinctness","title":"[source]  distinctness  <code>property</code>","text":"<pre><code>distinctness: float | None\n</code></pre> <p>Fraction of distinct values of a feature over the number of all its values. Distinct values occur at least once.</p> Example <p>[a, a, b] contains two distinct values a and b, so distinctness is 2/3.</p>"},{"location":"python-api/hsfs/transformation_statistics/#hsfs.transformation_statistics.FeatureTransformationStatistics.entropy","title":"[source]  entropy  <code>property</code>","text":"<pre><code>entropy: float | None\n</code></pre> <p>Entropy is a measure of the level of information contained in an event (feature value) when considering all possible events (all feature values).</p> <p>Entropy is estimated using observed value counts as the negative sum of (value_count/total_count) * log(value_count/total_count).</p> Example <p>[a, b, b, c, c] has three distinct values with counts [1, 2, 2].</p> <p>Entropy is then (-1/5*log(1/5)-2/5*log(2/5)-2/5*log(2/5)) = 1.055.</p>"},{"location":"python-api/hsfs/transformation_statistics/#hsfs.transformation_statistics.FeatureTransformationStatistics.uniqueness","title":"[source]  uniqueness  <code>property</code>","text":"<pre><code>uniqueness: float | None\n</code></pre> <p>Fraction of unique values over the number of all values of a column. Unique values occur exactly once.</p> Example <p>[a, a, b] contains one unique value b, so uniqueness is 1/3.</p>"},{"location":"python-api/hsfs/transformation_statistics/#hsfs.transformation_statistics.FeatureTransformationStatistics.exact_num_distinct_values","title":"[source]  exact_num_distinct_values  <code>property</code>","text":"<pre><code>exact_num_distinct_values: int | None\n</code></pre> <p>Exact number of distinct values.</p>"},{"location":"python-api/hsfs/transformation_statistics/#hsfs.transformation_statistics.FeatureTransformationStatistics.correlations","title":"[source]  correlations  <code>property</code>","text":"<pre><code>correlations: dict | None\n</code></pre> <p>Correlations of feature values.</p>"},{"location":"python-api/hsfs/transformation_statistics/#hsfs.transformation_statistics.FeatureTransformationStatistics.histogram","title":"[source]  histogram  <code>property</code>","text":"<pre><code>histogram: dict | None\n</code></pre> <p>Histogram of feature values.</p>"},{"location":"python-api/hsfs/transformation_statistics/#hsfs.transformation_statistics.FeatureTransformationStatistics.kll","title":"[source]  kll  <code>property</code>","text":"<pre><code>kll: dict | None\n</code></pre> <p>KLL of feature values.</p>"},{"location":"python-api/hsfs/transformation_statistics/#hsfs.transformation_statistics.FeatureTransformationStatistics.unique_values","title":"[source]  unique_values  <code>property</code>","text":"<pre><code>unique_values: dict | None\n</code></pre> <p>Number of Unique Values.</p>"},{"location":"python-api/hsfs/transformation_statistics/#hsfs.transformation_statistics.TransformationStatistics","title":"[source]  TransformationStatistics","text":"<p>Class that stores feature transformation statistics of all features that require training dataset statistics in a transformation function.</p> <p>All statistics for a feature is initially initialized with null values and will be populated with values when training dataset is created for the soe.</p> PARAMETER DESCRIPTION <code>*features</code> <p><code>str</code>. The features for which training dataset statistics need to be computed.</p> <p> TYPE: <code>str</code> DEFAULT: <code>()</code> </p> Example <pre><code># Defining transformation statistics\ntransformation_statistics = TransformationStatistics(\"feature1\", \"feature2\")\n\n# Accessing feature transformation statistics for a specific feature\nfeature_transformation_statistics_feature1 = transformation_statistics.feature1\n</code></pre>"},{"location":"python-api/hsfs/validation_report/","title":"hsfs.validation_report","text":""},{"location":"python-api/hsfs/validation_report/#hsfs.validation_report","title":"hsfs.validation_report","text":""},{"location":"python-api/hsfs/validation_report/#hsfs.validation_report.ValidationReport","title":"[source]  ValidationReport","text":"<p>Metadata object representing a validation report generated by Great Expectations in the Feature Store.</p>"},{"location":"python-api/hsfs/validation_report/#hsfs.validation_report.ValidationReport.id","title":"[source]  id  <code>property</code> <code>writable</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Id of the validation report, set by backend.</p>"},{"location":"python-api/hsfs/validation_report/#hsfs.validation_report.ValidationReport.success","title":"[source]  success  <code>property</code> <code>writable</code>","text":"<pre><code>success: bool\n</code></pre> <p>Overall success of the validation step.</p>"},{"location":"python-api/hsfs/validation_report/#hsfs.validation_report.ValidationReport.results","title":"[source]  results  <code>property</code> <code>writable</code>","text":"<pre><code>results: list[ValidationResult]\n</code></pre> <p>List of expectation results obtained after validation.</p>"},{"location":"python-api/hsfs/validation_report/#hsfs.validation_report.ValidationReport.meta","title":"[source]  meta  <code>property</code> <code>writable</code>","text":"<pre><code>meta: dict[str, Any] | None\n</code></pre> <p>Meta field of the validation report to store additional informations.</p>"},{"location":"python-api/hsfs/validation_report/#hsfs.validation_report.ValidationReport.statistics","title":"[source]  statistics  <code>property</code> <code>writable</code>","text":"<pre><code>statistics: dict[str, Any] | None\n</code></pre> <p>Statistics field of the validation report which store overall statistics about the validation result, e.g number of failing/successful expectations.</p>"},{"location":"python-api/hsfs/validation_report/#hsfs.validation_report.ValidationReport.evaluation_parameters","title":"[source]  evaluation_parameters  <code>property</code> <code>writable</code>","text":"<pre><code>evaluation_parameters: dict[str, Any] | None\n</code></pre> <p>Evaluation parameters field of the validation report which store kwargs of the validation.</p>"},{"location":"python-api/hsfs/validation_report/#hsfs.validation_report.ValidationReport.ingestion_result","title":"[source]  ingestion_result  <code>property</code> <code>writable</code>","text":"<pre><code>ingestion_result: str\n</code></pre> <p>Overall success of the validation run together with the ingestion validation policy, indicating if dataframe was ingested or rejected.</p>"},{"location":"python-api/hsfs/validation_report/#hsfs.validation_report.ValidationReport.to_ge_type","title":"[source]  to_ge_type","text":"<pre><code>to_ge_type() -&gt; (\n    great_expectations.core.ExpectationSuiteValidationResult\n)\n</code></pre> <p>Convert to Great Expectations ExpectationSuiteValidationResult type.</p>"},{"location":"python-api/hsml/core/explicit_provenance/","title":"hsml.core.explicit_provenance","text":""},{"location":"python-api/hsml/core/explicit_provenance/#hsml.core.explicit_provenance","title":"hsml.core.explicit_provenance","text":""},{"location":"python-api/hsml/core/explicit_provenance/#hsml.core.explicit_provenance.Artifact","title":"[source]  Artifact","text":""},{"location":"python-api/hsml/core/explicit_provenance/#hsml.core.explicit_provenance.Artifact.model_registry_id","title":"[source]  model_registry_id  <code>property</code>","text":"<p>Id of the model registry in which the artifact is located.</p>"},{"location":"python-api/hsml/core/explicit_provenance/#hsml.core.explicit_provenance.Artifact.name","title":"[source]  name  <code>property</code>","text":"<p>Name of the artifact.</p>"},{"location":"python-api/hsml/core/explicit_provenance/#hsml.core.explicit_provenance.Artifact.version","title":"[source]  version  <code>property</code>","text":"<p>Version of the artifact.</p>"},{"location":"python-api/hsml/core/explicit_provenance/#hsml.core.explicit_provenance.Links","title":"[source]  Links","text":""},{"location":"python-api/hsml/core/explicit_provenance/#hsml.core.explicit_provenance.Links.deleted","title":"[source]  deleted  <code>property</code>","text":"<p>List of [Artifact objects] which contains minimal information (name, version) about the entities (feature views, training datasets) they represent.</p> <p>These entities have been removed from the feature store.</p>"},{"location":"python-api/hsml/core/explicit_provenance/#hsml.core.explicit_provenance.Links.inaccessible","title":"[source]  inaccessible  <code>property</code>","text":"<p>List of [Artifact objects] which contains minimal information (name, version) about the entities (feature views, training datasets) they represent.</p> <p>These entities exist in the feature store, however the user does not have access to them anymore.</p>"},{"location":"python-api/hsml/core/explicit_provenance/#hsml.core.explicit_provenance.Links.accessible","title":"[source]  accessible  <code>property</code>","text":"<p>List of [FeatureView|TrainingDataset objects] objects which are part of the provenance graph requested.</p> <p>These entities exist in the feature store and the user has access to them.</p>"},{"location":"python-api/hsml/core/explicit_provenance/#hsml.core.explicit_provenance.Links.faulty","title":"[source]  faulty  <code>property</code>","text":"<p>List of [Artifact objects] which contains minimal information (name, version) about the entities (feature views, training datasets) they represent.</p> <p>These entities exist in the feature store, however they are corrupted.</p>"},{"location":"python-api/hsml/deployment/","title":"hsml.deployment","text":""},{"location":"python-api/hsml/deployment/#hsml.deployment","title":"hsml.deployment","text":""},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment","title":"[source]  Deployment","text":""},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment.NOT_FOUND_ERROR_CODE","title":"[source]  NOT_FOUND_ERROR_CODE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>NOT_FOUND_ERROR_CODE = 240000\n</code></pre> <p>Metadata object representing a deployment in Model Serving.</p>"},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment.id","title":"[source]  id  <code>property</code>","text":"<p>Id of the deployment.</p>"},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment.name","title":"[source]  name  <code>property</code> <code>writable</code>","text":"<p>Name of the deployment.</p>"},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment.version","title":"[source]  version  <code>property</code>","text":"<p>Version of the deployment.</p>"},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment.description","title":"[source]  description  <code>property</code> <code>writable</code>","text":"<p>Description of the deployment.</p>"},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment.has_model","title":"[source]  has_model  <code>property</code>","text":"<p>Whether the deployment has a model associated.</p>"},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment.predictor","title":"[source]  predictor  <code>property</code> <code>writable</code>","text":"<p>Predictor used in the deployment.</p>"},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment.requested_instances","title":"[source]  requested_instances  <code>property</code>","text":"<p>Total number of requested instances in the deployment.</p>"},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment.model_name","title":"[source]  model_name  <code>property</code> <code>writable</code>","text":"<p>Name of the model deployed by the predictor.</p>"},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment.model_path","title":"[source]  model_path  <code>property</code> <code>writable</code>","text":"<p>Model path deployed by the predictor.</p>"},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment.model_version","title":"[source]  model_version  <code>property</code> <code>writable</code>","text":"<p>Model version deployed by the predictor.</p>"},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment.artifact_version","title":"[source]  artifact_version  <code>property</code> <code>writable</code>","text":"<p>Artifact version deployed by the predictor.</p> Deprecated <p>Artifact versions are deprecated in favor of deployment versions.</p>"},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment.artifact_files_path","title":"[source]  artifact_files_path  <code>property</code>","text":"<p>Path of the artifact files deployed by the predictor.</p>"},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment.artifact_path","title":"[source]  artifact_path  <code>property</code>","text":"<p>Path of the model artifact deployed by the predictor.</p> Deprecated <p>Artifact versions are deprecated in favor of deployment versions.</p>"},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment.model_server","title":"[source]  model_server  <code>property</code> <code>writable</code>","text":"<p>Model server ran by the predictor.</p>"},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment.serving_tool","title":"[source]  serving_tool  <code>property</code> <code>writable</code>","text":"<p>Serving tool used to run the model server.</p>"},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment.script_file","title":"[source]  script_file  <code>property</code> <code>writable</code>","text":"<p>Script file used by the predictor.</p>"},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment.config_file","title":"[source]  config_file  <code>property</code> <code>writable</code>","text":"<p>Model server configuration file passed to the model deployment.</p> <p>It can be accessed via <code>CONFIG_FILE_PATH</code> environment variable from a predictor or transformer script. For LLM deployments without a predictor script, this file is used to configure the vLLM engine.</p>"},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment.resources","title":"[source]  resources  <code>property</code> <code>writable</code>","text":"<p>Resource configuration for the predictor.</p>"},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment.inference_logger","title":"[source]  inference_logger  <code>property</code> <code>writable</code>","text":"<p>Configuration of the inference logger attached to this predictor.</p>"},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment.inference_batcher","title":"[source]  inference_batcher  <code>property</code> <code>writable</code>","text":"<p>Configuration of the inference batcher attached to this predictor.</p>"},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment.transformer","title":"[source]  transformer  <code>property</code> <code>writable</code>","text":"<p>Transformer configured in the predictor.</p>"},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment.model_registry_id","title":"[source]  model_registry_id  <code>property</code> <code>writable</code>","text":"<p>Model Registry Id of the deployment.</p>"},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment.created_at","title":"[source]  created_at  <code>property</code>","text":"<p>Created at date of the predictor.</p>"},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment.creator","title":"[source]  creator  <code>property</code>","text":"<p>Creator of the predictor.</p>"},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment.api_protocol","title":"[source]  api_protocol  <code>property</code> <code>writable</code>","text":"<p>API protocol enabled in the deployment (e.g., HTTP or GRPC).</p>"},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment.environment","title":"[source]  environment  <code>property</code> <code>writable</code>","text":"<p>Name of inference environment.</p>"},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment.project_namespace","title":"[source]  project_namespace  <code>property</code> <code>writable</code>","text":"<p>Name of inference environment.</p>"},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment.project_name","title":"[source]  project_name  <code>property</code> <code>writable</code>","text":"<p>Name of the project the deployment belongs to.</p>"},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment.scaling_configuration","title":"[source]  scaling_configuration  <code>property</code> <code>writable</code>","text":"<p>Scaling configuration for the deployment.</p>"},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment.save","title":"[source]  save","text":"<pre><code>save(await_update: int | None = 600)\n</code></pre> <p>Persist this deployment including the predictor and metadata to Model Serving.</p> PARAMETER DESCRIPTION <code>await_update</code> <p>If the deployment is running, awaiting time (seconds) for the running instances to be updated.           If the running instances are not updated within this timespan, the call to this method returns while           the update in the background.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>600</code> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>In case the backend encounters an issue</p>"},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment.start","title":"[source]  start","text":"<pre><code>start(await_running: int | None = 600)\n</code></pre> <p>Start the deployment.</p> PARAMETER DESCRIPTION <code>await_running</code> <p>Awaiting time (seconds) for the deployment to start.            If the deployment has not started within this timespan, the call to this method returns while            it deploys in the background.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>600</code> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>In case the backend encounters an issue</p>"},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment.stop","title":"[source]  stop","text":"<pre><code>stop(await_stopped: int | None = 600)\n</code></pre> <p>Stop the deployment.</p> PARAMETER DESCRIPTION <code>await_stopped</code> <p>Awaiting time (seconds) for the deployment to stop.            If the deployment has not stopped within this timespan, the call to this method returns while            it stopping in the background.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>600</code> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>In case the backend encounters an issue</p>"},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment.delete","title":"[source]  delete","text":"<pre><code>delete(force=False)\n</code></pre> <p>Delete the deployment.</p> PARAMETER DESCRIPTION <code>force</code> <p>Force the deletion of the deployment. If the deployment is running, it will be stopped and deleted automatically.</p> <p> DEFAULT: <code>False</code> </p> Warning <p>A call to this method does not ask for a second confirmation.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>In case the backend encounters an issue</p>"},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment.get_state","title":"[source]  get_state","text":"<pre><code>get_state() -&gt; PredictorState\n</code></pre> <p>Get the current state of the deployment.</p> RETURNS DESCRIPTION <code>PredictorState</code> <p><code>PredictorState</code>. The state of the deployment.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>In case the backend encounters an issue</p>"},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment.is_created","title":"[source]  is_created","text":"<pre><code>is_created() -&gt; bool\n</code></pre> <p>Check whether the deployment is created.</p> RETURNS DESCRIPTION <code>bool</code> <p><code>bool</code>. Whether the deployment is created or not.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>In case the backend encounters an issue</p>"},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment.is_running","title":"[source]  is_running","text":"<pre><code>is_running(or_idle=True, or_updating=True) -&gt; bool\n</code></pre> <p>Check whether the deployment is ready to handle inference requests.</p> PARAMETER DESCRIPTION <code>or_idle</code> <p>Whether the idle state is considered as running (default is True)</p> <p> DEFAULT: <code>True</code> </p> <code>or_updating</code> <p>Whether the updating state is considered as running (default is True)</p> <p> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>bool</code> <p><code>bool</code>. Whether the deployment is ready or not.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>In case the backend encounters an issue</p>"},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment.is_stopped","title":"[source]  is_stopped","text":"<pre><code>is_stopped(or_created=True) -&gt; bool\n</code></pre> <p>Check whether the deployment is stopped.</p> PARAMETER DESCRIPTION <code>or_created</code> <p>Whether the creating and created state is considered as stopped (default is True)</p> <p> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>bool</code> <p><code>bool</code>. Whether the deployment is stopped or not.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>In case the backend encounters an issue</p>"},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment.predict","title":"[source]  predict","text":"<pre><code>predict(\n    data: dict | InferInput = None,\n    inputs: list | dict = None,\n)\n</code></pre> <p>Send inference requests to the deployment.</p> <p>One of data or inputs parameters must be set. If both are set, inputs will be ignored.</p> Example <pre><code># login into Hopsworks using hopsworks.login()\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n\n# retrieve deployment by name\nmy_deployment = ms.get_deployment(\"my_deployment\")\n\n# (optional) retrieve model input example\nmy_model = project.get_model_registry()                                .get_model(my_deployment.model_name, my_deployment.model_version)\n\n# make predictions using model inputs (single or batch)\npredictions = my_deployment.predict(inputs=my_model.input_example)\n\n# or using more sophisticated inference request payloads\ndata = { \"instances\": [ my_model.input_example ], \"key2\": \"value2\" }\npredictions = my_deployment.predict(data)\n</code></pre> PARAMETER DESCRIPTION <code>data</code> <p>Payload dictionary for the inference request including the model input(s)</p> <p> TYPE: <code>dict | InferInput</code> DEFAULT: <code>None</code> </p> <code>inputs</code> <p>Model inputs used in the inference requests</p> <p> TYPE: <code>list | dict</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <p><code>dict</code>. Inference response.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>In case the backend encounters an issue</p>"},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment.get_model","title":"[source]  get_model","text":"<pre><code>get_model()\n</code></pre> <p>Retrieve the metadata object for the model being used by this deployment.</p>"},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment.download_artifact_files","title":"[source]  download_artifact_files","text":"<pre><code>download_artifact_files(local_path=None)\n</code></pre> <p>Download the artifact files served by the deployment.</p> PARAMETER DESCRIPTION <code>local_path</code> <p>path where to download the artifact files in the local filesystem</p> <p> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>In case the backend encounters an issue</p>"},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment.get_logs","title":"[source]  get_logs","text":"<pre><code>get_logs(component='predictor', tail=10)\n</code></pre> <p>Prints the deployment logs of the predictor or transformer.</p> PARAMETER DESCRIPTION <code>component</code> <p>Deployment component to get the logs from (e.g., predictor or transformer)</p> <p> DEFAULT: <code>'predictor'</code> </p> <code>tail</code> <p>Number of most recent lines to retrieve from the logs.</p> <p> DEFAULT: <code>10</code> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>In case the backend encounters an issue</p>"},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment.get_url","title":"[source]  get_url","text":"<pre><code>get_url()\n</code></pre> <p>Get url to the deployment in Hopsworks.</p>"},{"location":"python-api/hsml/deployment/#hsml.deployment.Deployment.describe","title":"[source]  describe","text":"<pre><code>describe()\n</code></pre> <p>Print a JSON description of the deployment.</p>"},{"location":"python-api/hsml/inference_batcher/","title":"hsml.inference_batcher","text":""},{"location":"python-api/hsml/inference_batcher/#hsml.inference_batcher","title":"hsml.inference_batcher","text":""},{"location":"python-api/hsml/inference_batcher/#hsml.inference_batcher.InferenceBatcher","title":"[source]  InferenceBatcher","text":"<p>Configuration of an inference batcher for a predictor.</p> PARAMETER DESCRIPTION <code>enabled</code> <p>Whether the inference batcher is enabled or not. The default value is <code>false</code>.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>max_batch_size</code> <p>Maximum requests batch size.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>max_latency</code> <p>Maximum latency for request batching.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>timeout</code> <p>Maximum waiting time for request batching.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <p><code>InferenceLogger</code>. Configuration of an inference logger.</p>"},{"location":"python-api/hsml/inference_batcher/#hsml.inference_batcher.InferenceBatcher.enabled","title":"[source]  enabled  <code>property</code> <code>writable</code>","text":"<p>Whether the inference batcher is enabled or not.</p>"},{"location":"python-api/hsml/inference_batcher/#hsml.inference_batcher.InferenceBatcher.max_batch_size","title":"[source]  max_batch_size  <code>property</code> <code>writable</code>","text":"<p>Maximum requests batch size.</p>"},{"location":"python-api/hsml/inference_batcher/#hsml.inference_batcher.InferenceBatcher.max_latency","title":"[source]  max_latency  <code>property</code> <code>writable</code>","text":"<p>Maximum latency.</p>"},{"location":"python-api/hsml/inference_batcher/#hsml.inference_batcher.InferenceBatcher.timeout","title":"[source]  timeout  <code>property</code> <code>writable</code>","text":"<p>Maximum timeout.</p>"},{"location":"python-api/hsml/inference_batcher/#hsml.inference_batcher.InferenceBatcher.describe","title":"[source]  describe","text":"<pre><code>describe()\n</code></pre> <p>Print a JSON description of the inference batcher.</p>"},{"location":"python-api/hsml/inference_logger/","title":"hsml.inference_logger","text":""},{"location":"python-api/hsml/inference_logger/#hsml.inference_logger","title":"hsml.inference_logger","text":""},{"location":"python-api/hsml/inference_logger/#hsml.inference_logger.InferenceLogger","title":"[source]  InferenceLogger","text":"<p>Configuration of an inference logger for a predictor.</p> PARAMETER DESCRIPTION <code>kafka_topic</code> <p>Kafka topic to send the inference logs to. By default, a new Kafka topic is configured.</p> <p> TYPE: <code>KafkaTopic | dict | Default | None</code> DEFAULT: <code>DEFAULT</code> </p> <code>mode</code> <p>Inference logging mode. (e.g., <code>NONE</code>, <code>ALL</code>, <code>PREDICTIONS</code>, or <code>MODEL_INPUTS</code>). By default, <code>ALL</code> inference logs are sent.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>INFERENCE_LOGGER.MODE_ALL</code> </p> RETURNS DESCRIPTION <p><code>InferenceLogger</code>. Configuration of an inference logger.</p>"},{"location":"python-api/hsml/inference_logger/#hsml.inference_logger.InferenceLogger.kafka_topic","title":"[source]  kafka_topic  <code>property</code> <code>writable</code>","text":"<p>Kafka topic to send the inference logs to.</p>"},{"location":"python-api/hsml/inference_logger/#hsml.inference_logger.InferenceLogger.mode","title":"[source]  mode  <code>property</code> <code>writable</code>","text":"<p>Inference logging mode (\"NONE\", \"ALL\", \"PREDICTIONS\", or \"MODEL_INPUTS\").</p>"},{"location":"python-api/hsml/inference_logger/#hsml.inference_logger.InferenceLogger.describe","title":"[source]  describe","text":"<pre><code>describe()\n</code></pre> <p>Print a JSON description of the inference logger.</p>"},{"location":"python-api/hsml/model/","title":"hsml.model","text":""},{"location":"python-api/hsml/model/#hsml.model","title":"hsml.model","text":""},{"location":"python-api/hsml/model/#hsml.model.Model","title":"[source]  Model","text":""},{"location":"python-api/hsml/model/#hsml.model.Model.NOT_FOUND_ERROR_CODE","title":"[source]  NOT_FOUND_ERROR_CODE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>NOT_FOUND_ERROR_CODE = 360000\n</code></pre> <p>Metadata object representing a model in the Model Registry.</p>"},{"location":"python-api/hsml/model/#hsml.model.Model.id","title":"[source]  id  <code>property</code> <code>writable</code>","text":"<p>Id of the model.</p>"},{"location":"python-api/hsml/model/#hsml.model.Model.name","title":"[source]  name  <code>property</code> <code>writable</code>","text":"<p>Name of the model.</p>"},{"location":"python-api/hsml/model/#hsml.model.Model.version","title":"[source]  version  <code>property</code> <code>writable</code>","text":"<p>Version of the model.</p>"},{"location":"python-api/hsml/model/#hsml.model.Model.description","title":"[source]  description  <code>property</code> <code>writable</code>","text":"<p>Description of the model.</p>"},{"location":"python-api/hsml/model/#hsml.model.Model.created","title":"[source]  created  <code>property</code> <code>writable</code>","text":"<p>Creation date of the model.</p>"},{"location":"python-api/hsml/model/#hsml.model.Model.creator","title":"[source]  creator  <code>property</code> <code>writable</code>","text":"<p>Creator of the model.</p>"},{"location":"python-api/hsml/model/#hsml.model.Model.environment","title":"[source]  environment  <code>property</code> <code>writable</code>","text":"<p>Input example of the model.</p>"},{"location":"python-api/hsml/model/#hsml.model.Model.training_metrics","title":"[source]  training_metrics  <code>property</code> <code>writable</code>","text":"<p>Training metrics of the model.</p>"},{"location":"python-api/hsml/model/#hsml.model.Model.program","title":"[source]  program  <code>property</code> <code>writable</code>","text":"<p>Executable used to export the model.</p>"},{"location":"python-api/hsml/model/#hsml.model.Model.user","title":"[source]  user  <code>property</code> <code>writable</code>","text":"<p>User of the model.</p>"},{"location":"python-api/hsml/model/#hsml.model.Model.input_example","title":"[source]  input_example  <code>property</code> <code>writable</code>","text":"<p>input_example of the model.</p>"},{"location":"python-api/hsml/model/#hsml.model.Model.framework","title":"[source]  framework  <code>property</code> <code>writable</code>","text":"<p>Framework of the model.</p>"},{"location":"python-api/hsml/model/#hsml.model.Model.model_schema","title":"[source]  model_schema  <code>property</code> <code>writable</code>","text":"<p>Model schema of the model.</p>"},{"location":"python-api/hsml/model/#hsml.model.Model.project_name","title":"[source]  project_name  <code>property</code> <code>writable</code>","text":"<p>project_name of the model.</p>"},{"location":"python-api/hsml/model/#hsml.model.Model.model_registry_id","title":"[source]  model_registry_id  <code>property</code> <code>writable</code>","text":"<p>model_registry_id of the model.</p>"},{"location":"python-api/hsml/model/#hsml.model.Model.model_path","title":"[source]  model_path  <code>property</code>","text":"<p>Path of the model with version folder omitted.</p> <p>Resolves to <code>/Projects/{project_name}/Models/{name}</code>.</p>"},{"location":"python-api/hsml/model/#hsml.model.Model.version_path","title":"[source]  version_path  <code>property</code>","text":"<p>Path of the model including version folder.</p> <p>Resolves to <code>/Projects/{project_name}/Models/{name}/{version}</code>.</p>"},{"location":"python-api/hsml/model/#hsml.model.Model.model_files_path","title":"[source]  model_files_path  <code>property</code>","text":"<p>Path of the model files including version and files folder.</p> <p>Resolves to <code>/Projects/{project_name}/Models/{name}/{version}/Files</code>.</p>"},{"location":"python-api/hsml/model/#hsml.model.Model.shared_registry_project_name","title":"[source]  shared_registry_project_name  <code>property</code> <code>writable</code>","text":"<p>shared_registry_project_name of the model.</p>"},{"location":"python-api/hsml/model/#hsml.model.Model.save","title":"[source]  save","text":"<pre><code>save(\n    model_path,\n    await_registration=480,\n    keep_original_files=False,\n    upload_configuration: dict[str, Any] | None = None,\n)\n</code></pre> <p>Persist this model including model files and metadata to the model registry.</p> PARAMETER DESCRIPTION <code>model_path</code> <p>Local or remote (Hopsworks file system) path to the folder where the model files are located, or path to a specific model file.</p> <p> </p> <code>await_registration</code> <p>Awaiting time for the model to be registered in Hopsworks.</p> <p> DEFAULT: <code>480</code> </p> <code>keep_original_files</code> <p>If the model files are located in hopsfs, whether to move or copy those files into the Models dataset. Default is False (i.e., model files will be moved)</p> <p> DEFAULT: <code>False</code> </p> <code>upload_configuration</code> <p>When saving a model from outside Hopsworks, the model is uploaded to the model registry using the REST APIs. Each model artifact is divided into chunks and each chunk uploaded independently. This parameter can be used to control the upload chunk size, the parallelism and the number of retries. <code>upload_configuration</code> can contain the following keys: * key <code>chunk_size</code>: size of each chunk in megabytes. Default 10. * key <code>simultaneous_uploads</code>: number of chunks to upload in parallel. Default 3. * key <code>max_chunk_retries</code>: number of times to retry the upload of a chunk in case of failure. Default 1.</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <p><code>Model</code>: The model metadata object.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>In case the backend encounters an issue</p>"},{"location":"python-api/hsml/model/#hsml.model.Model.download","title":"[source]  download","text":"<pre><code>download(local_path=None) -&gt; str\n</code></pre> <p>Download the model files.</p> PARAMETER DESCRIPTION <code>local_path</code> <p>path where to download the model files in the local filesystem</p> <p> DEFAULT: <code>None</code> </p> <p>Returns:     <code>str</code>: Absolute path to local folder containing the model files.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>In case the backend encounters an issue</p>"},{"location":"python-api/hsml/model/#hsml.model.Model.delete","title":"[source]  delete","text":"<pre><code>delete()\n</code></pre> <p>Delete the model.</p> Potentially dangerous operation <p>This operation drops all metadata associated with this version of the model and deletes the model files.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>In case the backend encounters an issue</p>"},{"location":"python-api/hsml/model/#hsml.model.Model.deploy","title":"[source]  deploy","text":"<pre><code>deploy(\n    name: str | None = None,\n    description: str | None = None,\n    artifact_version: str | None = None,\n    serving_tool: str | None = None,\n    script_file: str | None = None,\n    config_file: str | None = None,\n    resources: PredictorResources | dict | None = None,\n    inference_logger: InferenceLogger | dict | None = None,\n    inference_batcher: InferenceBatcher\n    | dict\n    | None = None,\n    scaling_configuration: PredictorScalingConfig\n    | dict\n    | None = None,\n    transformer: Transformer | dict | None = None,\n    api_protocol: str | None = IE.API_PROTOCOL_REST,\n    environment: str | None = None,\n) -&gt; deployment.Deployment\n</code></pre> <p>Deploy the model.</p> Example <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# retrieve the trained model you want to deploy\nmy_model = mr.get_model(\"my_model\", version=1)\n\nmy_deployment = my_model.deploy()\n</code></pre> <p>Parameters:     name: Name of the deployment.     description: Description of the deployment.     artifact_version: (Deprecated) Version number of the model artifact to deploy, <code>CREATE</code> to create a new model artifact     or <code>MODEL-ONLY</code> to reuse the shared artifact containing only the model files.     serving_tool: Serving tool used to deploy the model server.     script_file: Path to a custom predictor script implementing the Predict class.     config_file: Model server configuration file to be passed to the model deployment.         It can be accessed via <code>CONFIG_FILE_PATH</code> environment variable from a predictor or transformer script.         For LLM deployments without a predictor script, this file is used to configure the vLLM engine.     resources: Resources to be allocated for the predictor.     inference_logger: Inference logger configuration.     inference_batcher: Inference batcher configuration.     scaling_configuration: Scaling configuration for the predictor.     transformer: Transformer to be deployed together with the predictor.     api_protocol: API protocol to be enabled in the deployment (i.e., 'REST' or 'GRPC'). Defaults to 'REST'.     environment: The inference environment to use.</p> RETURNS DESCRIPTION <code>deployment.Deployment</code> <p><code>Deployment</code>: The deployment metadata object of a new or existing deployment.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>In case the backend encounters an issue</p>"},{"location":"python-api/hsml/model/#hsml.model.Model.add_tag","title":"[source]  add_tag","text":"<pre><code>add_tag(name: str, value: str | dict)\n</code></pre> <p>Attach a tag to a model.</p> <p>A tag consists of a  pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. PARAMETER DESCRIPTION <code>name</code> <p>Name of the tag to be added.</p> <p> TYPE: <code>str</code> </p> <code>value</code> <p>Value of the tag to be added.</p> <p> TYPE: <code>str | dict</code> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>in case the backend fails to add the tag.</p>"},{"location":"python-api/hsml/model/#hsml.model.Model.set_tag","title":"[source]  set_tag","text":"<pre><code>set_tag(name: str, value: str | dict)\n</code></pre> <p>Deprecated: Use add_tag instead.</p>"},{"location":"python-api/hsml/model/#hsml.model.Model.delete_tag","title":"[source]  delete_tag","text":"<pre><code>delete_tag(name: str)\n</code></pre> <p>Delete a tag attached to a model.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the tag to be removed.</p> <p> TYPE: <code>str</code> </p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>in case the backend fails to delete the tag.</p>"},{"location":"python-api/hsml/model/#hsml.model.Model.get_tag","title":"[source]  get_tag","text":"<pre><code>get_tag(name: str) -&gt; str | None\n</code></pre> <p>Get the tags of a model.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the tag to get.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>str | None</code> <p>tag value or <code>None</code> if it does not exist.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>in case the backend fails to retrieve the tag.</p>"},{"location":"python-api/hsml/model/#hsml.model.Model.get_tags","title":"[source]  get_tags","text":"<pre><code>get_tags() -&gt; dict[str, tag.Tag]\n</code></pre> <p>Retrieves all tags attached to a model.</p> RETURNS DESCRIPTION <code>dict[str, tag.Tag]</code> <p><code>Dict[str, obj]</code> of tags.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>In case of a server error.</p>"},{"location":"python-api/hsml/model/#hsml.model.Model.get_url","title":"[source]  get_url","text":"<pre><code>get_url()\n</code></pre> <p>Get url to the model in Hopsworks.</p>"},{"location":"python-api/hsml/model/#hsml.model.Model.get_feature_view","title":"[source]  get_feature_view","text":"<pre><code>get_feature_view(init: bool = True, online: bool = False)\n</code></pre> <p>Get the parent feature view of this model, based on explicit provenance.</p> <p>Only accessible, usable feature view objects are returned. Otherwise an Exception is raised.  For more details, call the base method - get_feature_view_provenance</p> <p>Parameters:     init: By default this is set to True. If you require a more complex initialization of the feature view for online or batch scenarios, you should set <code>init</code> to False to retrieve a non initialized feature view and then call <code>init_batch_scoring()</code> or <code>init_serving()</code> with the required parameters.     online: By default this is set to False and the initialization for batch scoring is considered the default scenario. If you set <code>online</code> to True, the online scenario is enabled and the <code>init_serving()</code> method is called. When inside a deployment, the only available scenario is the online one, thus the parameter is ignored and init_serving is always called (if <code>init</code> is set to True). If you want to override this behaviour, you should set <code>init</code> to False and proceed with a custom initialization.</p> RETURNS DESCRIPTION <p><code>FeatureView</code>: Feature View Object or <code>None</code> if it does not exist.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>in case the backend fails to retrieve the feature view.</p>"},{"location":"python-api/hsml/model/#hsml.model.Model.get_feature_view_provenance","title":"[source]  get_feature_view_provenance","text":"<pre><code>get_feature_view_provenance() -&gt; explicit_provenance.Links\n</code></pre> <p>Get the parent feature view of this model, based on explicit provenance.</p> <p>This feature view can be accessible, deleted or inaccessible. For deleted and inaccessible feature views, only a minimal information is returned.</p> RETURNS DESCRIPTION <code>explicit_provenance.Links</code> <p><code>Links</code>: Object containing the section of provenance graph requested or <code>None</code> if it does not exist.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>in case the backend fails to retrieve the feature view provenance.</p>"},{"location":"python-api/hsml/model/#hsml.model.Model.get_training_dataset_provenance","title":"[source]  get_training_dataset_provenance","text":"<pre><code>get_training_dataset_provenance() -&gt; (\n    explicit_provenance.Links\n)\n</code></pre> <p>Get the parent training dataset of this model, based on explicit provenance.</p> <p>This training dataset can be accessible, deleted or inaccessible. For deleted and inaccessible training datasets, only a minimal information is returned.</p> RETURNS DESCRIPTION <code>explicit_provenance.Links</code> <p><code>Links</code>: Object containing the section of provenance graph requested or <code>None</code> if it does not exist.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>in case the backend fails to retrieve the training dataset provenance.</p>"},{"location":"python-api/hsml/model_registry/","title":"hsml.model_registry","text":""},{"location":"python-api/hsml/model_registry/#hsml.model_registry","title":"hsml.model_registry","text":""},{"location":"python-api/hsml/model_registry/#hsml.model_registry.ModelRegistry","title":"[source]  ModelRegistry","text":""},{"location":"python-api/hsml/model_registry/#hsml.model_registry.ModelRegistry.project_name","title":"[source]  project_name  <code>property</code>","text":"<p>Name of the project the registry is connected to.</p>"},{"location":"python-api/hsml/model_registry/#hsml.model_registry.ModelRegistry.project_path","title":"[source]  project_path  <code>property</code>","text":"<p>Path of the project the registry is connected to.</p>"},{"location":"python-api/hsml/model_registry/#hsml.model_registry.ModelRegistry.project_id","title":"[source]  project_id  <code>property</code>","text":"<p>Id of the project the registry is connected to.</p>"},{"location":"python-api/hsml/model_registry/#hsml.model_registry.ModelRegistry.shared_registry_project_name","title":"[source]  shared_registry_project_name  <code>property</code>","text":"<p>Name of the project the shared model registry originates from.</p>"},{"location":"python-api/hsml/model_registry/#hsml.model_registry.ModelRegistry.model_registry_id","title":"[source]  model_registry_id  <code>property</code>","text":"<p>Id of the model registry.</p>"},{"location":"python-api/hsml/model_registry/#hsml.model_registry.ModelRegistry.tensorflow","title":"[source]  tensorflow  <code>property</code>","text":"<p>Module for exporting a TensorFlow model.</p>"},{"location":"python-api/hsml/model_registry/#hsml.model_registry.ModelRegistry.sklearn","title":"[source]  sklearn  <code>property</code>","text":"<p>Module for exporting a sklearn model.</p>"},{"location":"python-api/hsml/model_registry/#hsml.model_registry.ModelRegistry.torch","title":"[source]  torch  <code>property</code>","text":"<p>Module for exporting a torch model.</p>"},{"location":"python-api/hsml/model_registry/#hsml.model_registry.ModelRegistry.python","title":"[source]  python  <code>property</code>","text":"<p>Module for exporting a generic Python model.</p>"},{"location":"python-api/hsml/model_registry/#hsml.model_registry.ModelRegistry.llm","title":"[source]  llm  <code>property</code>","text":"<p>Module for exporting a Large Language Model.</p>"},{"location":"python-api/hsml/model_registry/#hsml.model_registry.ModelRegistry.get_model","title":"[source]  get_model","text":"<pre><code>get_model(\n    name: str, version: int = None\n) -&gt; model.Model | None\n</code></pre> <p>Get a model entity from the model registry.</p> <p>Getting a model from the Model Registry means getting its metadata handle so you can subsequently download the model directory.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the model to get.</p> <p> TYPE: <code>str</code> </p> <code>version</code> <p>Version of the model to retrieve, defaults to <code>None</code> and will return the <code>version=1</code>.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>model.Model | None</code> <p><code>Model</code>: The model metadata object or <code>None</code> if it does not exist.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If unable to retrieve model from the model registry.</p>"},{"location":"python-api/hsml/model_registry/#hsml.model_registry.ModelRegistry.get_models","title":"[source]  get_models","text":"<pre><code>get_models(name: str) -&gt; list[model.Model]\n</code></pre> <p>Get all model entities from the model registry for a specified name.</p> <p>Getting all models from the Model Registry for a given name returns a list of model entities, one for each version registered under the specified model name.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the model to get.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>list[model.Model]</code> <p><code>List[Model]</code>: A list of model metadata objects.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If unable to retrieve model versions from the model registry.</p>"},{"location":"python-api/hsml/model_registry/#hsml.model_registry.ModelRegistry.get_best_model","title":"[source]  get_best_model","text":"<pre><code>get_best_model(\n    name: str, metric: str, direction: str\n) -&gt; model.Model | None\n</code></pre> <p>Get the best performing model entity from the model registry.</p> <p>Getting the best performing model from the Model Registry means specifying in addition to the name, also a metric name corresponding to one of the keys in the training_metrics dict of the model and a direction. For example, to get the model version with the highest accuracy, specify metric='accuracy' and direction='max'.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the model to get.</p> <p> TYPE: <code>str</code> </p> <code>metric</code> <p>Name of the key in the training metrics field to compare.</p> <p> TYPE: <code>str</code> </p> <code>direction</code> <p>'max' to get the model entity with the highest value of the set metric, or 'min' for the lowest.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>model.Model | None</code> <p><code>Model</code>: The model metadata object or <code>None</code> if it does not exist.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If unable to retrieve model from the model registry.</p>"},{"location":"python-api/hsml/model_schema/","title":"hsml.model_schema","text":""},{"location":"python-api/hsml/model_schema/#hsml.model_schema","title":"hsml.model_schema","text":""},{"location":"python-api/hsml/model_schema/#hsml.model_schema.ModelSchema","title":"[source]  ModelSchema","text":"<p>Create a schema for a model.</p> PARAMETER DESCRIPTION <code>input_schema</code> <p>Schema to describe the inputs.</p> <p> TYPE: <code>Schema | None</code> DEFAULT: <code>None</code> </p> <code>output_schema</code> <p>Schema to describe the outputs.</p> <p> TYPE: <code>Schema | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <p><code>ModelSchema</code>. The model schema object.</p>"},{"location":"python-api/hsml/model_schema/#hsml.model_schema.ModelSchema.to_dict","title":"[source]  to_dict","text":"<pre><code>to_dict()\n</code></pre> <p>Get dict representation of the ModelSchema.</p>"},{"location":"python-api/hsml/model_serving/","title":"hsml.model_serving","text":""},{"location":"python-api/hsml/model_serving/#hsml.model_serving","title":"hsml.model_serving","text":""},{"location":"python-api/hsml/model_serving/#hsml.model_serving.ModelServing","title":"[source]  ModelServing","text":""},{"location":"python-api/hsml/model_serving/#hsml.model_serving.ModelServing.project_name","title":"[source]  project_name  <code>property</code>","text":"<p>Name of the project in which Model Serving is located.</p>"},{"location":"python-api/hsml/model_serving/#hsml.model_serving.ModelServing.project_path","title":"[source]  project_path  <code>property</code>","text":"<p>Path of the project the registry is connected to.</p>"},{"location":"python-api/hsml/model_serving/#hsml.model_serving.ModelServing.project_id","title":"[source]  project_id  <code>property</code>","text":"<p>Id of the project in which Model Serving is located.</p>"},{"location":"python-api/hsml/model_serving/#hsml.model_serving.ModelServing.get_deployment_by_id","title":"[source]  get_deployment_by_id","text":"<pre><code>get_deployment_by_id(id: int) -&gt; Deployment | None\n</code></pre> <p>Get a deployment by id from Model Serving.</p> <p>Getting a deployment from Model Serving means getting its metadata handle so you can subsequently operate on it (e.g., start or stop).</p> Example <pre><code># login and get Hopsworks Model Serving handle using .login() and .get_model_serving()\n\n# get a deployment by id\nmy_deployment = ms.get_deployment_by_id(1)\n</code></pre> PARAMETER DESCRIPTION <code>id</code> <p>Id of the deployment to get.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>Deployment | None</code> <p><code>Deployment</code>: The deployment metadata object or <code>None</code> if it does not exist.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If unable to retrieve deployment from model serving.</p>"},{"location":"python-api/hsml/model_serving/#hsml.model_serving.ModelServing.get_deployment","title":"[source]  get_deployment","text":"<pre><code>get_deployment(name: str = None) -&gt; Deployment | None\n</code></pre> <p>Get a deployment by name from Model Serving.</p> Example <pre><code># login and get Hopsworks Model Serving handle using .login() and .get_model_serving()\n\n# get a deployment by name\nmy_deployment = ms.get_deployment('deployment_name')\n</code></pre> <p>Getting a deployment from Model Serving means getting its metadata handle so you can subsequently operate on it (e.g., start or stop).</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the deployment to get.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Deployment | None</code> <p><code>Deployment</code>: The deployment metadata object or <code>None</code> if it does not exist.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If unable to retrieve deployment from model serving.</p>"},{"location":"python-api/hsml/model_serving/#hsml.model_serving.ModelServing.get_deployments","title":"[source]  get_deployments","text":"<pre><code>get_deployments(\n    model: Model = None, status: str = None\n) -&gt; list[Deployment]\n</code></pre> <p>Get all deployments from model serving.</p> Example <pre><code># login into Hopsworks using hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n\n# retrieve the trained model you want to deploy\nmy_model = mr.get_model(\"my_model\", version=1)\n\nlist_deployments = ms.get_deployment(my_model)\n\nfor deployment in list_deployments:\n    print(deployment.get_state())\n</code></pre> <p>Parameters:     model: Filter by model served in the deployments     status: Filter by status of the deployments Returns:     <code>List[Deployment]</code>: A list of deployments.</p> RAISES DESCRIPTION <code>hopsworks.client.exceptions.RestAPIError</code> <p>If unable to retrieve deployments from model serving.</p>"},{"location":"python-api/hsml/model_serving/#hsml.model_serving.ModelServing.get_inference_endpoints","title":"[source]  get_inference_endpoints","text":"<pre><code>get_inference_endpoints() -&gt; list[InferenceEndpoint]\n</code></pre> <p>Get all inference endpoints available in the current project.</p> RETURNS DESCRIPTION <code>list[InferenceEndpoint]</code> <p><code>List[InferenceEndpoint]</code>: Inference endpoints for model inference</p>"},{"location":"python-api/hsml/model_serving/#hsml.model_serving.ModelServing.create_predictor","title":"[source]  create_predictor","text":"<pre><code>create_predictor(\n    model: Model,\n    name: str | None = None,\n    artifact_version: str | None = None,\n    serving_tool: str | None = None,\n    script_file: str | None = None,\n    config_file: str | None = None,\n    resources: PredictorResources | dict | None = None,\n    inference_logger: InferenceLogger\n    | dict\n    | str\n    | None = None,\n    inference_batcher: InferenceBatcher\n    | dict\n    | None = None,\n    transformer: Transformer | dict | None = None,\n    api_protocol: str | None = IE.API_PROTOCOL_REST,\n    environment: str | None = None,\n    scaling_configuration: PredictorScalingConfig\n    | dict\n    | None = None,\n) -&gt; Predictor\n</code></pre> <p>Create a Predictor metadata object.</p> Example <pre><code># login into Hopsworks using hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# retrieve the trained model you want to deploy\nmy_model = mr.get_model(\"my_model\", version=1)\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n\nmy_predictor = ms.create_predictor(my_model)\n\nmy_deployment = my_predictor.deploy()\n</code></pre> Lazy <p>This method is lazy and does not persist any metadata or deploy any model on its own. To create a deployment using this predictor, call the <code>deploy()</code> method.</p> PARAMETER DESCRIPTION <code>model</code> <p>Model to be deployed.</p> <p> TYPE: <code>Model</code> </p> <code>name</code> <p>Name of the predictor.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>artifact_version</code> <p>(Deprecated) Version number of the model artifact to deploy, <code>CREATE</code> to create a new model artifact</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>serving_tool</code> <p>Serving tool used to deploy the model server.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>script_file</code> <p>Path to a custom predictor script implementing the Predict class.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>config_file</code> <p>Model server configuration file to be passed to the model deployment. It can be accessed via <code>CONFIG_FILE_PATH</code> environment variable from a predictor script. For LLM deployments without a predictor script, this file is used to configure the vLLM engine.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>resources</code> <p>Resources to be allocated for the predictor.</p> <p> TYPE: <code>PredictorResources | dict | None</code> DEFAULT: <code>None</code> </p> <code>inference_logger</code> <p>Inference logger configuration.</p> <p> TYPE: <code>InferenceLogger | dict | str | None</code> DEFAULT: <code>None</code> </p> <code>inference_batcher</code> <p>Inference batcher configuration.</p> <p> TYPE: <code>InferenceBatcher | dict | None</code> DEFAULT: <code>None</code> </p> <code>transformer</code> <p>Transformer to be deployed together with the predictor.</p> <p> TYPE: <code>Transformer | dict | None</code> DEFAULT: <code>None</code> </p> <code>api_protocol</code> <p>API protocol to be enabled in the deployment (i.e., 'REST' or 'GRPC'). Defaults to 'REST'.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>IE.API_PROTOCOL_REST</code> </p> <code>environment</code> <p>The project Python environment to use</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>scaling_configuration</code> <p>Scaling configuration for the predictor.</p> <p> TYPE: <code>PredictorScalingConfig | dict | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Predictor</code> <p><code>Predictor</code>. The predictor metadata object.</p>"},{"location":"python-api/hsml/model_serving/#hsml.model_serving.ModelServing.create_transformer","title":"[source]  create_transformer","text":"<pre><code>create_transformer(\n    script_file: str | None = None,\n    resources: PredictorResources | dict | None = None,\n    scaling_configuration: TransformerScalingConfig\n    | dict\n    | None = None,\n) -&gt; Transformer\n</code></pre> <p>Create a Transformer metadata object.</p> Example <pre><code># login into Hopsworks using hopsworks.login()\n\n# get Dataset API instance\ndataset_api = project.get_dataset_api()\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n\n# create my_transformer.py Python script\nclass Transformer(object):\n    def __init__(self):\n        ''' Initialization code goes here '''\n        pass\n\n    def preprocess(self, inputs):\n        ''' Transform the requests inputs here. The object returned by this method will be used as model input to make predictions. '''\n        return inputs\n\n    def postprocess(self, outputs):\n        ''' Transform the predictions computed by the model before returning a response '''\n        return outputs\n\nuploaded_file_path = dataset_api.upload(\"my_transformer.py\", \"Resources\", overwrite=True)\ntransformer_script_path = os.path.join(\"/Projects\", project.name, uploaded_file_path)\n\nmy_transformer = ms.create_transformer(script_file=uploaded_file_path)\n\n# or\n\nfrom hsml.transformer import Transformer\n\nmy_transformer = Transformer(script_file)\n</code></pre> Create a deployment with the transformer <pre><code>my_predictor = ms.create_predictor(transformer=my_transformer)\nmy_deployment = my_predictor.deploy()\n\n# or\nmy_deployment = ms.create_deployment(my_predictor, transformer=my_transformer)\nmy_deployment.save()\n</code></pre> Lazy <p>This method is lazy and does not persist any metadata or deploy any transformer. To create a deployment using this transformer, set it in the <code>predictor.transformer</code> property.</p> PARAMETER DESCRIPTION <code>script_file</code> <p>Path to a custom predictor script implementing the Transformer class.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>resources</code> <p>Resources to be allocated for the transformer.</p> <p> TYPE: <code>PredictorResources | dict | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Transformer</code> <p><code>Transformer</code>. The transformer metadata object.</p>"},{"location":"python-api/hsml/model_serving/#hsml.model_serving.ModelServing.create_endpoint","title":"[source]  create_endpoint","text":"<pre><code>create_endpoint(\n    name: str,\n    script_file: str,\n    description: str | None = None,\n    resources: PredictorResources | dict | None = None,\n    inference_logger: InferenceLogger\n    | dict\n    | str\n    | None = None,\n    inference_batcher: InferenceBatcher\n    | dict\n    | None = None,\n    api_protocol: str | None = IE.API_PROTOCOL_REST,\n    environment: str | None = None,\n    scaling_configuration: PredictorScalingConfig\n    | dict\n    | None = None,\n) -&gt; Predictor\n</code></pre> <p>Create an Entrypoint metadata object.</p> Example <pre><code># login into Hopsworks using hopsworks.login()\n\n# get Hopsworks Model Registry handle\nms = project.get_model_serving()\n\nmy_endpoint = ms.create_entrypoint(name=\"feature_server\", entrypoint_file=\"feature_server.py\")\n\nmy_deployment = my_endpoint.deploy()\n</code></pre> Lazy <p>This method is lazy and does not persist any metadata or deploy any endpoint on its own. To create a deployment using this endpoint, call the <code>deploy()</code> method.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the endpoint.</p> <p> TYPE: <code>str</code> </p> <code>script_file</code> <p>Path to a custom script file implementing a HTTP server.</p> <p> TYPE: <code>str</code> </p> <code>description</code> <p>Description of the endpoint.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>resources</code> <p>Resources to be allocated for the predictor.</p> <p> TYPE: <code>PredictorResources | dict | None</code> DEFAULT: <code>None</code> </p> <code>inference_logger</code> <p>Inference logger configuration.</p> <p> TYPE: <code>InferenceLogger | dict | str | None</code> DEFAULT: <code>None</code> </p> <code>inference_batcher</code> <p>Inference batcher configuration.</p> <p> TYPE: <code>InferenceBatcher | dict | None</code> DEFAULT: <code>None</code> </p> <code>api_protocol</code> <p>API protocol to be enabled in the deployment (i.e., 'REST' or 'GRPC'). Defaults to 'REST'.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>IE.API_PROTOCOL_REST</code> </p> <code>environment</code> <p>The project Python environment to use</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>scaling_configuration</code> <p>Scaling configuration for the predictor.</p> <p> TYPE: <code>PredictorScalingConfig | dict | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Predictor</code> <p><code>Predictor</code>. The predictor metadata object.</p>"},{"location":"python-api/hsml/model_serving/#hsml.model_serving.ModelServing.create_deployment","title":"[source]  create_deployment","text":"<pre><code>create_deployment(\n    predictor: Predictor,\n    name: str | None = None,\n    environment: str | None = None,\n) -&gt; Deployment\n</code></pre> <p>Create a Deployment metadata object.</p> Example <pre><code># login into Hopsworks using hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# retrieve the trained model you want to deploy\nmy_model = mr.get_model(\"my_model\", version=1)\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n\nmy_predictor = ms.create_predictor(my_model)\n\nmy_deployment = ms.create_deployment(my_predictor)\nmy_deployment.save()\n</code></pre> Using the model object <pre><code># login into Hopsworks using hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# retrieve the trained model you want to deploy\nmy_model = mr.get_model(\"my_model\", version=1)\n\nmy_deployment = my_model.deploy()\n\nmy_deployment.get_state().describe()\n</code></pre> Using the Model Serving handle <pre><code># login into Hopsworks using hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# retrieve the trained model you want to deploy\nmy_model = mr.get_model(\"my_model\", version=1)\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n\nmy_predictor = ms.create_predictor(my_model)\n\nmy_deployment = my_predictor.deploy()\n\nmy_deployment.get_state().describe()\n</code></pre> Lazy <p>This method is lazy and does not persist any metadata or deploy any model. To create a deployment, call the <code>save()</code> method.</p> PARAMETER DESCRIPTION <code>predictor</code> <p>predictor to be used in the deployment</p> <p> TYPE: <code>Predictor</code> </p> <code>name</code> <p>name of the deployment</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>environment</code> <p>(Deprecated) The project Python environment to use. This argument will be ignored, use the argument <code>environment</code> in the <code>create_predictor()</code> or <code>create_endpoint()</code> methods instead.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Deployment</code> <p><code>Deployment</code>. The deployment metadata object.</p>"},{"location":"python-api/hsml/predictor/","title":"hsml.predictor","text":""},{"location":"python-api/hsml/predictor/#hsml.predictor","title":"hsml.predictor","text":""},{"location":"python-api/hsml/predictor/#hsml.predictor.Predictor","title":"[source]  Predictor","text":"<p>               Bases: <code>DeployableComponent</code></p> <p>Metadata object representing a predictor in Model Serving.</p>"},{"location":"python-api/hsml/predictor/#hsml.predictor.Predictor.id","title":"[source]  id  <code>property</code>","text":"<p>Id of the predictor.</p>"},{"location":"python-api/hsml/predictor/#hsml.predictor.Predictor.name","title":"[source]  name  <code>property</code> <code>writable</code>","text":"<p>Name of the predictor.</p>"},{"location":"python-api/hsml/predictor/#hsml.predictor.Predictor.version","title":"[source]  version  <code>property</code>","text":"<p>Version of the predictor.</p>"},{"location":"python-api/hsml/predictor/#hsml.predictor.Predictor.description","title":"[source]  description  <code>property</code> <code>writable</code>","text":"<p>Description of the predictor.</p>"},{"location":"python-api/hsml/predictor/#hsml.predictor.Predictor.model_name","title":"[source]  model_name  <code>property</code> <code>writable</code>","text":"<p>Name of the model deployed by the predictor.</p>"},{"location":"python-api/hsml/predictor/#hsml.predictor.Predictor.model_path","title":"[source]  model_path  <code>property</code> <code>writable</code>","text":"<p>Model path deployed by the predictor.</p>"},{"location":"python-api/hsml/predictor/#hsml.predictor.Predictor.model_version","title":"[source]  model_version  <code>property</code> <code>writable</code>","text":"<p>Model version deployed by the predictor.</p>"},{"location":"python-api/hsml/predictor/#hsml.predictor.Predictor.model_framework","title":"[source]  model_framework  <code>property</code> <code>writable</code>","text":"<p>Model framework of the model to be deployed by the predictor.</p>"},{"location":"python-api/hsml/predictor/#hsml.predictor.Predictor.artifact_version","title":"[source]  artifact_version  <code>property</code> <code>writable</code>","text":"<p>Artifact version deployed by the predictor.</p> Deprecated <p>Artifact versions are deprecated in favor of deployment versions.</p>"},{"location":"python-api/hsml/predictor/#hsml.predictor.Predictor.artifact_files_path","title":"[source]  artifact_files_path  <code>property</code>","text":"<p>Path of the artifact files deployed by the predictor.</p>"},{"location":"python-api/hsml/predictor/#hsml.predictor.Predictor.artifact_path","title":"[source]  artifact_path  <code>property</code>","text":"<p>Path of the model artifact deployed by the predictor. Resolves to /Projects/{project_name}/Models/{name}/{version}/Artifacts/{artifact_version}/{name}{version}.zip.</p>"},{"location":"python-api/hsml/predictor/#hsml.predictor.Predictor.model_server","title":"[source]  model_server  <code>property</code>","text":"<p>Model server used by the predictor.</p>"},{"location":"python-api/hsml/predictor/#hsml.predictor.Predictor.serving_tool","title":"[source]  serving_tool  <code>property</code> <code>writable</code>","text":"<p>Serving tool used to run the model server.</p>"},{"location":"python-api/hsml/predictor/#hsml.predictor.Predictor.script_file","title":"[source]  script_file  <code>property</code> <code>writable</code>","text":"<p>Script file used to load and run the model.</p>"},{"location":"python-api/hsml/predictor/#hsml.predictor.Predictor.config_file","title":"[source]  config_file  <code>property</code> <code>writable</code>","text":"<p>Model server configuration file passed to the model deployment.</p> <p>It can be accessed via <code>CONFIG_FILE_PATH</code> environment variable from a predictor or transformer script. For LLM deployments without a predictor script, this file is used to configure the vLLM engine.</p>"},{"location":"python-api/hsml/predictor/#hsml.predictor.Predictor.inference_logger","title":"[source]  inference_logger  <code>property</code> <code>writable</code>","text":"<p>Configuration of the inference logger attached to this predictor.</p>"},{"location":"python-api/hsml/predictor/#hsml.predictor.Predictor.transformer","title":"[source]  transformer  <code>property</code> <code>writable</code>","text":"<p>Transformer configuration attached to the predictor.</p>"},{"location":"python-api/hsml/predictor/#hsml.predictor.Predictor.created_at","title":"[source]  created_at  <code>property</code>","text":"<p>Created at date of the predictor.</p>"},{"location":"python-api/hsml/predictor/#hsml.predictor.Predictor.creator","title":"[source]  creator  <code>property</code>","text":"<p>Creator of the predictor.</p>"},{"location":"python-api/hsml/predictor/#hsml.predictor.Predictor.requested_instances","title":"[source]  requested_instances  <code>property</code>","text":"<p>Total number of requested instances in the predictor.</p>"},{"location":"python-api/hsml/predictor/#hsml.predictor.Predictor.api_protocol","title":"[source]  api_protocol  <code>property</code> <code>writable</code>","text":"<p>API protocol enabled in the predictor (e.g., HTTP or GRPC).</p>"},{"location":"python-api/hsml/predictor/#hsml.predictor.Predictor.environment","title":"[source]  environment  <code>property</code> <code>writable</code>","text":"<p>Name of the inference environment.</p>"},{"location":"python-api/hsml/predictor/#hsml.predictor.Predictor.project_namespace","title":"[source]  project_namespace  <code>property</code> <code>writable</code>","text":"<p>Kubernetes project namespace.</p>"},{"location":"python-api/hsml/predictor/#hsml.predictor.Predictor.project_name","title":"[source]  project_name  <code>property</code> <code>writable</code>","text":"<p>Name of the project the deployment belongs to.</p>"},{"location":"python-api/hsml/predictor/#hsml.predictor.Predictor.deploy","title":"[source]  deploy","text":"<pre><code>deploy()\n</code></pre> <p>Create a deployment for this predictor and persists it in the Model Serving.</p> Example <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# retrieve the trained model you want to deploy\nmy_model = mr.get_model(\"my_model\", version=1)\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n\nmy_predictor = ms.create_predictor(my_model)\nmy_deployment = my_predictor.deploy()\n\nprint(my_deployment.get_state())\n</code></pre> RETURNS DESCRIPTION <p><code>Deployment</code>. The deployment metadata object of a new or existing deployment.</p>"},{"location":"python-api/hsml/predictor/#hsml.predictor.Predictor.describe","title":"[source]  describe","text":"<pre><code>describe()\n</code></pre> <p>Print a JSON description of the predictor.</p>"},{"location":"python-api/hsml/predictor_state/","title":"hsml.predictor_state","text":""},{"location":"python-api/hsml/predictor_state/#hsml.predictor_state","title":"hsml.predictor_state","text":""},{"location":"python-api/hsml/predictor_state/#hsml.predictor_state.PredictorState","title":"[source]  PredictorState","text":"<p>State of a predictor.</p>"},{"location":"python-api/hsml/predictor_state/#hsml.predictor_state.PredictorState.available_predictor_instances","title":"[source]  available_predictor_instances  <code>property</code>","text":"<p>Available predicotr instances.</p>"},{"location":"python-api/hsml/predictor_state/#hsml.predictor_state.PredictorState.available_transformer_instances","title":"[source]  available_transformer_instances  <code>property</code>","text":"<p>Available transformer instances.</p>"},{"location":"python-api/hsml/predictor_state/#hsml.predictor_state.PredictorState.hopsworks_inference_path","title":"[source]  hopsworks_inference_path  <code>property</code>","text":"<p>Inference path in the Hopsworks REST API.</p>"},{"location":"python-api/hsml/predictor_state/#hsml.predictor_state.PredictorState.model_server_inference_path","title":"[source]  model_server_inference_path  <code>property</code>","text":"<p>Inference path in the model server.</p>"},{"location":"python-api/hsml/predictor_state/#hsml.predictor_state.PredictorState.internal_port","title":"[source]  internal_port  <code>property</code>","text":"<p>Internal port for the predictor.</p>"},{"location":"python-api/hsml/predictor_state/#hsml.predictor_state.PredictorState.revision","title":"[source]  revision  <code>property</code>","text":"<p>Last revision of the predictor.</p>"},{"location":"python-api/hsml/predictor_state/#hsml.predictor_state.PredictorState.deployed","title":"[source]  deployed  <code>property</code>","text":"<p>Whether the predictor is deployed or not.</p>"},{"location":"python-api/hsml/predictor_state/#hsml.predictor_state.PredictorState.condition","title":"[source]  condition  <code>property</code>","text":"<p>Condition of the current state of predictor.</p>"},{"location":"python-api/hsml/predictor_state/#hsml.predictor_state.PredictorState.status","title":"[source]  status  <code>property</code>","text":"<p>Status of the predictor.</p>"},{"location":"python-api/hsml/predictor_state/#hsml.predictor_state.PredictorState.describe","title":"[source]  describe","text":"<pre><code>describe()\n</code></pre> <p>Print a JSON description of the deployment state.</p>"},{"location":"python-api/hsml/predictor_state_condition/","title":"hsml.predictor_state_condition","text":""},{"location":"python-api/hsml/predictor_state_condition/#hsml.predictor_state_condition","title":"hsml.predictor_state_condition","text":""},{"location":"python-api/hsml/predictor_state_condition/#hsml.predictor_state_condition.PredictorStateCondition","title":"[source]  PredictorStateCondition","text":"<p>Condition of a predictor state.</p>"},{"location":"python-api/hsml/predictor_state_condition/#hsml.predictor_state_condition.PredictorStateCondition.type","title":"[source]  type  <code>property</code>","text":"<p>Condition type of the predictor state.</p>"},{"location":"python-api/hsml/predictor_state_condition/#hsml.predictor_state_condition.PredictorStateCondition.status","title":"[source]  status  <code>property</code>","text":"<p>Condition status of the predictor state.</p>"},{"location":"python-api/hsml/predictor_state_condition/#hsml.predictor_state_condition.PredictorStateCondition.reason","title":"[source]  reason  <code>property</code>","text":"<p>Condition reason of the predictor state.</p>"},{"location":"python-api/hsml/predictor_state_condition/#hsml.predictor_state_condition.PredictorStateCondition.describe","title":"[source]  describe","text":"<pre><code>describe()\n</code></pre> <p>Print a JSON description of the predictor state condition.</p>"},{"location":"python-api/hsml/resources/","title":"hsml.resources","text":""},{"location":"python-api/hsml/resources/#hsml.resources","title":"hsml.resources","text":""},{"location":"python-api/hsml/resources/#hsml.resources.Resources","title":"[source]  Resources","text":"<p>Resource configuration for a predictor or transformer.</p> PARAMETER DESCRIPTION <code>cores</code> <p>Number of CPUs.</p> <p> TYPE: <code>int</code> </p> <code>memory</code> <p>Memory (MB) resources.</p> <p> TYPE: <code>int</code> </p> <code>gpus</code> <p>Number of GPUs.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <p><code>Resources</code>. Resource configuration for a predictor or transformer.</p>"},{"location":"python-api/hsml/resources/#hsml.resources.Resources.cores","title":"[source]  cores  <code>property</code> <code>writable</code>","text":"<p>Number of CPUs to be allocated per instance.</p>"},{"location":"python-api/hsml/resources/#hsml.resources.Resources.memory","title":"[source]  memory  <code>property</code> <code>writable</code>","text":"<p>Memory resources to be allocated per instance.</p>"},{"location":"python-api/hsml/resources/#hsml.resources.Resources.gpus","title":"[source]  gpus  <code>property</code> <code>writable</code>","text":"<p>Number of GPUs to be allocated per instance.</p>"},{"location":"python-api/hsml/resources/#hsml.resources.Resources.describe","title":"[source]  describe","text":"<pre><code>describe()\n</code></pre> <p>Print a JSON description of the resource configuration.</p>"},{"location":"python-api/hsml/schema/","title":"hsml.schema","text":""},{"location":"python-api/hsml/schema/#hsml.schema","title":"hsml.schema","text":""},{"location":"python-api/hsml/schema/#hsml.schema.Schema","title":"[source]  Schema","text":"<p>Create a schema for a model input or output.</p> PARAMETER DESCRIPTION <code>object</code> <p>The object to construct the schema from.</p> <p> TYPE: <code>pandas.DataFrame | pandas.Series | TypeVar('pyspark.sql.dataframe.DataFrame') | TypeVar('hsfs.training_dataset.TrainingDataset') | numpy.ndarray | list | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <p><code>Schema</code>. The schema object.</p>"},{"location":"python-api/hsml/schema/#hsml.schema.Schema.to_dict","title":"[source]  to_dict","text":"<pre><code>to_dict()\n</code></pre> <p>Get dict representation of the Schema.</p>"},{"location":"python-api/hsml/transformer/","title":"hsml.transformer","text":""},{"location":"python-api/hsml/transformer/#hsml.transformer","title":"hsml.transformer","text":""},{"location":"python-api/hsml/transformer/#hsml.transformer.Transformer","title":"[source]  Transformer","text":"<p>               Bases: <code>DeployableComponent</code></p> <p>Metadata object representing a transformer to be used in a predictor.</p>"},{"location":"python-api/hsml/transformer/#hsml.transformer.Transformer.describe","title":"[source]  describe","text":"<pre><code>describe()\n</code></pre> <p>Print a JSON description of the transformer.</p>"}]}