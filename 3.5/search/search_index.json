{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":".md-typeset h1 { font-size: 0em; } Enterprise Data Feature Engineering Frameworks Pandas Flink Spark SQL Storage connectors JDBC BigQuery Object Store Snowflake RedShift Enterprise Feature Store Govern & Monitor Serve Share & Re-use Create .layer_02{pointer-events: none; } .round-frame{ pointer-events: initial; } Project Based Collaboration Write API Feature Groups External Feature Groups Read API Feature Views Training Data Feature Vectors Search , Versioning , Statistics Provenance & Lineage Azure AWS Google Cloud On-premise Enterprise AI MLOps Experiments & Model Training Model Registry Model Serving Operational ML Analytical ML BI Tools Vector DB OpenSearch Hopsworks is a data platform for ML with a Python-centric Feature Store and MLOps capabilities. Hopsworks is a modular platform. You can use it as a standalone Feature Store, you can use it to manage, govern, and serve your models, and you can even use it to develop and operate feature pipelines and training pipelines. Hopsworks brings collaboration for ML teams, providing a secure, governed platform for developing, managing, and sharing ML assets - features, models, training data, batch scoring data, logs, and more. Python-Centric Feature Store # Hopsworks is widely used as a standalone Feature Store. Hopsworks breaks the monolithic model development pipeline into separate feature and training pipelines, enabling both feature reuse and better tested ML assets. You can develop features by building feature pipelines in any Python (or Spark or Flink) environment, either inside or outside Hopsworks. You can use the Python frameworks you are familiar with to build production feature pipelines. You can compute aggregations in Pandas, validate feature data with Great Expectations, reduce your data dimensionality with embeddings and PCA, test your feature logic and features end-to-end with PyTest, and transform your categorical and numerical features with Scikit-Learn, TensorFlow, and PyTorch. You can orchestrate your feature pipelines with your Python framework of choice, including Hopsworks' own Airflow support. The Widest Feature Store Capabilities # Hopsworks Feature Store also supports feature pipelines in PySpark, Spark, Flink, and SQL. Offline features can either be stored in Hopsworks, as Hudi tables on object storage, or in external data lakehouses (Snowflake, Databricks, Redshift, BigQuery, any JDBC-enabled platform) via External Feature Groups. Online features are served by RonDB , developed by Hopsworks as the lowest latency, highest throughput, highest availability data store for your features. MLOps on Hopsworks # Hopsworks provides model serving capabilities through KServe, with additional support for feature/prediction logging to Kafka (also part of Hopsworks), and secure, low-latency model deployments via Istio. Hopsworks also has a Model Registry for KServe, with support for versioning both models and model assets (such as KServe transformers). Hopsworks also includes a vector database to provide similarity search capabilities for embeddings, based on OpenSearch . Project-based Multi-Tenancy and Team Collaboration # Hopsworks provides projects as a secure sandbox in which teams can collaborate and share ML assets. Hopsworks' unique multi-tenant project model even enables sensitive data to be stored in a shared cluster, while still providing fine-grained sharing capabilities for ML assets across project boundaries. Projects can be used to structure teams so that they have end-to-end responsibility from raw data to managed features and models. Projects can also be used to create development, staging, and production environments for data teams. All ML assets support versioning, lineage, and provenance provide all Hopsworks users with a complete view of the MLOps life cycle, from feature engineering through model serving. Development and Operations # Hopsworks provides development tools for Data Science, including conda environments for Python, Jupyter notebooks, jobs, or even notebooks as jobs. You can build production pipelines with the bundled Airflow, and even run ML training pipelines with GPUs in notebooks on Airflow. You can train models on as many GPUs as are installed in a Hopsworks cluster and easily share them among users. You can also run Spark, Spark Streaming, or Flink programs on Hopsworks, with support for elastic workers in the cloud (add/remove workers dynamically). Available on any Platform # Hopsworks is available as a both managed platform in the cloud on AWS, Azure, and GCP, and can be installed on any Linux-based virtual machines (Ubuntu/Redhat compatible), even in air-gapped data centers. Hopsworks is also available as a serverless platform that manages and serves both your features and models. Join the community # Ask questions and give us feedback in the Hopsworks Community Follow us on Twitter Check out all our latest product releases Join our public slack-channel Contribute # We are building the most complete and modular ML platform available in the market, and we count on your support to continuously improve Hopsworks. Feel free to give us suggestions , report bugs and add features to our library anytime. Open-Source # Hopsworks is available under the AGPL-V3 license. In plain English this means that you are free to use Hopsworks and even build paid services on it, but if you modify the source code, you should also release back your changes and any systems built around it as AGPL-V3. We're the best at what we do, and we strive to keep the same standard for our community! Our many thanks to the contributors of Hopsworks.","title":"Home"},{"location":"#python-centric-feature-store","text":"Hopsworks is widely used as a standalone Feature Store. Hopsworks breaks the monolithic model development pipeline into separate feature and training pipelines, enabling both feature reuse and better tested ML assets. You can develop features by building feature pipelines in any Python (or Spark or Flink) environment, either inside or outside Hopsworks. You can use the Python frameworks you are familiar with to build production feature pipelines. You can compute aggregations in Pandas, validate feature data with Great Expectations, reduce your data dimensionality with embeddings and PCA, test your feature logic and features end-to-end with PyTest, and transform your categorical and numerical features with Scikit-Learn, TensorFlow, and PyTorch. You can orchestrate your feature pipelines with your Python framework of choice, including Hopsworks' own Airflow support.","title":"Python-Centric Feature Store"},{"location":"#the-widest-feature-store-capabilities","text":"Hopsworks Feature Store also supports feature pipelines in PySpark, Spark, Flink, and SQL. Offline features can either be stored in Hopsworks, as Hudi tables on object storage, or in external data lakehouses (Snowflake, Databricks, Redshift, BigQuery, any JDBC-enabled platform) via External Feature Groups. Online features are served by RonDB , developed by Hopsworks as the lowest latency, highest throughput, highest availability data store for your features.","title":"The Widest Feature Store Capabilities"},{"location":"#mlops-on-hopsworks","text":"Hopsworks provides model serving capabilities through KServe, with additional support for feature/prediction logging to Kafka (also part of Hopsworks), and secure, low-latency model deployments via Istio. Hopsworks also has a Model Registry for KServe, with support for versioning both models and model assets (such as KServe transformers). Hopsworks also includes a vector database to provide similarity search capabilities for embeddings, based on OpenSearch .","title":"MLOps on Hopsworks"},{"location":"#project-based-multi-tenancy-and-team-collaboration","text":"Hopsworks provides projects as a secure sandbox in which teams can collaborate and share ML assets. Hopsworks' unique multi-tenant project model even enables sensitive data to be stored in a shared cluster, while still providing fine-grained sharing capabilities for ML assets across project boundaries. Projects can be used to structure teams so that they have end-to-end responsibility from raw data to managed features and models. Projects can also be used to create development, staging, and production environments for data teams. All ML assets support versioning, lineage, and provenance provide all Hopsworks users with a complete view of the MLOps life cycle, from feature engineering through model serving.","title":"Project-based Multi-Tenancy and Team Collaboration"},{"location":"#development-and-operations","text":"Hopsworks provides development tools for Data Science, including conda environments for Python, Jupyter notebooks, jobs, or even notebooks as jobs. You can build production pipelines with the bundled Airflow, and even run ML training pipelines with GPUs in notebooks on Airflow. You can train models on as many GPUs as are installed in a Hopsworks cluster and easily share them among users. You can also run Spark, Spark Streaming, or Flink programs on Hopsworks, with support for elastic workers in the cloud (add/remove workers dynamically).","title":"Development and Operations"},{"location":"#available-on-any-platform","text":"Hopsworks is available as a both managed platform in the cloud on AWS, Azure, and GCP, and can be installed on any Linux-based virtual machines (Ubuntu/Redhat compatible), even in air-gapped data centers. Hopsworks is also available as a serverless platform that manages and serves both your features and models.","title":"Available on any Platform"},{"location":"#join-the-community","text":"Ask questions and give us feedback in the Hopsworks Community Follow us on Twitter Check out all our latest product releases Join our public slack-channel","title":"Join the community"},{"location":"#contribute","text":"We are building the most complete and modular ML platform available in the market, and we count on your support to continuously improve Hopsworks. Feel free to give us suggestions , report bugs and add features to our library anytime.","title":"Contribute"},{"location":"#open-source","text":"Hopsworks is available under the AGPL-V3 license. In plain English this means that you are free to use Hopsworks and even build paid services on it, but if you modify the source code, you should also release back your changes and any systems built around it as AGPL-V3. We're the best at what we do, and we strive to keep the same standard for our community! Our many thanks to the contributors of Hopsworks.","title":"Open-Source"},{"location":"admin/","text":"Cluster Administration # Hopsworks has a cluster management page that allows you, the administrator, to perform management actions, monitor and control Hopsworks. To access the cluster management page you should log in into Hopsworks using your administrator account. In the top right corner, click on your name in the top right corner of the navigation bar and choose Cluster Settings from the dropdown menu.","title":"Introduction"},{"location":"admin/#cluster-administration","text":"Hopsworks has a cluster management page that allows you, the administrator, to perform management actions, monitor and control Hopsworks. To access the cluster management page you should log in into Hopsworks using your administrator account. In the top right corner, click on your name in the top right corner of the navigation bar and choose Cluster Settings from the dropdown menu.","title":"Cluster Administration"},{"location":"admin/alert/","text":"Configure Alerts # Introduction # Alerts are sent from Hopsworks using Prometheus' Alert manager . In order to send alerts we first need to configure the Alert manager . Prerequisites # Administrator account on a Hopsworks cluster. Step 1: Go to alerts configuration # To configure the Alert manager click on your name in the top right corner of the navigation bar and choose Cluster Settings from the dropdown menu. In the Cluster Settings' Alerts tab you can configure the alert manager to send alerts via email, slack or pagerduty. Configure alerts Step 2: Configure Email Alerts # To send alerts via email you need to configure an SMTP server. Click on the Configure button on the left side of the email row and fill out the form that pops up. Configure Email Alerts Default from : the address used as sender in the alert email. SMTP smarthost : the Simple Mail Transfer Protocol (SMTP) host through which emails are sent. Default hostname (optional) : hostname to identify to the SMTP server. Authentication method : how to authenticate to the SMTP server. CRAM-MD5, LOGIN or PLAIN. Optionally cluster wide Email alert receivers can be added in Default receiver emails . These receivers will be available to all users when they create event triggered alerts . Step 3: Configure Slack Alerts # Alerts can also be sent via Slack messages. To be able to send Slack messages you first need to configure a Slack webhook. Click on the Configure button on the left side of the slack row and past in your Slack webhook in Webhook . Configure slack Alerts Optionally cluster wide Slack alert receivers can be added in Slack channel/user . These receivers will be available to all users when they create event triggered alerts . Step 4: Configure Pagerduty Alerts # Pagerduty is another way you can send alerts from Hopsworks. Click on the Configure button on the left side of the pagerduty row and fill out the form that pops up. Configure Pagerduty Alerts Fill in Pagerduty URL: the URL to send API requests to. Optionally cluster wide Pagerduty alert receivers can be added in Service key/Routing key . By first choosing the PagerDuty integration type: global event routing (routing_key) : when using PagerDuty integration type Events API v2 . service (service_key) : when using PagerDuty integration type Prometheus . Then adding the Service key/Routing key of the receiver(s). PagerDuty provides documentation on how to integrate with Prometheus' Alert manager. Step 5: Advanced configuration # If you are familiar with Prometheus' Alert manager you can also configure alerts by editing the yaml/json file directly. Advanced configuration Example: Adding the yaml snippet shown below in the global section of the alert manager configuration will have the same effect as creating the SMTP configuration as shown in section 1 above. global : smtp_smarthost : smtp.gmail.com:587 smtp_from : hopsworks@gmail.com smtp_auth_username : hopsworks@gmail.com smtp_auth_password : XXXXXXXXX smtp_auth_identity : hopsworks@gmail.com ... To test the alerts by creating triggers from Jobs and Feature group validations see Alerts . The yaml syntax in the UI is slightly different in that it does not allow double quotes (it will ignore the values but give no error). Below is an example configuration, that can be used in the UI, with both email and slack receivers configured for system alerts. global : smtp_smarthost : smtp.gmail.com:587 smtp_from : hopsworks@gmail.com smtp_auth_username : hopsworks@gmail.com smtp_auth_password : XXXXXXXXX smtp_auth_identity : hopsworks@gmail.com resolveTimeout : 5m templates : - /srv/hops/alertmanager/alertmanager-0.17.0.linux-amd64/template/*.tmpl route : receiver : default routes : - receiver : email continue : true match : type : system-alert - receiver : slack continue : true match : type : system-alert groupBy : - alertname groupWait : 10s groupInterval : 10s receivers : - name : default - name : email emailConfigs : - to : someone@logicalclocks.com from : hopsworks@logicalclocks.com smarthost : mail.hello.com text : >- summary: {{ .CommonAnnotations.summary }} description: {{ .CommonAnnotations.description }} - name : slack slackConfigs : - apiUrl : >- https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX channel : '#general' text : >- <!channel> summary: {{ .Annotations.summary }} description: {{ .Annotations.description }} Conclusion # In this guide you learned how to configure alerts in Hopsworks.","title":"Configure Alerts"},{"location":"admin/alert/#configure-alerts","text":"","title":"Configure Alerts"},{"location":"admin/alert/#introduction","text":"Alerts are sent from Hopsworks using Prometheus' Alert manager . In order to send alerts we first need to configure the Alert manager .","title":"Introduction"},{"location":"admin/alert/#prerequisites","text":"Administrator account on a Hopsworks cluster.","title":"Prerequisites"},{"location":"admin/alert/#step-1-go-to-alerts-configuration","text":"To configure the Alert manager click on your name in the top right corner of the navigation bar and choose Cluster Settings from the dropdown menu. In the Cluster Settings' Alerts tab you can configure the alert manager to send alerts via email, slack or pagerduty. Configure alerts","title":"Step 1: Go to alerts configuration"},{"location":"admin/alert/#step-2-configure-email-alerts","text":"To send alerts via email you need to configure an SMTP server. Click on the Configure button on the left side of the email row and fill out the form that pops up. Configure Email Alerts Default from : the address used as sender in the alert email. SMTP smarthost : the Simple Mail Transfer Protocol (SMTP) host through which emails are sent. Default hostname (optional) : hostname to identify to the SMTP server. Authentication method : how to authenticate to the SMTP server. CRAM-MD5, LOGIN or PLAIN. Optionally cluster wide Email alert receivers can be added in Default receiver emails . These receivers will be available to all users when they create event triggered alerts .","title":"Step 2: Configure Email Alerts"},{"location":"admin/alert/#step-3-configure-slack-alerts","text":"Alerts can also be sent via Slack messages. To be able to send Slack messages you first need to configure a Slack webhook. Click on the Configure button on the left side of the slack row and past in your Slack webhook in Webhook . Configure slack Alerts Optionally cluster wide Slack alert receivers can be added in Slack channel/user . These receivers will be available to all users when they create event triggered alerts .","title":"Step 3: Configure Slack Alerts"},{"location":"admin/alert/#step-4-configure-pagerduty-alerts","text":"Pagerduty is another way you can send alerts from Hopsworks. Click on the Configure button on the left side of the pagerduty row and fill out the form that pops up. Configure Pagerduty Alerts Fill in Pagerduty URL: the URL to send API requests to. Optionally cluster wide Pagerduty alert receivers can be added in Service key/Routing key . By first choosing the PagerDuty integration type: global event routing (routing_key) : when using PagerDuty integration type Events API v2 . service (service_key) : when using PagerDuty integration type Prometheus . Then adding the Service key/Routing key of the receiver(s). PagerDuty provides documentation on how to integrate with Prometheus' Alert manager.","title":"Step 4: Configure Pagerduty Alerts"},{"location":"admin/alert/#step-5-advanced-configuration","text":"If you are familiar with Prometheus' Alert manager you can also configure alerts by editing the yaml/json file directly. Advanced configuration Example: Adding the yaml snippet shown below in the global section of the alert manager configuration will have the same effect as creating the SMTP configuration as shown in section 1 above. global : smtp_smarthost : smtp.gmail.com:587 smtp_from : hopsworks@gmail.com smtp_auth_username : hopsworks@gmail.com smtp_auth_password : XXXXXXXXX smtp_auth_identity : hopsworks@gmail.com ... To test the alerts by creating triggers from Jobs and Feature group validations see Alerts . The yaml syntax in the UI is slightly different in that it does not allow double quotes (it will ignore the values but give no error). Below is an example configuration, that can be used in the UI, with both email and slack receivers configured for system alerts. global : smtp_smarthost : smtp.gmail.com:587 smtp_from : hopsworks@gmail.com smtp_auth_username : hopsworks@gmail.com smtp_auth_password : XXXXXXXXX smtp_auth_identity : hopsworks@gmail.com resolveTimeout : 5m templates : - /srv/hops/alertmanager/alertmanager-0.17.0.linux-amd64/template/*.tmpl route : receiver : default routes : - receiver : email continue : true match : type : system-alert - receiver : slack continue : true match : type : system-alert groupBy : - alertname groupWait : 10s groupInterval : 10s receivers : - name : default - name : email emailConfigs : - to : someone@logicalclocks.com from : hopsworks@logicalclocks.com smarthost : mail.hello.com text : >- summary: {{ .CommonAnnotations.summary }} description: {{ .CommonAnnotations.description }} - name : slack slackConfigs : - apiUrl : >- https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX channel : '#general' text : >- <!channel> summary: {{ .Annotations.summary }} description: {{ .Annotations.description }}","title":"Step 5: Advanced configuration"},{"location":"admin/alert/#conclusion","text":"In this guide you learned how to configure alerts in Hopsworks.","title":"Conclusion"},{"location":"admin/auth/","text":"Authentication Methods # Introduction # Hopsworks can be configured to use different type of authentication methods. In this guide we will look at the different authentication methods available in Hopsworks. Prerequisites # Administrator account on a Hopsworks cluster. Step 1: Go to Authentication methods page # To configure Authentication methods click on your name in the top right corner of the navigation bar and choose Cluster Settings from the dropdown menu. Step 2: Configure Authentication methods # In the Cluster Settings Authentication tab you can configure how users authenticate. TOTP Two-factor Authentication : can be disabled , optional or mandatory . If set to mandatory all users are required to set up two-factor authentication when registering. Note If two-factor is set to mandatory on a cluster with preexisting users all users will need to go through lost device recovery step to enable two-factor. So consider setting it to optional first and allow users to enable it before setting it to mandatory. OAuth2 : if your organization already have an identity management system compatible with OpenID Connect (OIDC) you can configure Hopsworks to use your identity provider by enabling OAuth as shown in the figure below. After enabling OAuth you can register your identity provider by clicking on Add Identity Provider button. See Create client for details. LDAP/Kerberos : if your organization is using LDAP or Kerberos to manage users and services you can configure Hopsworks to use it as the user management system. You can enable LDAP/Kerberos by clicking on the checkbox, as shown in the figure below, and choosing LDAP or Kerberos. For more information on how to configure LDAP and Kerberos see Configure LDAP and Configure Kerberos . Setup Authentication Methods In the figure above we see a cluster with Two-factor authentication disabled, OAuth enabled with one registered identity provider and LDAP authentication enabled. Conclusion # In this guide you learned how to configure authentication methods in Hopsworks.","title":"Configure Authentication"},{"location":"admin/auth/#authentication-methods","text":"","title":"Authentication Methods"},{"location":"admin/auth/#introduction","text":"Hopsworks can be configured to use different type of authentication methods. In this guide we will look at the different authentication methods available in Hopsworks.","title":"Introduction"},{"location":"admin/auth/#prerequisites","text":"Administrator account on a Hopsworks cluster.","title":"Prerequisites"},{"location":"admin/auth/#step-1-go-to-authentication-methods-page","text":"To configure Authentication methods click on your name in the top right corner of the navigation bar and choose Cluster Settings from the dropdown menu.","title":"Step 1: Go to Authentication methods page"},{"location":"admin/auth/#step-2-configure-authentication-methods","text":"In the Cluster Settings Authentication tab you can configure how users authenticate. TOTP Two-factor Authentication : can be disabled , optional or mandatory . If set to mandatory all users are required to set up two-factor authentication when registering. Note If two-factor is set to mandatory on a cluster with preexisting users all users will need to go through lost device recovery step to enable two-factor. So consider setting it to optional first and allow users to enable it before setting it to mandatory. OAuth2 : if your organization already have an identity management system compatible with OpenID Connect (OIDC) you can configure Hopsworks to use your identity provider by enabling OAuth as shown in the figure below. After enabling OAuth you can register your identity provider by clicking on Add Identity Provider button. See Create client for details. LDAP/Kerberos : if your organization is using LDAP or Kerberos to manage users and services you can configure Hopsworks to use it as the user management system. You can enable LDAP/Kerberos by clicking on the checkbox, as shown in the figure below, and choosing LDAP or Kerberos. For more information on how to configure LDAP and Kerberos see Configure LDAP and Configure Kerberos . Setup Authentication Methods In the figure above we see a cluster with Two-factor authentication disabled, OAuth enabled with one registered identity provider and LDAP authentication enabled.","title":"Step 2: Configure Authentication methods"},{"location":"admin/auth/#conclusion","text":"In this guide you learned how to configure authentication methods in Hopsworks.","title":"Conclusion"},{"location":"admin/project/","text":"Manage Projects # Hopsworks provides an administrator with a view of the projects in a Hopsworks cluster. A Hopsworks administrator is not automatically a member of all the projects in a cluster. However, they can see which projects exist, who is the project owner, and they can limit the storage quota and compute quota for each project. Prerequisites # You need to be an administrator on a Hopsworks cluster. Changing project quotas # You can find the Project management page by clicking on your name, in the top right coner of the navigation bar, and choosing Cluster Settings from the dropdown menu and going to the Project tab. Project page This page will list all the projects in a cluster, their name, owner and when its quota was last updated. By clicking on the edit configuration link of a project you will be able to edit the quotas of that project. Project quotas Storage # Storage quota represents the amount of data a project can store. The storage quota is broken down in three different areas: Feature Store : This represents the storage quota for files and directories stored in the _featurestore.db dataset in the project. This dataset contains all the feature group offline data for the project. Hive DB : This represents the storage quota for files and directories stored in the [projectName].db dataset in the project. This is a general purpose Hive database for the project that can be used for analytics. Project : This represents the storage quota for all the data stored on any other dataset. Each storage quota is divided into space quota, i.e., how much space the files can consume, and namespace quota, i.e., how many files and directories there can be. If Hopsworks is deployed on-premise using hard drives to store the data, i.e., Hopsworks is not configured to store its data in a S3-compliant storage system, the data is replicated across multiple nodes (by default 3) and the space quota takes the replication factor into consideration. As an example, a 100MB file stored with a replication factor of 3, will consume 300MB of space quota. By default, all storage quotas are disabled and not enforced. Administrators can change this default by changing the following configuration in the Configuration UI and/or the cluster definition: hopsworks: featurestore_default_quota: [default quota in bytes, -1 to disable] hdfs_default_quota: [default quota in bytes, -1 to disable] hive_default_quota: [default quota in bytes, -1 to disable] The values specified will be set during project creation and administrators will be able to customize each project using this UI. Compute # Compute quotas represents the amount of compute a project can use to run Spark and Flink applications as well as Tez queries. Quota is expressed as number of seconds a container of size 1 CPU and 1GB of RAM can run for. If the Hopsworks cluster is connected to a Kubernetes cluster, Python jobs, Jupyter notebooks and KServe models are not subject to the compute quota. Currently, Hopsworks does not support defining quotas for compute scheduled on the connected Kubernetes cluster. By default, the compute quota is disabled. Administrators can change this default by changing the following configuration in the Condiguration UI and/or the cluster definition: hopsworks: yarn_default_payment_type: [NOLIMIT to disable the quota, PREPAID to enable it] yarn_default_quota: [default quota in seconds] The values specified will be set during project creation and administrators will be able to customize each project using this UI. Kakfa Topics # Kafka is used within Hopsworks to enable users to write data to the feature store in Real-Time and from a variety of different frameworks. If a user creates a feature group with the stream APIs enabled, then a Kafka topic will be created for that feature group. By default, a project can have up to 100 Kafka topics. Administrators can increase the number of Kafka topics a project is allowed to create by increasing the quota in the project admin UI. Force deleting a project # Administrators have the option to force delete a project. This is useful if the project was not created or deleted properly, e.g., because of an error. Controlling who can create projects # Every user on Hopsworks can create projects. By default, each user can create up to 10 projects. For production environments, the number of projects should be limited and controlled for resource allocation purposes as well as closer control over the data. Administrators can control how many projects a user can provision by setting the following configuration in the Configuration UI and/or cluster definition: hopsworks: max_num_proj_per_user: [Maximum number of projects each user can create] This value will be set when the user is provisioned. Administrators can grant additional projects to a specific user through the User Administration UI.","title":"Project Management"},{"location":"admin/project/#manage-projects","text":"Hopsworks provides an administrator with a view of the projects in a Hopsworks cluster. A Hopsworks administrator is not automatically a member of all the projects in a cluster. However, they can see which projects exist, who is the project owner, and they can limit the storage quota and compute quota for each project.","title":"Manage Projects"},{"location":"admin/project/#prerequisites","text":"You need to be an administrator on a Hopsworks cluster.","title":"Prerequisites"},{"location":"admin/project/#changing-project-quotas","text":"You can find the Project management page by clicking on your name, in the top right coner of the navigation bar, and choosing Cluster Settings from the dropdown menu and going to the Project tab. Project page This page will list all the projects in a cluster, their name, owner and when its quota was last updated. By clicking on the edit configuration link of a project you will be able to edit the quotas of that project. Project quotas","title":"Changing project quotas"},{"location":"admin/project/#storage","text":"Storage quota represents the amount of data a project can store. The storage quota is broken down in three different areas: Feature Store : This represents the storage quota for files and directories stored in the _featurestore.db dataset in the project. This dataset contains all the feature group offline data for the project. Hive DB : This represents the storage quota for files and directories stored in the [projectName].db dataset in the project. This is a general purpose Hive database for the project that can be used for analytics. Project : This represents the storage quota for all the data stored on any other dataset. Each storage quota is divided into space quota, i.e., how much space the files can consume, and namespace quota, i.e., how many files and directories there can be. If Hopsworks is deployed on-premise using hard drives to store the data, i.e., Hopsworks is not configured to store its data in a S3-compliant storage system, the data is replicated across multiple nodes (by default 3) and the space quota takes the replication factor into consideration. As an example, a 100MB file stored with a replication factor of 3, will consume 300MB of space quota. By default, all storage quotas are disabled and not enforced. Administrators can change this default by changing the following configuration in the Configuration UI and/or the cluster definition: hopsworks: featurestore_default_quota: [default quota in bytes, -1 to disable] hdfs_default_quota: [default quota in bytes, -1 to disable] hive_default_quota: [default quota in bytes, -1 to disable] The values specified will be set during project creation and administrators will be able to customize each project using this UI.","title":"Storage"},{"location":"admin/project/#compute","text":"Compute quotas represents the amount of compute a project can use to run Spark and Flink applications as well as Tez queries. Quota is expressed as number of seconds a container of size 1 CPU and 1GB of RAM can run for. If the Hopsworks cluster is connected to a Kubernetes cluster, Python jobs, Jupyter notebooks and KServe models are not subject to the compute quota. Currently, Hopsworks does not support defining quotas for compute scheduled on the connected Kubernetes cluster. By default, the compute quota is disabled. Administrators can change this default by changing the following configuration in the Condiguration UI and/or the cluster definition: hopsworks: yarn_default_payment_type: [NOLIMIT to disable the quota, PREPAID to enable it] yarn_default_quota: [default quota in seconds] The values specified will be set during project creation and administrators will be able to customize each project using this UI.","title":"Compute"},{"location":"admin/project/#kakfa-topics","text":"Kafka is used within Hopsworks to enable users to write data to the feature store in Real-Time and from a variety of different frameworks. If a user creates a feature group with the stream APIs enabled, then a Kafka topic will be created for that feature group. By default, a project can have up to 100 Kafka topics. Administrators can increase the number of Kafka topics a project is allowed to create by increasing the quota in the project admin UI.","title":"Kakfa Topics"},{"location":"admin/project/#force-deleting-a-project","text":"Administrators have the option to force delete a project. This is useful if the project was not created or deleted properly, e.g., because of an error.","title":"Force deleting a project"},{"location":"admin/project/#controlling-who-can-create-projects","text":"Every user on Hopsworks can create projects. By default, each user can create up to 10 projects. For production environments, the number of projects should be limited and controlled for resource allocation purposes as well as closer control over the data. Administrators can control how many projects a user can provision by setting the following configuration in the Configuration UI and/or cluster definition: hopsworks: max_num_proj_per_user: [Maximum number of projects each user can create] This value will be set when the user is provisioned. Administrators can grant additional projects to a specific user through the User Administration UI.","title":"Controlling who can create projects"},{"location":"admin/roleChaining/","text":"AWS IAM Role Chaining # Introduction # When running Hopsworks in the cloud you have several options to give the Hopsworks user access to AWS resources. The simplest is to setup the EC2 instances running Hopsworks with an instance profile giving access to the resources. But, this will make these resources accessible by all the Hopsworks users. To manage access to the resource on a project base you need to use Role chaining . In this document we will see how to configure AWS and Hopsworks to use Role chaining in your Hopsworks projects. Prerequisites # Before you begin this guide you'll need the following: A Hopsworks cluster running on EC2. Administrator account on a Hopsworks cluster. Step 1: Create an instance profile role # To use role chaining the head node need to be able to impersonate the roles you want to be linked to your project. For this you need to create an instance profile with assume role permissions and attach it to your head node. For more details about the creation of instance profile see the aws documentation . If running in managed.hopsworks.ai you can also refer to our getting started guide . Note To ensure that the Hopsworks users can't use the head node instance profile and impersonate the roles by their own means, you need to ensure that they can't execute code on the head node. This means having all jobs running on worker nodes and using EKS to run jupyter nodebooks. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"AssumeDataRoles\" , \"Effect\" : \"Allow\" , \"Action\" : \"sts:AssumeRole\" , \"Resource\" : [ \"arn:aws:iam::123456789011:role/test-role\" , \"arn:aws:iam::xxxxxxxxxxxx:role/s3-role\" , \"arn:aws:iam::xxxxxxxxxxxx:role/dev-s3-role\" , \"arn:aws:iam::xxxxxxxxxxxx:role/redshift\" ] } ] } Example policy for assuming four roles. Step 2: Create the resource roles # For the instance profile to be able to impersonate the roles you need to configure the roles themselves to allow it. This is dome by adding the instance profile to the role's Trust relationships . { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"arn:aws:iam::xxxxxxxxxxxx:role/instance-profile\" }, \"Action\" : \"sts:AssumeRole\" } ] } Example trust-policy document. Step 3: Create mappings # Now that the head node can assume the roles we need to configure Hopsworks to deletegate access to the roles on a project base. In Hopsworks, click on your name in the top right corner of the navigation bar and choose Cluster Settings from the dropdown menu. In the Cluster Settings' IAM Role Chaining tab you can configure the mappings between projects and IAM roles. Role Chaining Add mappings by clicking on New role chaining . Enter the project name. Select the type of user that can assume the role. Enter the role ARN. And click on Create new role chaining Create Role Chaining Project member can now create connectors using temporary credentials to assume the role you configured. More detail about using temporary credentials can be found here . Project member can see the list of role they can assume by going the Project Settings -> Assuming IAM Roles page. Conclusion # In this guide you learned how to configure and map AWS IAM roles to project roles in Hopsworks.","title":"IAM Role Chaining"},{"location":"admin/roleChaining/#aws-iam-role-chaining","text":"","title":"AWS IAM Role Chaining"},{"location":"admin/roleChaining/#introduction","text":"When running Hopsworks in the cloud you have several options to give the Hopsworks user access to AWS resources. The simplest is to setup the EC2 instances running Hopsworks with an instance profile giving access to the resources. But, this will make these resources accessible by all the Hopsworks users. To manage access to the resource on a project base you need to use Role chaining . In this document we will see how to configure AWS and Hopsworks to use Role chaining in your Hopsworks projects.","title":"Introduction"},{"location":"admin/roleChaining/#prerequisites","text":"Before you begin this guide you'll need the following: A Hopsworks cluster running on EC2. Administrator account on a Hopsworks cluster.","title":"Prerequisites"},{"location":"admin/roleChaining/#step-1-create-an-instance-profile-role","text":"To use role chaining the head node need to be able to impersonate the roles you want to be linked to your project. For this you need to create an instance profile with assume role permissions and attach it to your head node. For more details about the creation of instance profile see the aws documentation . If running in managed.hopsworks.ai you can also refer to our getting started guide . Note To ensure that the Hopsworks users can't use the head node instance profile and impersonate the roles by their own means, you need to ensure that they can't execute code on the head node. This means having all jobs running on worker nodes and using EKS to run jupyter nodebooks. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"AssumeDataRoles\" , \"Effect\" : \"Allow\" , \"Action\" : \"sts:AssumeRole\" , \"Resource\" : [ \"arn:aws:iam::123456789011:role/test-role\" , \"arn:aws:iam::xxxxxxxxxxxx:role/s3-role\" , \"arn:aws:iam::xxxxxxxxxxxx:role/dev-s3-role\" , \"arn:aws:iam::xxxxxxxxxxxx:role/redshift\" ] } ] } Example policy for assuming four roles.","title":"Step 1: Create an instance profile role"},{"location":"admin/roleChaining/#step-2-create-the-resource-roles","text":"For the instance profile to be able to impersonate the roles you need to configure the roles themselves to allow it. This is dome by adding the instance profile to the role's Trust relationships . { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"arn:aws:iam::xxxxxxxxxxxx:role/instance-profile\" }, \"Action\" : \"sts:AssumeRole\" } ] } Example trust-policy document.","title":"Step 2: Create the resource roles"},{"location":"admin/roleChaining/#step-3-create-mappings","text":"Now that the head node can assume the roles we need to configure Hopsworks to deletegate access to the roles on a project base. In Hopsworks, click on your name in the top right corner of the navigation bar and choose Cluster Settings from the dropdown menu. In the Cluster Settings' IAM Role Chaining tab you can configure the mappings between projects and IAM roles. Role Chaining Add mappings by clicking on New role chaining . Enter the project name. Select the type of user that can assume the role. Enter the role ARN. And click on Create new role chaining Create Role Chaining Project member can now create connectors using temporary credentials to assume the role you configured. More detail about using temporary credentials can be found here . Project member can see the list of role they can assume by going the Project Settings -> Assuming IAM Roles page.","title":"Step 3: Create mappings"},{"location":"admin/roleChaining/#conclusion","text":"In this guide you learned how to configure and map AWS IAM roles to project roles in Hopsworks.","title":"Conclusion"},{"location":"admin/services/","text":"Manage Services # Introduction # Hopsworks provides administrators with a view of the status/health of the cluster. This information is provided through the Services page. Prerequisites # Administrator account on a Hopsworks cluster. Step 1: Check service status # You can find the Services page by clicking on your name, in the top right corner of the navigation bar, and choosing Cluster Settings from the dropdown menu and going to the Services tab. Services page This page give administrators an overview of which services are running on the cluster. It provides information about their status as reported by agents that monitor the status of the different Systemd units. Columns in the services table represent machines in your cluster. Each service running on a machine will have a status running (green) or stopped (red). If a service is not installed on a machine it will have a status not installed (gray). Step 2: Find a service # Services are divided into groups, and you can search for a service by its name or group. You can also search for machines by their host name. Services Step 3: Manage a service # After you find the correct service you will be able to start , stop or restart it, by clicking on its status. Start, Stop and Restart a service Note Stopping some services like the web server (glassfish_domain1) is not recommended. If you stop it you will have to access the machine running the service and start it with systemctl start glassfish_domain1 . Conclusion # In this guide you learned how to manage services in Hopsworks.","title":"Manage Services"},{"location":"admin/services/#manage-services","text":"","title":"Manage Services"},{"location":"admin/services/#introduction","text":"Hopsworks provides administrators with a view of the status/health of the cluster. This information is provided through the Services page.","title":"Introduction"},{"location":"admin/services/#prerequisites","text":"Administrator account on a Hopsworks cluster.","title":"Prerequisites"},{"location":"admin/services/#step-1-check-service-status","text":"You can find the Services page by clicking on your name, in the top right corner of the navigation bar, and choosing Cluster Settings from the dropdown menu and going to the Services tab. Services page This page give administrators an overview of which services are running on the cluster. It provides information about their status as reported by agents that monitor the status of the different Systemd units. Columns in the services table represent machines in your cluster. Each service running on a machine will have a status running (green) or stopped (red). If a service is not installed on a machine it will have a status not installed (gray).","title":"Step 1: Check service status"},{"location":"admin/services/#step-2-find-a-service","text":"Services are divided into groups, and you can search for a service by its name or group. You can also search for machines by their host name. Services","title":"Step 2: Find a service"},{"location":"admin/services/#step-3-manage-a-service","text":"After you find the correct service you will be able to start , stop or restart it, by clicking on its status. Start, Stop and Restart a service Note Stopping some services like the web server (glassfish_domain1) is not recommended. If you stop it you will have to access the machine running the service and start it with systemctl start glassfish_domain1 .","title":"Step 3: Manage a service"},{"location":"admin/services/#conclusion","text":"In this guide you learned how to manage services in Hopsworks.","title":"Conclusion"},{"location":"admin/user/","text":"User Management # Introduction # Whether you run Hopsworks on-premise, or on the cloud using managed.hopsworks.ai , you have a Hopsworks cluster which contains all users and projects. Prerequisites # Administrator account on a Hopsworks cluster. Step 1: Go to user management # All the users of your Hopsworks instance have access to your cluster with different access rights. You can find them by clicking on your name in the top right corner of the navigation bar and choosing Cluster Settings from the dropdown menu and going to the Users tab (You need to have Admin role to get access to the Cluster Settings page). Active Users Step 2: Manage user roles # Roles let you manage the access rights of a user to the cluster. User: users with this role are only allowed to use the cluster by creating a limited number of projects. Admin: users with this role are allowed to manage the cluster. This includes accepting new users to the cluster or blocking them, managing user quota, configure alerts and setting up authentication methods . You can change the role of a user by clicking on the select dropdown that shows the current role of the user. Step 3: Validating and blocking users # By default, a user who register on Hopsworks using their own credentials are not granted access to the cluster. First, a user with an admin role needs to validate their account. By clicking on the Review Requests button you can open a user request review popup as shown in the image below. Review user request On the user request review popup you can activate or block users. Users with a validated email address will have a check mark on their email. Similarly, if a user is no longer allowed access to the cluster you can block them. To keep consistency with the history of your datasets, a user can not be deleted but only blocked. If necessary a user can be deleted manually in the cluster using the command line. You can block a user by clicking on the block icon on the right side of the user in the list. Blocked Users Blocked users will appear on the lower section of the page. Click on display blocked users to show all the blocked users in your cluster. If a user is blocked by mistake you can reactivate it by clicking on the check mark icon that corresponds to that user in the blocked users list. If there are too many users in your cluster, use the search box (available for blocked users too) to filter users by name or email. It is also possible to filter activated users by role. For example to see all administrators in you cluster click on the select dropdown to the right of the search box and choose Admin . Step 4: Create a new users # If you want to allow users to login without registering you can pre-create them by clicking on New user . Create new user After setting the user's name and email chose the type of user you want to create (Hopsworks, Kerberos or LDAP). To create a Kerberos or LDAP user you need to get the users UUID from the Kerberos or LDAP server. Hopsworks user can also be assigned a Role . Kerberos and LDAP users on the other hand can only be assigned a role through group mapping. A temporary password will be generated and displayed when you click on Create new user . Copy the password and pass it securely to the user. Copy temporary password Step 5: Reset user password # In the case where a user loses her/his password and can not recover it with the password recovery , an administrator can reset it for them. On the bottom of the Users page click on the Reset a user password link. A popup window with a dropdown for searching users by name or email will open. Find the user and click on Reset new password . Reset user password A temporary password will be displayed. Copy the password and pass it to the user securely. Copy temporary password A user with a temporary password will see a warning message when going to Account settings Authentication tab. Change password Note A temporary password should be changed as soon as possible. Conclusion # In this guide you learned how to manage users in Hopsworks.","title":"User Management"},{"location":"admin/user/#user-management","text":"","title":"User Management"},{"location":"admin/user/#introduction","text":"Whether you run Hopsworks on-premise, or on the cloud using managed.hopsworks.ai , you have a Hopsworks cluster which contains all users and projects.","title":"Introduction"},{"location":"admin/user/#prerequisites","text":"Administrator account on a Hopsworks cluster.","title":"Prerequisites"},{"location":"admin/user/#step-1-go-to-user-management","text":"All the users of your Hopsworks instance have access to your cluster with different access rights. You can find them by clicking on your name in the top right corner of the navigation bar and choosing Cluster Settings from the dropdown menu and going to the Users tab (You need to have Admin role to get access to the Cluster Settings page). Active Users","title":"Step 1: Go to user management"},{"location":"admin/user/#step-2-manage-user-roles","text":"Roles let you manage the access rights of a user to the cluster. User: users with this role are only allowed to use the cluster by creating a limited number of projects. Admin: users with this role are allowed to manage the cluster. This includes accepting new users to the cluster or blocking them, managing user quota, configure alerts and setting up authentication methods . You can change the role of a user by clicking on the select dropdown that shows the current role of the user.","title":"Step 2: Manage user roles"},{"location":"admin/user/#step-3-validating-and-blocking-users","text":"By default, a user who register on Hopsworks using their own credentials are not granted access to the cluster. First, a user with an admin role needs to validate their account. By clicking on the Review Requests button you can open a user request review popup as shown in the image below. Review user request On the user request review popup you can activate or block users. Users with a validated email address will have a check mark on their email. Similarly, if a user is no longer allowed access to the cluster you can block them. To keep consistency with the history of your datasets, a user can not be deleted but only blocked. If necessary a user can be deleted manually in the cluster using the command line. You can block a user by clicking on the block icon on the right side of the user in the list. Blocked Users Blocked users will appear on the lower section of the page. Click on display blocked users to show all the blocked users in your cluster. If a user is blocked by mistake you can reactivate it by clicking on the check mark icon that corresponds to that user in the blocked users list. If there are too many users in your cluster, use the search box (available for blocked users too) to filter users by name or email. It is also possible to filter activated users by role. For example to see all administrators in you cluster click on the select dropdown to the right of the search box and choose Admin .","title":"Step 3: Validating and blocking users"},{"location":"admin/user/#step-4-create-a-new-users","text":"If you want to allow users to login without registering you can pre-create them by clicking on New user . Create new user After setting the user's name and email chose the type of user you want to create (Hopsworks, Kerberos or LDAP). To create a Kerberos or LDAP user you need to get the users UUID from the Kerberos or LDAP server. Hopsworks user can also be assigned a Role . Kerberos and LDAP users on the other hand can only be assigned a role through group mapping. A temporary password will be generated and displayed when you click on Create new user . Copy the password and pass it securely to the user. Copy temporary password","title":"Step 4: Create a new users"},{"location":"admin/user/#step-5-reset-user-password","text":"In the case where a user loses her/his password and can not recover it with the password recovery , an administrator can reset it for them. On the bottom of the Users page click on the Reset a user password link. A popup window with a dropdown for searching users by name or email will open. Find the user and click on Reset new password . Reset user password A temporary password will be displayed. Copy the password and pass it to the user securely. Copy temporary password A user with a temporary password will see a warning message when going to Account settings Authentication tab. Change password Note A temporary password should be changed as soon as possible.","title":"Step 5: Reset user password"},{"location":"admin/user/#conclusion","text":"In this guide you learned how to manage users in Hopsworks.","title":"Conclusion"},{"location":"admin/variables/","text":"Cluster Configuration # Introduction # Whether you run Hopsworks on-premise, or on the cloud using managed.hopsworks.ai , it is possible to change a variety of configurations on the cluster, changing its default behaviour. This section is not going into detail for every setting, since every Hopsworks cluster comes with a robust default setup. However, this guide is to explain where to find the configurations and if necessary, how to change them. Note In most cases you will be only be prompted to change these configurations by a Hopsworks Solutions Engineer or similar. Prerequisites # An administrator account on a Hopsworks cluster. Step 1: The configuration page # You can find the configuration page by navigating in the UI: Click on your user name in the top right corner, then select Cluster Settings . Among the cluster settings, you will find a tab Configuration Configuration settings Step 2: Editing existing configurations # To edit an existing configuration, simply find the property using the search field, then click the edit button to change the value of the setting or its visibility. Once you have made the change, don't forget to click save to persist the changes. Visibility # The visibility setting indicates whether a setting can be read only by Hops Admins or also by simple Hops Users , that is everyone. Additionally, you can also allow to read the setting even when not authenticated . If the setting contains a password or sensitive information, you can also hide the value so it's not shown in the UI. Step 3: Adding a new configuration # In rare cases it might be necessary to add additional configurations. To do so, click on New Variable , where you can then configure the new setting with a key, value and visibility. Once you have set the desired properties, you can persist them by clicking Create Configuration Adding a new configuration property","title":"Cluster Configuration"},{"location":"admin/variables/#cluster-configuration","text":"","title":"Cluster Configuration"},{"location":"admin/variables/#introduction","text":"Whether you run Hopsworks on-premise, or on the cloud using managed.hopsworks.ai , it is possible to change a variety of configurations on the cluster, changing its default behaviour. This section is not going into detail for every setting, since every Hopsworks cluster comes with a robust default setup. However, this guide is to explain where to find the configurations and if necessary, how to change them. Note In most cases you will be only be prompted to change these configurations by a Hopsworks Solutions Engineer or similar.","title":"Introduction"},{"location":"admin/variables/#prerequisites","text":"An administrator account on a Hopsworks cluster.","title":"Prerequisites"},{"location":"admin/variables/#step-1-the-configuration-page","text":"You can find the configuration page by navigating in the UI: Click on your user name in the top right corner, then select Cluster Settings . Among the cluster settings, you will find a tab Configuration Configuration settings","title":"Step 1: The configuration page"},{"location":"admin/variables/#step-2-editing-existing-configurations","text":"To edit an existing configuration, simply find the property using the search field, then click the edit button to change the value of the setting or its visibility. Once you have made the change, don't forget to click save to persist the changes.","title":"Step 2: Editing existing configurations"},{"location":"admin/variables/#visibility","text":"The visibility setting indicates whether a setting can be read only by Hops Admins or also by simple Hops Users , that is everyone. Additionally, you can also allow to read the setting even when not authenticated . If the setting contains a password or sensitive information, you can also hide the value so it's not shown in the UI.","title":"Visibility"},{"location":"admin/variables/#step-3-adding-a-new-configuration","text":"In rare cases it might be necessary to add additional configurations. To do so, click on New Variable , where you can then configure the new setting with a key, value and visibility. Once you have set the desired properties, you can persist them by clicking Create Configuration Adding a new configuration property","title":"Step 3: Adding a new configuration"},{"location":"admin/audit/audit-logs/","text":"Access Audit Logs # Introduction # Hopsworks collects audit logs on all URL requests to the application server. These logs are saved in Payara log directory under <payara-log-dir>/audit by default. Prerequisites # In order to access the audit logs you need the following: Administrator account on the Hopsworks cluster. SSH access to the Hopsworks cluster with a user in the glassfish group. Step 1: Configure Audit logs # Audit logs can be configured from the Cluster Settings Configuration tab. You can access the Configuration page of your Hopsworks cluster by clicking on your name, in the top right corner, and choosing Cluster Settings from the dropdown menu. Audit log configuration Type audit in the search box to see the configuration variables associated with audit logs. To edit a configuration variable, you can click on the edit button ( ), insert the new value and save changes clicking on the check mark ( ). Audit logs configuration variables Name Description audit_log_count the number of files to keep when rotating logs (java.util.logging.FileHandler.count) audit_log_dir the path where audit logs are saved audit_log_file_format log file name pattern. (java.util.logging.FileHandler.pattern) audit_log_file_type the output format of the log file. Can be one of java.util.logging.SimpleFormatter (default), io.hops.hopsworks.audit.helper.JSONLogFormatter, or io.hops.hopsworks.audit.helper.HtmlLogFormatter. audit_log_size_limit the maximum number of bytes to write to any one file. (java.util.logging.FileHandler.limit) audit_log_date_format if io.hops.hopsworks.audit.helper.JSONLogFormatter is used as audit log file type, this will set the date format of the output JSON. The format should be java.text.SimpleDateFormat compatible string. Warning Hopsworks application needs to be reloaded for any changes to be applied. For doing that, go to the Payara admin panel ( https://<your-domain>:4848 ), click on Applications on the side menu and reload the hopsworks-ear application. Step 2: Access the Logs # To access the audit logs, SSH into the head node of your Hopsworks cluster and navigate to the path set in the audit_log_dir configuration variable. Audit logs follow the format set in the audit_log_file_type configuration variable. Example of audit logs using JSONLogFormatter { \"className\" : \"io.hops.hopsworks.api.user.AuthService\" , \"methodName\" : \"login\" , \"parameters\" : \"[admin@hopsworks.ai, org.apache.catalina.connector.ResponseFacade@2de6dd0b, org.apache.catalina.connector.RequestFacade@7a82f674]\" , \"outcome\" : \"200\" , \"caller\" :{ \"username\" : null , \"email\" : \"admin@hopsworks.ai\" , \"userId\" : null }, \"clientIp\" : \"10.0.2.2\" , \"userAgent\" : \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36\" , \"pathInfo\" : \"/auth/login\" , \"dateTime\" : \"2022-11-09 12:00:08\" } Regardless the format, each line in the audit logs can contain the following variables: Audit log variables Name Description className the class called by the request methodName the method called by the request parameters parameters sent from the client outcome response code sent from the server caller the logged in user that made the request. Can be username, email, or userId clientIp the IP address of the client userAgent the browser used by the client pathInfo the URL path called by the client dateTime time of the request Conclusion # In this guide we showed how you can configure audit logs in Hopsworks from the admin page and access the audit log files via SSH. To learn how to export audit logs, see Export Audit Logs .","title":"Access Audit Logs"},{"location":"admin/audit/audit-logs/#access-audit-logs","text":"","title":"Access Audit Logs"},{"location":"admin/audit/audit-logs/#introduction","text":"Hopsworks collects audit logs on all URL requests to the application server. These logs are saved in Payara log directory under <payara-log-dir>/audit by default.","title":"Introduction"},{"location":"admin/audit/audit-logs/#prerequisites","text":"In order to access the audit logs you need the following: Administrator account on the Hopsworks cluster. SSH access to the Hopsworks cluster with a user in the glassfish group.","title":"Prerequisites"},{"location":"admin/audit/audit-logs/#step-1-configure-audit-logs","text":"Audit logs can be configured from the Cluster Settings Configuration tab. You can access the Configuration page of your Hopsworks cluster by clicking on your name, in the top right corner, and choosing Cluster Settings from the dropdown menu. Audit log configuration Type audit in the search box to see the configuration variables associated with audit logs. To edit a configuration variable, you can click on the edit button ( ), insert the new value and save changes clicking on the check mark ( ). Audit logs configuration variables Name Description audit_log_count the number of files to keep when rotating logs (java.util.logging.FileHandler.count) audit_log_dir the path where audit logs are saved audit_log_file_format log file name pattern. (java.util.logging.FileHandler.pattern) audit_log_file_type the output format of the log file. Can be one of java.util.logging.SimpleFormatter (default), io.hops.hopsworks.audit.helper.JSONLogFormatter, or io.hops.hopsworks.audit.helper.HtmlLogFormatter. audit_log_size_limit the maximum number of bytes to write to any one file. (java.util.logging.FileHandler.limit) audit_log_date_format if io.hops.hopsworks.audit.helper.JSONLogFormatter is used as audit log file type, this will set the date format of the output JSON. The format should be java.text.SimpleDateFormat compatible string. Warning Hopsworks application needs to be reloaded for any changes to be applied. For doing that, go to the Payara admin panel ( https://<your-domain>:4848 ), click on Applications on the side menu and reload the hopsworks-ear application.","title":"Step 1: Configure Audit logs"},{"location":"admin/audit/audit-logs/#step-2-access-the-logs","text":"To access the audit logs, SSH into the head node of your Hopsworks cluster and navigate to the path set in the audit_log_dir configuration variable. Audit logs follow the format set in the audit_log_file_type configuration variable. Example of audit logs using JSONLogFormatter { \"className\" : \"io.hops.hopsworks.api.user.AuthService\" , \"methodName\" : \"login\" , \"parameters\" : \"[admin@hopsworks.ai, org.apache.catalina.connector.ResponseFacade@2de6dd0b, org.apache.catalina.connector.RequestFacade@7a82f674]\" , \"outcome\" : \"200\" , \"caller\" :{ \"username\" : null , \"email\" : \"admin@hopsworks.ai\" , \"userId\" : null }, \"clientIp\" : \"10.0.2.2\" , \"userAgent\" : \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36\" , \"pathInfo\" : \"/auth/login\" , \"dateTime\" : \"2022-11-09 12:00:08\" } Regardless the format, each line in the audit logs can contain the following variables: Audit log variables Name Description className the class called by the request methodName the method called by the request parameters parameters sent from the client outcome response code sent from the server caller the logged in user that made the request. Can be username, email, or userId clientIp the IP address of the client userAgent the browser used by the client pathInfo the URL path called by the client dateTime time of the request","title":"Step 2: Access the Logs"},{"location":"admin/audit/audit-logs/#conclusion","text":"In this guide we showed how you can configure audit logs in Hopsworks from the admin page and access the audit log files via SSH. To learn how to export audit logs, see Export Audit Logs .","title":"Conclusion"},{"location":"admin/audit/export-audit-logs/","text":"Export Audit Logs # Introduction # Audit logs can be exported to your storage of preference. In case audit logs have not been configured yet in your Hopsworks cluster, please see Access Audit Logs . Note As an example, in this guide we will show how to export audit logs to BigQuery using the bq command-line tool. Prerequisites # In order to export audit logs you need SSH access to the Hopsworks cluster. Step 1: Create a BigQuery Table # Create a dataset and a table in BigQuery . The table schema is shown below. fullname mode type description pathInfo NULLABLE STRING methodName NULLABLE STRING caller NULLABLE RECORD dateTime NULLABLE TIMESTAMP bq-datetime userAgent NULLABLE STRING clientIp NULLABLE STRING outcome NULLABLE STRING parameters NULLABLE STRING className NULLABLE STRING caller.userId NULLABLE STRING caller.email NULLABLE STRING caller.username NULLABLE STRING Step 2: Export Audit Logs to the BigQuery Table # Audit logs can be exported in different formats. For instance, to export audit logs in JSON format set audit_log_file_type=io.hops.hopsworks.audit.helper.JSONLogFormatter . Info For more information on how to configure the audit log file type see the audit_log_file_type configuration variable in Audit logs . To export the audit logs to the BigQuery table created in the previous step, run the following command. bq load --project_id <projectId> \\ --source_format = NEWLINE_DELIMITED_JSON \\ <DATASET.TABLE> \\ /srv/hops/domains/domain1/logs/audit/server_audit_log0.log Tip This command can be configured to run periodically on a given schedule by setting up a cronjob. Conclusion # In this guide you showed how you can export audit logs in your Hopsworks cluster to a BigQuery table.","title":"Export Audit Logs"},{"location":"admin/audit/export-audit-logs/#export-audit-logs","text":"","title":"Export Audit Logs"},{"location":"admin/audit/export-audit-logs/#introduction","text":"Audit logs can be exported to your storage of preference. In case audit logs have not been configured yet in your Hopsworks cluster, please see Access Audit Logs . Note As an example, in this guide we will show how to export audit logs to BigQuery using the bq command-line tool.","title":"Introduction"},{"location":"admin/audit/export-audit-logs/#prerequisites","text":"In order to export audit logs you need SSH access to the Hopsworks cluster.","title":"Prerequisites"},{"location":"admin/audit/export-audit-logs/#step-1-create-a-bigquery-table","text":"Create a dataset and a table in BigQuery . The table schema is shown below. fullname mode type description pathInfo NULLABLE STRING methodName NULLABLE STRING caller NULLABLE RECORD dateTime NULLABLE TIMESTAMP bq-datetime userAgent NULLABLE STRING clientIp NULLABLE STRING outcome NULLABLE STRING parameters NULLABLE STRING className NULLABLE STRING caller.userId NULLABLE STRING caller.email NULLABLE STRING caller.username NULLABLE STRING","title":"Step 1: Create a BigQuery Table"},{"location":"admin/audit/export-audit-logs/#step-2-export-audit-logs-to-the-bigquery-table","text":"Audit logs can be exported in different formats. For instance, to export audit logs in JSON format set audit_log_file_type=io.hops.hopsworks.audit.helper.JSONLogFormatter . Info For more information on how to configure the audit log file type see the audit_log_file_type configuration variable in Audit logs . To export the audit logs to the BigQuery table created in the previous step, run the following command. bq load --project_id <projectId> \\ --source_format = NEWLINE_DELIMITED_JSON \\ <DATASET.TABLE> \\ /srv/hops/domains/domain1/logs/audit/server_audit_log0.log Tip This command can be configured to run periodically on a given schedule by setting up a cronjob.","title":"Step 2: Export Audit Logs to the BigQuery Table"},{"location":"admin/audit/export-audit-logs/#conclusion","text":"In this guide you showed how you can export audit logs in your Hopsworks cluster to a BigQuery table.","title":"Conclusion"},{"location":"admin/ha-dr/dr/","text":"Disaster Recovery # Backup # The state of the Hopsworks cluster is divided into data and metadata and distributed across the different node groups. This section of the guide allows you to take a consistent backup between data in the offline and online feature store as well as the metadata. The following services contain critical state that should be backed up: RonDB : as mentioned above, the RonDB is used by Hopsworks to store the cluster metadata as well as the data for the online feature store. HopsFS : HopsFS stores the data for the batch feature store as well as checkpoints and logs for feature engineering applications. Backing up service/application metrics and services/applications logs are out of the scope of this guide. By default metrics and logs are rotated after 7 days. Application logs are available on HopsFS when the application has finished and, as such, are backed up with the rest of HopsFS\u2019 data. Apache Kafka and OpenSearch are additional services maintaining state. The OpenSearch metadata can be reconstructed from the metadata stored on RonDB. Apache Kafka is used in Hopsworks to store the in-flight data that is on its way to the online feature store. In the event of a total loss of the cluster, running jobs with inflight data will have to be replayed. Configuration Backup # Hopsworks adopts an Infrastructure-as-code philosophy, as such all the configuration files for the different Hopsworks services are generated during the deployment phase. Cluster-specific customizations should be centralized in the cluster definition used to deploy the cluster. As such the cluster definition should be backed up (e.g., by committing it to a git repository) to be able to recreate the same cluster in case it needs to be recreated. RonDB Backup # The RonDB backup is divided into two parts: user and privileges backup and data backup. To take the backup of users and privileges you can run the following command from any of the nodes in the head node group. This command generates a SQL file containing all the user definitions for both the metadata services (Hopsworks, HopsFS, Metastore) as well as the user and permission grants for the online feature store. This command needs to be run as user \u2018mysql\u2019 or with sudo privileges. /srv/hops/mysql/bin/mysqlpump -S /srv/hops/mysql-cluster/mysql.sock --exclude-databases = % --exclude-users = root,mysql.sys,mysql.session,mysql.infoschema --users > users.sql The second step is to trigger the backup of the data. This can be achieved by running the following command as user \u2018mysql\u2019 on one of the nodes of the head node group. /srv/hops/mysql-cluster/ndb/scripts/mgm-client.sh -e \"START BACKUP [replace_backup_id] SNAPSHOTEND WAIT COMPLETED\" The backup ID is an integer greater or equal than 1. The script uses the following: $(date +'%y%m%d%H%M') instead of an integer as backup id to make it easier to identify backups over time. The command instructs each RonDB datanode to backup the data it is responsible for. The backup will be located locally on each datanode under the following path: /srv/hops/mysql-cluster/ndb/backups/BACKUP - the directory name will be BACKUP- [ backup_id ] A more comprehensive backup script is available here - The script includes the steps above as well as collecting all the partial RonDB backups on a single node. The script is a good starting point and can be adapted to ship the database backup outside the cluster. HopsFS Backup # HopsFS is a distributed file system based on Apache HDFS. HopsFS stores its metadata in RonDB, as such metadata backup has already been discussed in the section above. The data is stored in the form of blocks on the different data nodes. For availability reasons, the blocks are replicated across three different data nodes. Within a node, the blocks are stored by default under the following directory, under the ownership of the \u2018hdfs\u2019 user: /srv/hopsworks-data/hops/hopsdata/hdfs/dn/ To safely backup all the data, a copy of all the datanodes should be taken. As the data is replicated across the different nodes, excluding a set of nodes might result in data loss. Additionally, as HopsFS blocks are files on the file system and the filesystem can be quite large, the backup is not transactional. Consistency is dictated by the metadata. Blocks being added during the copying process will not be visible when restoring as they are not part of the metadata backup taken prior to cloning the HopsFS blocks. When the HopsFS data blocks are stored in a cloud block storage, for example, Amazon S3, then it is sufficient to only backup the metadata. The blob cloud storage service will ensure durability of the data blocks. Restore # As with the backup phase, the restore operation is broken down in different steps. Cluster deployment # The first step to redeploy the cluster is to redeploy the binaries and configuration. You should reuse the same cluster definition used to deploy the first (original) cluster. This will re-create the same cluster with the same configuration. RonDB restore # The deployment step above created a functioning empty cluster. To restore the cluster, the first step is to restore the metadata and online feature store data stored on RonDB. To restore the state of RonDB, we first need to restore its schemas and tables, then its data, rebuild the indices, and finally restore the users and grants. Restore RonDB schemas and tables # This command should be executed on one of the nodes in the head node group and is going to recreate the schemas, tables, and internal RonDB metadata. In the command below, you should replace the node_id with the id of the node you are running the command on, backup_id with the id of the backup you want to restore. Finally, you should replace the mgm_node_ip with the address of the node where the RonDB management service is running. /srv/hops/mysql/bin/ndb_restore -n [ node_id ] -b [ backup_id ] -m --disable-indexes --ndb-connectstring =[ mgm_node_ip ] :1186 --backup_path = /srv/hops/mysql-cluster/ndb/backups/BACKUP/BACKUP- [ backup_id ] Restore RonDB data # This command should be executed on all the RonDB datanodes. Each command should be customized with the node id of the node you are trying to restore (i.e., replace the node_id). As for the command above you should replace the backup_id and mgm_node_ip. /srv/hops/mysql/bin/ndb_restore -n [ node_id ] -b [ backup_id ] -r --ndb-connectstring =[ mgm_node_ip ] :1186 --backup_path = /srv/hops/mysql-cluster/ndb/backups/BACKUP/BACKUP- [ backup_id ] Rebuild the indices # In the first command we disable the indices for recovery. This last command will take care of enabling them again. This command needs to run only once on one of the nodes of the head node group. As for the commands above, you should replace node_id, backup_id and mgm_node_id. /srv/hops/mysql/bin/ndb_restore -n [ node_id ] -b [ backup_id ] --rebuild-indexes --ndb-connectstring =[ mgm_node_ip ] :1186 --backup_path = /srv/hops/mysql-cluster/ndb/backups/BACKUP/BACKUP- [ backup_ip ] Restore Users and Grants # In the backup phase, we took the backup of the user and grants separately. The last step of the RonDB restore process is to re-create all the users and grants both for Hopsworks services as well as for the online feature store users. This can be achieved by running the following command on one node of the head node group: /srv/hops/mysql-cluster/ndb/scripts/mysql-client.sh source users.sql HopsFS restore # With the metadata restored, you can now proceed to restore the file system blocks on HopsFS and restart the file system. When starting the datanode, it will advertise it\u2019s ID/ClusterID and Storage ID based on the VERSION file that can be found in this directory: /srv/hopsworks-data/hops/hopsdata/hdfs/dn/current It\u2019s important that all the datanodes are restored and they report their block to the namenodes processes running on the head nodes. By default the namenodes in HopsFS will exit \u201cSAFE MODE\u201d (i.e., the mode that allows only read operations) only when the datanodes have reported 99.9% of the blocks the namenodes have in the metadata. As such, the namenodes will not resume operations until all the file blocks have been restored. OpenSearch state rebuild # The OpenSearch state can be rebuilt using the Hopsworks metadata stored on RonDB. The rebuild process is done by using the re-indexing mechanism provided by ePipe. The re-indexing can be triggered by running the following command on the head node where ePipe is running: /srv/hops/epipe/bin/reindex-epipe.sh The script is deployed and configured during the platform deployment. Kafka topics rebuild # The backup and restore plan doesn\u2019t cover the data in transit in Kafka, for which the jobs producing it will have to be replayed. However, the RonDB backup contains the information necessary to recreate the topics of all the feature groups. You can run the following command, as super user, to recreate all the topics with the correct partitioning and replication factors: /srv/hops/kafka/bin/kafka-restore.sh The script is deployed and configured during the platform deployment.","title":"Disaster Recovery"},{"location":"admin/ha-dr/dr/#disaster-recovery","text":"","title":"Disaster Recovery"},{"location":"admin/ha-dr/dr/#backup","text":"The state of the Hopsworks cluster is divided into data and metadata and distributed across the different node groups. This section of the guide allows you to take a consistent backup between data in the offline and online feature store as well as the metadata. The following services contain critical state that should be backed up: RonDB : as mentioned above, the RonDB is used by Hopsworks to store the cluster metadata as well as the data for the online feature store. HopsFS : HopsFS stores the data for the batch feature store as well as checkpoints and logs for feature engineering applications. Backing up service/application metrics and services/applications logs are out of the scope of this guide. By default metrics and logs are rotated after 7 days. Application logs are available on HopsFS when the application has finished and, as such, are backed up with the rest of HopsFS\u2019 data. Apache Kafka and OpenSearch are additional services maintaining state. The OpenSearch metadata can be reconstructed from the metadata stored on RonDB. Apache Kafka is used in Hopsworks to store the in-flight data that is on its way to the online feature store. In the event of a total loss of the cluster, running jobs with inflight data will have to be replayed.","title":"Backup"},{"location":"admin/ha-dr/dr/#configuration-backup","text":"Hopsworks adopts an Infrastructure-as-code philosophy, as such all the configuration files for the different Hopsworks services are generated during the deployment phase. Cluster-specific customizations should be centralized in the cluster definition used to deploy the cluster. As such the cluster definition should be backed up (e.g., by committing it to a git repository) to be able to recreate the same cluster in case it needs to be recreated.","title":"Configuration Backup"},{"location":"admin/ha-dr/dr/#rondb-backup","text":"The RonDB backup is divided into two parts: user and privileges backup and data backup. To take the backup of users and privileges you can run the following command from any of the nodes in the head node group. This command generates a SQL file containing all the user definitions for both the metadata services (Hopsworks, HopsFS, Metastore) as well as the user and permission grants for the online feature store. This command needs to be run as user \u2018mysql\u2019 or with sudo privileges. /srv/hops/mysql/bin/mysqlpump -S /srv/hops/mysql-cluster/mysql.sock --exclude-databases = % --exclude-users = root,mysql.sys,mysql.session,mysql.infoschema --users > users.sql The second step is to trigger the backup of the data. This can be achieved by running the following command as user \u2018mysql\u2019 on one of the nodes of the head node group. /srv/hops/mysql-cluster/ndb/scripts/mgm-client.sh -e \"START BACKUP [replace_backup_id] SNAPSHOTEND WAIT COMPLETED\" The backup ID is an integer greater or equal than 1. The script uses the following: $(date +'%y%m%d%H%M') instead of an integer as backup id to make it easier to identify backups over time. The command instructs each RonDB datanode to backup the data it is responsible for. The backup will be located locally on each datanode under the following path: /srv/hops/mysql-cluster/ndb/backups/BACKUP - the directory name will be BACKUP- [ backup_id ] A more comprehensive backup script is available here - The script includes the steps above as well as collecting all the partial RonDB backups on a single node. The script is a good starting point and can be adapted to ship the database backup outside the cluster.","title":"RonDB Backup"},{"location":"admin/ha-dr/dr/#hopsfs-backup","text":"HopsFS is a distributed file system based on Apache HDFS. HopsFS stores its metadata in RonDB, as such metadata backup has already been discussed in the section above. The data is stored in the form of blocks on the different data nodes. For availability reasons, the blocks are replicated across three different data nodes. Within a node, the blocks are stored by default under the following directory, under the ownership of the \u2018hdfs\u2019 user: /srv/hopsworks-data/hops/hopsdata/hdfs/dn/ To safely backup all the data, a copy of all the datanodes should be taken. As the data is replicated across the different nodes, excluding a set of nodes might result in data loss. Additionally, as HopsFS blocks are files on the file system and the filesystem can be quite large, the backup is not transactional. Consistency is dictated by the metadata. Blocks being added during the copying process will not be visible when restoring as they are not part of the metadata backup taken prior to cloning the HopsFS blocks. When the HopsFS data blocks are stored in a cloud block storage, for example, Amazon S3, then it is sufficient to only backup the metadata. The blob cloud storage service will ensure durability of the data blocks.","title":"HopsFS Backup"},{"location":"admin/ha-dr/dr/#restore","text":"As with the backup phase, the restore operation is broken down in different steps.","title":"Restore"},{"location":"admin/ha-dr/dr/#cluster-deployment","text":"The first step to redeploy the cluster is to redeploy the binaries and configuration. You should reuse the same cluster definition used to deploy the first (original) cluster. This will re-create the same cluster with the same configuration.","title":"Cluster deployment"},{"location":"admin/ha-dr/dr/#rondb-restore","text":"The deployment step above created a functioning empty cluster. To restore the cluster, the first step is to restore the metadata and online feature store data stored on RonDB. To restore the state of RonDB, we first need to restore its schemas and tables, then its data, rebuild the indices, and finally restore the users and grants.","title":"RonDB restore"},{"location":"admin/ha-dr/dr/#restore-rondb-schemas-and-tables","text":"This command should be executed on one of the nodes in the head node group and is going to recreate the schemas, tables, and internal RonDB metadata. In the command below, you should replace the node_id with the id of the node you are running the command on, backup_id with the id of the backup you want to restore. Finally, you should replace the mgm_node_ip with the address of the node where the RonDB management service is running. /srv/hops/mysql/bin/ndb_restore -n [ node_id ] -b [ backup_id ] -m --disable-indexes --ndb-connectstring =[ mgm_node_ip ] :1186 --backup_path = /srv/hops/mysql-cluster/ndb/backups/BACKUP/BACKUP- [ backup_id ]","title":"Restore RonDB schemas and tables"},{"location":"admin/ha-dr/dr/#restore-rondb-data","text":"This command should be executed on all the RonDB datanodes. Each command should be customized with the node id of the node you are trying to restore (i.e., replace the node_id). As for the command above you should replace the backup_id and mgm_node_ip. /srv/hops/mysql/bin/ndb_restore -n [ node_id ] -b [ backup_id ] -r --ndb-connectstring =[ mgm_node_ip ] :1186 --backup_path = /srv/hops/mysql-cluster/ndb/backups/BACKUP/BACKUP- [ backup_id ]","title":"Restore RonDB data"},{"location":"admin/ha-dr/dr/#rebuild-the-indices","text":"In the first command we disable the indices for recovery. This last command will take care of enabling them again. This command needs to run only once on one of the nodes of the head node group. As for the commands above, you should replace node_id, backup_id and mgm_node_id. /srv/hops/mysql/bin/ndb_restore -n [ node_id ] -b [ backup_id ] --rebuild-indexes --ndb-connectstring =[ mgm_node_ip ] :1186 --backup_path = /srv/hops/mysql-cluster/ndb/backups/BACKUP/BACKUP- [ backup_ip ]","title":"Rebuild the indices"},{"location":"admin/ha-dr/dr/#restore-users-and-grants","text":"In the backup phase, we took the backup of the user and grants separately. The last step of the RonDB restore process is to re-create all the users and grants both for Hopsworks services as well as for the online feature store users. This can be achieved by running the following command on one node of the head node group: /srv/hops/mysql-cluster/ndb/scripts/mysql-client.sh source users.sql","title":"Restore Users and Grants"},{"location":"admin/ha-dr/dr/#hopsfs-restore","text":"With the metadata restored, you can now proceed to restore the file system blocks on HopsFS and restart the file system. When starting the datanode, it will advertise it\u2019s ID/ClusterID and Storage ID based on the VERSION file that can be found in this directory: /srv/hopsworks-data/hops/hopsdata/hdfs/dn/current It\u2019s important that all the datanodes are restored and they report their block to the namenodes processes running on the head nodes. By default the namenodes in HopsFS will exit \u201cSAFE MODE\u201d (i.e., the mode that allows only read operations) only when the datanodes have reported 99.9% of the blocks the namenodes have in the metadata. As such, the namenodes will not resume operations until all the file blocks have been restored.","title":"HopsFS restore"},{"location":"admin/ha-dr/dr/#opensearch-state-rebuild","text":"The OpenSearch state can be rebuilt using the Hopsworks metadata stored on RonDB. The rebuild process is done by using the re-indexing mechanism provided by ePipe. The re-indexing can be triggered by running the following command on the head node where ePipe is running: /srv/hops/epipe/bin/reindex-epipe.sh The script is deployed and configured during the platform deployment.","title":"OpenSearch state rebuild"},{"location":"admin/ha-dr/dr/#kafka-topics-rebuild","text":"The backup and restore plan doesn\u2019t cover the data in transit in Kafka, for which the jobs producing it will have to be replayed. However, the RonDB backup contains the information necessary to recreate the topics of all the feature groups. You can run the following command, as super user, to recreate all the topics with the correct partitioning and replication factors: /srv/hops/kafka/bin/kafka-restore.sh The script is deployed and configured during the platform deployment.","title":"Kafka topics rebuild"},{"location":"admin/ha-dr/ha/","text":"High Availability # At a high level a Hopsworks cluster can be divided into 4 groups of nodes. Each node group should be deployed according to the requirements (e.g., 3/5/7 nodes for the head node group) to guarantee the availability of the components. Head nodes : The head node is responsible for running all the metadata, public API, and user interface services that are required for Hopsworks to provide its functionality. They need to be deployed in an odd number (1, 3, 5) as the head nodes run services like Zookeeper and OpenSearch which enforce consistency through quorum based protocols. The head nodes are also responsible for managing the services running on the remaining group of nodes. Worker nodes : The worker node is responsible for executing the feature engineering pipeline code as well as storing the data for the offline feature store (HopsFS). In an on-prem deployment, the data is stored and replicated on the workers\u2019 local hard drives. By default the data is replicated across 3 workers. In a cloud deployment, HopsFS\u2019 data is persisted in a cloud object store (Amazon S3, Azure Blob Storage, Google Cloud Blob Storage) and the HopsFS datanodes are responsible for persisting, retrieving and caching of blocks from the object store. RonDB Data nodes : These nodes are responsible for storing the services\u2019 metadata (Hopsworks, HopsFS, Hive Metastore, Airflow) as well as the data for the online feature store. For high availability, at least two data nodes should be deployed and RonDB is typically configured with a replication factor of 2, as it uses synchronous replication with 2-phase commit, not a quorum-based replication protocol. More advanced deployment patterns and best practices are covered in the RonDB documentation (https://docs.rondb.ai) . Query brokers : The query brokers are the entry point for querying the online feature store. They handle authentication, authorization and execution of the requests for online feature data being submitted from the feature store APIs. At least two query brokers should be deployed to achieve high availability. Query brokers are stateless. Additional query brokers should be deployed to handle additional load and clients. Example deployment: Example High Available deployment For higher availability, a Hopsworks cluster should be deployed across multiple availability zones, however, a single cluster cannot be deployed across multiple regions. Multiple region deployments are out of the scope of this guide. A different service placement is also possible, e.g., separating RonDB data nodes between metadata and online feature store or adding more replicas of a metadata service without necessarily adding a whole new head node, however, this is outside the scope of this guide.","title":"High Availability"},{"location":"admin/ha-dr/ha/#high-availability","text":"At a high level a Hopsworks cluster can be divided into 4 groups of nodes. Each node group should be deployed according to the requirements (e.g., 3/5/7 nodes for the head node group) to guarantee the availability of the components. Head nodes : The head node is responsible for running all the metadata, public API, and user interface services that are required for Hopsworks to provide its functionality. They need to be deployed in an odd number (1, 3, 5) as the head nodes run services like Zookeeper and OpenSearch which enforce consistency through quorum based protocols. The head nodes are also responsible for managing the services running on the remaining group of nodes. Worker nodes : The worker node is responsible for executing the feature engineering pipeline code as well as storing the data for the offline feature store (HopsFS). In an on-prem deployment, the data is stored and replicated on the workers\u2019 local hard drives. By default the data is replicated across 3 workers. In a cloud deployment, HopsFS\u2019 data is persisted in a cloud object store (Amazon S3, Azure Blob Storage, Google Cloud Blob Storage) and the HopsFS datanodes are responsible for persisting, retrieving and caching of blocks from the object store. RonDB Data nodes : These nodes are responsible for storing the services\u2019 metadata (Hopsworks, HopsFS, Hive Metastore, Airflow) as well as the data for the online feature store. For high availability, at least two data nodes should be deployed and RonDB is typically configured with a replication factor of 2, as it uses synchronous replication with 2-phase commit, not a quorum-based replication protocol. More advanced deployment patterns and best practices are covered in the RonDB documentation (https://docs.rondb.ai) . Query brokers : The query brokers are the entry point for querying the online feature store. They handle authentication, authorization and execution of the requests for online feature data being submitted from the feature store APIs. At least two query brokers should be deployed to achieve high availability. Query brokers are stateless. Additional query brokers should be deployed to handle additional load and clients. Example deployment: Example High Available deployment For higher availability, a Hopsworks cluster should be deployed across multiple availability zones, however, a single cluster cannot be deployed across multiple regions. Multiple region deployments are out of the scope of this guide. A different service placement is also possible, e.g., separating RonDB data nodes between metadata and online feature store or adding more replicas of a metadata service without necessarily adding a whole new head node, however, this is outside the scope of this guide.","title":"High Availability"},{"location":"admin/ha-dr/intro/","text":"Hopsworks High Availability and Disaster Recovery Documentation # The Hopsworks Feature Store is the underlying component powering enterprise ML pipelines as well as serving feature data to model making user facing predictions. Sometimes the Hopsworks cluster can experience hardware failures or power loss, to help you plan for these occasions and avoid Hopsworks Feature Store downtime, we put together this guide. This guide is divided into three sections: High availability : deployment patterns and best practices to make sure individual component failures do not impact the availability of the Hopsworks cluster. Backup : configuration policies and best practices to make sure you have have fresh copy of the data and metadata in case of necessity Restore : procedures and best practices to restore a previous backup if needed.","title":"Overview"},{"location":"admin/ha-dr/intro/#hopsworks-high-availability-and-disaster-recovery-documentation","text":"The Hopsworks Feature Store is the underlying component powering enterprise ML pipelines as well as serving feature data to model making user facing predictions. Sometimes the Hopsworks cluster can experience hardware failures or power loss, to help you plan for these occasions and avoid Hopsworks Feature Store downtime, we put together this guide. This guide is divided into three sections: High availability : deployment patterns and best practices to make sure individual component failures do not impact the availability of the Hopsworks cluster. Backup : configuration policies and best practices to make sure you have have fresh copy of the data and metadata in case of necessity Restore : procedures and best practices to restore a previous backup if needed.","title":"Hopsworks High Availability and Disaster Recovery Documentation"},{"location":"admin/ldap/configure-krb/","text":"Configure Kerberos # Introduction # Kerberos is a network authentication protocol that allow nodes to communicating over a non-secure network to prove their identity to one another in a secure manner. This tutorial shows an administrator how to configure Kerberos authentication. Kerberos need some server configuration before you can enable it from the UI. Prerequisites # A server configured with Kerberos. See Server Configuration for Kerberos for instruction on how to do this. Step 1: Enable Kerberos # After configuring the server you can configure Authentication methods by clicking on your name in the top right corner of the navigation bar and choosing Cluster Settings from the dropdown menu. In the Authentication tab you can find in Cluster Settings , you can enable Kerberos by clicking on the Kerberos checkbox. If LDAP/Kerberos checkbox is not checked, make sure that you configured your application server and enable it by clicking on the checkbox. Setup Authentication Methods Step 2: Edit configuration # Finally, click on edit configuration and fill in the attributes. Configure Kerberos Account status: the status a user will be assigned when logging in for the first time. If a user is assigned a status different from Activated an admin needs to manually activate each user from the User management . Group mapping: allows you to specify a mapping between LDAP groups and Hopsworks groups. The mapping is a semicolon separated string in the form Directory Administrators->HOPS_ADMIN;IT People-> HOPS_USER . Default is empty. If no mapping is specified, users need to be assigned a role by an admin before they can log in. User id: the id field in LDAP with a string placeholder. Default uid=%s . User given name: the given name field in LDAP. Default givenName . User surname: the surname field in LDAP. Default sn . User email: the email field in LDAP. Default mail . User search filter: the search filter for user. Default uid=%s . Principal search filter: the search filter for principal name. Default krbPrincipalName=%s . Group search filter: the search filter for groups. Default member=%d . Group target: the target to search for groups in the LDAP directory tree. Default cn . Dynamic group target: the target to search for dynamic groups in the LDAP directory tree. Default memberOf . User dn: specify the distinguished name (DN) of the container or base point where the users are stored. Default is empty. Group dn: specify the DN of the container or base point where the groups are stored. Default is empty. All defaults are taken from OpenLDAP . Note Group mapping can be disabled by setting ldap_group_mapping_enabled=false in the Configuration UI. When group mapping is disabled an administrator needs to activate each user from the User Management page. If group mapping is disabled then Account status in the Kerberos configuration above should be set to Verified . The login page will now have the choice to use Kerberos for authentication. Log in using Kerberos Note Make sure that you have Kerberos properly configured on your computer and you are logged in. Kerberos support must also be configured on the browser to use Kerberos for authentication. Conclusion # In this guide you learned how to configure Kerberos for authentication.","title":"Configure Kerberos"},{"location":"admin/ldap/configure-krb/#configure-kerberos","text":"","title":"Configure Kerberos"},{"location":"admin/ldap/configure-krb/#introduction","text":"Kerberos is a network authentication protocol that allow nodes to communicating over a non-secure network to prove their identity to one another in a secure manner. This tutorial shows an administrator how to configure Kerberos authentication. Kerberos need some server configuration before you can enable it from the UI.","title":"Introduction"},{"location":"admin/ldap/configure-krb/#prerequisites","text":"A server configured with Kerberos. See Server Configuration for Kerberos for instruction on how to do this.","title":"Prerequisites"},{"location":"admin/ldap/configure-krb/#step-1-enable-kerberos","text":"After configuring the server you can configure Authentication methods by clicking on your name in the top right corner of the navigation bar and choosing Cluster Settings from the dropdown menu. In the Authentication tab you can find in Cluster Settings , you can enable Kerberos by clicking on the Kerberos checkbox. If LDAP/Kerberos checkbox is not checked, make sure that you configured your application server and enable it by clicking on the checkbox. Setup Authentication Methods","title":"Step 1: Enable Kerberos"},{"location":"admin/ldap/configure-krb/#step-2-edit-configuration","text":"Finally, click on edit configuration and fill in the attributes. Configure Kerberos Account status: the status a user will be assigned when logging in for the first time. If a user is assigned a status different from Activated an admin needs to manually activate each user from the User management . Group mapping: allows you to specify a mapping between LDAP groups and Hopsworks groups. The mapping is a semicolon separated string in the form Directory Administrators->HOPS_ADMIN;IT People-> HOPS_USER . Default is empty. If no mapping is specified, users need to be assigned a role by an admin before they can log in. User id: the id field in LDAP with a string placeholder. Default uid=%s . User given name: the given name field in LDAP. Default givenName . User surname: the surname field in LDAP. Default sn . User email: the email field in LDAP. Default mail . User search filter: the search filter for user. Default uid=%s . Principal search filter: the search filter for principal name. Default krbPrincipalName=%s . Group search filter: the search filter for groups. Default member=%d . Group target: the target to search for groups in the LDAP directory tree. Default cn . Dynamic group target: the target to search for dynamic groups in the LDAP directory tree. Default memberOf . User dn: specify the distinguished name (DN) of the container or base point where the users are stored. Default is empty. Group dn: specify the DN of the container or base point where the groups are stored. Default is empty. All defaults are taken from OpenLDAP . Note Group mapping can be disabled by setting ldap_group_mapping_enabled=false in the Configuration UI. When group mapping is disabled an administrator needs to activate each user from the User Management page. If group mapping is disabled then Account status in the Kerberos configuration above should be set to Verified . The login page will now have the choice to use Kerberos for authentication. Log in using Kerberos Note Make sure that you have Kerberos properly configured on your computer and you are logged in. Kerberos support must also be configured on the browser to use Kerberos for authentication.","title":"Step 2: Edit configuration"},{"location":"admin/ldap/configure-krb/#conclusion","text":"In this guide you learned how to configure Kerberos for authentication.","title":"Conclusion"},{"location":"admin/ldap/configure-ldap/","text":"Configure LDAP/Kerberos # Introduction # LDAP (Lightweight Directory Access Protocol) is a software protocol for enabling anyone in a network to gain access to resources such as files and devices. This tutorial shows an administrator how to configure LDAP authentication. LDAP need some server configuration before you can enable it from the UI. Prerequisites # A server configured with LDAP. See Server Configuration for LDAP for instruction on how to do this. Step 1: Enable LDAP # After configuring the server you can configure Authentication methods by clicking on your name in the top right corner of the navigation bar and choosing Cluster Settings from the dropdown menu. In the Authentication tab you can find in Cluster Settings , you can enable LDAP by clicking on the LDAP checkbox. If LDAP/Kerberos checkbox is not checked make sure that you configured your application server and enable it by clicking on the checkbox. Setup Authentication Methods Step 2: Edit configuration # Finally, click on edit configuration and fill in the attributes. Configure LDAP Account status: the status a user will be assigned when logging in for the first time. If a use is assigned a status different from Activated an admin needs to manually activate each user from the User management . Group mapping: allows you to specify a mapping between LDAP groups and Hopsworks groups. The mapping is a semicolon separated string in the form Directory Administrators->HOPS_ADMIN;IT People-> HOPS_USER . Default is empty. If no mapping is specified, users need to be assigned a role by an admin before they can log in. User id: the id field in LDAP with a string placeholder. Default uid=%s . User given name: the given name field in LDAP. Default givenName . User surname: the surname field in LDAP. Default sn . User email: the email field in LDAP. Default mail . User search filter: the search filter for user. Default uid=%s . Group search filter: the search filter for groups. Default member=%d . Group target: the target to search for groups in the LDAP directory tree. Default cn . Dynamic group target: the target to search for dynamic groups in the LDAP directory tree. Default memberOf . User dn: specify the distinguished name (DN) of the container or base point where the users are stored. Default is empty. Group dn: specify the DN of the container or base point where the groups are stored. Default is empty. All defaults are taken from OpenLDAP . The login page will now have the choice to use LDAP for authentication. Log in using LDAP Note Group mapping can be disabled by setting ldap_group_mapping_enabled=false in the Configuration UI. When group mapping is disabled an administrator needs to activate each user from the User Management page. If group mapping is disabled then Account status in LDAP configuration above should be set to Verified . Conclusion # In this guide you learned how to configure LDAP for authentication.","title":"Configure LDAP"},{"location":"admin/ldap/configure-ldap/#configure-ldapkerberos","text":"","title":"Configure LDAP/Kerberos"},{"location":"admin/ldap/configure-ldap/#introduction","text":"LDAP (Lightweight Directory Access Protocol) is a software protocol for enabling anyone in a network to gain access to resources such as files and devices. This tutorial shows an administrator how to configure LDAP authentication. LDAP need some server configuration before you can enable it from the UI.","title":"Introduction"},{"location":"admin/ldap/configure-ldap/#prerequisites","text":"A server configured with LDAP. See Server Configuration for LDAP for instruction on how to do this.","title":"Prerequisites"},{"location":"admin/ldap/configure-ldap/#step-1-enable-ldap","text":"After configuring the server you can configure Authentication methods by clicking on your name in the top right corner of the navigation bar and choosing Cluster Settings from the dropdown menu. In the Authentication tab you can find in Cluster Settings , you can enable LDAP by clicking on the LDAP checkbox. If LDAP/Kerberos checkbox is not checked make sure that you configured your application server and enable it by clicking on the checkbox. Setup Authentication Methods","title":"Step 1: Enable LDAP"},{"location":"admin/ldap/configure-ldap/#step-2-edit-configuration","text":"Finally, click on edit configuration and fill in the attributes. Configure LDAP Account status: the status a user will be assigned when logging in for the first time. If a use is assigned a status different from Activated an admin needs to manually activate each user from the User management . Group mapping: allows you to specify a mapping between LDAP groups and Hopsworks groups. The mapping is a semicolon separated string in the form Directory Administrators->HOPS_ADMIN;IT People-> HOPS_USER . Default is empty. If no mapping is specified, users need to be assigned a role by an admin before they can log in. User id: the id field in LDAP with a string placeholder. Default uid=%s . User given name: the given name field in LDAP. Default givenName . User surname: the surname field in LDAP. Default sn . User email: the email field in LDAP. Default mail . User search filter: the search filter for user. Default uid=%s . Group search filter: the search filter for groups. Default member=%d . Group target: the target to search for groups in the LDAP directory tree. Default cn . Dynamic group target: the target to search for dynamic groups in the LDAP directory tree. Default memberOf . User dn: specify the distinguished name (DN) of the container or base point where the users are stored. Default is empty. Group dn: specify the DN of the container or base point where the groups are stored. Default is empty. All defaults are taken from OpenLDAP . The login page will now have the choice to use LDAP for authentication. Log in using LDAP Note Group mapping can be disabled by setting ldap_group_mapping_enabled=false in the Configuration UI. When group mapping is disabled an administrator needs to activate each user from the User Management page. If group mapping is disabled then Account status in LDAP configuration above should be set to Verified .","title":"Step 2: Edit configuration"},{"location":"admin/ldap/configure-ldap/#conclusion","text":"In this guide you learned how to configure LDAP for authentication.","title":"Conclusion"},{"location":"admin/ldap/configure-project-mapping/","text":"Configure LDAP/Kerberos group to project mapping # Introduction # A group to project mapping allows you to add members of your LDAP group to a project without having to add each user manually. A mapping is created by specifying a group from LDAP that will be mapped to a project in Hopsworks and what role the members of that group will be assigned in the project. Once a mapping is created, project membership is managed by LDAP group membership. Any change to group membership in LDAP will be reflected in Hopsworks i.e. removing a user from the LDAP group will also remove them from the project. Prerequisites # A server configured with LDAP or Kerberos. See Server Configuration for Kerberos and Server Configuration for LDAP for instructions on how to do this. LDAP group mapping sync enabled. This can be done by setting the variable ldap_group_mapping_sync_enabled=true . See Cluster Configuration on how to change variable values in Hopsworks. Step 1: Create a mapping # To create a mapping go to Cluster Settings by clicking on your name in the top right corner of the navigation bar and choosing Cluster Settings from the dropdown menu. In the Project mapping tab, you can create a new mapping by clicking on Create new mapping . Project mapping This will take you to the create mapping page shown below Create mapping Here you can choose multiple Remote groups from your LDAP groups and map them to a project from the Project drop down list. You can also choose the Project role users will be assigned when they are added to the project. Finally, click on Create mapping and go back to mappings. You should see the newly created mapping(s) as shown below. Project mappings Note If there are no groups in the Remote group drop down list check if ldap_groups_search_filter is correct by using the value in ldapsearch replacing %c with * , as shown in the example below. ldapsearch -LLL -H ldap:/// -b '<base dn>' -D '<user dn>' -w <password> '(&(objectClass=groupOfNames)(cn=*))' This should return all the groups in your LDAP. See Cluster Configuration on how to find and update the value of this variable. Step 2: Edit a mapping # From the list of mappings click on the edit button ( ). This will make the row editable and allow you to change the remote group , project name , and project role of a mapping. Edit mapping Warning Updating a mapping's remote group or project name will remove all members of the previous group from the project. Step 3: Delete a mapping # To delete a mapping click on the delete button. Warning Deleting a mapping will remove all members of that group from the project. Step 4: Configure sync interval # After configuring all the group mappings users will be added to or removed from the projects in the mapping when they login to Hopsworks. It is also possible to synchronize mappings without requiring users to log out. This can be done by setting ldap_group_mapping_sync_interval to an interval greater or equal to 2 minutes. If ldap_group_mapping_sync_interval is set group mapping sync will run periodically based on the interval and add or remove users from projects. Conclusion # In this guide you learned how to configure LDAP group to project mapping.","title":"Configure Project Mapping"},{"location":"admin/ldap/configure-project-mapping/#configure-ldapkerberos-group-to-project-mapping","text":"","title":"Configure LDAP/Kerberos group to project mapping"},{"location":"admin/ldap/configure-project-mapping/#introduction","text":"A group to project mapping allows you to add members of your LDAP group to a project without having to add each user manually. A mapping is created by specifying a group from LDAP that will be mapped to a project in Hopsworks and what role the members of that group will be assigned in the project. Once a mapping is created, project membership is managed by LDAP group membership. Any change to group membership in LDAP will be reflected in Hopsworks i.e. removing a user from the LDAP group will also remove them from the project.","title":"Introduction"},{"location":"admin/ldap/configure-project-mapping/#prerequisites","text":"A server configured with LDAP or Kerberos. See Server Configuration for Kerberos and Server Configuration for LDAP for instructions on how to do this. LDAP group mapping sync enabled. This can be done by setting the variable ldap_group_mapping_sync_enabled=true . See Cluster Configuration on how to change variable values in Hopsworks.","title":"Prerequisites"},{"location":"admin/ldap/configure-project-mapping/#step-1-create-a-mapping","text":"To create a mapping go to Cluster Settings by clicking on your name in the top right corner of the navigation bar and choosing Cluster Settings from the dropdown menu. In the Project mapping tab, you can create a new mapping by clicking on Create new mapping . Project mapping This will take you to the create mapping page shown below Create mapping Here you can choose multiple Remote groups from your LDAP groups and map them to a project from the Project drop down list. You can also choose the Project role users will be assigned when they are added to the project. Finally, click on Create mapping and go back to mappings. You should see the newly created mapping(s) as shown below. Project mappings Note If there are no groups in the Remote group drop down list check if ldap_groups_search_filter is correct by using the value in ldapsearch replacing %c with * , as shown in the example below. ldapsearch -LLL -H ldap:/// -b '<base dn>' -D '<user dn>' -w <password> '(&(objectClass=groupOfNames)(cn=*))' This should return all the groups in your LDAP. See Cluster Configuration on how to find and update the value of this variable.","title":"Step 1: Create a mapping"},{"location":"admin/ldap/configure-project-mapping/#step-2-edit-a-mapping","text":"From the list of mappings click on the edit button ( ). This will make the row editable and allow you to change the remote group , project name , and project role of a mapping. Edit mapping Warning Updating a mapping's remote group or project name will remove all members of the previous group from the project.","title":"Step 2: Edit a mapping"},{"location":"admin/ldap/configure-project-mapping/#step-3-delete-a-mapping","text":"To delete a mapping click on the delete button. Warning Deleting a mapping will remove all members of that group from the project.","title":"Step 3: Delete a mapping"},{"location":"admin/ldap/configure-project-mapping/#step-4-configure-sync-interval","text":"After configuring all the group mappings users will be added to or removed from the projects in the mapping when they login to Hopsworks. It is also possible to synchronize mappings without requiring users to log out. This can be done by setting ldap_group_mapping_sync_interval to an interval greater or equal to 2 minutes. If ldap_group_mapping_sync_interval is set group mapping sync will run periodically based on the interval and add or remove users from projects.","title":"Step 4: Configure sync interval"},{"location":"admin/ldap/configure-project-mapping/#conclusion","text":"In this guide you learned how to configure LDAP group to project mapping.","title":"Conclusion"},{"location":"admin/ldap/configure-server/","text":"Configure Server for LDAP and Kerberos # Introduction # LDAP and Kerberos integration need some configuration in the Karamel cluster definition used to deploy your Hopsworks cluster. This tutorial shows an administrator how to configure the application server for LDAP and Kerberos integration. Prerequisites # An accessable LDAP domain. A Kerberos Key Distribution Center (KDC) running on the same domain as Hopsworks (Only for Kerberos). Step 1: Server Configuration for LDAP # The LDAP attributes below are used to configure JNDI external resource in Payara. The JNDI resource will communicate with your LDAP server to perform the authentication. ldap : enabled : true jndilookupname : \"dc=hopsworks,dc=ai\" provider_url : \"ldap://193.10.66.104:1389\" attr_binary_val : \"entryUUID\" security_auth : \"none\" security_principal : \"\" security_credentials : \"\" referral : \"ignore\" additional_props : \"\" jndilookupname: should contain the LDAP domain. attr_binary_val: is the binary unique identifier that will be used in subsequent logins to identify the user. security_auth: how to authenticate to the LDAP server. security_principal: contains the username of the user that will be used to query LDAP. security_credentials: contains the password of the user that will be used to query LDAP. referral: whether to follow or ignore an alternate location in which an LDAP Request may be processed. Without Karamel/Chef # An already deployed instance can be configured to connect to LDAP without re-running Karamel/Chef. Go to the payara admin UI and create a new JNDI external resource. The name of the resource should be ldap/LdapResource . LDAP Resource This can also be achived by running the bellow asadmin command. asadmin create-jndi-resource \\ --restype javax.naming.ldap.LdapContext \\ --factoryclass com.sun.jndi.ldap.LdapCtxFactory \\ --jndilookupname dc \\= hopsworks \\, dc \\= ai \\ --property java.naming.provider.url = ldap \\\\ ://193 \\. 10 \\. 66 \\. 104 \\\\ :1389: \\ hopsworks.ldap.basedn = dc \\\\\\= hopsworks \\, dc \\\\\\= ai: \\ java.naming.ldap.attributes.binary = entryUUID: \\ java.naming.security.authentication = simple: \\ java.naming.security.principal = <username>: \\ java.naming.security.credentials = <password>: \\ java.naming.referral = ignore \\ ldap/LdapResource Step 2: Server Configuration for Kerberos # The Kerberos attributes are used to configure SPNEGO . SPNEGO is used to establish a secure context between the requester and the application server when using Kerberos authentication. kerberos : enabled : true krb_conf_path : \"/etc/krb5.conf\" krb_server_key_tab_path : \"/etc/security/keytabs/service.keytab\" krb_server_key_tab_name : \"service.keytab\" spnego_server_conf : '\\nuseKeyTab=true\\nprincipal=\\\"HTTP/server.hopsworks.ai@HOPSWORKS.AI\\\"\\nstoreKey=true\\nisInitiator=false' ldap : jndilookupname : \"dc=hopsworks,dc=ai\" provider_url : \"ldap://193.10.66.104:1389\" attr_binary_val : \"objectGUID\" security_auth : \"none\" security_principal : \"\" security_credentials : \"\" referral : \"ignore\" additional_props : \"\" Both Kerberos and LDAP attributes need to be specified to configure Kerberos. The LDAP attributes are explained above. krb_conf_path: contains the path to the krb5.conf used by SPNEGO to get information about the default domain and the location of the Kerberos KDC. The file is copied by the recipe in to /srv/hops/domains/domain1/config. krb_server_key_tab_path: contains the path to the Kerberos service keytab. The keytab is copied by the recipe in to /srv/hops/domains/domain/config with the name set in the krb_server_key_tab_name attribute. spnego_server_conf: contains the configuration that will be appended to Payara's (application serve used to host hopsworks) login.conf. In particular, it should contain useKeyTab=true, and the principal name to be used in the authentication phase. Initiator should be set to false. Conclusion # In this guide you learned how to configure the application server for LDAP and Kerberos.","title":"Configure server for LDAP and Kerberos"},{"location":"admin/ldap/configure-server/#configure-server-for-ldap-and-kerberos","text":"","title":"Configure Server for LDAP and Kerberos"},{"location":"admin/ldap/configure-server/#introduction","text":"LDAP and Kerberos integration need some configuration in the Karamel cluster definition used to deploy your Hopsworks cluster. This tutorial shows an administrator how to configure the application server for LDAP and Kerberos integration.","title":"Introduction"},{"location":"admin/ldap/configure-server/#prerequisites","text":"An accessable LDAP domain. A Kerberos Key Distribution Center (KDC) running on the same domain as Hopsworks (Only for Kerberos).","title":"Prerequisites"},{"location":"admin/ldap/configure-server/#step-1-server-configuration-for-ldap","text":"The LDAP attributes below are used to configure JNDI external resource in Payara. The JNDI resource will communicate with your LDAP server to perform the authentication. ldap : enabled : true jndilookupname : \"dc=hopsworks,dc=ai\" provider_url : \"ldap://193.10.66.104:1389\" attr_binary_val : \"entryUUID\" security_auth : \"none\" security_principal : \"\" security_credentials : \"\" referral : \"ignore\" additional_props : \"\" jndilookupname: should contain the LDAP domain. attr_binary_val: is the binary unique identifier that will be used in subsequent logins to identify the user. security_auth: how to authenticate to the LDAP server. security_principal: contains the username of the user that will be used to query LDAP. security_credentials: contains the password of the user that will be used to query LDAP. referral: whether to follow or ignore an alternate location in which an LDAP Request may be processed.","title":"Step 1: Server Configuration for LDAP"},{"location":"admin/ldap/configure-server/#without-karamelchef","text":"An already deployed instance can be configured to connect to LDAP without re-running Karamel/Chef. Go to the payara admin UI and create a new JNDI external resource. The name of the resource should be ldap/LdapResource . LDAP Resource This can also be achived by running the bellow asadmin command. asadmin create-jndi-resource \\ --restype javax.naming.ldap.LdapContext \\ --factoryclass com.sun.jndi.ldap.LdapCtxFactory \\ --jndilookupname dc \\= hopsworks \\, dc \\= ai \\ --property java.naming.provider.url = ldap \\\\ ://193 \\. 10 \\. 66 \\. 104 \\\\ :1389: \\ hopsworks.ldap.basedn = dc \\\\\\= hopsworks \\, dc \\\\\\= ai: \\ java.naming.ldap.attributes.binary = entryUUID: \\ java.naming.security.authentication = simple: \\ java.naming.security.principal = <username>: \\ java.naming.security.credentials = <password>: \\ java.naming.referral = ignore \\ ldap/LdapResource","title":"Without Karamel/Chef"},{"location":"admin/ldap/configure-server/#step-2-server-configuration-for-kerberos","text":"The Kerberos attributes are used to configure SPNEGO . SPNEGO is used to establish a secure context between the requester and the application server when using Kerberos authentication. kerberos : enabled : true krb_conf_path : \"/etc/krb5.conf\" krb_server_key_tab_path : \"/etc/security/keytabs/service.keytab\" krb_server_key_tab_name : \"service.keytab\" spnego_server_conf : '\\nuseKeyTab=true\\nprincipal=\\\"HTTP/server.hopsworks.ai@HOPSWORKS.AI\\\"\\nstoreKey=true\\nisInitiator=false' ldap : jndilookupname : \"dc=hopsworks,dc=ai\" provider_url : \"ldap://193.10.66.104:1389\" attr_binary_val : \"objectGUID\" security_auth : \"none\" security_principal : \"\" security_credentials : \"\" referral : \"ignore\" additional_props : \"\" Both Kerberos and LDAP attributes need to be specified to configure Kerberos. The LDAP attributes are explained above. krb_conf_path: contains the path to the krb5.conf used by SPNEGO to get information about the default domain and the location of the Kerberos KDC. The file is copied by the recipe in to /srv/hops/domains/domain1/config. krb_server_key_tab_path: contains the path to the Kerberos service keytab. The keytab is copied by the recipe in to /srv/hops/domains/domain/config with the name set in the krb_server_key_tab_name attribute. spnego_server_conf: contains the configuration that will be appended to Payara's (application serve used to host hopsworks) login.conf. In particular, it should contain useKeyTab=true, and the principal name to be used in the authentication phase. Initiator should be set to false.","title":"Step 2: Server Configuration for Kerberos"},{"location":"admin/ldap/configure-server/#conclusion","text":"In this guide you learned how to configure the application server for LDAP and Kerberos.","title":"Conclusion"},{"location":"admin/monitoring/export-metrics/","text":"Exporting Hopsworks metrics # Introduction # Hopsworks services produce metrics which are centrally gathered by Prometheus and visualized in Grafana . Although the system is self-contained, it is possible to export these metrics to third-party services or another Prometheus instance. Prerequisites # In order to configure Prometheus to export metrics you need root SSH access to either Hopsworks or to the target server depending on the export method you choose below. Exporting metrics # Prometheus can be configured to export metrics to another Prometheus instance (cross-service federation) or to a custom service which knows how to handle them. Prometheus federation # Prometheus servers can be federated to scale better or to just clone all metrics (cross-service federation). Prometheus federation is well documented but there are some specificities to Hopsworks. In the guide below we assume Prometheus A is the service running in Hopsworks and Prometheus B is the server you want to clone metrics to. Step 1 # Prometheus B needs to be able to connect to TCP port 9089 of Prometheus B to scrape metrics. If you have any firewall (or Security Group) in place, allow ingress for that port. Step 2 # SSH into Prometheus B server, edit Prometheus configuration file and add the following under the scrape_configs Note Replace IP_ADDRESS with the actual address of Hopsworks server - job_name : 'federate' scrape_interval : 15s honor_labels : true metrics_path : '/federate' params : 'match[]' : - '{job=\"airflow\"}' - '{job=\"pushgateway\"}' - '{job=\"hadoop\"}' - '{job=\"hopsworks\"}' static_configs : - targets : - 'IP_ADDRESS:9089' These are the basic labels gathered by Hopsworks. If your Hopsworks cluster runs without Kubernetes append '{job=\"cadvisor\"}' to match[] list If your Hopsworks cluster runs with Kubernetes append the following labels to match[] '{job=~\"knative.+\"}' '{job=\"kubernetes-cadvisor\"}' '{job=\"istio-envoy\"}' '{job=\"kube-state-metrics\"}' '{job=\"cadvisor\"}' '{job=\"cadvisor\"}' '{job=\"cadvisor\"}' Step 3 # Finally restart Prometheus service with sudo systemctl restart prometheus Custom service # Prometheus can push metrics to another custom resource via HTTP. The custom service is responsible for handling the received metrics. To push metrics with this method we use the remote_write configuration. We will only give a sample configuration as remote_write is extensively documented in Prometheus documentation In the example below we push metrics to a custom service listening on port 9096 which transforms the metrics and forwards them. remote_write : - url : \"http://localhost:9096\" queue_config : capacity : 10000 max_samples_per_send : 5000 batch_send_deadline : 60s Conclusion # In this guide we showed how you can push metrics outside of Hopsworks cluster using two methods, (a) federated Prometheus or (b) remote write to a custom service. This configuration is useful if you already have a centralized monitoring system with alerts already configured.","title":"Export metrics"},{"location":"admin/monitoring/export-metrics/#exporting-hopsworks-metrics","text":"","title":"Exporting Hopsworks metrics"},{"location":"admin/monitoring/export-metrics/#introduction","text":"Hopsworks services produce metrics which are centrally gathered by Prometheus and visualized in Grafana . Although the system is self-contained, it is possible to export these metrics to third-party services or another Prometheus instance.","title":"Introduction"},{"location":"admin/monitoring/export-metrics/#prerequisites","text":"In order to configure Prometheus to export metrics you need root SSH access to either Hopsworks or to the target server depending on the export method you choose below.","title":"Prerequisites"},{"location":"admin/monitoring/export-metrics/#exporting-metrics","text":"Prometheus can be configured to export metrics to another Prometheus instance (cross-service federation) or to a custom service which knows how to handle them.","title":"Exporting metrics"},{"location":"admin/monitoring/export-metrics/#prometheus-federation","text":"Prometheus servers can be federated to scale better or to just clone all metrics (cross-service federation). Prometheus federation is well documented but there are some specificities to Hopsworks. In the guide below we assume Prometheus A is the service running in Hopsworks and Prometheus B is the server you want to clone metrics to.","title":"Prometheus federation"},{"location":"admin/monitoring/export-metrics/#step-1","text":"Prometheus B needs to be able to connect to TCP port 9089 of Prometheus B to scrape metrics. If you have any firewall (or Security Group) in place, allow ingress for that port.","title":"Step 1"},{"location":"admin/monitoring/export-metrics/#step-2","text":"SSH into Prometheus B server, edit Prometheus configuration file and add the following under the scrape_configs Note Replace IP_ADDRESS with the actual address of Hopsworks server - job_name : 'federate' scrape_interval : 15s honor_labels : true metrics_path : '/federate' params : 'match[]' : - '{job=\"airflow\"}' - '{job=\"pushgateway\"}' - '{job=\"hadoop\"}' - '{job=\"hopsworks\"}' static_configs : - targets : - 'IP_ADDRESS:9089' These are the basic labels gathered by Hopsworks. If your Hopsworks cluster runs without Kubernetes append '{job=\"cadvisor\"}' to match[] list If your Hopsworks cluster runs with Kubernetes append the following labels to match[] '{job=~\"knative.+\"}' '{job=\"kubernetes-cadvisor\"}' '{job=\"istio-envoy\"}' '{job=\"kube-state-metrics\"}' '{job=\"cadvisor\"}' '{job=\"cadvisor\"}' '{job=\"cadvisor\"}'","title":"Step 2"},{"location":"admin/monitoring/export-metrics/#step-3","text":"Finally restart Prometheus service with sudo systemctl restart prometheus","title":"Step 3"},{"location":"admin/monitoring/export-metrics/#custom-service","text":"Prometheus can push metrics to another custom resource via HTTP. The custom service is responsible for handling the received metrics. To push metrics with this method we use the remote_write configuration. We will only give a sample configuration as remote_write is extensively documented in Prometheus documentation In the example below we push metrics to a custom service listening on port 9096 which transforms the metrics and forwards them. remote_write : - url : \"http://localhost:9096\" queue_config : capacity : 10000 max_samples_per_send : 5000 batch_send_deadline : 60s","title":"Custom service"},{"location":"admin/monitoring/export-metrics/#conclusion","text":"In this guide we showed how you can push metrics outside of Hopsworks cluster using two methods, (a) federated Prometheus or (b) remote write to a custom service. This configuration is useful if you already have a centralized monitoring system with alerts already configured.","title":"Conclusion"},{"location":"admin/monitoring/grafana/","text":"Services Dashboards # Introduction # The Hopsworks platform is composed of different services. Hopsworks uses Prometheus to collect health and performance metrics from the different services and Grafana to display them to the Hopsworks administrators. In this guide you will learn how to access the Grafana dashboards to monitor the health of the cluster or to troubleshoot performance issues. Prerequisites # To access the services dashboards in Grafana, you need to have an administrator account on the Hopsworks cluster. Step 1: Access Grafana # You can access the admin page of your Hopsworks cluster by clicking on your name, in the top right corner, and choosing Cluster Settings from the dropdown menu. You can then navigate to the Monitoring tab. The Monitoring tab gives you access to several of the observability tools that are already deployed to help you manage the health of the cluster. Monitoring tab Click on the Grafana link to open the Grafana web application and navigate through the dashboards. Step 2: Navigate through the dashboards # In the Grafana web application, you can click on the Home button on the top left corner and navigate through the available dashboards. Dashboards are organized into three folders: Hops : This folder contains all the dashboards of the Hopsworks services (e.g. the web application, the file system, resource manager) as well as the dashboards of the hosts (e.g. EC2 instances, virtual machines, servers) on which the cluster is deployed. RonDB : This folder contains all the dashboard related to the database. The Database dashboard contains a general overview of the RonDB cluster, while the remaining dashboards focus on specific items (e.g. thread activity, memory management, etc). Kubernetes : If you have integrated Hopsworks with a Kubernetes cluster, this folder contains the dashboards to monitor the health of the Kubernetes cluster. Grafana view The default dashboards are read only and cannot be edited. Additional dashboards can be created by logging in to Grafana. You can log in into Grafana using the username and password specified in the cluster definition. Warning By default Hopsworks keeps metrics information only for the past 15 days. This means that, by default, you will not be able to access health and performance metrics which are older than 15 days. Conclusion # In this guide you learned how to access the Grafana dashboards to monitor the health and performance of the Hopsworks services. You can find additional documentation on Grafana itself at: https://grafana.com/docs/","title":"Services Dashboards"},{"location":"admin/monitoring/grafana/#services-dashboards","text":"","title":"Services Dashboards"},{"location":"admin/monitoring/grafana/#introduction","text":"The Hopsworks platform is composed of different services. Hopsworks uses Prometheus to collect health and performance metrics from the different services and Grafana to display them to the Hopsworks administrators. In this guide you will learn how to access the Grafana dashboards to monitor the health of the cluster or to troubleshoot performance issues.","title":"Introduction"},{"location":"admin/monitoring/grafana/#prerequisites","text":"To access the services dashboards in Grafana, you need to have an administrator account on the Hopsworks cluster.","title":"Prerequisites"},{"location":"admin/monitoring/grafana/#step-1-access-grafana","text":"You can access the admin page of your Hopsworks cluster by clicking on your name, in the top right corner, and choosing Cluster Settings from the dropdown menu. You can then navigate to the Monitoring tab. The Monitoring tab gives you access to several of the observability tools that are already deployed to help you manage the health of the cluster. Monitoring tab Click on the Grafana link to open the Grafana web application and navigate through the dashboards.","title":"Step 1: Access Grafana"},{"location":"admin/monitoring/grafana/#step-2-navigate-through-the-dashboards","text":"In the Grafana web application, you can click on the Home button on the top left corner and navigate through the available dashboards. Dashboards are organized into three folders: Hops : This folder contains all the dashboards of the Hopsworks services (e.g. the web application, the file system, resource manager) as well as the dashboards of the hosts (e.g. EC2 instances, virtual machines, servers) on which the cluster is deployed. RonDB : This folder contains all the dashboard related to the database. The Database dashboard contains a general overview of the RonDB cluster, while the remaining dashboards focus on specific items (e.g. thread activity, memory management, etc). Kubernetes : If you have integrated Hopsworks with a Kubernetes cluster, this folder contains the dashboards to monitor the health of the Kubernetes cluster. Grafana view The default dashboards are read only and cannot be edited. Additional dashboards can be created by logging in to Grafana. You can log in into Grafana using the username and password specified in the cluster definition. Warning By default Hopsworks keeps metrics information only for the past 15 days. This means that, by default, you will not be able to access health and performance metrics which are older than 15 days.","title":"Step 2: Navigate through the dashboards"},{"location":"admin/monitoring/grafana/#conclusion","text":"In this guide you learned how to access the Grafana dashboards to monitor the health and performance of the Hopsworks services. You can find additional documentation on Grafana itself at: https://grafana.com/docs/","title":"Conclusion"},{"location":"admin/monitoring/services-logs/","text":"Services Logs # Introduction # The Hopsworks platform is composed of different services running on different nodes. Hopsworks uses Filebeat, Logstash and OpenSearch to collect, parse, index and present the logs to the Hopsworks administrators. In this guide you will learn how to access the Hopsworks logs using OpenSearch Dashboards. Prerequisites # To access the services logs, you need to have an administrator account on the Hopsworks cluster. Step 1: Access the Logs # You can access the admin page of your Hopsworks cluster by clicking on your name, in the top right corner, and choosing Cluster Settings from the dropdown menu. You can then navigate to the Monitoring tab. The Monitoring tab gives you access to several of the observability tools that are already deployed to help you manage the health of the cluster. Monitoring tab Click on the Service Logs link to open the OpenSearch Dashboards web application and navigate through the logs. Step 2: Search the logs # In the OpenSearch dashboard web application you will see by default all the logs generated by all monitored services in the last 15 minutes. You can filter the logs of a specific service by searching for the term service:[service name] . As shown in the picture below, you can search for the namenode logs by querying service:namenode . Currently only the logs of the following services are collected and indexed: Hopsworks web application (called domain1 in the log entires), namenodes, resource managers, datanodes, nodemanagers, Kafka brokers, Hive services and RonDB. These are the core component of the platform, additional logs will be added in the future. OpenSearch Dashboards displaying the logs Warning By default, logs are rotated automatically after 7 days. This means that by default, you will not be able to access logs through OpenSearch Dashboards which are older than 7 days. Depending on the service and on the Hopsworks configuration, you can still access the logs by SSH directly into the machines of the cluster. Conclusion # In this guide you learned how to access the services logs using OpenSearch Dashboards from the Hopsworks admin page. You can find additional documentation on OpenSearch Dashboards at: https://opensearch.org/docs/latest/","title":"Services Logs"},{"location":"admin/monitoring/services-logs/#services-logs","text":"","title":"Services Logs"},{"location":"admin/monitoring/services-logs/#introduction","text":"The Hopsworks platform is composed of different services running on different nodes. Hopsworks uses Filebeat, Logstash and OpenSearch to collect, parse, index and present the logs to the Hopsworks administrators. In this guide you will learn how to access the Hopsworks logs using OpenSearch Dashboards.","title":"Introduction"},{"location":"admin/monitoring/services-logs/#prerequisites","text":"To access the services logs, you need to have an administrator account on the Hopsworks cluster.","title":"Prerequisites"},{"location":"admin/monitoring/services-logs/#step-1-access-the-logs","text":"You can access the admin page of your Hopsworks cluster by clicking on your name, in the top right corner, and choosing Cluster Settings from the dropdown menu. You can then navigate to the Monitoring tab. The Monitoring tab gives you access to several of the observability tools that are already deployed to help you manage the health of the cluster. Monitoring tab Click on the Service Logs link to open the OpenSearch Dashboards web application and navigate through the logs.","title":"Step 1: Access the Logs"},{"location":"admin/monitoring/services-logs/#step-2-search-the-logs","text":"In the OpenSearch dashboard web application you will see by default all the logs generated by all monitored services in the last 15 minutes. You can filter the logs of a specific service by searching for the term service:[service name] . As shown in the picture below, you can search for the namenode logs by querying service:namenode . Currently only the logs of the following services are collected and indexed: Hopsworks web application (called domain1 in the log entires), namenodes, resource managers, datanodes, nodemanagers, Kafka brokers, Hive services and RonDB. These are the core component of the platform, additional logs will be added in the future. OpenSearch Dashboards displaying the logs Warning By default, logs are rotated automatically after 7 days. This means that by default, you will not be able to access logs through OpenSearch Dashboards which are older than 7 days. Depending on the service and on the Hopsworks configuration, you can still access the logs by SSH directly into the machines of the cluster.","title":"Step 2: Search the logs"},{"location":"admin/monitoring/services-logs/#conclusion","text":"In this guide you learned how to access the services logs using OpenSearch Dashboards from the Hopsworks admin page. You can find additional documentation on OpenSearch Dashboards at: https://opensearch.org/docs/latest/","title":"Conclusion"},{"location":"admin/oauth2/create-azure-client/","text":"Create An Application in Azure Active Directory. # Introduction # This example uses Azure Active Directory as the identity provider, but the same can be done with any identity provider supporting OAuth2 OpenID Connect protocol. Prerequisites # Azure account. Step 1: Register Hopsworks as an application in your identity provider # To use OAuth2 in Hopsworks you first need to create and configure an OAuth client in your identity provider. We will take the example of Azure AD for the remaining of this documentation, but equivalent steps can be taken on other identity providers. Navigate to the Microsoft Azure Portal and authenticate. Navigate to Azure Active Directory . Click on App Registrations . Click on New Registration . Create application Enter a name for the client such as hopsworks_oauth_client . Verify the Supported account type is set to Accounts in this organizational directory only . And Click Register. Name application Step 2: Get the nessary fields for client registration # In the Overview section, copy the Application (client) ID field . We will use it in Identity Provider registration under the name Client id . Copy client ID Click on Endpoints and copy the OpenId Connect metadata document endpoint excluding the .well-known/openid-configuration part. We will use it in Identity Provider registration under the name Connection URL . Endpoint Click on Certificates & secrets , then Click on New client secret . New client secret Add a description of the secret. Select an expiration period. And, Click Add . Client secret creation Copy the secret. This will be used in Identity Provider registration under the name Client Secret . Client secret creation Click on Authentication . Then click on Add a platform Add a platform In Configure platforms click on Web . Configure platform: Web Enter the Redirect URI and click on Configure . The redirect URI is HOPSWORKS-URI/callback with HOPSWORKS-URI the URI of your Hopsworks cluster. Configure platform: Redirect Note If your Hopsworks cluster is created on the cloud (managed.hopsworks.ai), you can find your HOPSWORKS-URI by going to the managed.hopsworks.ai dashboard in the General tab of your cluster and copying the URI. Conclusion # In this guide you learned how to create a client in your Azure identity provider and acquire a client id and a client secret .","title":"Create Azure Client"},{"location":"admin/oauth2/create-azure-client/#create-an-application-in-azure-active-directory","text":"","title":"Create An Application in Azure Active Directory."},{"location":"admin/oauth2/create-azure-client/#introduction","text":"This example uses Azure Active Directory as the identity provider, but the same can be done with any identity provider supporting OAuth2 OpenID Connect protocol.","title":"Introduction"},{"location":"admin/oauth2/create-azure-client/#prerequisites","text":"Azure account.","title":"Prerequisites"},{"location":"admin/oauth2/create-azure-client/#step-1-register-hopsworks-as-an-application-in-your-identity-provider","text":"To use OAuth2 in Hopsworks you first need to create and configure an OAuth client in your identity provider. We will take the example of Azure AD for the remaining of this documentation, but equivalent steps can be taken on other identity providers. Navigate to the Microsoft Azure Portal and authenticate. Navigate to Azure Active Directory . Click on App Registrations . Click on New Registration . Create application Enter a name for the client such as hopsworks_oauth_client . Verify the Supported account type is set to Accounts in this organizational directory only . And Click Register. Name application","title":"Step 1: Register Hopsworks as an application in your identity provider"},{"location":"admin/oauth2/create-azure-client/#step-2-get-the-nessary-fields-for-client-registration","text":"In the Overview section, copy the Application (client) ID field . We will use it in Identity Provider registration under the name Client id . Copy client ID Click on Endpoints and copy the OpenId Connect metadata document endpoint excluding the .well-known/openid-configuration part. We will use it in Identity Provider registration under the name Connection URL . Endpoint Click on Certificates & secrets , then Click on New client secret . New client secret Add a description of the secret. Select an expiration period. And, Click Add . Client secret creation Copy the secret. This will be used in Identity Provider registration under the name Client Secret . Client secret creation Click on Authentication . Then click on Add a platform Add a platform In Configure platforms click on Web . Configure platform: Web Enter the Redirect URI and click on Configure . The redirect URI is HOPSWORKS-URI/callback with HOPSWORKS-URI the URI of your Hopsworks cluster. Configure platform: Redirect Note If your Hopsworks cluster is created on the cloud (managed.hopsworks.ai), you can find your HOPSWORKS-URI by going to the managed.hopsworks.ai dashboard in the General tab of your cluster and copying the URI.","title":"Step 2: Get the nessary fields for client registration"},{"location":"admin/oauth2/create-azure-client/#conclusion","text":"In this guide you learned how to create a client in your Azure identity provider and acquire a client id and a client secret .","title":"Conclusion"},{"location":"admin/oauth2/create-client/","text":"Register Identity Provider in Hopsworks # Introduction # Before registering your identity provider in Hopsworks you need to create a client application in your identity provider and acquire a client id and a client secret . An example on how to create a client using Okta and Azure Active Directory identity providers can be found here and here respectively. Prerequisites # Acquired a client id and a client secret from your identity provider. Step 1: Register a client # After acquiring the client id and client secret create the client in Hopsworks by enabling OAuth2 and clicking on add another identity provider in the Authentication configuration page . Then set base uri of your identity provider in Connection URL give a name to your identity provider (the name will be used in the login page as an alternative login method) and set the client id and client secret in their respective fields, as shown in the figure below. Application overview Connection URL : (provider Uri) is the base uri of the identity provider's API (URI should contain scheme http:// or https://). Additional configuration can be set here: Verify email : if checked only users with verified email address (in the identity provider) can log in to Hopsworks. Code challenge : if your identity provider requires code challenge for authorization request check the code challenge check box. This will allow you to choose code challenge method that can be either plain or S256 . Logo URL : optionally a logo URL to an image can be added. The logo will be shown on the login page with the name as shown in the figure below. Claim names for given name, family name, email and group can also be set here. If left empty the default openid claim names will be used. Step 2: Add Group mappings # Optionally you can add a group mapping from your identity provider to Hopsworks groups, by clicking on your name in the top right corner of the navigation bar and choosing Cluster Settings from the dropdown menu. In the Cluster Settings Configuration tab search for oauth_group_mapping and click on the edit button. Set Configuration variables Note Setting oauth_group_mapping to ANY_GROUP->HOPS_USER will assign the role user to any user from any group in your identity provider when they log into Hopsworks with OAuth for the first time. You can replace ANY_GROUP with the group of your choice in the identity provider. You can replace HOPS_USER by HOPS_ADMIN if you want the users of that group to be admins in Hopsworks. You can do several mappings by separating them with a semicolon. Group mapping can be disabled by setting oauth_group_mapping_enabled=false in the Configuration UI. When group mapping is disabled an administrator needs to activate each user from the User Management page. If group mapping is disabled then oauth_account_status in the Configuration UI should be set to 1 (Verified). Users will now see a new button on the login page. The button has the name you set above for Name and will redirect to your identity provider. Login with OAuth2 Note When creating a client make sure you can access the provider metadata by making a GET request on the well known endpoint of the provider. The well-known URL, will typically be the Connection URL plus .well-known/openid-configuration . For the above client it would be https://dev-86723251.okta.com/.well-known/openid-configuration . Conclusion # In this guide you learned how to register an identity provider in Hopsworks.","title":"Register an Identity Provider"},{"location":"admin/oauth2/create-client/#register-identity-provider-in-hopsworks","text":"","title":"Register Identity Provider in Hopsworks"},{"location":"admin/oauth2/create-client/#introduction","text":"Before registering your identity provider in Hopsworks you need to create a client application in your identity provider and acquire a client id and a client secret . An example on how to create a client using Okta and Azure Active Directory identity providers can be found here and here respectively.","title":"Introduction"},{"location":"admin/oauth2/create-client/#prerequisites","text":"Acquired a client id and a client secret from your identity provider.","title":"Prerequisites"},{"location":"admin/oauth2/create-client/#step-1-register-a-client","text":"After acquiring the client id and client secret create the client in Hopsworks by enabling OAuth2 and clicking on add another identity provider in the Authentication configuration page . Then set base uri of your identity provider in Connection URL give a name to your identity provider (the name will be used in the login page as an alternative login method) and set the client id and client secret in their respective fields, as shown in the figure below. Application overview Connection URL : (provider Uri) is the base uri of the identity provider's API (URI should contain scheme http:// or https://). Additional configuration can be set here: Verify email : if checked only users with verified email address (in the identity provider) can log in to Hopsworks. Code challenge : if your identity provider requires code challenge for authorization request check the code challenge check box. This will allow you to choose code challenge method that can be either plain or S256 . Logo URL : optionally a logo URL to an image can be added. The logo will be shown on the login page with the name as shown in the figure below. Claim names for given name, family name, email and group can also be set here. If left empty the default openid claim names will be used.","title":"Step 1: Register a client"},{"location":"admin/oauth2/create-client/#step-2-add-group-mappings","text":"Optionally you can add a group mapping from your identity provider to Hopsworks groups, by clicking on your name in the top right corner of the navigation bar and choosing Cluster Settings from the dropdown menu. In the Cluster Settings Configuration tab search for oauth_group_mapping and click on the edit button. Set Configuration variables Note Setting oauth_group_mapping to ANY_GROUP->HOPS_USER will assign the role user to any user from any group in your identity provider when they log into Hopsworks with OAuth for the first time. You can replace ANY_GROUP with the group of your choice in the identity provider. You can replace HOPS_USER by HOPS_ADMIN if you want the users of that group to be admins in Hopsworks. You can do several mappings by separating them with a semicolon. Group mapping can be disabled by setting oauth_group_mapping_enabled=false in the Configuration UI. When group mapping is disabled an administrator needs to activate each user from the User Management page. If group mapping is disabled then oauth_account_status in the Configuration UI should be set to 1 (Verified). Users will now see a new button on the login page. The button has the name you set above for Name and will redirect to your identity provider. Login with OAuth2 Note When creating a client make sure you can access the provider metadata by making a GET request on the well known endpoint of the provider. The well-known URL, will typically be the Connection URL plus .well-known/openid-configuration . For the above client it would be https://dev-86723251.okta.com/.well-known/openid-configuration .","title":"Step 2: Add Group mappings"},{"location":"admin/oauth2/create-client/#conclusion","text":"In this guide you learned how to register an identity provider in Hopsworks.","title":"Conclusion"},{"location":"admin/oauth2/create-okta-client/","text":"Create An Application in Okta # Introduction # This example uses an Okta development account to create an application that will represent a Hopsworks client in the identity provider. Prerequisites # Okta development account. To create a developer account go to Okta developer . Step 1: Register Hopsworks as an application in your identity provider # After creating a developer account register a client by going to Applications and click on Create App Integration . Okta Applications This will open a popup as shown in the figure below. Select OIDC as Sign-in-method and Web Application as Application type and click next. Create new Application Give your application a name and select Client credential as Grant Type . Then add a Sign-in redirect URI that is your Hopsworks cluster domain name (including the port number if needed) with path /callback , and a Sign-out redirect URI that is Hopsworks cluster domain name (including the port number if needed) with no path. New Application If you want to limit who can access your Hopsworks cluster select Limit access to selected groups and select group(s) you want to give access to. Here we will allow everyone in the organization to access the cluster. Group assignment Group mapping # You can also create mappings from groups in Okta to groups in Hopsworks. To achieve this you need to configure Okta to send Groups with user information. To do this go to Applications and select your application name. In the Sign On tab click edit OpenID Connect ID Token and select Filter for Groups claim type , then for Groups claim filter add groups as the claim name, select Match Regex from the dropdown and .* (dot star) as Regex to match all groups. See Group mapping on how to do the mapping in Hopsworks. Group claim Step 2: Get the nessary fields for client registration # After the application is created go back to Applications and click on the application you just created. Use the Okta domain ( Connection URL ), client id and client secret generated for your app in the Identity Provider registration in Hopsworks. Application overview Note When copying the domain in the figure above make sure to add the url scheme (http:// or https://) when using it in the Connection URL in the Identity Provider registration form . Conclusion # In this guide you learned how to create a client in your Okta identity provider and acquire a client id and a client secret .","title":"Create Okta Client"},{"location":"admin/oauth2/create-okta-client/#create-an-application-in-okta","text":"","title":"Create An Application in Okta"},{"location":"admin/oauth2/create-okta-client/#introduction","text":"This example uses an Okta development account to create an application that will represent a Hopsworks client in the identity provider.","title":"Introduction"},{"location":"admin/oauth2/create-okta-client/#prerequisites","text":"Okta development account. To create a developer account go to Okta developer .","title":"Prerequisites"},{"location":"admin/oauth2/create-okta-client/#step-1-register-hopsworks-as-an-application-in-your-identity-provider","text":"After creating a developer account register a client by going to Applications and click on Create App Integration . Okta Applications This will open a popup as shown in the figure below. Select OIDC as Sign-in-method and Web Application as Application type and click next. Create new Application Give your application a name and select Client credential as Grant Type . Then add a Sign-in redirect URI that is your Hopsworks cluster domain name (including the port number if needed) with path /callback , and a Sign-out redirect URI that is Hopsworks cluster domain name (including the port number if needed) with no path. New Application If you want to limit who can access your Hopsworks cluster select Limit access to selected groups and select group(s) you want to give access to. Here we will allow everyone in the organization to access the cluster. Group assignment","title":"Step 1: Register Hopsworks as an application in your identity provider"},{"location":"admin/oauth2/create-okta-client/#group-mapping","text":"You can also create mappings from groups in Okta to groups in Hopsworks. To achieve this you need to configure Okta to send Groups with user information. To do this go to Applications and select your application name. In the Sign On tab click edit OpenID Connect ID Token and select Filter for Groups claim type , then for Groups claim filter add groups as the claim name, select Match Regex from the dropdown and .* (dot star) as Regex to match all groups. See Group mapping on how to do the mapping in Hopsworks. Group claim","title":"Group mapping"},{"location":"admin/oauth2/create-okta-client/#step-2-get-the-nessary-fields-for-client-registration","text":"After the application is created go back to Applications and click on the application you just created. Use the Okta domain ( Connection URL ), client id and client secret generated for your app in the Identity Provider registration in Hopsworks. Application overview Note When copying the domain in the figure above make sure to add the url scheme (http:// or https://) when using it in the Connection URL in the Identity Provider registration form .","title":"Step 2: Get the nessary fields for client registration"},{"location":"admin/oauth2/create-okta-client/#conclusion","text":"In this guide you learned how to create a client in your Okta identity provider and acquire a client id and a client secret .","title":"Conclusion"},{"location":"concepts/hopsworks/","text":"Hopsworks is a modular MLOps platform with: a feature store (available as standalone) model registry and model serving based on KServe vector database based on OpenSearch a data science and data engineering platform Standalone Feature Store # Hopsworks was the first open-source and first enterprise feature store for ML. You can use Hopsworks as a standalone feature store with the HSFS API. Model Management # Hopsworks includes support for model management, with model deployments using the KServe framework and a model registry designed for KServe. Hopsworks logs all inference requests to Kafka to enable easy monitoring of deployed models, and provides model metrics with grafana/prometheus. Vector DB # Hopsworks provides a vector database (or embedding store) based on OpenSearch kNN ( FAISS and nmslib ). Hopsworks Vector DB includes out-of-the-box support for authentication, access control, filtering, backup-and-restore, and horizontal scalability. Hopsworks' Feature Store and vector DB are often used together to build scalable recommender systems, such as ranking-and-retrieval for real-time recommendations. Governance # Hopsworks provides a data-mesh architecture for managing ML assets and teams, with multi-tenant projects. Not unlike a GitHub repository, a project is a sandbox containing team members, data, and ML assets. In Hopsworks, all ML assets (features, models, training data) are versioned, taggable, lineage-tracked, and support free-text search. Data can be also be securely shared between projects. Data Science Platform # You can develop feature engineering pipelines and training pipelines in Hopsworks. There is support for version control (GitHub, GitLab, BitBucket), Jupyter notebooks, a shared distributed file system, per project conda environments for managing python dependencies without needing to write Dockerfiles, jobs (Python, Spark, Flink), and workflow orchestration with Airflow.","title":"Hopsworks Platform"},{"location":"concepts/hopsworks/#standalone-feature-store","text":"Hopsworks was the first open-source and first enterprise feature store for ML. You can use Hopsworks as a standalone feature store with the HSFS API.","title":"Standalone Feature Store"},{"location":"concepts/hopsworks/#model-management","text":"Hopsworks includes support for model management, with model deployments using the KServe framework and a model registry designed for KServe. Hopsworks logs all inference requests to Kafka to enable easy monitoring of deployed models, and provides model metrics with grafana/prometheus.","title":"Model Management"},{"location":"concepts/hopsworks/#vector-db","text":"Hopsworks provides a vector database (or embedding store) based on OpenSearch kNN ( FAISS and nmslib ). Hopsworks Vector DB includes out-of-the-box support for authentication, access control, filtering, backup-and-restore, and horizontal scalability. Hopsworks' Feature Store and vector DB are often used together to build scalable recommender systems, such as ranking-and-retrieval for real-time recommendations.","title":"Vector DB"},{"location":"concepts/hopsworks/#governance","text":"Hopsworks provides a data-mesh architecture for managing ML assets and teams, with multi-tenant projects. Not unlike a GitHub repository, a project is a sandbox containing team members, data, and ML assets. In Hopsworks, all ML assets (features, models, training data) are versioned, taggable, lineage-tracked, and support free-text search. Data can be also be securely shared between projects.","title":"Governance"},{"location":"concepts/hopsworks/#data-science-platform","text":"You can develop feature engineering pipelines and training pipelines in Hopsworks. There is support for version control (GitHub, GitLab, BitBucket), Jupyter notebooks, a shared distributed file system, per project conda environments for managing python dependencies without needing to write Dockerfiles, jobs (Python, Spark, Flink), and workflow orchestration with Airflow.","title":"Data Science Platform"},{"location":"concepts/dev/inside/","text":"Hopsworks provides a complete self-service development environment for feature engineering and model training. You can develop programs as Jupyter notebooks or jobs, you can manage the Python libraries in a project using its conda environment, you can manage your source code with Git, and you can orchestrate jobs with Airflow. Jupyter Notebooks # Hopsworks provides a Jupyter notebook development environment for programs written in Python, Spark, Flink, and SparkSQL. You can also develop in your IDE (PyCharm, IntelliJ, etc), test locally, and then run your programs as Jobs in Hopsworks. Jupyter notebooks can also be run as Jobs. Source Code Control # Hopsworks provides source code control support using Git (GitHub, GitLab or BitBucket). You can securely checkout code into your project and commit and push updates to your code to your source code repository. Conda Environment per Project # Hopsworks supports the self-service installation of Python libraries using PyPi, Conda, Wheel files, or GitHub URLs. The Python libraries are installed in a Conda environment linked with your project. Each project has a base Docker image and its custom conda environment. Jobs are run as Docker images, but they are compiled transparently for you when you update your Conda environment. That is, there is no need to write a Dockerfile, users install Python libraries in their project. You can setup custom development and production environments by creating new projects, each with their own conda environment. Jobs # In Hopsworks, a Job is a schedulable program that is allocated compute and memory resources. You can run a Job in Hopsworks: from the UI; programmatically with the Hopsworks SDK (Python, Java) or REST API; from Airflow programs (either inside our outside Hopsworks); from your IDE using a plugin ( PyCharm/IntelliJ plugin ); Orchestration # Airflow comes out-of-the box with Hopsworks, but you can also use an external Airflow cluster (with the Hopsworks Job operator) if you have one. Airflow can be used to schedule the execution of Jobs, individually or as part of Airflow DAGs.","title":"Inside Hopsworks"},{"location":"concepts/dev/inside/#jupyter-notebooks","text":"Hopsworks provides a Jupyter notebook development environment for programs written in Python, Spark, Flink, and SparkSQL. You can also develop in your IDE (PyCharm, IntelliJ, etc), test locally, and then run your programs as Jobs in Hopsworks. Jupyter notebooks can also be run as Jobs.","title":"Jupyter Notebooks"},{"location":"concepts/dev/inside/#source-code-control","text":"Hopsworks provides source code control support using Git (GitHub, GitLab or BitBucket). You can securely checkout code into your project and commit and push updates to your code to your source code repository.","title":"Source Code Control"},{"location":"concepts/dev/inside/#conda-environment-per-project","text":"Hopsworks supports the self-service installation of Python libraries using PyPi, Conda, Wheel files, or GitHub URLs. The Python libraries are installed in a Conda environment linked with your project. Each project has a base Docker image and its custom conda environment. Jobs are run as Docker images, but they are compiled transparently for you when you update your Conda environment. That is, there is no need to write a Dockerfile, users install Python libraries in their project. You can setup custom development and production environments by creating new projects, each with their own conda environment.","title":"Conda Environment per Project"},{"location":"concepts/dev/inside/#jobs","text":"In Hopsworks, a Job is a schedulable program that is allocated compute and memory resources. You can run a Job in Hopsworks: from the UI; programmatically with the Hopsworks SDK (Python, Java) or REST API; from Airflow programs (either inside our outside Hopsworks); from your IDE using a plugin ( PyCharm/IntelliJ plugin );","title":"Jobs"},{"location":"concepts/dev/inside/#orchestration","text":"Airflow comes out-of-the box with Hopsworks, but you can also use an external Airflow cluster (with the Hopsworks Job operator) if you have one. Airflow can be used to schedule the execution of Jobs, individually or as part of Airflow DAGs.","title":"Orchestration"},{"location":"concepts/dev/outside/","text":"You can write programs that use Hopsworks in any Python, Spark, PySpark, or Flink environment . Hopsworks also running SQL queries to compute features in external data warehouses. The Feature Store can also be queried with SQL. There is REST API for Hopsworks that can be used with a valid API key, generated in Hopsworks. However, it is often easier to develop your programs against SDKs available in Python and Java/Scala for HSFS, in Python for HSML, and in Python for the Hopsworks API.","title":"Outside Hopsworks"},{"location":"concepts/fs/","text":"What is Hopsworks Feature Store? # Hopsworks and its Feature Store are an open source data-intensive AI platform used for the development and operation of machine learning models at scale. The Hopsworks Feature Store provides the HSFS API to enable clients to write features to feature groups in the feature store, and to read features from feature views - either through a low latency Online API to retrieve pre-computed features for operational models or through a high throughput, latency insensitive Offline API, used to create training data and to retrieve batch data for scoring. HSFS API # The HSFS (HopsworkS Feature Store) API is how you, as a developer, will use the feature store. The HSFS API helps simplify some of the problems that feature stores address including: consistent features for training and serving centralized, secure access to features point-in-time JOINs of features to create training data with no data leakage easier connection and backfilling of features from external data sources use of external tables as features transparent computation of statistics and usage data for features. Write to feature groups, read from feature views. # You write to feature groups with a feature pipeline program. The program can be written in Python, Spark, Flink, or SQL. You read from views on top of the feature groups, called feature views. That is, a feature view does not store feature data, but is a logical grouping of features. Typically, you define a feature view because you want to train/deploy a model with exactly those features in the feature view. Feature views enable the reuse of feature data from different feature groups across different models.","title":"Architecture"},{"location":"concepts/fs/#what-is-hopsworks-feature-store","text":"Hopsworks and its Feature Store are an open source data-intensive AI platform used for the development and operation of machine learning models at scale. The Hopsworks Feature Store provides the HSFS API to enable clients to write features to feature groups in the feature store, and to read features from feature views - either through a low latency Online API to retrieve pre-computed features for operational models or through a high throughput, latency insensitive Offline API, used to create training data and to retrieve batch data for scoring.","title":"What is Hopsworks Feature Store?"},{"location":"concepts/fs/#hsfs-api","text":"The HSFS (HopsworkS Feature Store) API is how you, as a developer, will use the feature store. The HSFS API helps simplify some of the problems that feature stores address including: consistent features for training and serving centralized, secure access to features point-in-time JOINs of features to create training data with no data leakage easier connection and backfilling of features from external data sources use of external tables as features transparent computation of statistics and usage data for features.","title":"HSFS API"},{"location":"concepts/fs/#write-to-feature-groups-read-from-feature-views","text":"You write to feature groups with a feature pipeline program. The program can be written in Python, Spark, Flink, or SQL. You read from views on top of the feature groups, called feature views. That is, a feature view does not store feature data, but is a logical grouping of features. Typically, you define a feature view because you want to train/deploy a model with exactly those features in the feature view. Feature views enable the reuse of feature data from different feature groups across different models.","title":"Write to feature groups, read from feature views."},{"location":"concepts/fs/feature_group/external_fg/","text":"External feature groups are offline feature groups where their data is stored in an external table. An external table requires a storage connector, defined with the Connector API (or more typically in the user interface), to enable HSFS to retrieve data from the external table. An external table includes a user-defined SQL string for retrieving data, but you also perform SQL operations, including projections, aggregations, and so on. The SQL query is executed on-demand when HSFS retrieves data from the external Feature Group, for example, when creating training data using features in the external table. In the image below, we can see that HSFS currently supports a large number of data sources, including any JDBC-enabled source, Snowflake, Data Lake, Redshift, BigQuery, S3, ADLS, GCS, and Kafka","title":"External Feature Groups"},{"location":"concepts/fs/feature_group/feature_pipelines/","text":"A feature pipeline is a program that orchestrates the execution of a dataflow graph of data validation, aggregation, dimensionality reduction, transformation, and other feature engineering steps on input data to create and/or update feature data. With HSFS, you can write feature pipelines in different languages as shown in the figure below. Data Sources # Your feature pipeline needs to connect to some (external) data source to read the data to be processed. Python, Spark, and Flink have connectors to a huge number of different data sources, while SQL feature pipelines are often restricted to a single data source (for example, your connector to SnowFlake only runs SQL on SnowFlake). SparkSQL, in contrast, can be used over tables that originate in different data sources. Data Validation # In order to be able to train and serve models that you can rely on, you need clean, high quality features. Data validation operations include removing bad data, removing or imputing missing values, and identifying problems such as feature shift. HSFS supports Great Expectations to specify data validation rules that are executed in the client before features are written to the Feature Store. The validation results are collected and shown in Hopsworks. Aggregations # Aggregations are used to summarize large datasets into more concise, signal-rich features. Popular aggregations include count(), sum(), mean(), median(), stddev(), min(), and max(). These aggregations produce a single number (a numerical feature) that captures information about a potentially large dataset. Both numerical and categorical features are often transformed before being used to train or serve models. Dimensionality Reduction # If input data is impractically large or if it has a significant amount of redundancy, it can often be transformed into a reduced set of features with dimensionality reduction (often called feature extraction). Popular dimensionality algorithms include embedding algorithms, PCA, and TSNE. Transformations # Transformations are covered in more detail in training/inference pipelines , as transformations typically happen after the feature store. If you store transformed features in feature groups, the feature data is no longer useful for EDA (as it near to impossible for Data Scientists to understand the transformed values). It also makes it impossible for inference pipelines to log untransformed feature values and predictions for an operational model. There is one use case for storing transformed features in feature groups - when you need to have ultra low latency when reading precomputed features (and online transformations when reading features add too much latency for your use case). The figure below shows to include transformations in your feature pipelines. Feature Engineering in Python # Python is the most widely used framework for feature engineering due to its extensive library support for aggregations (Pandas), data validation (Great Expectations), and dimensionality reduction (embeddings, PCA), and transformations (in Scikit-Learn, TensorFlow, PyTorch). Python also supports open-source feature engineering frameworks used for automated feature engineering, such as featuretools that supports relational and temporal sources. Feature Engineering in Spark/PySpark # Spark is popular as a feature engineering framework as it can scale to process larger volumes of data than Python, and provides native support for aggregations, and it supports many of the same data validation (Great Expectations), and dimensionality reduction algorithms (embeddings, PCA) as Python. Spark also has native support for transformations, which are useful for analytical models (batch scoring), but less useful for operational models, where online transformations are required, and Spark environments are less common. Online model serving environments typically only support online transformations in Python. Feature Engineering in SQL # SQL has grown in popularity for performing heavy lifting in feature pipelines - computing aggregates on data - when the input data already resides in a data warehouse. Data warehouses also support data validation, for example, through Great Expectations in DBT. However, SQL is not mature as a platform for transformations and dimensionality reductions, where UDFs are applied row-wise. You can do aggregation in SQL for data in your data warehouse or database. Feature Engineering in Flink # Apache Flink is a powerful and flexible framework for stateful feature computation operations over unbounded and bounded data streams. It is used for feature engineering when you need very fresh features computed in real-time. Flink provides a rich set of operators and functions such as time windows and aggregation operations that can be applied to keyed and/or global window streams. Flink\u2019s stateful operations allow users to maintain and update state across multiple data records or events, which is particularly useful for feature engineering tasks such as sessionization and/or maintaining rolling aggregates over a sliding window of data. Flink feature engineering pipelines are supported in Java/Scala only. Feature Engineering in Beam # Beam feature engineering pipelines are supported in Java/Scala only.","title":"Feature Pipelines"},{"location":"concepts/fs/feature_group/feature_pipelines/#data-sources","text":"Your feature pipeline needs to connect to some (external) data source to read the data to be processed. Python, Spark, and Flink have connectors to a huge number of different data sources, while SQL feature pipelines are often restricted to a single data source (for example, your connector to SnowFlake only runs SQL on SnowFlake). SparkSQL, in contrast, can be used over tables that originate in different data sources.","title":"Data Sources"},{"location":"concepts/fs/feature_group/feature_pipelines/#data-validation","text":"In order to be able to train and serve models that you can rely on, you need clean, high quality features. Data validation operations include removing bad data, removing or imputing missing values, and identifying problems such as feature shift. HSFS supports Great Expectations to specify data validation rules that are executed in the client before features are written to the Feature Store. The validation results are collected and shown in Hopsworks.","title":"Data Validation"},{"location":"concepts/fs/feature_group/feature_pipelines/#aggregations","text":"Aggregations are used to summarize large datasets into more concise, signal-rich features. Popular aggregations include count(), sum(), mean(), median(), stddev(), min(), and max(). These aggregations produce a single number (a numerical feature) that captures information about a potentially large dataset. Both numerical and categorical features are often transformed before being used to train or serve models.","title":"Aggregations"},{"location":"concepts/fs/feature_group/feature_pipelines/#dimensionality-reduction","text":"If input data is impractically large or if it has a significant amount of redundancy, it can often be transformed into a reduced set of features with dimensionality reduction (often called feature extraction). Popular dimensionality algorithms include embedding algorithms, PCA, and TSNE.","title":"Dimensionality Reduction"},{"location":"concepts/fs/feature_group/feature_pipelines/#transformations","text":"Transformations are covered in more detail in training/inference pipelines , as transformations typically happen after the feature store. If you store transformed features in feature groups, the feature data is no longer useful for EDA (as it near to impossible for Data Scientists to understand the transformed values). It also makes it impossible for inference pipelines to log untransformed feature values and predictions for an operational model. There is one use case for storing transformed features in feature groups - when you need to have ultra low latency when reading precomputed features (and online transformations when reading features add too much latency for your use case). The figure below shows to include transformations in your feature pipelines.","title":"Transformations"},{"location":"concepts/fs/feature_group/feature_pipelines/#feature-engineering-in-python","text":"Python is the most widely used framework for feature engineering due to its extensive library support for aggregations (Pandas), data validation (Great Expectations), and dimensionality reduction (embeddings, PCA), and transformations (in Scikit-Learn, TensorFlow, PyTorch). Python also supports open-source feature engineering frameworks used for automated feature engineering, such as featuretools that supports relational and temporal sources.","title":"Feature Engineering in Python"},{"location":"concepts/fs/feature_group/feature_pipelines/#feature-engineering-in-sparkpyspark","text":"Spark is popular as a feature engineering framework as it can scale to process larger volumes of data than Python, and provides native support for aggregations, and it supports many of the same data validation (Great Expectations), and dimensionality reduction algorithms (embeddings, PCA) as Python. Spark also has native support for transformations, which are useful for analytical models (batch scoring), but less useful for operational models, where online transformations are required, and Spark environments are less common. Online model serving environments typically only support online transformations in Python.","title":"Feature Engineering in Spark/PySpark"},{"location":"concepts/fs/feature_group/feature_pipelines/#feature-engineering-in-sql","text":"SQL has grown in popularity for performing heavy lifting in feature pipelines - computing aggregates on data - when the input data already resides in a data warehouse. Data warehouses also support data validation, for example, through Great Expectations in DBT. However, SQL is not mature as a platform for transformations and dimensionality reductions, where UDFs are applied row-wise. You can do aggregation in SQL for data in your data warehouse or database.","title":"Feature Engineering in SQL"},{"location":"concepts/fs/feature_group/feature_pipelines/#feature-engineering-in-flink","text":"Apache Flink is a powerful and flexible framework for stateful feature computation operations over unbounded and bounded data streams. It is used for feature engineering when you need very fresh features computed in real-time. Flink provides a rich set of operators and functions such as time windows and aggregation operations that can be applied to keyed and/or global window streams. Flink\u2019s stateful operations allow users to maintain and update state across multiple data records or events, which is particularly useful for feature engineering tasks such as sessionization and/or maintaining rolling aggregates over a sliding window of data. Flink feature engineering pipelines are supported in Java/Scala only.","title":"Feature Engineering in Flink"},{"location":"concepts/fs/feature_group/feature_pipelines/#feature-engineering-in-beam","text":"Beam feature engineering pipelines are supported in Java/Scala only.","title":"Feature Engineering in Beam"},{"location":"concepts/fs/feature_group/fg_overview/","text":"As a programmer, you can consider a feature, in machine learning, to be a variable associated with some entity that contains a value that is useful for helping train a model to solve a prediction problem. That is, the feature is just a variable with predictive power for a machine learning problem, or task. A feature group is a table of features, where each feature group has a primary key, and optionally an event_time column (indicating when the features in that row were observed), and a partition key. Collectively, they are referred to as columns. The partition key determines how to layout the feature group rows on disk such that you can efficiently query the data using queries with the partition key. For example, if your partition key is the day and you have hundreds of days worth of data, with a partition key, you can query the day for only a given day or a range of days, and only the data for those days will be read from disk. Online and offline Storage # Feature groups can be stored in a low-latency \"online\" database and/or in low cost, high throughput \"offline\" storage, typically a data lake or data warehouse. The online store stores only the latest values of features for a feature group. It is used to serve pre-computed features to models at runtime. The offline store stores the historical values of features for a feature group, so it may store many times more data than the online store. Offline feature groups are used, typically, to create training data for models, but also to retrieve data for batch scoring of models:","title":"Overview"},{"location":"concepts/fs/feature_group/fg_overview/#online-and-offline-storage","text":"Feature groups can be stored in a low-latency \"online\" database and/or in low cost, high throughput \"offline\" storage, typically a data lake or data warehouse. The online store stores only the latest values of features for a feature group. It is used to serve pre-computed features to models at runtime. The offline store stores the historical values of features for a feature group, so it may store many times more data than the online store. Offline feature groups are used, typically, to create training data for models, but also to retrieve data for batch scoring of models:","title":"Online and offline Storage"},{"location":"concepts/fs/feature_group/fg_statistics/","text":"HSFS supports monitoring, validation, and alerting for features: transparently compute statistics over features on writing to a feature group; validation of data written to feature groups using Great Expectations alerting users when there was a problem writing or update features. Statistics # When you create a Feature Group in HSFS, you can configure it to compute statistics over the features inserted into the fFeature Group by setting the statistics_config dict parameter, see Feature Group Statistics for details. Every time you write to the Feature Group, new statistics will be computed over all of the data in the Feature Group. Data Validation # You can define expectation suites in Great Expectations and associate them with feature groups. When you write to a feature group, the expectations are executed, then you can define a policy on the feature group for what to do if any expectation fails. Alerting # HSFS also supports alerts, that can be triggered when there are problems in your feature pipelines, for example, when a write fails due to an error or a failed expectation. You can send alerts to different alerting endpoints, such as email or Slack, that can be configured in the Hopsworks UI. For example, you can send a slack message if features being written to a feature group are missing some input data.","title":"Data Validation/Stats/Alerts"},{"location":"concepts/fs/feature_group/fg_statistics/#statistics","text":"When you create a Feature Group in HSFS, you can configure it to compute statistics over the features inserted into the fFeature Group by setting the statistics_config dict parameter, see Feature Group Statistics for details. Every time you write to the Feature Group, new statistics will be computed over all of the data in the Feature Group.","title":"Statistics"},{"location":"concepts/fs/feature_group/fg_statistics/#data-validation","text":"You can define expectation suites in Great Expectations and associate them with feature groups. When you write to a feature group, the expectations are executed, then you can define a policy on the feature group for what to do if any expectation fails.","title":"Data Validation"},{"location":"concepts/fs/feature_group/fg_statistics/#alerting","text":"HSFS also supports alerts, that can be triggered when there are problems in your feature pipelines, for example, when a write fails due to an error or a failed expectation. You can send alerts to different alerting endpoints, such as email or Slack, that can be configured in the Hopsworks UI. For example, you can send a slack message if features being written to a feature group are missing some input data.","title":"Alerting"},{"location":"concepts/fs/feature_group/spine_group/","text":"Spine Group # It is possible to maintain labels or prediction events among the regular features in a regular feature group with a feature pipeline updating the labels at a specific cadence. Often times, however, it is more convenient to provide the training events or entities in a Dataframe when reading feature data from the feature store through a feature view. We call such a Dataframe a Spine as it is the structure around which the training data or batch data is built. In order to retrieve the correct feature values for the entities in the Dataframe, using a point-in-time correct join, some additional metadata apart from the Dataframe schema is necessary. Namely, the information about which columns define the primary key , and which column indicates the event time at which the label was valid. The spine Dataframe together with this additional metadata is what we call a Spine Group . For example, in the following spine, we want to retrieve the features for the three locations, no later than the event time of each of the rainfall measurements, which is our prediction target: location_id event_time rainfall (label) 1 2022-06-01 13:11 44 2 2022-06-01 09:14 5 3 2022-06-01 06:36 2 A Spine Group does not materialize any data to the feature store itself, and always needs to be provided when retrieving features from the offline API . You can think of it as a place holder or a temporary feature group, to be replaced by a Dataframe in point-in-time joins. When using the online API , it is not necessary to provide the spine, since the online feature store contains only the latest feature values, and therefore no point in time join is required, the label is not required, as the inference pipeline is going to compute the prediction and the primary key values are specified when calling the online API.","title":"Spine Group"},{"location":"concepts/fs/feature_group/spine_group/#spine-group","text":"It is possible to maintain labels or prediction events among the regular features in a regular feature group with a feature pipeline updating the labels at a specific cadence. Often times, however, it is more convenient to provide the training events or entities in a Dataframe when reading feature data from the feature store through a feature view. We call such a Dataframe a Spine as it is the structure around which the training data or batch data is built. In order to retrieve the correct feature values for the entities in the Dataframe, using a point-in-time correct join, some additional metadata apart from the Dataframe schema is necessary. Namely, the information about which columns define the primary key , and which column indicates the event time at which the label was valid. The spine Dataframe together with this additional metadata is what we call a Spine Group . For example, in the following spine, we want to retrieve the features for the three locations, no later than the event time of each of the rainfall measurements, which is our prediction target: location_id event_time rainfall (label) 1 2022-06-01 13:11 44 2 2022-06-01 09:14 5 3 2022-06-01 06:36 2 A Spine Group does not materialize any data to the feature store itself, and always needs to be provided when retrieving features from the offline API . You can think of it as a place holder or a temporary feature group, to be replaced by a Dataframe in point-in-time joins. When using the online API , it is not necessary to provide the spine, since the online feature store contains only the latest feature values, and therefore no point in time join is required, the label is not required, as the inference pipeline is going to compute the prediction and the primary key values are specified when calling the online API.","title":"Spine Group"},{"location":"concepts/fs/feature_group/versioning/","text":"See here for information about version of feature views . Schema Versioning # The schema of feature groups is versioned. If you make a breaking change to the schema of a feature group, you need to increment the version of the feature group, and then backfill the new feature group. A breaking schema change is when you: drop a column from the schema add a new feature without any default value for the new feature change how a feature is computed, such that, for training models, the data for the old feature is not compatible with the data for the new feature. For example, if you have an embedding as a feature and change the algorithm to compute that embedding, you probably should not mix feature values computed with the old embedding model with feature values computed with the new embedding model. Data Versioning for Feature Groups # Data Versioning of a feature group involves tracking updates to the feature group, so that you can recover the state of the feature group at a given point-in-time in the past.","title":"Versioning"},{"location":"concepts/fs/feature_group/versioning/#schema-versioning","text":"The schema of feature groups is versioned. If you make a breaking change to the schema of a feature group, you need to increment the version of the feature group, and then backfill the new feature group. A breaking schema change is when you: drop a column from the schema add a new feature without any default value for the new feature change how a feature is computed, such that, for training models, the data for the old feature is not compatible with the data for the new feature. For example, if you have an embedding as a feature and change the algorithm to compute that embedding, you probably should not mix feature values computed with the old embedding model with feature values computed with the new embedding model.","title":"Schema Versioning"},{"location":"concepts/fs/feature_group/versioning/#data-versioning-for-feature-groups","text":"Data Versioning of a feature group involves tracking updates to the feature group, so that you can recover the state of the feature group at a given point-in-time in the past.","title":"Data Versioning for Feature Groups"},{"location":"concepts/fs/feature_group/write_apis/","text":"You write to feature groups, and read from feature views. There are 3 APIs for writing to feature groups, as shown in the table below: Stream API Batch API Connector API Python X - - Spark X X - Flink X - - External Table - - X Stream API # The Stream API is the only API for Python and Flink clients, and is the preferred API for Spark, as it ensures consistent features between offline and online feature stores. The Stream API first writes data to be ingested to a Kafka topic, and then Hopsworks ensures that the data is synchronized to the Online and Offline Feature Groups through the OnlineFS service and Hudi DeltaStreamer jobs, respectively. The data in the feature groups is guaranteed to arrive at-most-once, through idempotent writes to the online feature group (only the latest values of features are stored there, and duplicates in Kafka only cause idempotent updates) and duplicate removal by Apache Hudi for the offline feature group. Batch API # For very large updates to feature groups, such as when you are backfilling large amounts of data to an offline feature group, it is often preferential to write directly to the Hudi tables in Hopsworks, instead of via Kafka - thus reducing write amplification. Spark clients can write directly to Hudi tables on Hopsworks with Hopsworks libraries and certificates using a HDFS API. This requires network connectivity between the Spark clients and the datanodes in Hopsworks. Connector API # Hopsworks supports external tables as feature groups. You can mount a table from an external database as an offline feature group using the Connector API - you create an external table using the connector. This enables you to use features from your external data source (Snowflake, Redshift, Delta Lake, etc) as you would any feature in an offline feature group in Hopsworks. You can, for example, join features from different feature groups (external or not) together to create feature views and training data for models.","title":"Write APIs"},{"location":"concepts/fs/feature_group/write_apis/#stream-api","text":"The Stream API is the only API for Python and Flink clients, and is the preferred API for Spark, as it ensures consistent features between offline and online feature stores. The Stream API first writes data to be ingested to a Kafka topic, and then Hopsworks ensures that the data is synchronized to the Online and Offline Feature Groups through the OnlineFS service and Hudi DeltaStreamer jobs, respectively. The data in the feature groups is guaranteed to arrive at-most-once, through idempotent writes to the online feature group (only the latest values of features are stored there, and duplicates in Kafka only cause idempotent updates) and duplicate removal by Apache Hudi for the offline feature group.","title":"Stream API"},{"location":"concepts/fs/feature_group/write_apis/#batch-api","text":"For very large updates to feature groups, such as when you are backfilling large amounts of data to an offline feature group, it is often preferential to write directly to the Hudi tables in Hopsworks, instead of via Kafka - thus reducing write amplification. Spark clients can write directly to Hudi tables on Hopsworks with Hopsworks libraries and certificates using a HDFS API. This requires network connectivity between the Spark clients and the datanodes in Hopsworks.","title":"Batch API"},{"location":"concepts/fs/feature_group/write_apis/#connector-api","text":"Hopsworks supports external tables as feature groups. You can mount a table from an external database as an offline feature group using the Connector API - you create an external table using the connector. This enables you to use features from your external data source (Snowflake, Redshift, Delta Lake, etc) as you would any feature in an offline feature group in Hopsworks. You can, for example, join features from different feature groups (external or not) together to create feature views and training data for models.","title":"Connector API"},{"location":"concepts/fs/feature_view/fv_overview/","text":"A feature view is a logical view over (or interface to) a set of features that may come from different feature groups. You create a feature view by joining together features from existing feature groups. In the illustration below, we can see that features are joined together from the two feature groups: seller_delivery_time_monthly and the seller_reviews_quarterly. You can also see that features in the feature view inherit not only the feature type from their feature groups, but also whether they are the primary key and/or the event_time. The image also includes transformation functions that are applied to individual features. Transformation functions are a part of the feature types included in the feature view. That is, a feature in a feature view is not only defined by its data type (int, string, etc) or its feature type (categorical, numerical, embedding), but also by its transformation. Feature views can also include: the label for the supervised ML problem transformation functions that should be applied to specified features consistently between training and serving the ability to create training data the ability to retrieve a feature vector with the most recent feature values In the flow chart below, we can see the decisions that can be taken when creating (1) a feature view, and (2) creating training data with the feature view. We can see here how the feature view is a representation for a model in the feature store - the same feature view is used to retrieve feature vectors for operational model that was created with training data from this feature view. As such, you can see that the most common use case for creating a feature view is to define the features that will be used in a model. In this way, feature views enable features from different feature groups to be reused across different models, and if features are stored untransformed in feature groups, they become even more reusable, as different feature views can apply different transformations to the same feature.","title":"Overview"},{"location":"concepts/fs/feature_view/offline_api/","text":"The feature view provides an Offline API for creating training data creating batch (scoring) data Training Data # Training data is created using a feature view. You can create training data as either: in-memory Pandas DataFrames, useful when you have a small amount of training data; materialized training data in files, in a file format of your choice (such as .tfrecord, .csv, or .parquet). You can apply filters when creating training data from a feature view: start-time and end-time, for example, to create the train-set from an earlier time range, and the test-set from a later (unseen) time range; feature value features, for example, only train a model on customers from a particular country. Note that filters are not applied when retrieving feature vectors using feature views, as we only look up features for a specific entity, like a customer. In this case, the application should know that predictions for this customer should be made on the model trained on customers in USA, for example. Point-in-time Correct Training Data # When you create training data from features in different feature groups, it is possible that the feature groups are updated at different cadences. For example, maybe one feature group is updated hourly, while another feature group is updated daily. It is very complex to write code that joins features together from such feature groups and ensures there is no data leakage in the resultant training data. HSFS hides this complexity by performing the point-in-time JOIN transparently, similar to the illustration below: HSFS uses the event_time columns on both feature groups to determine the most recent (but not newer) feature values that are joined together with the feature values from the feature group containing the label. That is, the features in the feature group containing the label are the observation times for the features in the resulting training data, and we want feature values from the other feature groups that have the most recent timestamps, but not newer than the timestamp in the label-containing feature group. Spine Dataframes # The left side of the point-in-time join is typically the set of training entities/primary key values for which the relevant features need to be retrieved. This left side of the join can also be replaced by a spine group . When using feature groups also so save labels/prediction targets, it can happen that you end up with the same entity multiple times in the training dataset depending on the cadence at which the label group was updated and the length of the event time interval that is being used to generate the training dataset. This can lead to bias in the training dataset and should be avoided. To avoid this kind of situation, users can either narrow down the event time interval during training dataset creation or use a spine in order to precisely define the entities to be included in the training dataset. This is just one example where spines are helpful. Splitting Training Data # You can create random train/validation/test splits of your training data using the HSFS API. You can also time-based splits with the HSFS API. Evaluation Sets # Test data can also be split into evaluation sets to help evaluate a model for potential bias. First, you have to identify the classes of samples that could be at risk of bias, and generate evaluation sets from your unseen test set - one evaluation set for each group of samples at risk of bias. For example, if you have a feature group of users, where one of the features is gender, and you want to evaluate the risk of bias due to gender, you can use filters to generate 3 evaluation sets from your test set - one for male, female, and non-binary. Then you score your model against all 3 evaluation sets to ensure that the prediction performance is comparable and non-biased across all 3 gender. Batch (Scoring) Data # Batch data for scoring models is created using a feature view. Similar to training data, you can create batch data as either: in-memory Pandas DataFrames, useful when you have a small amount of data to score; materialized data in files, in a file format of your choice (such as .tfrecord, .csv, or .parquet) Batch data requires specification of a start_time for the start of the batch scoring data. You can also specify the end_time (default is the current date). Spine Dataframes # Similar to training dataset generation, it might be helpful to specify a spine when retrieving features for batch inference. The only difference in this case is that the spine dataframe doesn't need to contain the label, as this will be the output of the inference pipeline. A typical use case is the handling of opt-ins, where certain customers have to be excluded from an inference pipeline due to a missing marketing opt-in.","title":"Offline API"},{"location":"concepts/fs/feature_view/offline_api/#training-data","text":"Training data is created using a feature view. You can create training data as either: in-memory Pandas DataFrames, useful when you have a small amount of training data; materialized training data in files, in a file format of your choice (such as .tfrecord, .csv, or .parquet). You can apply filters when creating training data from a feature view: start-time and end-time, for example, to create the train-set from an earlier time range, and the test-set from a later (unseen) time range; feature value features, for example, only train a model on customers from a particular country. Note that filters are not applied when retrieving feature vectors using feature views, as we only look up features for a specific entity, like a customer. In this case, the application should know that predictions for this customer should be made on the model trained on customers in USA, for example.","title":"Training Data"},{"location":"concepts/fs/feature_view/offline_api/#point-in-time-correct-training-data","text":"When you create training data from features in different feature groups, it is possible that the feature groups are updated at different cadences. For example, maybe one feature group is updated hourly, while another feature group is updated daily. It is very complex to write code that joins features together from such feature groups and ensures there is no data leakage in the resultant training data. HSFS hides this complexity by performing the point-in-time JOIN transparently, similar to the illustration below: HSFS uses the event_time columns on both feature groups to determine the most recent (but not newer) feature values that are joined together with the feature values from the feature group containing the label. That is, the features in the feature group containing the label are the observation times for the features in the resulting training data, and we want feature values from the other feature groups that have the most recent timestamps, but not newer than the timestamp in the label-containing feature group.","title":"Point-in-time Correct Training Data"},{"location":"concepts/fs/feature_view/offline_api/#spine-dataframes","text":"The left side of the point-in-time join is typically the set of training entities/primary key values for which the relevant features need to be retrieved. This left side of the join can also be replaced by a spine group . When using feature groups also so save labels/prediction targets, it can happen that you end up with the same entity multiple times in the training dataset depending on the cadence at which the label group was updated and the length of the event time interval that is being used to generate the training dataset. This can lead to bias in the training dataset and should be avoided. To avoid this kind of situation, users can either narrow down the event time interval during training dataset creation or use a spine in order to precisely define the entities to be included in the training dataset. This is just one example where spines are helpful.","title":"Spine Dataframes"},{"location":"concepts/fs/feature_view/offline_api/#splitting-training-data","text":"You can create random train/validation/test splits of your training data using the HSFS API. You can also time-based splits with the HSFS API.","title":"Splitting Training Data"},{"location":"concepts/fs/feature_view/offline_api/#evaluation-sets","text":"Test data can also be split into evaluation sets to help evaluate a model for potential bias. First, you have to identify the classes of samples that could be at risk of bias, and generate evaluation sets from your unseen test set - one evaluation set for each group of samples at risk of bias. For example, if you have a feature group of users, where one of the features is gender, and you want to evaluate the risk of bias due to gender, you can use filters to generate 3 evaluation sets from your test set - one for male, female, and non-binary. Then you score your model against all 3 evaluation sets to ensure that the prediction performance is comparable and non-biased across all 3 gender.","title":"Evaluation Sets"},{"location":"concepts/fs/feature_view/offline_api/#batch-scoring-data","text":"Batch data for scoring models is created using a feature view. Similar to training data, you can create batch data as either: in-memory Pandas DataFrames, useful when you have a small amount of data to score; materialized data in files, in a file format of your choice (such as .tfrecord, .csv, or .parquet) Batch data requires specification of a start_time for the start of the batch scoring data. You can also specify the end_time (default is the current date).","title":"Batch (Scoring) Data"},{"location":"concepts/fs/feature_view/offline_api/#spine-dataframes_1","text":"Similar to training dataset generation, it might be helpful to specify a spine when retrieving features for batch inference. The only difference in this case is that the spine dataframe doesn't need to contain the label, as this will be the output of the inference pipeline. A typical use case is the handling of opt-ins, where certain customers have to be excluded from an inference pipeline due to a missing marketing opt-in.","title":"Spine Dataframes"},{"location":"concepts/fs/feature_view/online_api/","text":"The Feature View provides an Online API to return an individual feature vector, or a batch of feature vectors, containing the latest feature values. To retrieve a feature vector, a client needs to provide the primary key(s) for the feature groups backing the feature view. For example, if you have customer_profile and customer_purchases Feature Groups both with customer_id as a primary key, and a Feature View made up from features from both Feature Groups, then, you would use customer_id to retrieve a feature vector using the Feature View object. Feature Vectors # A feature vector is a row of features (without the primary key(s) and event timestamp): It may be the case that for any given feature vector, not all features will come pre-engineered from the feature store. Some features will be provided by the client (or at least the raw data to compute the feature will come from the client). We call these 'passed' features and, similar to precomputed features from the feature store, they can also be transformed by the HSFS client in the method: feature_view.get_feature_vector(entry, passed_features={...})","title":"Online API"},{"location":"concepts/fs/feature_view/online_api/#feature-vectors","text":"A feature vector is a row of features (without the primary key(s) and event timestamp): It may be the case that for any given feature vector, not all features will come pre-engineered from the feature store. Some features will be provided by the client (or at least the raw data to compute the feature will come from the client). We call these 'passed' features and, similar to precomputed features from the feature store, they can also be transformed by the HSFS client in the method: feature_view.get_feature_vector(entry, passed_features={...})","title":"Feature Vectors"},{"location":"concepts/fs/feature_view/statistics/","text":"The feature view does not contain any statistics, as it is simply an interface consisting of a number of features and any transformation functions applied to those features. However, training data can have descriptive statistics over it computed by HSFS. Descriptive statistics for training data is important for model monitoring, as it can enable model monitoring. If you compute the same descriptive statistics over windows of input features to models, you can help determine when there is a significant change in the distribution of an input feature, so-called feature shift.","title":"Statistics"},{"location":"concepts/fs/feature_view/training_inference_pipelines/","text":"A training pipeline is a program that orchestrates the training of a machine learning model. For supervised machine learning, a training pipeline requires both features and labels, and these can typically be retrieved from the feature store as either in-memory Pandas DataFrames or read as training data files, created from the feature store. An inference pipeline is a program that takes user input, optionally enriches it with features from the feature store, and builds a feature vector (or batch of feature vectors) with with it uses a model to make a prediction. Transformations # Feature transformations are mathematical operations that change feature values with the goal of improving model convergence or performance properties. Transformation functions take as input a single value (or small number of values), they often require state (such as the mean value of a feature to normalize the input), and they output a single value or a list of values. Training Serving Skew # It is crucial that the transformations performed when creating features (for training or serving) are consistent - use the same code - to avoid training/serving skew. In the image below, you can see that transformations happen after the Feature Store, but that the implementation of the transformation functions need to be consistent between the training and inference pipelines. There are 3 main approaches to prevent training/serving skew that we support in Hopsworks. These are (1) perform transformations in models, (2) perform transformations in pipelines (sklearn, TF, PyTorch) and use the model registry to save the transformation pipeline so that the same transformation is used in your inference pipeline, and (3) use HSFS transformations, defined as UDFs in Python. Transformations as Pre-Processing Layers in Models # Transformation functions can be implemented as preprocessing steps within a model. For example, you can write a transformation function as a pre-processing layer in Keras/TensorFlow. When you save the model, the preprocessing steps will also be saved as part of the model. Any state required to compute the transformation, such as the arithmetic mean of a numerical feature in the train set, is also stored with the function, enabling consistent transformations during inference. When data preprocessing is part of the model, users can just send the untransformed feature values to the model and the model itself will apply any transformation functions as preprocessing layers (such as encoding categorical variables or normalizing numerical variables). Transformation Pipelines in Scikit-Learn/TensorFlow/PyTorch # You have to save your transformation pipeline (serialize the object or the parameters) and make sure you apply exactly the same transformations in your inference pipeline. This means you should version the transformations. In Hopsworks, you can store the transformations with your versioned models in the Model Registry, helping you to ensure the same transformation pipeline is applied to both training/serving for the same model version. Transformations as Python UDFs in HSFS # Hopsworks feature store also supports consistent transformation functions by enabling a Python UDF, that implements a transformation, to be attached a to feature in a feature view. When training data is created with a feature view or when a feature vector is retrieved from a feature view, HSFS ensures that any transformation functions defined over any features will be applied before returning feature values. You can use built-in transformation objects in HSFS or write your own custom transformation functions as Python UDFs. The benefit of this approach is that transformations are applied consistently when creating training data and when retrieving feature data from the online feature store. Transformations no longer need to be included in either your training pipeline or inference pipeline, as they are applied transparently when creating training data and retrieving feature vectors. Hopsworks uses Spark to create training data as files, and any transformation functions for features are executed as Python UDFs in Spark - enabling transformation functions to be applied on large volumes of data and removing potentially CPU-intensive transformations from training pipelines.","title":"Consistent Transformations"},{"location":"concepts/fs/feature_view/training_inference_pipelines/#transformations","text":"Feature transformations are mathematical operations that change feature values with the goal of improving model convergence or performance properties. Transformation functions take as input a single value (or small number of values), they often require state (such as the mean value of a feature to normalize the input), and they output a single value or a list of values.","title":"Transformations"},{"location":"concepts/fs/feature_view/training_inference_pipelines/#training-serving-skew","text":"It is crucial that the transformations performed when creating features (for training or serving) are consistent - use the same code - to avoid training/serving skew. In the image below, you can see that transformations happen after the Feature Store, but that the implementation of the transformation functions need to be consistent between the training and inference pipelines. There are 3 main approaches to prevent training/serving skew that we support in Hopsworks. These are (1) perform transformations in models, (2) perform transformations in pipelines (sklearn, TF, PyTorch) and use the model registry to save the transformation pipeline so that the same transformation is used in your inference pipeline, and (3) use HSFS transformations, defined as UDFs in Python.","title":"Training Serving Skew"},{"location":"concepts/fs/feature_view/training_inference_pipelines/#transformations-as-pre-processing-layers-in-models","text":"Transformation functions can be implemented as preprocessing steps within a model. For example, you can write a transformation function as a pre-processing layer in Keras/TensorFlow. When you save the model, the preprocessing steps will also be saved as part of the model. Any state required to compute the transformation, such as the arithmetic mean of a numerical feature in the train set, is also stored with the function, enabling consistent transformations during inference. When data preprocessing is part of the model, users can just send the untransformed feature values to the model and the model itself will apply any transformation functions as preprocessing layers (such as encoding categorical variables or normalizing numerical variables).","title":"Transformations as Pre-Processing Layers in Models"},{"location":"concepts/fs/feature_view/training_inference_pipelines/#transformation-pipelines-in-scikit-learntensorflowpytorch","text":"You have to save your transformation pipeline (serialize the object or the parameters) and make sure you apply exactly the same transformations in your inference pipeline. This means you should version the transformations. In Hopsworks, you can store the transformations with your versioned models in the Model Registry, helping you to ensure the same transformation pipeline is applied to both training/serving for the same model version.","title":"Transformation Pipelines in Scikit-Learn/TensorFlow/PyTorch"},{"location":"concepts/fs/feature_view/training_inference_pipelines/#transformations-as-python-udfs-in-hsfs","text":"Hopsworks feature store also supports consistent transformation functions by enabling a Python UDF, that implements a transformation, to be attached a to feature in a feature view. When training data is created with a feature view or when a feature vector is retrieved from a feature view, HSFS ensures that any transformation functions defined over any features will be applied before returning feature values. You can use built-in transformation objects in HSFS or write your own custom transformation functions as Python UDFs. The benefit of this approach is that transformations are applied consistently when creating training data and when retrieving feature data from the online feature store. Transformations no longer need to be included in either your training pipeline or inference pipeline, as they are applied transparently when creating training data and retrieving feature vectors. Hopsworks uses Spark to create training data as files, and any transformation functions for features are executed as Python UDFs in Spark - enabling transformation functions to be applied on large volumes of data and removing potentially CPU-intensive transformations from training pipelines.","title":"Transformations as Python UDFs in HSFS"},{"location":"concepts/fs/feature_view/versioning/","text":"Feature views are interfaces, and if there is a change in the interface (the types of the features, the transformations applied to the features), then you need to change the version, to prevent breaking existing clients. Training datasets are associated with a specific feature view version. Training data also has its own version number (along with the version of its parent feature view). For example, online transformation functions often need training data statistics (e.g., normalizing a numerical feature requires you to divide the feature value by the mean value for that feature in the training dataset). As many training datasets can be created from a feature view, when you initialize the feature view you need to tell it which version of the training data to use - feature_view.init(1) means use version 1 of the training data for this feature view.","title":"Versioning"},{"location":"concepts/mlops/bi_tools/","text":"The Hopsworks Feature Store is based on an offline data store, queryable via an Apache Hive API, and an online data store, queryable via a MySQL Server API. Given that Feature Groups in Hopsworks have well-defined schemas, features in the Hopsworks Feature Store can be analyzed and reports can be generated from them using any BI Tools that include connectors for MySQL (JDBC) and Apache Hive (2-way TLS required). One platform we use with customers is Apache Superset , as it can be configured alongside Hopsworks to provide BI Tooling capabilities.","title":"BI Tools"},{"location":"concepts/mlops/kserve/","text":"KServe is an open-source framework for model serving on Kubernetes. In Hopsworks, you can easily deploy models from the model registry in KServe or in Docker containers (for Hopsworks Community). You can deploy models in either programs, using the HSML library, or in the UI. A KServe model deployment can include the following components: Transformer A pre-processing and post-processing component that can transform model inputs before predictions are made Predictor A predictor is a ML model in a Python object that takes a feature vector as input and returns a prediction as output Inference Logger Hopsworks logs inputs and outputs of transformers and predictors to a Kafka topic that is part of the same project as the model Inference Batcher Inference requests can be batched in different ways to adjust the trade-off between throughput and latencies of the model predictions Versioned Deployments Model deployments are versioned, enabling A/B testing and more. Istio Model Endpoint You can publish model via a REST Endpoint using Istio and access it over HTTP using a Hopsworks API key (with serving scope). Secure and authorized access is guaranteed by Hopsworks. Models deployed on KServe in Hopsworks can be easily integrated with the Hopsworks feature store using a Transformer Python script, that builds the predictor's input feature vector using the application input and pre-computed features from the feature store.","title":"Model Serving"},{"location":"concepts/mlops/mlops/","text":"","title":"Mlops"},{"location":"concepts/mlops/opensearch/","text":"Hopsworks includes OpenSearch as a multi-tenant service in projects. OpenSearch provides vector database capabilities through its k-NN plugin, that supports the FAISS and nsmlib embedding indexes. Through Hopsworks, OpenSearch also provides enterprise capabilities, including authentication and access control to indexes (an index can be private to a Hopsworks project), filtering, scalability, high availability, and disaster recovery support.","title":"Vector Database"},{"location":"concepts/mlops/prediction_services/","text":"A prediction service is an end-to-end analytical or operational machine learning system that takes in data and outputs predictions that are consumed by users of the prediction service. A prediction service consists of the following components: feature pipeline(s), training pipeline, inference pipeline (for either batch predictions or online predictions) a sink for predictions - either a store or a user-interface. Analytical ML # In the analytical ML figure below, we can see an analytical prediction service, where feature pipelines update the feature store with new feature data, running at some schedule (e.g., hourly, daily), and a batch program also runs on a schedule, reading batch scoring data from the feature store, computing predictions for that data with an embedded model, and writing those predictions to a database sink, from where the predictions are used for (predictive, prescriptive) analytical reports and/or to AI-enable operational services. Operational ML # In the operational ML figure below, we can see an operational prediction service, where feature pipelines update the feature store with new feature data, running at some schedule (e.g., streaming, hourly, daily), and the operational service sends prediction requests to a model deployed on KServe via its secured Istio endpoint. A deployed model on KServer handles the prediction request by first retrieving pre-computed features from the feature store for the given request, and then building a feature vector that is scored by the model. The prediction result is returned to the client (the operational service). KServe logs both the feature values and the prediction results back to Hopsworks for further analysis and to help create new training data. MLOps Flywheel # Once you have built your analytical or operational ML system, the MLOps flywheel is the path to building a self-managing system that automatically collects and processes feature logs, prediction logs, and outcomes to help create new training data for models. This enables a ML flywheel where new training data and insights are generated from your operational or analytical ML service, by feeding logs back into the feature store. More training data enables the training of better models, and with better models, you should hopefully improve you operational/batch services, so that you attract more clients, who in turn produce more data for training models. And, thus, the ML flywheel is bootstrapped and leads to a virtuous cycle of more data leading to better models and more models leading to more users, who produce more data, and so on.","title":"Prediction Services"},{"location":"concepts/mlops/prediction_services/#analytical-ml","text":"In the analytical ML figure below, we can see an analytical prediction service, where feature pipelines update the feature store with new feature data, running at some schedule (e.g., hourly, daily), and a batch program also runs on a schedule, reading batch scoring data from the feature store, computing predictions for that data with an embedded model, and writing those predictions to a database sink, from where the predictions are used for (predictive, prescriptive) analytical reports and/or to AI-enable operational services.","title":"Analytical ML"},{"location":"concepts/mlops/prediction_services/#operational-ml","text":"In the operational ML figure below, we can see an operational prediction service, where feature pipelines update the feature store with new feature data, running at some schedule (e.g., streaming, hourly, daily), and the operational service sends prediction requests to a model deployed on KServe via its secured Istio endpoint. A deployed model on KServer handles the prediction request by first retrieving pre-computed features from the feature store for the given request, and then building a feature vector that is scored by the model. The prediction result is returned to the client (the operational service). KServe logs both the feature values and the prediction results back to Hopsworks for further analysis and to help create new training data.","title":"Operational ML"},{"location":"concepts/mlops/prediction_services/#mlops-flywheel","text":"Once you have built your analytical or operational ML system, the MLOps flywheel is the path to building a self-managing system that automatically collects and processes feature logs, prediction logs, and outcomes to help create new training data for models. This enables a ML flywheel where new training data and insights are generated from your operational or analytical ML service, by feeding logs back into the feature store. More training data enables the training of better models, and with better models, you should hopefully improve you operational/batch services, so that you attract more clients, who in turn produce more data for training models. And, thus, the ML flywheel is bootstrapped and leads to a virtuous cycle of more data leading to better models and more models leading to more users, who produce more data, and so on.","title":"MLOps Flywheel"},{"location":"concepts/mlops/registry/","text":"Hopsworks Model Registry is designed with specific support for KServe and MLOps, through versioning. It enables developers to publish, test, monitor, govern and share models for collaboration with other teams. The model registry is where developers publish their models during the experimentation phase. The model registry can also be used to share models with the team and stakeholders. Like other project-based multi-tenant services in Hopsworks, a model registry is private to a project. That means you can easily add a development, staging, and production model registry to a cluster, and implement CI/CD processes for transitioning a model from development to staging to production. The model registry for KServe's capability are shown in the diagram below: The model registry centralizes model management, enabling models to be securely accessed and governed. Models are more than just the model itself - the registry also stores sample data for testing, configuration information, provenance information, environment variables, links to the code used to generate the model, the model version, and tags/descriptions). When you save a model, you can also save model metrics with the model, enabling users to understand, for example, performance of the model on test (or unseen) data. Model Package # A ML model consists of a number of different components in a model package: - Model Input/Output Schema - Model artifacts - Model version information - Model format (based on the ML framework used to train the model - e.g., .pkl or .tb files) You can also optionally include in your packaged model: - Sample data (used to test the model in KServe) - The source notebook/program/experiment used to create the model","title":"Model Registry"},{"location":"concepts/mlops/registry/#model-package","text":"A ML model consists of a number of different components in a model package: - Model Input/Output Schema - Model artifacts - Model version information - Model format (based on the ML framework used to train the model - e.g., .pkl or .tb files) You can also optionally include in your packaged model: - Sample data (used to test the model in KServe) - The source notebook/program/experiment used to create the model","title":"Model Package"},{"location":"concepts/mlops/training/","text":"Hopsworks supports running model training pipelines on any Python environment, whether on an external Python client or on a Hopsworks cluster. The outputs of a training pipeline are typically experiment results, including logs, and possibly a trained model. You can plugin your own experimentation tracking platform or model registry, or you can use Hopsworks. Training Pipelines on Hopsworks # If you train models with Hopsworks, you can setup CI/CD pipelines as shown below, where the experiments are tracked by Hopsworks, and any model created is published to a model registry. Each project has its own private model registry, so when you are working in a development project, you typically publish models to your project's private development registry, and if all model validation tests pass, and the model performance is good enough, the same training pipeline can be submitted via a CI/CD pipeline (e.g., GitHub push request) to a staging project, and the same procedure can be repeated to push the training pipeline to a production project. Hopsworks Model Registry and Model Serving capabilities can then be used to build a batch or online prediction service using the model.","title":"Model Training"},{"location":"concepts/mlops/training/#training-pipelines-on-hopsworks","text":"If you train models with Hopsworks, you can setup CI/CD pipelines as shown below, where the experiments are tracked by Hopsworks, and any model created is published to a model registry. Each project has its own private model registry, so when you are working in a development project, you typically publish models to your project's private development registry, and if all model validation tests pass, and the model performance is good enough, the same training pipeline can be submitted via a CI/CD pipeline (e.g., GitHub push request) to a staging project, and the same procedure can be repeated to push the training pipeline to a production project. Hopsworks Model Registry and Model Serving capabilities can then be used to build a batch or online prediction service using the model.","title":"Training Pipelines on Hopsworks"},{"location":"concepts/projects/cicd/","text":"You can setup traditional development, staging, and production environment in Hopsworks using Projects. A project enables you provide access control for the different environments - just like a GitHub repository, owners of projects can add and remove members of projects and assign different roles to project members - the \"data owner\" role can write to feature store, while a \"data scientist\" can only read from the feature store and create training data. Dev, Staging, Prod # You can create dev, staging, and prod projects - either on the same cluster, but mostly commonly, with production on its own cluster: Versioning # Hopsworks supports the versioning of ML assets, including: Feature Groups: the version of its schema - breaking schema changes require a new version and backfilling the new version; Feature Views: the version of its schema, and breaking schema changes only require a new version; Models: the version of a model; Deployments: the version of the deployment of a model - a model with the same version can be found in >1 deployment. Pytest for feature logic and feature pipeline tests # Pytest and Great Expectations can be used for testing feature pipelines. Pytest is used to test feature logic and for end-to-end feature pipeline tests, while Great Expectations is used for data validation tests. Here, we can see how a feature pipeline test uses sample data to compute features and validate they have been written successfully, first to a development feature store, and then they can be pushed to a staging feature store, before finally being promoted to production.","title":"CI/CD"},{"location":"concepts/projects/cicd/#dev-staging-prod","text":"You can create dev, staging, and prod projects - either on the same cluster, but mostly commonly, with production on its own cluster:","title":"Dev, Staging, Prod"},{"location":"concepts/projects/cicd/#versioning","text":"Hopsworks supports the versioning of ML assets, including: Feature Groups: the version of its schema - breaking schema changes require a new version and backfilling the new version; Feature Views: the version of its schema, and breaking schema changes only require a new version; Models: the version of a model; Deployments: the version of the deployment of a model - a model with the same version can be found in >1 deployment.","title":"Versioning"},{"location":"concepts/projects/cicd/#pytest-for-feature-logic-and-feature-pipeline-tests","text":"Pytest and Great Expectations can be used for testing feature pipelines. Pytest is used to test feature logic and for end-to-end feature pipeline tests, while Great Expectations is used for data validation tests. Here, we can see how a feature pipeline test uses sample data to compute features and validate they have been written successfully, first to a development feature store, and then they can be pushed to a staging feature store, before finally being promoted to production.","title":"Pytest for feature logic and feature pipeline tests"},{"location":"concepts/projects/governance/","text":"Hopsworks provides project-level multi-tenancy, a data mesh enabling technology. Think of it as a GitHub repository for your teams and ML assets. More specifically, a project is a sandbox for team members, ML assets (features, training data, models, vector database, model deployments), and optionally feature pipelines and training pipelines. The ML assets can only be accessed by project members, and there is role-based access control (RBAC) for project members within a project. Dev/Staging/Prod for Data # Projects enable you to define development, staging, and even production projects on the same cluster. Often, companies deploy production projects on dedicated clusters, but development projects and staging projects on a shared cluster. This way, projects can be easily used to implement CI/CD workflows. Data Mesh of Feature Stores # Projects enable you to move beyond the traditional dev/staging/prod ownership model for data. Different teams or lines of business can have their own private feature stores, you can mix them with a group-wide feature store, and feature stores can be securely shared between teams/organizations. Effectively, you can have decentralized ownership of feature stores, with domain-specific projects, and each project managing its own feature pipelines. Hopsworks provides data/feature sharing support between these self-service projects. Audit Logs with REST API # Hopsworks stores audit logs for all calls on its REST API in its file system, HopsFS. The audit log can be used to analyze the historical usage of services by users.","title":"Governance"},{"location":"concepts/projects/governance/#devstagingprod-for-data","text":"Projects enable you to define development, staging, and even production projects on the same cluster. Often, companies deploy production projects on dedicated clusters, but development projects and staging projects on a shared cluster. This way, projects can be easily used to implement CI/CD workflows.","title":"Dev/Staging/Prod for Data"},{"location":"concepts/projects/governance/#data-mesh-of-feature-stores","text":"Projects enable you to move beyond the traditional dev/staging/prod ownership model for data. Different teams or lines of business can have their own private feature stores, you can mix them with a group-wide feature store, and feature stores can be securely shared between teams/organizations. Effectively, you can have decentralized ownership of feature stores, with domain-specific projects, and each project managing its own feature pipelines. Hopsworks provides data/feature sharing support between these self-service projects.","title":"Data Mesh of Feature Stores"},{"location":"concepts/projects/governance/#audit-logs-with-rest-api","text":"Hopsworks stores audit logs for all calls on its REST API in its file system, HopsFS. The audit log can be used to analyze the historical usage of services by users.","title":"Audit Logs with REST API"},{"location":"concepts/projects/search/","text":"Hopsworks supports free-text search for ML assets: features, feature groups, feature views, training data, models, and deployments. You can use the search bar at the top of your project to free-text search for the names or descriptions of any ML asset. You can also search using keywords or tags that are attached to an ML asset. A keyword is a single user-defined word attached to an ML asset. Keywords can be used to help it make it easier to find ML assets or understand the context in which they should be used (for example, PII could be used to indicate that the ML asset is based on personally identifiable information. However, it may be preferable to have a stronger governance framework for ML assets than keywords alone. For this, you can define a schematized tag , defining a list of key/value tags along with a type for a value. In the figure below, you can see an example of a schematized tag with two key/value pairs: pii of type boolean (indicating if this feature group contains PII data), and owner of type string (indicating who the owner of the data in this feature group is). Note there is also a keyword defined for this feature group called eu_region , indicating the data has its origins in the EU. Lineage # Hopsworks tracks the lineage (or provenance) of ML assets automatically for you. You can see what features are used in which feature view or training dataset. You can see what training dataset was used to train a given model. For assets that are managed outside of Hopsworks, there is support for the explicit definition of lineage dependencies.","title":"Tags/Search/Lineage"},{"location":"concepts/projects/search/#lineage","text":"Hopsworks tracks the lineage (or provenance) of ML assets automatically for you. You can see what features are used in which feature view or training dataset. You can see what training dataset was used to train a given model. For assets that are managed outside of Hopsworks, there is support for the explicit definition of lineage dependencies.","title":"Lineage"},{"location":"concepts/projects/storage/","text":"Every project in Hopsworks has its own private assets: a Feature Store (including both Online and Offline Stores) a Filesystem subtree (all directory and files under /Projects/ /) a Model Registry Model Deployments Kafka topics OpenSearch indexes (including KNN indexes - the vector DB) a Hive Database Access control to these assets is controlled using project membership ACLs (access-control lists). Users in a project who have a Data Owner role have read/write access to these assets. Users in a project who have a Data Scientist role have mostly read-only access to these assets, with the exception of the ability to write to well-known directories (Resources, Jupyter, Logs). However, it is often desirable to share assets between projects, with read-only, read/write privileges, and to restrict the privileges to specific role (e.g., Data Owners) in the target project. In Hopsworks, you can explicitly share assets between projects without copying the assets. Sharing is managed by ACLs in Hopsworks, see example below:","title":"Data Storage/Sharing"},{"location":"reference_guides/","text":".md-typeset h1 { font-size: 0em; } Hopsworks Feature Store (hsfs) Model Registry & Model Serving (hsml)","title":"Index"},{"location":"setup_installation/","text":"Setup and Installation # This section contains installation guides for the Hopsworks Platform , on AWS Azure GCP On-Prem environments and common setup instructions. For instructions on installing the Hopsworks Client libraries, see the Client Installation guide.","title":"Setup and Installation"},{"location":"setup_installation/#setup-and-installation","text":"This section contains installation guides for the Hopsworks Platform , on AWS Azure GCP On-Prem environments and common setup instructions. For instructions on installing the Hopsworks Client libraries, see the Client Installation guide.","title":"Setup and Installation"},{"location":"setup_installation/aws/cluster_creation/","text":"Getting started with managed.hopsworks.ai (AWS) # This guide goes into detail for each of the steps of the cluster creation in managed.hopsworks.ai Step 1 starting to create a cluster # In managed.hopsworks.ai , select Create cluster : Create a Hopsworks cluster Step 2 setting the General information # Create a Hopsworks cluster, general information Select the Region in which you want your cluster to run (1), name your cluster (2). Select the Instance type (3) and Local storage (4) size for the cluster Head node . Enable EBS encryption # Select the checkbox (5) to enable encryption of EBS drives and shapshots. After enabling, the KMS key to be used for encryption can be specified by its alias, ID or ARN. Leaving the KMS key unspecified results in the EC2 default encryption key being used. S3 bucket configuration # Enter the name of the S3 bucket (6) you want the cluster to store its data in. Note The S3 bucket you are using must be empty. Premium users have the option to use encrypted S3 buckets . To configure an encrypted bucket click on the Advanced tab and select the appropriate encryption type. Note Encryption must have been already enabled for the bucket We support the following encryption schemes: SSE-S3 SSE-KMS S3 managed key User managed key Users can also select an AWS canned ACLs for the objects: bucket-owner-full-control ECR AWS Account Id # Enter the AWS account Id (7) to setup ECR repositories for the cluster. It is set by default to the AWS account id where you set the cross account role. On the main page, you can also choose to aggregate logs in CloudWatch and to opt out of managed.hopsworks.ai log collection. The first one is to aggregate the logs of services running in your cluster in CloudWatch service in your configured AWS account. This can be useful if you want to understand what is happening on your cluster, without having to ssh into the instances. The second one is for managed.hopsworks.ai to collect logs about the services running in your cluster. These logs will help us improve our system and provide support. If you choose to opt-out from the log collection and need support you will have to provide us the log by yourself, which will slow down the support process. Step 3 workers configuration # In this step, you configure the workers. There are two possible setups static or autoscaling. In the static setup, the cluster has a fixed number of workers that you decide. You can then add and remove workers manually, for more details: the documentation . In the autoscaling setup, you configure conditions to add and remove workers and the cluster will automatically add and remove workers depending on the demand. Static workers configuration # You can set the static configuration by selecting Disabled in the first drop-down (1). Then you select the number of workers you want to start the cluster with (2). And, select the Instance type (3) and Local storage size (4) for the worker nodes . Create a Hopsworks cluster, static workers configuration Autoscaling workers configuration # You can set the autoscaling configuration by selecting enabled in the first drop-down (1). You can configure: The instance type you want to use. The size of the instances' disk. The minimum number of workers. The maximum number of workers. The targeted number of standby workers. Setting some resources in standby ensures that there are always some free resources in your cluster. This ensures that requests for new resources are fulfilled promptly. You configure the standby by setting the amount of workers you want to be in standby. For example, if you set a value of 0.5 the system will start a new worker every time the aggregated free cluster resources drop below 50% of a worker's resources. If you set this value to 0 new workers will only be started when a job or notebook request the resources. The time to wait before removing unused resources. One often starts a new computation shortly after finishing the previous one. To avoid having to wait for workers to stop and start between each computation it is recommended to wait before shutting down workers. Here you set the amount of time in seconds resources need to be unused before they get removed from the system. Note The standby will not be taken into account if you set the minimum number of workers to 0 and no resources are used in the cluster. This ensures that the number of nodes can fall to 0 when no resources are used. The standby will start to take effect as soon as you start using resources. Create a Hopsworks cluster, autoscale workers configuration Step 4 select an SSH key # When deploying clusters, managed.hopsworks.ai installs an ssh key on the cluster's instances so that you can access them if necessary. Select the SSH key that you want to use to access cluster instances. For more detail on how to add a shh key in AWS refer to Create an ssh key Choose SSH key Step 5 select the Instance Profile # To let the cluster instances access the S3 bucket we need to attach an instance profile to the virtual machines. In this step, you choose which profile to use. This profile needs to have access right to the S3 bucket you selected in Step 2 . For more details on how to create the instance profile and give it access to the S3 bucket refer to Creating an instance profile and giving it access to the bucket If you want to use role chaining , it is recommanded to use a different instance profile for the head node and the other cluster's nodes. You do this by clicking the Advanced configuration check box and selecting instance profile for the head node. This profile should have the same permission as the profile you selected above, plus the extra permissions for the role chaining. Choose the instance profile Step 6 set the backup retention policy # To backup the S3 bucket data when taking a cluster backup we need to set a retention policy for S3. In this step, you choose the retention period in days. You can deactivate the retention policy by setting this value to 0 but this will block you from taking any backup of your cluster. Choose the backup retention policy Step 7 Managed Containers # Hopsworks can integrate with Amazon Elastic Kubernetes Service (EKS) to launch Python jobs, Jupyter servers, and ML model servings on top of Amazon EKS. For more detail on how to set up this integration refer to Integration with Amazon EKS . Add EKS cluster name Step 8 VPC selection # In this step, you can select the VPC which will be used by the Hopsworks cluster. You can either select an existing VPC or let managed.hopsworks.ai create one for you. If you decide to let managed.hopsworks.ai create the VPC for you, you can choose the CIDR block for this virtual network. Refer to Create a VPC for more details on how to create your own VPC in AWS. Choose a VPC Note If the VPC uses a custom domain read our guide on how to set this up Step 9 Availability Zone selection # If you selected an existing VPC in the previous step, this step lets you select which availability zone of this VPC to use. If you did not select an existing virtual network in the previous step managed.hopsworks.ai will create an availability zone for you. You can choose the CIDR block this subnet will use. Choose an availability zone Step 10 Security group selection # If you selected an existing VPC in the previous step, this step lets you select which security group to use. Note Managed.hopsworks.ai require some rules for inbound and outbound traffic in your security group, for more details refer to inbound traffic rules and outbound traffic rules . Note Managed.hopsworks.ai attaches a public ip to your cluster by default. However, you can disable this behavior by unchecking the Attach Public IP checkbox. Choose security group Limiting outbound traffic to managed.hopsworks.ai # Clusters created on managed.hopsworks.ai need to be able to send http requests to api.hopsworks.ai. If you have strict regulation regarding outbound traffic, you can enable the Use static IPs to communicate with managed.hopsworks.ai checkbox to get a list of IPs to be allowed as shown below: Enable static IPs Step 11 User management selection # In this step, you can choose which user management system to use. You have four choices: Managed : Managed.hopsworks.ai automatically adds and removes users from the Hopsworks cluster when you add and remove users from your organization (more details here ). OAuth2 : integrate the cluster with your organization's OAuth2 identity provider. See Use OAuth2 for user management for more detail. LDAP : integrate the cluster with your organization's LDAP/ActiveDirectory server. See Use LDAP for user management for more detail. Disabled : let you manage users manually from within Hopsworks. Choose user management type Step 12 Managed RonDB # Hopsworks uses RonDB as a database engine for its online Feature Store. By default database will run on its own VM. Premium users can scale-out database services to multiple VMs to handle increased workload. For details on how to configure RonDB check our guide here . Configure RonDB If you need to deploy a RonDB cluster instead of a single node please contact us . Step 13 add tags to your instances. # In this step, you can define tags that will be added to the cluster virtual machines. Add tags Step 14 add an init script to your instances. # In this step, you can enter an initialization script that will be run at startup on every instance. You can select whether this script will run before or after the VM configuration. Be cautious if you select to run it before the VM configuration as this might affect Cluster creation. Note this init script must be a bash script starting with #!/usr/bin/env bash Add initialization script Step 15 Review and create # Review all information and select Create : Review cluster information The cluster will start. This will take a few minutes: Booting Hopsworks cluster As soon as the cluster has started, you will be able to log in to your new Hopsworks cluster. You will also be able to stop, restart, or terminate the cluster. Running Hopsworks cluster","title":"Cluster Creation"},{"location":"setup_installation/aws/cluster_creation/#getting-started-with-managedhopsworksai-aws","text":"This guide goes into detail for each of the steps of the cluster creation in managed.hopsworks.ai","title":"Getting started with managed.hopsworks.ai (AWS)"},{"location":"setup_installation/aws/cluster_creation/#step-1-starting-to-create-a-cluster","text":"In managed.hopsworks.ai , select Create cluster : Create a Hopsworks cluster","title":"Step 1 starting to create a cluster"},{"location":"setup_installation/aws/cluster_creation/#step-2-setting-the-general-information","text":"Create a Hopsworks cluster, general information Select the Region in which you want your cluster to run (1), name your cluster (2). Select the Instance type (3) and Local storage (4) size for the cluster Head node .","title":"Step 2 setting the General information"},{"location":"setup_installation/aws/cluster_creation/#enable-ebs-encryption","text":"Select the checkbox (5) to enable encryption of EBS drives and shapshots. After enabling, the KMS key to be used for encryption can be specified by its alias, ID or ARN. Leaving the KMS key unspecified results in the EC2 default encryption key being used.","title":"Enable EBS encryption"},{"location":"setup_installation/aws/cluster_creation/#s3-bucket-configuration","text":"Enter the name of the S3 bucket (6) you want the cluster to store its data in. Note The S3 bucket you are using must be empty. Premium users have the option to use encrypted S3 buckets . To configure an encrypted bucket click on the Advanced tab and select the appropriate encryption type. Note Encryption must have been already enabled for the bucket We support the following encryption schemes: SSE-S3 SSE-KMS S3 managed key User managed key Users can also select an AWS canned ACLs for the objects: bucket-owner-full-control","title":"S3 bucket configuration"},{"location":"setup_installation/aws/cluster_creation/#ecr-aws-account-id","text":"Enter the AWS account Id (7) to setup ECR repositories for the cluster. It is set by default to the AWS account id where you set the cross account role. On the main page, you can also choose to aggregate logs in CloudWatch and to opt out of managed.hopsworks.ai log collection. The first one is to aggregate the logs of services running in your cluster in CloudWatch service in your configured AWS account. This can be useful if you want to understand what is happening on your cluster, without having to ssh into the instances. The second one is for managed.hopsworks.ai to collect logs about the services running in your cluster. These logs will help us improve our system and provide support. If you choose to opt-out from the log collection and need support you will have to provide us the log by yourself, which will slow down the support process.","title":"ECR AWS Account Id"},{"location":"setup_installation/aws/cluster_creation/#step-3-workers-configuration","text":"In this step, you configure the workers. There are two possible setups static or autoscaling. In the static setup, the cluster has a fixed number of workers that you decide. You can then add and remove workers manually, for more details: the documentation . In the autoscaling setup, you configure conditions to add and remove workers and the cluster will automatically add and remove workers depending on the demand.","title":"Step 3 workers configuration"},{"location":"setup_installation/aws/cluster_creation/#static-workers-configuration","text":"You can set the static configuration by selecting Disabled in the first drop-down (1). Then you select the number of workers you want to start the cluster with (2). And, select the Instance type (3) and Local storage size (4) for the worker nodes . Create a Hopsworks cluster, static workers configuration","title":"Static workers configuration"},{"location":"setup_installation/aws/cluster_creation/#autoscaling-workers-configuration","text":"You can set the autoscaling configuration by selecting enabled in the first drop-down (1). You can configure: The instance type you want to use. The size of the instances' disk. The minimum number of workers. The maximum number of workers. The targeted number of standby workers. Setting some resources in standby ensures that there are always some free resources in your cluster. This ensures that requests for new resources are fulfilled promptly. You configure the standby by setting the amount of workers you want to be in standby. For example, if you set a value of 0.5 the system will start a new worker every time the aggregated free cluster resources drop below 50% of a worker's resources. If you set this value to 0 new workers will only be started when a job or notebook request the resources. The time to wait before removing unused resources. One often starts a new computation shortly after finishing the previous one. To avoid having to wait for workers to stop and start between each computation it is recommended to wait before shutting down workers. Here you set the amount of time in seconds resources need to be unused before they get removed from the system. Note The standby will not be taken into account if you set the minimum number of workers to 0 and no resources are used in the cluster. This ensures that the number of nodes can fall to 0 when no resources are used. The standby will start to take effect as soon as you start using resources. Create a Hopsworks cluster, autoscale workers configuration","title":"Autoscaling workers configuration"},{"location":"setup_installation/aws/cluster_creation/#step-4-select-an-ssh-key","text":"When deploying clusters, managed.hopsworks.ai installs an ssh key on the cluster's instances so that you can access them if necessary. Select the SSH key that you want to use to access cluster instances. For more detail on how to add a shh key in AWS refer to Create an ssh key Choose SSH key","title":"Step 4 select an SSH key"},{"location":"setup_installation/aws/cluster_creation/#step-5-select-the-instance-profile","text":"To let the cluster instances access the S3 bucket we need to attach an instance profile to the virtual machines. In this step, you choose which profile to use. This profile needs to have access right to the S3 bucket you selected in Step 2 . For more details on how to create the instance profile and give it access to the S3 bucket refer to Creating an instance profile and giving it access to the bucket If you want to use role chaining , it is recommanded to use a different instance profile for the head node and the other cluster's nodes. You do this by clicking the Advanced configuration check box and selecting instance profile for the head node. This profile should have the same permission as the profile you selected above, plus the extra permissions for the role chaining. Choose the instance profile","title":"Step 5 select the Instance Profile"},{"location":"setup_installation/aws/cluster_creation/#step-6-set-the-backup-retention-policy","text":"To backup the S3 bucket data when taking a cluster backup we need to set a retention policy for S3. In this step, you choose the retention period in days. You can deactivate the retention policy by setting this value to 0 but this will block you from taking any backup of your cluster. Choose the backup retention policy","title":"Step 6 set the backup retention policy"},{"location":"setup_installation/aws/cluster_creation/#step-7-managed-containers","text":"Hopsworks can integrate with Amazon Elastic Kubernetes Service (EKS) to launch Python jobs, Jupyter servers, and ML model servings on top of Amazon EKS. For more detail on how to set up this integration refer to Integration with Amazon EKS . Add EKS cluster name","title":"Step 7 Managed Containers"},{"location":"setup_installation/aws/cluster_creation/#step-8-vpc-selection","text":"In this step, you can select the VPC which will be used by the Hopsworks cluster. You can either select an existing VPC or let managed.hopsworks.ai create one for you. If you decide to let managed.hopsworks.ai create the VPC for you, you can choose the CIDR block for this virtual network. Refer to Create a VPC for more details on how to create your own VPC in AWS. Choose a VPC Note If the VPC uses a custom domain read our guide on how to set this up","title":"Step 8 VPC selection"},{"location":"setup_installation/aws/cluster_creation/#step-9-availability-zone-selection","text":"If you selected an existing VPC in the previous step, this step lets you select which availability zone of this VPC to use. If you did not select an existing virtual network in the previous step managed.hopsworks.ai will create an availability zone for you. You can choose the CIDR block this subnet will use. Choose an availability zone","title":"Step 9 Availability Zone selection"},{"location":"setup_installation/aws/cluster_creation/#step-10-security-group-selection","text":"If you selected an existing VPC in the previous step, this step lets you select which security group to use. Note Managed.hopsworks.ai require some rules for inbound and outbound traffic in your security group, for more details refer to inbound traffic rules and outbound traffic rules . Note Managed.hopsworks.ai attaches a public ip to your cluster by default. However, you can disable this behavior by unchecking the Attach Public IP checkbox. Choose security group","title":"Step 10 Security group selection"},{"location":"setup_installation/aws/cluster_creation/#limiting-outbound-traffic-to-managedhopsworksai","text":"Clusters created on managed.hopsworks.ai need to be able to send http requests to api.hopsworks.ai. If you have strict regulation regarding outbound traffic, you can enable the Use static IPs to communicate with managed.hopsworks.ai checkbox to get a list of IPs to be allowed as shown below: Enable static IPs","title":"Limiting outbound traffic to managed.hopsworks.ai"},{"location":"setup_installation/aws/cluster_creation/#step-11-user-management-selection","text":"In this step, you can choose which user management system to use. You have four choices: Managed : Managed.hopsworks.ai automatically adds and removes users from the Hopsworks cluster when you add and remove users from your organization (more details here ). OAuth2 : integrate the cluster with your organization's OAuth2 identity provider. See Use OAuth2 for user management for more detail. LDAP : integrate the cluster with your organization's LDAP/ActiveDirectory server. See Use LDAP for user management for more detail. Disabled : let you manage users manually from within Hopsworks. Choose user management type","title":"Step 11 User management selection"},{"location":"setup_installation/aws/cluster_creation/#step-12-managed-rondb","text":"Hopsworks uses RonDB as a database engine for its online Feature Store. By default database will run on its own VM. Premium users can scale-out database services to multiple VMs to handle increased workload. For details on how to configure RonDB check our guide here . Configure RonDB If you need to deploy a RonDB cluster instead of a single node please contact us .","title":"Step 12 Managed RonDB"},{"location":"setup_installation/aws/cluster_creation/#step-13-add-tags-to-your-instances","text":"In this step, you can define tags that will be added to the cluster virtual machines. Add tags","title":"Step 13 add tags to your instances."},{"location":"setup_installation/aws/cluster_creation/#step-14-add-an-init-script-to-your-instances","text":"In this step, you can enter an initialization script that will be run at startup on every instance. You can select whether this script will run before or after the VM configuration. Be cautious if you select to run it before the VM configuration as this might affect Cluster creation. Note this init script must be a bash script starting with #!/usr/bin/env bash Add initialization script","title":"Step 14 add an init script to your instances."},{"location":"setup_installation/aws/cluster_creation/#step-15-review-and-create","text":"Review all information and select Create : Review cluster information The cluster will start. This will take a few minutes: Booting Hopsworks cluster As soon as the cluster has started, you will be able to log in to your new Hopsworks cluster. You will also be able to stop, restart, or terminate the cluster. Running Hopsworks cluster","title":"Step 15 Review and create"},{"location":"setup_installation/aws/custom_domain_name/","text":"Deploy in a VPC with a custom domain name # Some organizations follow network patterns which impose a specific domain name for Instances. In that case, the instance's hostname instead of ip-10-0-0-175.us-east-2.compute.internal would be ip-10-0-0-175.bar.foo The control plane at managed.hopsworks.ai needs to be aware of such case in order to properly initialize the Cluster. Note This feature is enabled only upon request. If you want this feature to be enable for your account please contact sales There are multiple ways to use custom domain names in your organization which are beyond the scope of this guide. We assume your cloud/network team has already setup the infrastructure. If you are using a resolver such as Amazon Route 53 , it is advised to update the record sets automatically. See our guide below for more information. Set cluster domain name # If this feature is enabled for your account, then in the VPC selection step you will have the option to specify the custom domain name as shown in the figure below. VPC with custom domain name In this case, the hostname of the Instance would be INSTANCE_ID.dev.hopsworks.domain Hostnames must be resolvable by all Virtual Machines in the cluster. For that reason we suggest, if possible, to automatically register the hostnames with your DNS. In the following section we present an example of automatic name registration in Amazon Route 53 Auto registration with Amazon Route 53 # It is quite common for organizations in AWS to use Route 53 for DNS or for hosted zones. You can configure a cluster in managed.hopsworks.ai to execute some custom initialization script before any other action. This script will be executed on all nodes of the cluster. Since the hostname of the VM is in the form of INSTANCE_ID.DOMAIN_NAME it is easy to automate the zone update. The script below creates an A record in a configured Route 53 hosted zone. Warning If you want the VM to register itself with Route 53 you must amend the Instance Profile with the following permissions { \"Sid\" : \"Route53RecordSet\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"route53:ChangeResourceRecordSets\" ], \"Resource\" : \"arn:aws:route53:::hostedzone/YOUR_HOSTED_ZONE_ID\" }, { \"Sid\" : \"Route53GetChange\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"route53:GetChange\" ], \"Resource\" : \"arn:aws:route53:::change/*\" } The following script will get the instance ID from the EC2 metadata server and add an A record to the hosted zone in Route53. Update the YOUR_HOSTED_ZONE_ID and YOUR_CUSTOM_DOMAIN_NAME to match yours. #!/usr/bin/env bash set -e HOSTED_ZONE_ID = YOUR_HOSTED_ZONE_ID ZONE = YOUR_CUSTOM_DOMAIN_NAME record_set_file = /tmp/record_set.json instance_id = $( curl --silent http://169.254.169.254/2016-09-02/meta-data/instance-id ) domain_name = \" ${ instance_id } . ${ ZONE } \" local_ip = $( curl --silent http://169.254.169.254/2016-09-02/meta-data/local-ipv4 ) cat << EOC | tee $record_set_file { \"Changes\": [ { \"Action\": \"UPSERT\", \"ResourceRecordSet\": { \"Name\": \"${domain_name}\", \"Type\": \"A\", \"TTL\": 60, \"ResourceRecords\": [ { \"Value\": \"${local_ip}\" } ] } } ] } EOC echo \"Adding A record ${ domain_name } -> ${ local_ip } to Hosted Zone ${ HOSTED_ZONE_ID } \" change_resource_id = $( aws route53 change-resource-record-sets --hosted-zone-id ${ HOSTED_ZONE_ID } --change-batch file:// ${ record_set_file } | jq -r '.ChangeInfo.Id' ) echo \"Change resource ID: ${ change_resource_id } \" aws route53 wait resource-record-sets-changed --id ${ change_resource_id } echo \"Added resource record set\" rm -f ${ record_set_file } Set VM initialization script # As a final step you need to configure the Cluster to use the script above during VM creation with the user init script option. Paste the script to the text box and make sure you select this script to be executed before anything else on the VM. Automatic domain name registration with Route53","title":"Deploy in a VPC with a custom domain name"},{"location":"setup_installation/aws/custom_domain_name/#deploy-in-a-vpc-with-a-custom-domain-name","text":"Some organizations follow network patterns which impose a specific domain name for Instances. In that case, the instance's hostname instead of ip-10-0-0-175.us-east-2.compute.internal would be ip-10-0-0-175.bar.foo The control plane at managed.hopsworks.ai needs to be aware of such case in order to properly initialize the Cluster. Note This feature is enabled only upon request. If you want this feature to be enable for your account please contact sales There are multiple ways to use custom domain names in your organization which are beyond the scope of this guide. We assume your cloud/network team has already setup the infrastructure. If you are using a resolver such as Amazon Route 53 , it is advised to update the record sets automatically. See our guide below for more information.","title":"Deploy in a VPC with a custom domain name"},{"location":"setup_installation/aws/custom_domain_name/#set-cluster-domain-name","text":"If this feature is enabled for your account, then in the VPC selection step you will have the option to specify the custom domain name as shown in the figure below. VPC with custom domain name In this case, the hostname of the Instance would be INSTANCE_ID.dev.hopsworks.domain Hostnames must be resolvable by all Virtual Machines in the cluster. For that reason we suggest, if possible, to automatically register the hostnames with your DNS. In the following section we present an example of automatic name registration in Amazon Route 53","title":"Set cluster domain name"},{"location":"setup_installation/aws/custom_domain_name/#auto-registration-with-amazon-route-53","text":"It is quite common for organizations in AWS to use Route 53 for DNS or for hosted zones. You can configure a cluster in managed.hopsworks.ai to execute some custom initialization script before any other action. This script will be executed on all nodes of the cluster. Since the hostname of the VM is in the form of INSTANCE_ID.DOMAIN_NAME it is easy to automate the zone update. The script below creates an A record in a configured Route 53 hosted zone. Warning If you want the VM to register itself with Route 53 you must amend the Instance Profile with the following permissions { \"Sid\" : \"Route53RecordSet\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"route53:ChangeResourceRecordSets\" ], \"Resource\" : \"arn:aws:route53:::hostedzone/YOUR_HOSTED_ZONE_ID\" }, { \"Sid\" : \"Route53GetChange\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"route53:GetChange\" ], \"Resource\" : \"arn:aws:route53:::change/*\" } The following script will get the instance ID from the EC2 metadata server and add an A record to the hosted zone in Route53. Update the YOUR_HOSTED_ZONE_ID and YOUR_CUSTOM_DOMAIN_NAME to match yours. #!/usr/bin/env bash set -e HOSTED_ZONE_ID = YOUR_HOSTED_ZONE_ID ZONE = YOUR_CUSTOM_DOMAIN_NAME record_set_file = /tmp/record_set.json instance_id = $( curl --silent http://169.254.169.254/2016-09-02/meta-data/instance-id ) domain_name = \" ${ instance_id } . ${ ZONE } \" local_ip = $( curl --silent http://169.254.169.254/2016-09-02/meta-data/local-ipv4 ) cat << EOC | tee $record_set_file { \"Changes\": [ { \"Action\": \"UPSERT\", \"ResourceRecordSet\": { \"Name\": \"${domain_name}\", \"Type\": \"A\", \"TTL\": 60, \"ResourceRecords\": [ { \"Value\": \"${local_ip}\" } ] } } ] } EOC echo \"Adding A record ${ domain_name } -> ${ local_ip } to Hosted Zone ${ HOSTED_ZONE_ID } \" change_resource_id = $( aws route53 change-resource-record-sets --hosted-zone-id ${ HOSTED_ZONE_ID } --change-batch file:// ${ record_set_file } | jq -r '.ChangeInfo.Id' ) echo \"Change resource ID: ${ change_resource_id } \" aws route53 wait resource-record-sets-changed --id ${ change_resource_id } echo \"Added resource record set\" rm -f ${ record_set_file }","title":"Auto registration with Amazon Route 53"},{"location":"setup_installation/aws/custom_domain_name/#set-vm-initialization-script","text":"As a final step you need to configure the Cluster to use the script above during VM creation with the user init script option. Paste the script to the text box and make sure you select this script to be executed before anything else on the VM. Automatic domain name registration with Route53","title":"Set VM initialization script"},{"location":"setup_installation/aws/eks_ecr_integration/","text":"Integration with Amazon EKS # This guide shows how to create a cluster in managed.hopsworks.ai with integrated support for Amazon Elastic Kubernetes Service (EKS). So that Hopsworks can launch Python jobs, Jupyter servers, and ML model servings on top of Amazon EKS. Warning In the current version, we don't support sharing EKS clusters between Hopsworks clusters. That is, an EKS cluster can be only used by one Hopsworks cluster. Step 1: Create an EKS cluster on AWS # If you have an existing EKS cluster, skip this step and go directly to Step 2. Amazon provides two getting started guides using AWS management console or eksctl to help you create an EKS cluster. The easiest way is to use the eksctl command. Step 1.1: Installing eksctl, aws, and kubectl # Follow the prerequisites section in getting started with eksctl to install aws, eksctl, and kubectl. Step 1.2: Create an EKS cluster using eksctl # You can create a sample EKS cluster with the name my-eks-cluster using Kubernetes version 1.20 with 2 managed nodes in the us-east-2 region by running the following command. For more details on the eksctl usage, check the eksctl documentation . eksctl create cluster --name my-eks-cluster --version 1 .20 --region us-east-2 --nodegroup-name my-nodes --nodes 2 --managed Output: [ \u2139 ] eksctl version 0 .26.0 [ \u2139 ] using region us-east-2 [ \u2139 ] setting availability zones to [ us-east-2b us-east-2a us-east-2c ] [ \u2139 ] subnets for us-east-2b - public:192.168.0.0/19 private:192.168.96.0/19 [ \u2139 ] subnets for us-east-2a - public:192.168.32.0/19 private:192.168.128.0/19 [ \u2139 ] subnets for us-east-2c - public:192.168.64.0/19 private:192.168.160.0/19 [ \u2139 ] using Kubernetes version 1 .20 [ \u2139 ] creating EKS cluster \"my-eks-cluster\" in \"us-east-2\" region with managed nodes [ \u2139 ] will create 2 separate CloudFormation stacks for cluster itself and the initial managed nodegroup [ \u2139 ] if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-east-2 --cluster=my-eks-cluster' [ \u2139 ] CloudWatch logging will not be enabled for cluster \"my-eks-cluster\" in \"us-east-2\" [ \u2139 ] you can enable it with 'eksctl utils update-cluster-logging --region=us-east-2 --cluster=my-eks-cluster' [ \u2139 ] Kubernetes API endpoint access will use default of { publicAccess = true, privateAccess = false } for cluster \"my-eks-cluster\" in \"us-east-2\" [ \u2139 ] 2 sequential tasks: { create cluster control plane \"my-eks-cluster\" , 2 sequential sub-tasks: { no tasks, create managed nodegroup \"my-nodes\" } } [ \u2139 ] building cluster stack \"eksctl-my-eks-cluster-cluster\" [ \u2139 ] deploying stack \"eksctl-my-eks-cluster-cluster\" [ \u2139 ] building managed nodegroup stack \"eksctl-my-eks-cluster-nodegroup-my-nodes\" [ \u2139 ] deploying stack \"eksctl-my-eks-cluster-nodegroup-my-nodes\" [ \u2139 ] waiting for the control plane availability... [ \u2714 ] saved kubeconfig as \"/Users/maism/.kube/config\" [ \u2139 ] no tasks [ \u2714 ] all EKS cluster resources for \"my-eks-cluster\" have been created [ \u2139 ] nodegroup \"my-nodes\" has 2 node ( s ) [ \u2139 ] node \"ip-192-168-21-142.us-east-2.compute.internal\" is ready [ \u2139 ] node \"ip-192-168-62-117.us-east-2.compute.internal\" is ready [ \u2139 ] waiting for at least 2 node ( s ) to become ready in \"my-nodes\" [ \u2139 ] nodegroup \"my-nodes\" has 2 node ( s ) [ \u2139 ] node \"ip-192-168-21-142.us-east-2.compute.internal\" is ready [ \u2139 ] node \"ip-192-168-62-117.us-east-2.compute.internal\" is ready [ \u2139 ] kubectl command should work with \"/Users/maism/.kube/config\" , try 'kubectl get nodes' [ \u2714 ] EKS cluster \"my-eks-cluster\" in \"us-east-2\" region is ready Once the cluster is created, eksctl will write the cluster credentials for the newly created cluster to your local kubeconfig file (~/.kube/config). To test the cluster credentials, you can run the following command to get the list of nodes in the cluster. kubectl get nodes Output: NAME STATUS ROLES AGE VERSION ip-192-168-21-142.us-east-2.compute.internal Ready <none> 2m35s v1.17.9-eks-4c6976 ip-192-168-62-117.us-east-2.compute.internal Ready <none> 2m34s v1.17.9-eks-4c6976 Step 2: Create an instance profile role on AWS # You need to add permission to the instance profile you use for instances deployed by managed.hopsworks.ai to give them access to EKS. Go to the IAM service in the AWS management console , click Roles , search for your role, and click on it. Click on Add inline policy . Go to the JSON tab and replace the existing JSON permissions with the JSON permissions below.. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"AllowDescirbeEKS\" , \"Effect\" : \"Allow\" , \"Action\" : \"eks:DescribeCluster\" , \"Resource\" : \"arn:aws:eks:*:*:cluster/*\" } ] } Click on Review policy . Give a name to your policy and click on Create policy . Copy the Role ARN of your profile (not to be confused with the Instance Profile ARNs two lines bellow). Copy the *Role ARN* Step 3: Allow your role to use your EKS cluster # You need to configure your EKS cluster to accept connections from the role you created above. This is done by using the following kubectl command. For more details, check Managing users or IAM roles for your cluster . Note The kubectl edit command uses the vi editor by default, however, you can override this behavior by setting KUBE_EDITOR to your preferred editor . KUBE_EDITOR = \"vi\" kubectl edit configmap aws-auth -n kube-system Output: # Please edit the object below. Lines beginning with a '#' will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion: v1 data: mapRoles: | - groups: - system:bootstrappers - system:nodes rolearn: arn:aws:iam::xxxxxxxxxxxx:role/eksctl-my-eks-cluster-nodegroup-m-NodeInstanceRole-FQ7L0HQI4NCC username: system:node: {{ EC2PrivateDNSName }} kind: ConfigMap metadata: creationTimestamp: \"2020-08-24T07:42:31Z\" name: aws-auth namespace: kube-system resourceVersion: \"770\" selfLink: /api/v1/namespaces/kube-system/configmaps/aws-auth uid: c794b2d8-9f10-443d-9072-c65d0f2eb552 Follow the example below (lines 13-16) to add your role to mapRoles and assign the system:masters group to your role. Make sure to replace 'YOUR ROLE RoleARN' with the Role ARN you copied in the previous step before saving. Warning Make sure to keep the same formatting as in the example below. The configuration format is sensitive to indentation and copy-pasting does not always keep the correct indentation. # Please edit the object below. Lines beginning with a '#' will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion: v1 data: mapRoles: | - groups: - system:bootstrappers - system:nodes rolearn: arn:aws:iam::xxxxxxxxxxxx:role/eksctl-my-eks-cluster-nodegroup-m-NodeInstanceRole-FQ7L0HQI4NCC username: system:node: {{ EC2PrivateDNSName }} - groups: - system:masters rolearn: <YOUR ROLE RoleARN> username: hopsworks kind: ConfigMap metadata: creationTimestamp: \"2020-08-24T07:42:31Z\" name: aws-auth namespace: kube-system resourceVersion: \"770\" selfLink: /api/v1/namespaces/kube-system/configmaps/aws-auth uid: c794b2d8-9f10-443d-9072-c65d0f2eb552 Once you are done with editing the configmap, save it and exit the editor. The output should be: configmap/aws-auth edited Step 4: Setup network connectivity # For Hopsworks to be able to start containers in the EKS cluster and for these containers to be able to use Hopsworks we need to establish network connectivity between Hopsworks and EKS. For this, we have two solutions. The first option ( A ) is to run Hopsworks and EKS in the same virtual network and security group. The second option ( B ) is to pair the EKS and Hopsworks virtual networks. If you choose this option, make sure to create the peering before starting the Hopsworks cluster as it connects to EKS at startup. Option A : run Hopsworks and EKS in the same virtual network. # Running EKS and Hopsworks in the same security group is the simplest of the two solutions when it comes to setting up the system. All you need to do is to open the ports needed by Hopsworks in the security group created by the EKS cluster. Then you can just select this security group during the Hopsworks cluster creation. We will now see how to open the ports for HTTP (80) and HTTPS (443) to allow Hopsworks to run with all its functionalities. Note It is possible not to open ports 80 and 443 at the cost of some features. See Limiting permissions for more details. First, you need to get the name of the security group of your EKS cluster by using the following eksctl command. Notice that you need to replace my-eks-cluster with the name of your cluster. eksctl utils describe-stacks --region = us-east-2 --cluster = my-eks-cluster | grep 'OutputKey: \"ClusterSecurityGroupId\"' -a1 Check the output for OutputValue , which will be the id of your EKS security group. ExportName: \"eksctl-my-eks-cluster-cluster::ClusterSecurityGroupId\" , OutputKey: \"ClusterSecurityGroupId\" , OutputValue: \"YOUR_EKS_SECURITY_GROUP_ID\" Go to the Security Groups section of EC2 in the AWS management console and search for your security group using the id obtained above. Note the VPC ID , you will need it when creating the Hopsworks cluster. Then, click on it then go to the Inbound rules tab and click on Edit inbound rules . You should now see the following screen. Edit inbound rules Add two rules for HTTP and HTTPS as follows: Edit inbound rules Click Save rules to save the updated rules to the security group. Option B : create a pairing between Hopsworks and EKS # To establish virtual peering between the Kubernetes cluster and Hopsworks, you need to select or create a virtual network for Hopsworks. You can create the virtual network by following the AWS documentation steps to create the virtual private network . Make sure to configure your subnet to use an address space that does not overlap with the address space in the Kubernetes network. You then need to select or create a security group for the Hopsworks VPC. You can create the security group by following the steps in the AWS documentation . Remember to open the port for HTTP (80) and HTTPS (443) to allow Hopsworks to run with all its functionalities. Note It is possible not to open ports 80 and 443 at the cost of some features. See Limiting permissions for more details. Once the Hopsworks VPC and security group are created you need to create a peering between the Hopsworks VPC and the EKS VPC. For this follow the AWS documentation here . Finally, you need to edit the security groups for the EKS cluster and for Hopsworks to allow full communication between both VPC. This can be done following the instruction in the AWS documentation . You can get the name of the security group of your EKS cluster by using the following eksctl command. Notice that you need to replace my-eks-cluster with the name of your cluster. eksctl utils describe-stacks --region = us-east-2 --cluster = my-eks-cluster | grep 'OutputKey: \"ClusterSecurityGroupId\"' -a1 Check the output for OutputValue , which will be the id of your EKS security group. ExportName: \"eksctl-my-eks-cluster-cluster::ClusterSecurityGroupId\" , OutputKey: \"ClusterSecurityGroupId\" , OutputValue: \"YOUR_EKS_SECURITY_GROUP_ID\" Step 6: Create a Hopsworks cluster with EKS support # In managed.hopsworks.ai , follow the same instructions as in the cluster creation guide except when setting Managed Containers , VPC , Subnet , and Security group . Choose Enabled to enable the use of Amazon EKS: Choose Enabled Add your EKS cluster name, then click Next: Add EKS cluster name If you followed option A when setting up the network Choose the VPC of your EKS cluster. Its name should have the form eksctl-YOUR-CLUSTER-NAME-cluster (click on the refresh button if the VPC is not in the list). If you followed option B choose the VPC you created during Step 4 . Choose VPC Choose any of the subnets in the VPC, then click Next. Note Avoid private subnets if you want to enjoy all the Hopsworks features . Choose Subnet Choose the security group that you have updated/created in Step 4 , then click Next: Note If you followed option A select the Security Group with the same id as in Step 4 and NOT the ones containing ControlPlaneSecurity or ClusterSharedNode in their name. Choose Security Group Click Review and submit , then Create. Once the cluster is created, Hopsworks will use EKS to launch Python jobs, Jupyter servers, and ML model servings.","title":"EKS integration"},{"location":"setup_installation/aws/eks_ecr_integration/#integration-with-amazon-eks","text":"This guide shows how to create a cluster in managed.hopsworks.ai with integrated support for Amazon Elastic Kubernetes Service (EKS). So that Hopsworks can launch Python jobs, Jupyter servers, and ML model servings on top of Amazon EKS. Warning In the current version, we don't support sharing EKS clusters between Hopsworks clusters. That is, an EKS cluster can be only used by one Hopsworks cluster.","title":"Integration with Amazon EKS"},{"location":"setup_installation/aws/eks_ecr_integration/#step-1-create-an-eks-cluster-on-aws","text":"If you have an existing EKS cluster, skip this step and go directly to Step 2. Amazon provides two getting started guides using AWS management console or eksctl to help you create an EKS cluster. The easiest way is to use the eksctl command.","title":"Step 1: Create an EKS cluster on AWS"},{"location":"setup_installation/aws/eks_ecr_integration/#step-11-installing-eksctl-aws-and-kubectl","text":"Follow the prerequisites section in getting started with eksctl to install aws, eksctl, and kubectl.","title":"Step 1.1: Installing eksctl, aws, and kubectl"},{"location":"setup_installation/aws/eks_ecr_integration/#step-12-create-an-eks-cluster-using-eksctl","text":"You can create a sample EKS cluster with the name my-eks-cluster using Kubernetes version 1.20 with 2 managed nodes in the us-east-2 region by running the following command. For more details on the eksctl usage, check the eksctl documentation . eksctl create cluster --name my-eks-cluster --version 1 .20 --region us-east-2 --nodegroup-name my-nodes --nodes 2 --managed Output: [ \u2139 ] eksctl version 0 .26.0 [ \u2139 ] using region us-east-2 [ \u2139 ] setting availability zones to [ us-east-2b us-east-2a us-east-2c ] [ \u2139 ] subnets for us-east-2b - public:192.168.0.0/19 private:192.168.96.0/19 [ \u2139 ] subnets for us-east-2a - public:192.168.32.0/19 private:192.168.128.0/19 [ \u2139 ] subnets for us-east-2c - public:192.168.64.0/19 private:192.168.160.0/19 [ \u2139 ] using Kubernetes version 1 .20 [ \u2139 ] creating EKS cluster \"my-eks-cluster\" in \"us-east-2\" region with managed nodes [ \u2139 ] will create 2 separate CloudFormation stacks for cluster itself and the initial managed nodegroup [ \u2139 ] if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-east-2 --cluster=my-eks-cluster' [ \u2139 ] CloudWatch logging will not be enabled for cluster \"my-eks-cluster\" in \"us-east-2\" [ \u2139 ] you can enable it with 'eksctl utils update-cluster-logging --region=us-east-2 --cluster=my-eks-cluster' [ \u2139 ] Kubernetes API endpoint access will use default of { publicAccess = true, privateAccess = false } for cluster \"my-eks-cluster\" in \"us-east-2\" [ \u2139 ] 2 sequential tasks: { create cluster control plane \"my-eks-cluster\" , 2 sequential sub-tasks: { no tasks, create managed nodegroup \"my-nodes\" } } [ \u2139 ] building cluster stack \"eksctl-my-eks-cluster-cluster\" [ \u2139 ] deploying stack \"eksctl-my-eks-cluster-cluster\" [ \u2139 ] building managed nodegroup stack \"eksctl-my-eks-cluster-nodegroup-my-nodes\" [ \u2139 ] deploying stack \"eksctl-my-eks-cluster-nodegroup-my-nodes\" [ \u2139 ] waiting for the control plane availability... [ \u2714 ] saved kubeconfig as \"/Users/maism/.kube/config\" [ \u2139 ] no tasks [ \u2714 ] all EKS cluster resources for \"my-eks-cluster\" have been created [ \u2139 ] nodegroup \"my-nodes\" has 2 node ( s ) [ \u2139 ] node \"ip-192-168-21-142.us-east-2.compute.internal\" is ready [ \u2139 ] node \"ip-192-168-62-117.us-east-2.compute.internal\" is ready [ \u2139 ] waiting for at least 2 node ( s ) to become ready in \"my-nodes\" [ \u2139 ] nodegroup \"my-nodes\" has 2 node ( s ) [ \u2139 ] node \"ip-192-168-21-142.us-east-2.compute.internal\" is ready [ \u2139 ] node \"ip-192-168-62-117.us-east-2.compute.internal\" is ready [ \u2139 ] kubectl command should work with \"/Users/maism/.kube/config\" , try 'kubectl get nodes' [ \u2714 ] EKS cluster \"my-eks-cluster\" in \"us-east-2\" region is ready Once the cluster is created, eksctl will write the cluster credentials for the newly created cluster to your local kubeconfig file (~/.kube/config). To test the cluster credentials, you can run the following command to get the list of nodes in the cluster. kubectl get nodes Output: NAME STATUS ROLES AGE VERSION ip-192-168-21-142.us-east-2.compute.internal Ready <none> 2m35s v1.17.9-eks-4c6976 ip-192-168-62-117.us-east-2.compute.internal Ready <none> 2m34s v1.17.9-eks-4c6976","title":"Step 1.2: Create an EKS cluster using eksctl"},{"location":"setup_installation/aws/eks_ecr_integration/#step-2-create-an-instance-profile-role-on-aws","text":"You need to add permission to the instance profile you use for instances deployed by managed.hopsworks.ai to give them access to EKS. Go to the IAM service in the AWS management console , click Roles , search for your role, and click on it. Click on Add inline policy . Go to the JSON tab and replace the existing JSON permissions with the JSON permissions below.. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"AllowDescirbeEKS\" , \"Effect\" : \"Allow\" , \"Action\" : \"eks:DescribeCluster\" , \"Resource\" : \"arn:aws:eks:*:*:cluster/*\" } ] } Click on Review policy . Give a name to your policy and click on Create policy . Copy the Role ARN of your profile (not to be confused with the Instance Profile ARNs two lines bellow). Copy the *Role ARN*","title":"Step 2: Create an instance profile role on AWS"},{"location":"setup_installation/aws/eks_ecr_integration/#step-3-allow-your-role-to-use-your-eks-cluster","text":"You need to configure your EKS cluster to accept connections from the role you created above. This is done by using the following kubectl command. For more details, check Managing users or IAM roles for your cluster . Note The kubectl edit command uses the vi editor by default, however, you can override this behavior by setting KUBE_EDITOR to your preferred editor . KUBE_EDITOR = \"vi\" kubectl edit configmap aws-auth -n kube-system Output: # Please edit the object below. Lines beginning with a '#' will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion: v1 data: mapRoles: | - groups: - system:bootstrappers - system:nodes rolearn: arn:aws:iam::xxxxxxxxxxxx:role/eksctl-my-eks-cluster-nodegroup-m-NodeInstanceRole-FQ7L0HQI4NCC username: system:node: {{ EC2PrivateDNSName }} kind: ConfigMap metadata: creationTimestamp: \"2020-08-24T07:42:31Z\" name: aws-auth namespace: kube-system resourceVersion: \"770\" selfLink: /api/v1/namespaces/kube-system/configmaps/aws-auth uid: c794b2d8-9f10-443d-9072-c65d0f2eb552 Follow the example below (lines 13-16) to add your role to mapRoles and assign the system:masters group to your role. Make sure to replace 'YOUR ROLE RoleARN' with the Role ARN you copied in the previous step before saving. Warning Make sure to keep the same formatting as in the example below. The configuration format is sensitive to indentation and copy-pasting does not always keep the correct indentation. # Please edit the object below. Lines beginning with a '#' will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion: v1 data: mapRoles: | - groups: - system:bootstrappers - system:nodes rolearn: arn:aws:iam::xxxxxxxxxxxx:role/eksctl-my-eks-cluster-nodegroup-m-NodeInstanceRole-FQ7L0HQI4NCC username: system:node: {{ EC2PrivateDNSName }} - groups: - system:masters rolearn: <YOUR ROLE RoleARN> username: hopsworks kind: ConfigMap metadata: creationTimestamp: \"2020-08-24T07:42:31Z\" name: aws-auth namespace: kube-system resourceVersion: \"770\" selfLink: /api/v1/namespaces/kube-system/configmaps/aws-auth uid: c794b2d8-9f10-443d-9072-c65d0f2eb552 Once you are done with editing the configmap, save it and exit the editor. The output should be: configmap/aws-auth edited","title":"Step 3: Allow your role to use your EKS cluster"},{"location":"setup_installation/aws/eks_ecr_integration/#step-4-setup-network-connectivity","text":"For Hopsworks to be able to start containers in the EKS cluster and for these containers to be able to use Hopsworks we need to establish network connectivity between Hopsworks and EKS. For this, we have two solutions. The first option ( A ) is to run Hopsworks and EKS in the same virtual network and security group. The second option ( B ) is to pair the EKS and Hopsworks virtual networks. If you choose this option, make sure to create the peering before starting the Hopsworks cluster as it connects to EKS at startup.","title":"Step 4: Setup network connectivity"},{"location":"setup_installation/aws/eks_ecr_integration/#option-a-run-hopsworks-and-eks-in-the-same-virtual-network","text":"Running EKS and Hopsworks in the same security group is the simplest of the two solutions when it comes to setting up the system. All you need to do is to open the ports needed by Hopsworks in the security group created by the EKS cluster. Then you can just select this security group during the Hopsworks cluster creation. We will now see how to open the ports for HTTP (80) and HTTPS (443) to allow Hopsworks to run with all its functionalities. Note It is possible not to open ports 80 and 443 at the cost of some features. See Limiting permissions for more details. First, you need to get the name of the security group of your EKS cluster by using the following eksctl command. Notice that you need to replace my-eks-cluster with the name of your cluster. eksctl utils describe-stacks --region = us-east-2 --cluster = my-eks-cluster | grep 'OutputKey: \"ClusterSecurityGroupId\"' -a1 Check the output for OutputValue , which will be the id of your EKS security group. ExportName: \"eksctl-my-eks-cluster-cluster::ClusterSecurityGroupId\" , OutputKey: \"ClusterSecurityGroupId\" , OutputValue: \"YOUR_EKS_SECURITY_GROUP_ID\" Go to the Security Groups section of EC2 in the AWS management console and search for your security group using the id obtained above. Note the VPC ID , you will need it when creating the Hopsworks cluster. Then, click on it then go to the Inbound rules tab and click on Edit inbound rules . You should now see the following screen. Edit inbound rules Add two rules for HTTP and HTTPS as follows: Edit inbound rules Click Save rules to save the updated rules to the security group.","title":"Option A: run Hopsworks and EKS in the same virtual network."},{"location":"setup_installation/aws/eks_ecr_integration/#option-b-create-a-pairing-between-hopsworks-and-eks","text":"To establish virtual peering between the Kubernetes cluster and Hopsworks, you need to select or create a virtual network for Hopsworks. You can create the virtual network by following the AWS documentation steps to create the virtual private network . Make sure to configure your subnet to use an address space that does not overlap with the address space in the Kubernetes network. You then need to select or create a security group for the Hopsworks VPC. You can create the security group by following the steps in the AWS documentation . Remember to open the port for HTTP (80) and HTTPS (443) to allow Hopsworks to run with all its functionalities. Note It is possible not to open ports 80 and 443 at the cost of some features. See Limiting permissions for more details. Once the Hopsworks VPC and security group are created you need to create a peering between the Hopsworks VPC and the EKS VPC. For this follow the AWS documentation here . Finally, you need to edit the security groups for the EKS cluster and for Hopsworks to allow full communication between both VPC. This can be done following the instruction in the AWS documentation . You can get the name of the security group of your EKS cluster by using the following eksctl command. Notice that you need to replace my-eks-cluster with the name of your cluster. eksctl utils describe-stacks --region = us-east-2 --cluster = my-eks-cluster | grep 'OutputKey: \"ClusterSecurityGroupId\"' -a1 Check the output for OutputValue , which will be the id of your EKS security group. ExportName: \"eksctl-my-eks-cluster-cluster::ClusterSecurityGroupId\" , OutputKey: \"ClusterSecurityGroupId\" , OutputValue: \"YOUR_EKS_SECURITY_GROUP_ID\"","title":"Option B: create a pairing between Hopsworks and EKS"},{"location":"setup_installation/aws/eks_ecr_integration/#step-6-create-a-hopsworks-cluster-with-eks-support","text":"In managed.hopsworks.ai , follow the same instructions as in the cluster creation guide except when setting Managed Containers , VPC , Subnet , and Security group . Choose Enabled to enable the use of Amazon EKS: Choose Enabled Add your EKS cluster name, then click Next: Add EKS cluster name If you followed option A when setting up the network Choose the VPC of your EKS cluster. Its name should have the form eksctl-YOUR-CLUSTER-NAME-cluster (click on the refresh button if the VPC is not in the list). If you followed option B choose the VPC you created during Step 4 . Choose VPC Choose any of the subnets in the VPC, then click Next. Note Avoid private subnets if you want to enjoy all the Hopsworks features . Choose Subnet Choose the security group that you have updated/created in Step 4 , then click Next: Note If you followed option A select the Security Group with the same id as in Step 4 and NOT the ones containing ControlPlaneSecurity or ClusterSharedNode in their name. Choose Security Group Click Review and submit , then Create. Once the cluster is created, Hopsworks will use EKS to launch Python jobs, Jupyter servers, and ML model servings.","title":"Step 6: Create a Hopsworks cluster with EKS support"},{"location":"setup_installation/aws/getting_started/","text":"Getting started with managed.hopsworks.ai (AWS) # Managed.hopsworks.ai is our managed platform for running Hopsworks and the Feature Store in the cloud. It integrates seamlessly with third-party platforms such as Databricks, SageMaker and KubeFlow. This guide shows how to set up managed.hopsworks.ai with your organization's AWS account. Prerequisites # To run the commands in this guide, you must have the AWS CLI installed and configured and your user must have at least the set of permission listed below. See the Getting started guide in the AWS CLI User Guide for more information about installing and configuring the AWS CLI. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"VisualEditor0\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"iam:CreateInstanceProfile\" , \"iam:PassRole\" , \"iam:CreateRole\" , \"iam:PutRolePolicy\" , \"iam:AddRoleToInstanceProfile\" , \"ec2:ImportKeyPair\" , \"ec2:CreateKeyPair\" , \"s3:CreateBucket\" ], \"Resource\" : \"*\" } ] } All the commands have unix-like quotation rules. These commands will need to be adapted to your terminal's quoting rules. See Using quotation marks with strings in the AWS CLI User Guide. All the commands use the default AWS profile. Add the --profile parameter to use another profile. Step 1: Connecting your AWS account # Managed.hopsworks.ai deploys Hopsworks clusters to your AWS account. To enable this you have to permit us to do so. This is done using an AWS cross-account role. In managed.hopsworks.ai click on Connect to AWS or go to Settings and click on Configure next to AWS . This will direct you to a page with the instructions needed to create the Cross account role and set up the connection. Follow the instructions. Note it is possible to limit the permissions that are set up during this phase. For more details see restrictive-permissions . Instructions to create the cross account role Step 2: Creating storage # Note If you prefer using terraform, you can skip this step and the remaining steps, and instead, follow this guide . The Hopsworks clusters deployed by managed.hopsworks.ai store their data in an S3 bucket in your AWS account. To create the bucket run the following command, replacing BUCKET_NAME with the name you want for your bucket and setting the region to the aws region in which you want to run your cluster. Warning The bucket must be in the same region as the hopsworks cluster you are going to run aws s3 mb s3://BUCKET_NAME --region us-east-2 Step 3: Creating Instance profile # Hopsworks cluster nodes need access to certain resources such as the S3 bucket you created above, an ecr repository, and CloudWatch. First, create an instance profile by running: aws iam create-instance-profile --instance-profile-name hopsworksai-instances We will now create a role with the needed permissions for this instance profile. Start by creating a file named assume-role-policy.json containing the following: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"Service\" : \"ec2.amazonaws.com\" }, \"Action\" : \"sts:AssumeRole\" } ] } Run the following to create the role: aws iam create-role --role-name hopsworksai-instances \\ --description \"Role for the hopsworks cluster instances\" \\ --assume-role-policy-document file://assume-role-policy.json Create a file called instances-policy.json containing the following permissions. Replace the following placeholders with their appropiate values BUCKET_NAME - S3 bucket name REGION - region where the cluster is deployed ECR_AWS_ACCOUNT_ID - AWS account id for ECR repositories Note Some of these permissions can be removed. Refer to this guide for more information. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"hopsworksaiInstanceProfile\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"S3:PutObject\" , \"S3:ListBucket\" , \"S3:GetObject\" , \"S3:DeleteObject\" , \"S3:AbortMultipartUpload\" , \"S3:ListBucketMultipartUploads\" , \"S3:PutLifecycleConfiguration\" , \"S3:GetLifecycleConfiguration\" , \"S3:PutBucketVersioning\" , \"S3:GetBucketVersioning\" , \"S3:ListBucketVersions\" , \"S3:DeleteObjectVersion\" ], \"Resource\" : [ \"arn:aws:s3:::BUCKET_NAME/*\" , \"arn:aws:s3:::BUCKET_NAME\" ] }, { \"Sid\" : \"AllowPullImagesFromHopsworkAi\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ecr:GetDownloadUrlForLayer\" , \"ecr:BatchGetImage\" ], \"Resource\" : [ \"arn:aws:ecr:REGION:822623301872:repository/filebeat\" , \"arn:aws:ecr:REGION:822623301872:repository/base\" , \"arn:aws:ecr:REGION:822623301872:repository/onlinefs\" , \"arn:aws:ecr:REGION:822623301872:repository/airflow\" , \"arn:aws:ecr:REGION:822623301872:repository/git\" , \"arn:aws:ecr:REGION:822623301872:repository/testconnector\" , \"arn:aws:ecr:REGION:822623301872:repository/flyingduck\" ] }, { \"Sid\" : \"AllowCreateRespositry\" , \"Effect\" : \"Allow\" , \"Action\" : \"ecr:CreateRepository\" , \"Resource\" : \"*\" }, { \"Sid\" : \"AllowPushandPullImagesToUserRepo\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ecr:GetDownloadUrlForLayer\" , \"ecr:BatchGetImage\" , \"ecr:CompleteLayerUpload\" , \"ecr:UploadLayerPart\" , \"ecr:InitiateLayerUpload\" , \"ecr:BatchCheckLayerAvailability\" , \"ecr:PutImage\" , \"ecr:ListImages\" , \"ecr:BatchDeleteImage\" , \"ecr:GetLifecyclePolicy\" , \"ecr:PutLifecyclePolicy\" , \"ecr:TagResource\" ], \"Resource\" : [ \"arn:aws:ecr:REGION:ECR_AWS_ACCOUNT_ID:repository/*/filebeat\" , \"arn:aws:ecr:REGION:ECR_AWS_ACCOUNT_ID:repository/*/base\" , \"arn:aws:ecr:REGION:ECR_AWS_ACCOUNT_ID:repository/*/onlinefs\" , \"arn:aws:ecr:REGION:ECR_AWS_ACCOUNT_ID:repository/*/airflow\" , \"arn:aws:ecr:REGION:ECR_AWS_ACCOUNT_ID:repository/*/git\" , \"arn:aws:ecr:REGION:ECR_AWS_ACCOUNT_ID:repository/*/testconnector\" , \"arn:aws:ecr:REGION:ECR_AWS_ACCOUNT_ID:repository/*/flyingduck\" ] }, { \"Sid\" : \"AllowGetAuthToken\" , \"Effect\" : \"Allow\" , \"Action\" : \"ecr:GetAuthorizationToken\" , \"Resource\" : \"*\" }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"cloudwatch:PutMetricData\" , \"ec2:DescribeVolumes\" , \"ec2:DescribeTags\" , \"logs:PutLogEvents\" , \"logs:DescribeLogStreams\" , \"logs:DescribeLogGroups\" , \"logs:CreateLogStream\" , \"logs:CreateLogGroup\" ], \"Resource\" : \"*\" }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"ssm:GetParameter\" ], \"Resource\" : \"arn:aws:ssm:*:*:parameter/AmazonCloudWatch-*\" } ] } Attach the permission to the role by running: aws iam put-role-policy --role-name hopsworksai-instances \\ --policy-name hopsworksai-instances \\ --policy-document file://instances-policy.json Finally, attach the role to the instance profile by running: aws iam add-role-to-instance-profile \\ --role-name hopsworksai-instances \\ --instance-profile-name hopsworksai-instances Step 4: Create an SSH key # When deploying clusters, managed.hopsworks.ai installs an ssh key on the cluster's instances so that you can access them if necessary. For this purpose, you need to add an ssh key to your AWS EC2 environment. This can be done in two ways: creating a new key pair or importing an existing key pair . Step 4.1: Create a new key pair # To create a new key pair run the following command replacing REGION by the region in which you want to run the hopsworks cluster. aws ec2 create-key-pair --key-name hopsworksai \\ --region REGION The output is an ASCII version of the private key and key fingerprint. You need to save the key to a file. Step 4.2: Import a key pair # To import an existing key pair run the following command replacing PATH_TO_PUBLIC_KEY by the path to the public key on your machine and REGION by the region in which you want to run the hopsworks cluster. aws ec2 import-key-pair --key-name hopsworskai \\ --public-key-material fileb://PATH_TO_PUBLIC_KEY \\ --region REGION Step 5: Deploying a Hopsworks cluster # In managed.hopsworks.ai , select Create cluster : Create a Hopsworks cluster Select the Region in which you want your cluster to run (1), name your cluster (2). Select the Instance type (3) and Local storage (4) size for the cluster Head node . Check if you want to Enable EBS encryption (5) Enter the name of the S3 bucket (6) you created in step 2 . Note The S3 bucket you are using must be empty. Make sure that the ECR AWS Account Id (7) is correct. It is set by default to the AWS account id where you set the cross-account role and need to match the permissions you set in step 3 . Press Next : Create a Hopsworks cluster, general information Select the number of workers you want to start the cluster with (2). Select the Instance type (3) and Local storage size (4) for the worker nodes . Note It is possible to add or remove workers or to enable autoscaling once the cluster is running. Press Next : Create a Hopsworks cluster, static workers configuration Select the SSH key you created in step 4 : Choose SSH key Select the Instance Profile that you created in step 3 : Choose the instance profile To backup the S3 bucket data when taking a cluster backup we need to set a retention policy for S3. You can deactivate the retention policy by setting this value to 0 but this will block you from taking any backup of your cluster. Choose the retention period in days and click on Review and submit : Choose the backup retention policy Review all information and select Create : Review cluster information The cluster will start. This will take a few minutes: Booting Hopsworks cluster As soon as the cluster has started, you will be able to log in to your new Hopsworks cluster. You will also be able to stop, restart, or terminate the cluster. Running Hopsworks cluster Step 6: Next steps # Check out our other guides for how to get started with Hopsworks and the Feature Store: Make Hopsworks services accessible from outside services Get started with the Hopsworks Feature Store Follow one of our tutorials Follow one of our Guide Code examples and notebooks: hops-examples","title":"Getting Started"},{"location":"setup_installation/aws/getting_started/#getting-started-with-managedhopsworksai-aws","text":"Managed.hopsworks.ai is our managed platform for running Hopsworks and the Feature Store in the cloud. It integrates seamlessly with third-party platforms such as Databricks, SageMaker and KubeFlow. This guide shows how to set up managed.hopsworks.ai with your organization's AWS account.","title":"Getting started with managed.hopsworks.ai (AWS)"},{"location":"setup_installation/aws/getting_started/#prerequisites","text":"To run the commands in this guide, you must have the AWS CLI installed and configured and your user must have at least the set of permission listed below. See the Getting started guide in the AWS CLI User Guide for more information about installing and configuring the AWS CLI. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"VisualEditor0\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"iam:CreateInstanceProfile\" , \"iam:PassRole\" , \"iam:CreateRole\" , \"iam:PutRolePolicy\" , \"iam:AddRoleToInstanceProfile\" , \"ec2:ImportKeyPair\" , \"ec2:CreateKeyPair\" , \"s3:CreateBucket\" ], \"Resource\" : \"*\" } ] } All the commands have unix-like quotation rules. These commands will need to be adapted to your terminal's quoting rules. See Using quotation marks with strings in the AWS CLI User Guide. All the commands use the default AWS profile. Add the --profile parameter to use another profile.","title":"Prerequisites"},{"location":"setup_installation/aws/getting_started/#step-1-connecting-your-aws-account","text":"Managed.hopsworks.ai deploys Hopsworks clusters to your AWS account. To enable this you have to permit us to do so. This is done using an AWS cross-account role. In managed.hopsworks.ai click on Connect to AWS or go to Settings and click on Configure next to AWS . This will direct you to a page with the instructions needed to create the Cross account role and set up the connection. Follow the instructions. Note it is possible to limit the permissions that are set up during this phase. For more details see restrictive-permissions . Instructions to create the cross account role","title":"Step 1: Connecting your AWS account"},{"location":"setup_installation/aws/getting_started/#step-2-creating-storage","text":"Note If you prefer using terraform, you can skip this step and the remaining steps, and instead, follow this guide . The Hopsworks clusters deployed by managed.hopsworks.ai store their data in an S3 bucket in your AWS account. To create the bucket run the following command, replacing BUCKET_NAME with the name you want for your bucket and setting the region to the aws region in which you want to run your cluster. Warning The bucket must be in the same region as the hopsworks cluster you are going to run aws s3 mb s3://BUCKET_NAME --region us-east-2","title":"Step 2: Creating storage"},{"location":"setup_installation/aws/getting_started/#step-3-creating-instance-profile","text":"Hopsworks cluster nodes need access to certain resources such as the S3 bucket you created above, an ecr repository, and CloudWatch. First, create an instance profile by running: aws iam create-instance-profile --instance-profile-name hopsworksai-instances We will now create a role with the needed permissions for this instance profile. Start by creating a file named assume-role-policy.json containing the following: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"Service\" : \"ec2.amazonaws.com\" }, \"Action\" : \"sts:AssumeRole\" } ] } Run the following to create the role: aws iam create-role --role-name hopsworksai-instances \\ --description \"Role for the hopsworks cluster instances\" \\ --assume-role-policy-document file://assume-role-policy.json Create a file called instances-policy.json containing the following permissions. Replace the following placeholders with their appropiate values BUCKET_NAME - S3 bucket name REGION - region where the cluster is deployed ECR_AWS_ACCOUNT_ID - AWS account id for ECR repositories Note Some of these permissions can be removed. Refer to this guide for more information. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"hopsworksaiInstanceProfile\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"S3:PutObject\" , \"S3:ListBucket\" , \"S3:GetObject\" , \"S3:DeleteObject\" , \"S3:AbortMultipartUpload\" , \"S3:ListBucketMultipartUploads\" , \"S3:PutLifecycleConfiguration\" , \"S3:GetLifecycleConfiguration\" , \"S3:PutBucketVersioning\" , \"S3:GetBucketVersioning\" , \"S3:ListBucketVersions\" , \"S3:DeleteObjectVersion\" ], \"Resource\" : [ \"arn:aws:s3:::BUCKET_NAME/*\" , \"arn:aws:s3:::BUCKET_NAME\" ] }, { \"Sid\" : \"AllowPullImagesFromHopsworkAi\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ecr:GetDownloadUrlForLayer\" , \"ecr:BatchGetImage\" ], \"Resource\" : [ \"arn:aws:ecr:REGION:822623301872:repository/filebeat\" , \"arn:aws:ecr:REGION:822623301872:repository/base\" , \"arn:aws:ecr:REGION:822623301872:repository/onlinefs\" , \"arn:aws:ecr:REGION:822623301872:repository/airflow\" , \"arn:aws:ecr:REGION:822623301872:repository/git\" , \"arn:aws:ecr:REGION:822623301872:repository/testconnector\" , \"arn:aws:ecr:REGION:822623301872:repository/flyingduck\" ] }, { \"Sid\" : \"AllowCreateRespositry\" , \"Effect\" : \"Allow\" , \"Action\" : \"ecr:CreateRepository\" , \"Resource\" : \"*\" }, { \"Sid\" : \"AllowPushandPullImagesToUserRepo\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ecr:GetDownloadUrlForLayer\" , \"ecr:BatchGetImage\" , \"ecr:CompleteLayerUpload\" , \"ecr:UploadLayerPart\" , \"ecr:InitiateLayerUpload\" , \"ecr:BatchCheckLayerAvailability\" , \"ecr:PutImage\" , \"ecr:ListImages\" , \"ecr:BatchDeleteImage\" , \"ecr:GetLifecyclePolicy\" , \"ecr:PutLifecyclePolicy\" , \"ecr:TagResource\" ], \"Resource\" : [ \"arn:aws:ecr:REGION:ECR_AWS_ACCOUNT_ID:repository/*/filebeat\" , \"arn:aws:ecr:REGION:ECR_AWS_ACCOUNT_ID:repository/*/base\" , \"arn:aws:ecr:REGION:ECR_AWS_ACCOUNT_ID:repository/*/onlinefs\" , \"arn:aws:ecr:REGION:ECR_AWS_ACCOUNT_ID:repository/*/airflow\" , \"arn:aws:ecr:REGION:ECR_AWS_ACCOUNT_ID:repository/*/git\" , \"arn:aws:ecr:REGION:ECR_AWS_ACCOUNT_ID:repository/*/testconnector\" , \"arn:aws:ecr:REGION:ECR_AWS_ACCOUNT_ID:repository/*/flyingduck\" ] }, { \"Sid\" : \"AllowGetAuthToken\" , \"Effect\" : \"Allow\" , \"Action\" : \"ecr:GetAuthorizationToken\" , \"Resource\" : \"*\" }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"cloudwatch:PutMetricData\" , \"ec2:DescribeVolumes\" , \"ec2:DescribeTags\" , \"logs:PutLogEvents\" , \"logs:DescribeLogStreams\" , \"logs:DescribeLogGroups\" , \"logs:CreateLogStream\" , \"logs:CreateLogGroup\" ], \"Resource\" : \"*\" }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"ssm:GetParameter\" ], \"Resource\" : \"arn:aws:ssm:*:*:parameter/AmazonCloudWatch-*\" } ] } Attach the permission to the role by running: aws iam put-role-policy --role-name hopsworksai-instances \\ --policy-name hopsworksai-instances \\ --policy-document file://instances-policy.json Finally, attach the role to the instance profile by running: aws iam add-role-to-instance-profile \\ --role-name hopsworksai-instances \\ --instance-profile-name hopsworksai-instances","title":"Step 3: Creating Instance profile"},{"location":"setup_installation/aws/getting_started/#step-4-create-an-ssh-key","text":"When deploying clusters, managed.hopsworks.ai installs an ssh key on the cluster's instances so that you can access them if necessary. For this purpose, you need to add an ssh key to your AWS EC2 environment. This can be done in two ways: creating a new key pair or importing an existing key pair .","title":"Step 4: Create an SSH key"},{"location":"setup_installation/aws/getting_started/#step-41-create-a-new-key-pair","text":"To create a new key pair run the following command replacing REGION by the region in which you want to run the hopsworks cluster. aws ec2 create-key-pair --key-name hopsworksai \\ --region REGION The output is an ASCII version of the private key and key fingerprint. You need to save the key to a file.","title":"Step 4.1: Create a new key pair"},{"location":"setup_installation/aws/getting_started/#step-42-import-a-key-pair","text":"To import an existing key pair run the following command replacing PATH_TO_PUBLIC_KEY by the path to the public key on your machine and REGION by the region in which you want to run the hopsworks cluster. aws ec2 import-key-pair --key-name hopsworskai \\ --public-key-material fileb://PATH_TO_PUBLIC_KEY \\ --region REGION","title":"Step 4.2: Import a key pair"},{"location":"setup_installation/aws/getting_started/#step-5-deploying-a-hopsworks-cluster","text":"In managed.hopsworks.ai , select Create cluster : Create a Hopsworks cluster Select the Region in which you want your cluster to run (1), name your cluster (2). Select the Instance type (3) and Local storage (4) size for the cluster Head node . Check if you want to Enable EBS encryption (5) Enter the name of the S3 bucket (6) you created in step 2 . Note The S3 bucket you are using must be empty. Make sure that the ECR AWS Account Id (7) is correct. It is set by default to the AWS account id where you set the cross-account role and need to match the permissions you set in step 3 . Press Next : Create a Hopsworks cluster, general information Select the number of workers you want to start the cluster with (2). Select the Instance type (3) and Local storage size (4) for the worker nodes . Note It is possible to add or remove workers or to enable autoscaling once the cluster is running. Press Next : Create a Hopsworks cluster, static workers configuration Select the SSH key you created in step 4 : Choose SSH key Select the Instance Profile that you created in step 3 : Choose the instance profile To backup the S3 bucket data when taking a cluster backup we need to set a retention policy for S3. You can deactivate the retention policy by setting this value to 0 but this will block you from taking any backup of your cluster. Choose the retention period in days and click on Review and submit : Choose the backup retention policy Review all information and select Create : Review cluster information The cluster will start. This will take a few minutes: Booting Hopsworks cluster As soon as the cluster has started, you will be able to log in to your new Hopsworks cluster. You will also be able to stop, restart, or terminate the cluster. Running Hopsworks cluster","title":"Step 5: Deploying a Hopsworks cluster"},{"location":"setup_installation/aws/getting_started/#step-6-next-steps","text":"Check out our other guides for how to get started with Hopsworks and the Feature Store: Make Hopsworks services accessible from outside services Get started with the Hopsworks Feature Store Follow one of our tutorials Follow one of our Guide Code examples and notebooks: hops-examples","title":"Step 6: Next steps"},{"location":"setup_installation/aws/instance_profile_permissions/","text":"Replace the following placeholders with their appropiate values BUCKET_NAME - S3 bucket name REGION - region where the cluster is deployed ECR_AWS_ACCOUNT_ID - AWS account id for ECR repositories Note Some of these permissions can be removed. Refer to this guide for more information. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"hopsworksaiInstanceProfile\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"S3:PutObject\" , \"S3:ListBucket\" , \"S3:GetObject\" , \"S3:DeleteObject\" , \"S3:AbortMultipartUpload\" , \"S3:ListBucketMultipartUploads\" , \"S3:PutLifecycleConfiguration\" , \"S3:GetLifecycleConfiguration\" , \"S3:PutBucketVersioning\" , \"S3:GetBucketVersioning\" , \"S3:ListBucketVersions\" , \"S3:DeleteObjectVersion\" ], \"Resource\" : [ \"arn:aws:s3:::BUCKET_NAME/*\" , \"arn:aws:s3:::BUCKET_NAME\" ] }, { \"Sid\" : \"AllowPullImagesFromHopsworkAi\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ecr:GetDownloadUrlForLayer\" , \"ecr:BatchGetImage\" ], \"Resource\" : [ \"arn:aws:ecr:REGION:822623301872:repository/filebeat\" , \"arn:aws:ecr:REGION:822623301872:repository/base\" , \"arn:aws:ecr:REGION:822623301872:repository/onlinefs\" , \"arn:aws:ecr:REGION:822623301872:repository/airflow\" , \"arn:aws:ecr:REGION:822623301872:repository/git\" , \"arn:aws:ecr:REGION:822623301872:repository/testconnector\" , \"arn:aws:ecr:REGION:822623301872:repository/flyingduck\" ] }, { \"Sid\" : \"AllowCreateRespositry\" , \"Effect\" : \"Allow\" , \"Action\" : \"ecr:CreateRepository\" , \"Resource\" : \"*\" }, { \"Sid\" : \"AllowPushandPullImagesToUserRepo\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ecr:GetDownloadUrlForLayer\" , \"ecr:BatchGetImage\" , \"ecr:CompleteLayerUpload\" , \"ecr:UploadLayerPart\" , \"ecr:InitiateLayerUpload\" , \"ecr:BatchCheckLayerAvailability\" , \"ecr:PutImage\" , \"ecr:ListImages\" , \"ecr:BatchDeleteImage\" , \"ecr:GetLifecyclePolicy\" , \"ecr:PutLifecyclePolicy\" , \"ecr:TagResource\" ], \"Resource\" : [ \"arn:aws:ecr:REGION:ECR_AWS_ACCOUNT_ID:repository/*/filebeat\" , \"arn:aws:ecr:REGION:ECR_AWS_ACCOUNT_ID:repository/*/base\" , \"arn:aws:ecr:REGION:ECR_AWS_ACCOUNT_ID:repository/*/onlinefs\" , \"arn:aws:ecr:REGION:ECR_AWS_ACCOUNT_ID:repository/*/airflow\" , \"arn:aws:ecr:REGION:ECR_AWS_ACCOUNT_ID:repository/*/git\" , \"arn:aws:ecr:REGION:ECR_AWS_ACCOUNT_ID:repository/*/testconnector\" , \"arn:aws:ecr:REGION:ECR_AWS_ACCOUNT_ID:repository/*/flyingduck\" ] }, { \"Sid\" : \"AllowGetAuthToken\" , \"Effect\" : \"Allow\" , \"Action\" : \"ecr:GetAuthorizationToken\" , \"Resource\" : \"*\" }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"cloudwatch:PutMetricData\" , \"ec2:DescribeVolumes\" , \"ec2:DescribeTags\" , \"logs:PutLogEvents\" , \"logs:DescribeLogStreams\" , \"logs:DescribeLogGroups\" , \"logs:CreateLogStream\" , \"logs:CreateLogGroup\" ], \"Resource\" : \"*\" }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"ssm:GetParameter\" ], \"Resource\" : \"arn:aws:ssm:*:*:parameter/AmazonCloudWatch-*\" } ] }","title":"Instance profile permissions"},{"location":"setup_installation/aws/restrictive_permissions/","text":"Limiting AWS permissions # Managed.hopsworks.ai requires a set of permissions to be able to manage resources in the user\u2019s AWS account. By default, these permissions are set to easily allow a wide range of different configurations and allow us to automate as many steps as possible. While we ensure to never access resources we shouldn\u2019t, we do understand that this might not be enough for your organization or security policy. This guide explains how to lock down AWS permissions following the IT security policy principle of least privilege allowing managed.hopsworks.ai to only access resources in a specific VPC. Default permissions # This is the list of default permissions that are required by managed.hopsworks.ai . If you prefer to limit these permissions, then proceed to the next section . { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"ListResources\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:DescribeInstances\" , \"ec2:DescribeVpcs\" , \"ec2:DescribeVolumes\" , \"ec2:DescribeSubnets\" , \"ec2:DescribeKeyPairs\" , \"ec2:DescribeInstanceStatus\" , \"iam:ListInstanceProfiles\" , \"ec2:DescribeSecurityGroups\" , \"ec2:DescribeVpcAttribute\" , \"ec2:DescribeRouteTables\" , \"iam:GetInstanceProfile\" ], \"Resource\" : \"*\" }, { \"Sid\" : \"IAMPassRoleToInstance\" , \"Effect\" : \"Allow\" , \"Action\" : \"iam:PassRole\" , \"Resource\" : \"*\" }, { \"Sid\" : \"AllowInstanceActions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:RunInstances\" , \"ec2:StopInstances\" , \"ec2:TerminateInstances\" , \"ec2:StartInstances\" , \"ec2:CreateTags\" , \"ec2:AssociateIamInstanceProfile\" ], \"Resource\" : \"*\" }, { \"Sid\" : \"AllowNetworkRessourcesCreation\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:CreateVpc\" , \"ec2:ModifyVpcAttribute\" , \"ec2:CreateInternetGateway\" , \"ec2:AttachInternetGateway\" , \"ec2:CreateSubnet\" , \"ec2:CreateSecurityGroup\" , \"ec2:CreateRouteTable\" , \"ec2:CreateRoute\" , \"ec2:AssociateRouteTable\" ], \"Resource\" : \"*\" }, { \"Sid\" : \"AllowNetworkRessourcesDeletion\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:DeleteSubnet\" , \"ec2:DeleteRouteTable\" , \"ec2:DeleteInternetGateway\" , \"ec2:DeleteRoute\" , \"ec2:DetachInternetGateway\" , \"ec2:DisassociateRouteTable\" , \"ec2:DeleteSecurityGroup\" , \"ec2:DeleteVpc\" ], \"Resource\" : \"*\" }, { \"Sid\" : \"AllowOpeningPorts\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:AuthorizeSecurityGroupIngress\" , \"ec2:RevokeSecurityGroupIngress\" ], \"Resource\" : \"*\" }, { \"Sid\" : \"AllowTestingOfInstanceRole\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"iam:SimulatePrincipalPolicy\" ], \"Resource\" : \"*\" }, { \"Sid\" : \"HopsworksAIBackup\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:RegisterImage\" , \"ec2:DeregisterImage\" , \"ec2:DescribeImages\" , \"ec2:CreateSnapshot\" , \"ec2:DeleteSnapshot\" , \"ec2:DescribeSnapshots\" ], \"Resource\" : \"*\" }, { \"Sid\" : \"HopsworksAIModifyInstanceType\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:ModifyInstanceAttribute\" ], \"Resource\" : \"*\" }, { \"Sid\" : \"ManageLoadBalancersForExternalAccess\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"elasticloadbalancing:CreateLoadBalancer\" , \"elasticloadbalancing:CreateListener\" , \"elasticloadbalancing:CreateTargetGroup\" , \"elasticloadbalancing:RegisterTargets\" , \"elasticloadbalancing:AddTags\" , \"elasticloadbalancing:DescribeTargetGroups\" , \"elasticloadbalancing:DeleteLoadBalancer\" , \"elasticloadbalancing:DeleteTargetGroup\" ], \"Resource\" : \"*\" } ] } Limiting the cross-account role permissions # Step 1: Create a VPC # To restrict managed.hopsworks.ai from accessing resources outside of a specific VPC, you need to create a new VPC connected to an Internet Gateway. This can be achieved in the AWS Management Console following this guide: Create the VPC . Follow the steps of Create a VPC, subnets, and other VPC resources , naming your vpc and changing the Number of Availability Zones to 1 are the only changes you need to make to the default configuration. Alternatively, an existing VPC such as the default VPC can be used and managed.hopsworks.ai will be restricted to this VPC. Note the VPC ID of the VPC you want to use for the following steps. Note Make sure you enable DNS hostnames for your VPC Note If you use VPC endpoints to managed access to services such as S3 and ECR you need to ensure that the endpoints provide the same permissions as set in the instance profile After you have created the VPC either Create a Security Group or use VPC's default. Make sure that the VPC allow the following traffic. Inbound traffic # It is imperative that the Security Group allows Inbound traffic from any Instance within the same Security Group in any (TCP) port. All VMs of the Cluster should be able to communicate with each other. It is also recommended to open TCP port 80 to sign the certificates. If you do not open port 80 you will have to use a self-signed certificate in your Hopsworks cluster. This can be done by checking the Continue with self-signed certificate check box in the Security Group step of the cluster creation. We recommend configuring the Network ACLs to be open to all inbound traffic and let the security group handle the access restriction. But if you want to set limitations at the Network ACLs level, they must be configured so that at least the TCP ephemeral port 32768 - 65535 are open to the internet (this is so that outbound trafic can receive answers). It is also recommended to open TCP port 80 to sign the certificates. If you do not open port 80 you will have to use a self-signed certificate in your Hopsworks cluster. This can be done by checking the Continue with self-signed certificate check box in the Security Group step of the cluster creation. Outbound traffic # Clusters created on managed.hopsworks.ai need to be able to send http requests to api.hopsworks.ai . The api.hopsworks.ai domain use a content delivery network for better performance. This result in the impossibility to predict which IP the request will be sent to. If you require a list of static IPs to allow outbound traffic from your security group, use the static IPs option during cluster creation . Similar to Inbound traffic, the Security Group in place must allow Outbound traffic in any (TCP) port towards any VM withing the same Security Group. Note If you intend to use the managed users option on your Hopsworks cluster you should also allow outbound traffic to cognito-idp.us-east-2.amazonaws.com and managedhopsworks-prod.auth.us-east-2.amazoncognito.com . Step 2: Create an instance profile # You need to create an instance profile that will identify all instances started by managed.hopsworks.ai . Follow this guide to create a role to be used by EC2 with no permissions attached: Creating a Role for an AWS Service (Console) . Take note of the ARN of the role you just created. You will need to add permissions to the instance profile to give access to the S3 bucket where Hopsworks will store its data. For more details about these permissions check our guide here . Check bellow for more information on restricting the permissions given the instance profile. Step 3: Set permissions of the cross-account role # During the account setup for managed.hopsworks.ai , you were asked to create and provide a cross-account role. If you don\u2019t remember which role you used then you can find it in Settings/Account Settings in managed.hopsworks.ai . Edit this role in the AWS Management Console and overwrite the existing inline policy with the following policy. Note that you have to replace [INSTANCE_PROFILE_NAME] and [VPC_ID] for multiple occurrences in the given policy. If you want to learn more about how this policy works check out: How to Help Lock Down a User\u2019s Amazon EC2 Capabilities to a Single VPC . { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"NonResourceBasedPermissions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:DescribeInstances\" , \"ec2:DescribeVpcs\" , \"ec2:DescribeVolumes\" , \"ec2:DescribeSubnets\" , \"ec2:DescribeKeyPairs\" , \"ec2:DescribeInstanceStatus\" , \"iam:ListInstanceProfiles\" , \"ec2:DescribeSecurityGroups\" , \"ec2:DescribeVpcAttribute\" , \"ec2:DescribeRouteTables\" ], \"Resource\" : \"*\" }, { \"Sid\" : \"IAMPassRoleToInstance\" , \"Effect\" : \"Allow\" , \"Action\" : \"iam:PassRole\" , \"Resource\" : \"arn:aws:iam::*:role/[INSTANCE_PROFILE_NAME]\" }, { \"Sid\" : \"EC2RunInstancesOnlyWithGivenRole\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:RunInstances\" , \"Resource\" : \"arn:aws:ec2:*:*:instance/*\" , \"Condition\" : { \"ArnLike\" : { \"ec2:InstanceProfile\" : \"arn:aws:iam::*:instance-profile/[INSTANCE_PROFILE_NAME]\" } } }, { \"Sid\" : \"EC2RunInstancesOnlyInGivenVpc\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:RunInstances\" , \"Resource\" : \"arn:aws:ec2:*:*:subnet/*\" , \"Condition\" : { \"ArnLike\" : { \"ec2:vpc\" : \"arn:aws:ec2:*:*:vpc/[VPC_ID]\" } } }, { \"Sid\" : \"AllowInstanceActions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:StopInstances\" , \"ec2:TerminateInstances\" , \"ec2:StartInstances\" , \"ec2:CreateTags\" , \"ec2:AssociateIamInstanceProfile\" ], \"Resource\" : \"arn:aws:ec2:*:*:instance/*\" , \"Condition\" : { \"ArnLike\" : { \"ec2:InstanceProfile\" : \"arn:aws:iam::*:instance-profile/[INSTANCE_PROFILE_NAME]\" } } }, { \"Sid\" : \"RemainingRunInstancePermissions\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:RunInstances\" , \"Resource\" : [ \"arn:aws:ec2:*:*:volume/*\" , \"arn:aws:ec2:*::image/*\" , \"arn:aws:ec2:*::snapshot/*\" , \"arn:aws:ec2:*:*:network-interface/*\" , \"arn:aws:ec2:*:*:key-pair/*\" , \"arn:aws:ec2:*:*:security-group/*\" ] }, { \"Sid\" : \"EC2VpcNonResourceSpecificActions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:AuthorizeSecurityGroupIngress\" , \"ec2:RevokeSecurityGroupIngress\" ], \"Resource\" : \"*\" , \"Condition\" : { \"ArnLike\" : { \"ec2:vpc\" : \"arn:aws:ec2:*:*:vpc/[VPC_ID]\" } } }, { \"Sid\" : \"EC2BackupCreation\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:RegisterImage\" , \"ec2:DeregisterImage\" , \"ec2:DescribeImages\" , \"ec2:CreateSnapshot\" , \"ec2:DeleteSnapshot\" , \"ec2:DescribeSnapshots\" ], \"Resource\" : \"*\" } ] } Step 4: Create your Hopsworks instance # You can now create a new Hopsworks instance in managed.hopsworks.ai by selecting the configured instance profile, VPC and security group during instance configuration. Selecting any other VPCs or instance profiles will result in permissions errors. Step 5: Supporting multiple VPCs # The policy can be extended to give managed.hopsworks.ai access to multiple VPCs. See: Creating a Condition with Multiple Keys or Values . Other removable permissions # There are other permissions that are required by Hopsworks to provide certain product capabilities to the users. In this section, we go through these permissions and what are the implication or removing them. Backup permissions # The following permissions are only needed for the backup feature. You can remove them if you are not going to create backups or if you do not have access to this Enterprise feature. { \"Sid\" : \"EC2BackupCreation\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:RegisterImage\" , \"ec2:DeregisterImage\" , \"ec2:DescribeImages\" , \"ec2:CreateSnapshot\" , \"ec2:DeleteSnapshot\" , \"ec2:DescribeSnapshots\" ], \"Resource\" : \"*\" , } Early warnings if VPC is not configured correctly # The following permissions are needed to give an early warning if your VPC and security groups are badly configured. You can remove them if you don't need it. \"ec2:DescribeVpcAttribute\" , \"ec2:DescribeRouteTables\" Open and close ports from within Hopsworks.ai # The following permissions are used to let you close and open ports on your cluster from hopswork.ai, you can remove them if you do not want to open ports on your cluster or if you want to manually open ports in EC2. { \"Sid\" : \"EC2VpcNonResourceSpecificActions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:AuthorizeSecurityGroupIngress\" , \"ec2:RevokeSecurityGroupIngress\" , ], \"Resource\" : \"*\" , \"Condition\" : { \"ArnLike\" : { \"ec2:vpc\" : \"arn:aws:ec2:*:*:vpc/[VPC_ID]\" } } } Non resource based permissions used for listing # If you are using terraform, then you can also remove most of the Describe permissions in NonResourceBasedPermissions and use the following permissions instead { \"Sid\" : \"NonResourceBasedPermissions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:DescribeInstances\" , \"ec2:DescribeVolumes\" , \"ec2:DescribeSecurityGroups\" ], \"Resource\" : \"*\" }, Load balancers permissions for external access # If you plan to access your Hopsworks cluster from an external python environment, especially if you plan to use the ArrowFlight with DuckDB , then it is required to create a network load balancer that forward requests to the ArrowFlight server(s) co-located with the RonDB MySQL Server(s). If you are not planning to use ArrowFlight server(s) or multiple mysql server(s), you can skip adding the following permissions. If you still wish to use the ArrowFlight server(s) but without adding the following permissions to your cross account role, check this advanced terraform example for more details . { \"Sid\" : \"ManageLoadBalancersForExternalAccess\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"elasticloadbalancing:CreateLoadBalancer\" , \"elasticloadbalancing:CreateListener\" , \"elasticloadbalancing:CreateTargetGroup\" , \"elasticloadbalancing:RegisterTargets\" , \"elasticloadbalancing:AddTags\" , \"elasticloadbalancing:DescribeTargetGroups\" , \"elasticloadbalancing:DeleteLoadBalancer\" , \"elasticloadbalancing:DeleteTargetGroup\" ], \"Resource\" : \"*\" } Limiting the instance profile permissions # Backups # If you do not intend to take backups or if you do not have access to this Enterprise feature you can remove the permissions that are only used by the backup feature when configuring instance profile permissions . For this remove the following permissions from the instance profile: \"S3:PutLifecycleConfiguration\" , \"S3:GetLifecycleConfiguration\" , \"S3:PutBucketVersioning\" , \"S3:ListBucketVersions\" , \"S3:DeleteObjectVersion\" , CloudWatch Logs # Hopsworks put its logs in Amazon CloudWatch so that you can access them without having to ssh into the machine. If you are not interested in this feature you can remove the following from your instance profile policy: { \"Effect\" : \"Allow\" , \"Action\" : [ \"cloudwatch:PutMetricData\" , \"ec2:DescribeVolumes\" , \"ec2:DescribeTags\" , \"logs:PutLogEvents\" , \"logs:DescribeLogStreams\" , \"logs:DescribeLogGroups\" , \"logs:CreateLogStream\" , \"logs:CreateLogGroup\" ], \"Resource\" : \"*\" }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"ssm:GetParameter\" ], \"Resource\" : \"arn:aws:ssm:*:*:parameter/AmazonCloudWatch-*\" }","title":"Limiting Permissions"},{"location":"setup_installation/aws/restrictive_permissions/#limiting-aws-permissions","text":"Managed.hopsworks.ai requires a set of permissions to be able to manage resources in the user\u2019s AWS account. By default, these permissions are set to easily allow a wide range of different configurations and allow us to automate as many steps as possible. While we ensure to never access resources we shouldn\u2019t, we do understand that this might not be enough for your organization or security policy. This guide explains how to lock down AWS permissions following the IT security policy principle of least privilege allowing managed.hopsworks.ai to only access resources in a specific VPC.","title":"Limiting AWS permissions"},{"location":"setup_installation/aws/restrictive_permissions/#default-permissions","text":"This is the list of default permissions that are required by managed.hopsworks.ai . If you prefer to limit these permissions, then proceed to the next section . { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"ListResources\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:DescribeInstances\" , \"ec2:DescribeVpcs\" , \"ec2:DescribeVolumes\" , \"ec2:DescribeSubnets\" , \"ec2:DescribeKeyPairs\" , \"ec2:DescribeInstanceStatus\" , \"iam:ListInstanceProfiles\" , \"ec2:DescribeSecurityGroups\" , \"ec2:DescribeVpcAttribute\" , \"ec2:DescribeRouteTables\" , \"iam:GetInstanceProfile\" ], \"Resource\" : \"*\" }, { \"Sid\" : \"IAMPassRoleToInstance\" , \"Effect\" : \"Allow\" , \"Action\" : \"iam:PassRole\" , \"Resource\" : \"*\" }, { \"Sid\" : \"AllowInstanceActions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:RunInstances\" , \"ec2:StopInstances\" , \"ec2:TerminateInstances\" , \"ec2:StartInstances\" , \"ec2:CreateTags\" , \"ec2:AssociateIamInstanceProfile\" ], \"Resource\" : \"*\" }, { \"Sid\" : \"AllowNetworkRessourcesCreation\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:CreateVpc\" , \"ec2:ModifyVpcAttribute\" , \"ec2:CreateInternetGateway\" , \"ec2:AttachInternetGateway\" , \"ec2:CreateSubnet\" , \"ec2:CreateSecurityGroup\" , \"ec2:CreateRouteTable\" , \"ec2:CreateRoute\" , \"ec2:AssociateRouteTable\" ], \"Resource\" : \"*\" }, { \"Sid\" : \"AllowNetworkRessourcesDeletion\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:DeleteSubnet\" , \"ec2:DeleteRouteTable\" , \"ec2:DeleteInternetGateway\" , \"ec2:DeleteRoute\" , \"ec2:DetachInternetGateway\" , \"ec2:DisassociateRouteTable\" , \"ec2:DeleteSecurityGroup\" , \"ec2:DeleteVpc\" ], \"Resource\" : \"*\" }, { \"Sid\" : \"AllowOpeningPorts\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:AuthorizeSecurityGroupIngress\" , \"ec2:RevokeSecurityGroupIngress\" ], \"Resource\" : \"*\" }, { \"Sid\" : \"AllowTestingOfInstanceRole\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"iam:SimulatePrincipalPolicy\" ], \"Resource\" : \"*\" }, { \"Sid\" : \"HopsworksAIBackup\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:RegisterImage\" , \"ec2:DeregisterImage\" , \"ec2:DescribeImages\" , \"ec2:CreateSnapshot\" , \"ec2:DeleteSnapshot\" , \"ec2:DescribeSnapshots\" ], \"Resource\" : \"*\" }, { \"Sid\" : \"HopsworksAIModifyInstanceType\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:ModifyInstanceAttribute\" ], \"Resource\" : \"*\" }, { \"Sid\" : \"ManageLoadBalancersForExternalAccess\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"elasticloadbalancing:CreateLoadBalancer\" , \"elasticloadbalancing:CreateListener\" , \"elasticloadbalancing:CreateTargetGroup\" , \"elasticloadbalancing:RegisterTargets\" , \"elasticloadbalancing:AddTags\" , \"elasticloadbalancing:DescribeTargetGroups\" , \"elasticloadbalancing:DeleteLoadBalancer\" , \"elasticloadbalancing:DeleteTargetGroup\" ], \"Resource\" : \"*\" } ] }","title":"Default permissions"},{"location":"setup_installation/aws/restrictive_permissions/#limiting-the-cross-account-role-permissions","text":"","title":"Limiting the cross-account role permissions"},{"location":"setup_installation/aws/restrictive_permissions/#step-1-create-a-vpc","text":"To restrict managed.hopsworks.ai from accessing resources outside of a specific VPC, you need to create a new VPC connected to an Internet Gateway. This can be achieved in the AWS Management Console following this guide: Create the VPC . Follow the steps of Create a VPC, subnets, and other VPC resources , naming your vpc and changing the Number of Availability Zones to 1 are the only changes you need to make to the default configuration. Alternatively, an existing VPC such as the default VPC can be used and managed.hopsworks.ai will be restricted to this VPC. Note the VPC ID of the VPC you want to use for the following steps. Note Make sure you enable DNS hostnames for your VPC Note If you use VPC endpoints to managed access to services such as S3 and ECR you need to ensure that the endpoints provide the same permissions as set in the instance profile After you have created the VPC either Create a Security Group or use VPC's default. Make sure that the VPC allow the following traffic.","title":"Step 1: Create a VPC"},{"location":"setup_installation/aws/restrictive_permissions/#inbound-traffic","text":"It is imperative that the Security Group allows Inbound traffic from any Instance within the same Security Group in any (TCP) port. All VMs of the Cluster should be able to communicate with each other. It is also recommended to open TCP port 80 to sign the certificates. If you do not open port 80 you will have to use a self-signed certificate in your Hopsworks cluster. This can be done by checking the Continue with self-signed certificate check box in the Security Group step of the cluster creation. We recommend configuring the Network ACLs to be open to all inbound traffic and let the security group handle the access restriction. But if you want to set limitations at the Network ACLs level, they must be configured so that at least the TCP ephemeral port 32768 - 65535 are open to the internet (this is so that outbound trafic can receive answers). It is also recommended to open TCP port 80 to sign the certificates. If you do not open port 80 you will have to use a self-signed certificate in your Hopsworks cluster. This can be done by checking the Continue with self-signed certificate check box in the Security Group step of the cluster creation.","title":"Inbound traffic"},{"location":"setup_installation/aws/restrictive_permissions/#outbound-traffic","text":"Clusters created on managed.hopsworks.ai need to be able to send http requests to api.hopsworks.ai . The api.hopsworks.ai domain use a content delivery network for better performance. This result in the impossibility to predict which IP the request will be sent to. If you require a list of static IPs to allow outbound traffic from your security group, use the static IPs option during cluster creation . Similar to Inbound traffic, the Security Group in place must allow Outbound traffic in any (TCP) port towards any VM withing the same Security Group. Note If you intend to use the managed users option on your Hopsworks cluster you should also allow outbound traffic to cognito-idp.us-east-2.amazonaws.com and managedhopsworks-prod.auth.us-east-2.amazoncognito.com .","title":"Outbound traffic"},{"location":"setup_installation/aws/restrictive_permissions/#step-2-create-an-instance-profile","text":"You need to create an instance profile that will identify all instances started by managed.hopsworks.ai . Follow this guide to create a role to be used by EC2 with no permissions attached: Creating a Role for an AWS Service (Console) . Take note of the ARN of the role you just created. You will need to add permissions to the instance profile to give access to the S3 bucket where Hopsworks will store its data. For more details about these permissions check our guide here . Check bellow for more information on restricting the permissions given the instance profile.","title":"Step 2: Create an instance profile"},{"location":"setup_installation/aws/restrictive_permissions/#step-3-set-permissions-of-the-cross-account-role","text":"During the account setup for managed.hopsworks.ai , you were asked to create and provide a cross-account role. If you don\u2019t remember which role you used then you can find it in Settings/Account Settings in managed.hopsworks.ai . Edit this role in the AWS Management Console and overwrite the existing inline policy with the following policy. Note that you have to replace [INSTANCE_PROFILE_NAME] and [VPC_ID] for multiple occurrences in the given policy. If you want to learn more about how this policy works check out: How to Help Lock Down a User\u2019s Amazon EC2 Capabilities to a Single VPC . { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"NonResourceBasedPermissions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:DescribeInstances\" , \"ec2:DescribeVpcs\" , \"ec2:DescribeVolumes\" , \"ec2:DescribeSubnets\" , \"ec2:DescribeKeyPairs\" , \"ec2:DescribeInstanceStatus\" , \"iam:ListInstanceProfiles\" , \"ec2:DescribeSecurityGroups\" , \"ec2:DescribeVpcAttribute\" , \"ec2:DescribeRouteTables\" ], \"Resource\" : \"*\" }, { \"Sid\" : \"IAMPassRoleToInstance\" , \"Effect\" : \"Allow\" , \"Action\" : \"iam:PassRole\" , \"Resource\" : \"arn:aws:iam::*:role/[INSTANCE_PROFILE_NAME]\" }, { \"Sid\" : \"EC2RunInstancesOnlyWithGivenRole\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:RunInstances\" , \"Resource\" : \"arn:aws:ec2:*:*:instance/*\" , \"Condition\" : { \"ArnLike\" : { \"ec2:InstanceProfile\" : \"arn:aws:iam::*:instance-profile/[INSTANCE_PROFILE_NAME]\" } } }, { \"Sid\" : \"EC2RunInstancesOnlyInGivenVpc\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:RunInstances\" , \"Resource\" : \"arn:aws:ec2:*:*:subnet/*\" , \"Condition\" : { \"ArnLike\" : { \"ec2:vpc\" : \"arn:aws:ec2:*:*:vpc/[VPC_ID]\" } } }, { \"Sid\" : \"AllowInstanceActions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:StopInstances\" , \"ec2:TerminateInstances\" , \"ec2:StartInstances\" , \"ec2:CreateTags\" , \"ec2:AssociateIamInstanceProfile\" ], \"Resource\" : \"arn:aws:ec2:*:*:instance/*\" , \"Condition\" : { \"ArnLike\" : { \"ec2:InstanceProfile\" : \"arn:aws:iam::*:instance-profile/[INSTANCE_PROFILE_NAME]\" } } }, { \"Sid\" : \"RemainingRunInstancePermissions\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:RunInstances\" , \"Resource\" : [ \"arn:aws:ec2:*:*:volume/*\" , \"arn:aws:ec2:*::image/*\" , \"arn:aws:ec2:*::snapshot/*\" , \"arn:aws:ec2:*:*:network-interface/*\" , \"arn:aws:ec2:*:*:key-pair/*\" , \"arn:aws:ec2:*:*:security-group/*\" ] }, { \"Sid\" : \"EC2VpcNonResourceSpecificActions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:AuthorizeSecurityGroupIngress\" , \"ec2:RevokeSecurityGroupIngress\" ], \"Resource\" : \"*\" , \"Condition\" : { \"ArnLike\" : { \"ec2:vpc\" : \"arn:aws:ec2:*:*:vpc/[VPC_ID]\" } } }, { \"Sid\" : \"EC2BackupCreation\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:RegisterImage\" , \"ec2:DeregisterImage\" , \"ec2:DescribeImages\" , \"ec2:CreateSnapshot\" , \"ec2:DeleteSnapshot\" , \"ec2:DescribeSnapshots\" ], \"Resource\" : \"*\" } ] }","title":"Step 3: Set permissions of the cross-account role"},{"location":"setup_installation/aws/restrictive_permissions/#step-4-create-your-hopsworks-instance","text":"You can now create a new Hopsworks instance in managed.hopsworks.ai by selecting the configured instance profile, VPC and security group during instance configuration. Selecting any other VPCs or instance profiles will result in permissions errors.","title":"Step 4: Create your Hopsworks instance"},{"location":"setup_installation/aws/restrictive_permissions/#step-5-supporting-multiple-vpcs","text":"The policy can be extended to give managed.hopsworks.ai access to multiple VPCs. See: Creating a Condition with Multiple Keys or Values .","title":"Step 5: Supporting multiple VPCs"},{"location":"setup_installation/aws/restrictive_permissions/#other-removable-permissions","text":"There are other permissions that are required by Hopsworks to provide certain product capabilities to the users. In this section, we go through these permissions and what are the implication or removing them.","title":"Other removable permissions"},{"location":"setup_installation/aws/restrictive_permissions/#backup-permissions","text":"The following permissions are only needed for the backup feature. You can remove them if you are not going to create backups or if you do not have access to this Enterprise feature. { \"Sid\" : \"EC2BackupCreation\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:RegisterImage\" , \"ec2:DeregisterImage\" , \"ec2:DescribeImages\" , \"ec2:CreateSnapshot\" , \"ec2:DeleteSnapshot\" , \"ec2:DescribeSnapshots\" ], \"Resource\" : \"*\" , }","title":"Backup permissions"},{"location":"setup_installation/aws/restrictive_permissions/#early-warnings-if-vpc-is-not-configured-correctly","text":"The following permissions are needed to give an early warning if your VPC and security groups are badly configured. You can remove them if you don't need it. \"ec2:DescribeVpcAttribute\" , \"ec2:DescribeRouteTables\"","title":"Early warnings if VPC is not configured correctly"},{"location":"setup_installation/aws/restrictive_permissions/#open-and-close-ports-from-within-hopsworksai","text":"The following permissions are used to let you close and open ports on your cluster from hopswork.ai, you can remove them if you do not want to open ports on your cluster or if you want to manually open ports in EC2. { \"Sid\" : \"EC2VpcNonResourceSpecificActions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:AuthorizeSecurityGroupIngress\" , \"ec2:RevokeSecurityGroupIngress\" , ], \"Resource\" : \"*\" , \"Condition\" : { \"ArnLike\" : { \"ec2:vpc\" : \"arn:aws:ec2:*:*:vpc/[VPC_ID]\" } } }","title":"Open and close ports from within Hopsworks.ai"},{"location":"setup_installation/aws/restrictive_permissions/#non-resource-based-permissions-used-for-listing","text":"If you are using terraform, then you can also remove most of the Describe permissions in NonResourceBasedPermissions and use the following permissions instead { \"Sid\" : \"NonResourceBasedPermissions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:DescribeInstances\" , \"ec2:DescribeVolumes\" , \"ec2:DescribeSecurityGroups\" ], \"Resource\" : \"*\" },","title":"Non resource based permissions used for listing"},{"location":"setup_installation/aws/restrictive_permissions/#load-balancers-permissions-for-external-access","text":"If you plan to access your Hopsworks cluster from an external python environment, especially if you plan to use the ArrowFlight with DuckDB , then it is required to create a network load balancer that forward requests to the ArrowFlight server(s) co-located with the RonDB MySQL Server(s). If you are not planning to use ArrowFlight server(s) or multiple mysql server(s), you can skip adding the following permissions. If you still wish to use the ArrowFlight server(s) but without adding the following permissions to your cross account role, check this advanced terraform example for more details . { \"Sid\" : \"ManageLoadBalancersForExternalAccess\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"elasticloadbalancing:CreateLoadBalancer\" , \"elasticloadbalancing:CreateListener\" , \"elasticloadbalancing:CreateTargetGroup\" , \"elasticloadbalancing:RegisterTargets\" , \"elasticloadbalancing:AddTags\" , \"elasticloadbalancing:DescribeTargetGroups\" , \"elasticloadbalancing:DeleteLoadBalancer\" , \"elasticloadbalancing:DeleteTargetGroup\" ], \"Resource\" : \"*\" }","title":"Load balancers permissions for external access"},{"location":"setup_installation/aws/restrictive_permissions/#limiting-the-instance-profile-permissions","text":"","title":"Limiting the instance profile permissions"},{"location":"setup_installation/aws/restrictive_permissions/#backups","text":"If you do not intend to take backups or if you do not have access to this Enterprise feature you can remove the permissions that are only used by the backup feature when configuring instance profile permissions . For this remove the following permissions from the instance profile: \"S3:PutLifecycleConfiguration\" , \"S3:GetLifecycleConfiguration\" , \"S3:PutBucketVersioning\" , \"S3:ListBucketVersions\" , \"S3:DeleteObjectVersion\" ,","title":"Backups"},{"location":"setup_installation/aws/restrictive_permissions/#cloudwatch-logs","text":"Hopsworks put its logs in Amazon CloudWatch so that you can access them without having to ssh into the machine. If you are not interested in this feature you can remove the following from your instance profile policy: { \"Effect\" : \"Allow\" , \"Action\" : [ \"cloudwatch:PutMetricData\" , \"ec2:DescribeVolumes\" , \"ec2:DescribeTags\" , \"logs:PutLogEvents\" , \"logs:DescribeLogStreams\" , \"logs:DescribeLogGroups\" , \"logs:CreateLogStream\" , \"logs:CreateLogGroup\" ], \"Resource\" : \"*\" }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"ssm:GetParameter\" ], \"Resource\" : \"arn:aws:ssm:*:*:parameter/AmazonCloudWatch-*\" }","title":"CloudWatch Logs"},{"location":"setup_installation/aws/troubleshooting/","text":"Troubleshooting # A list of common problems that you might encounter during cluster creation and how to solve them. Unauthorized error during cluster creation # If you encounter the following error right after creating your cluster, then it is likely that you have either missed or misconfigured one of the permissions in the cross account role setup . Unauthorized error during cluster creation In order to get more details regarding the authorization error above, you need to use the AWS Command Line Interface (AWS CLI) to decode the message using the aws sts decode-authorization-message --encoded-message command. $ aws sts decode-authorization-message --encoded-message hg-Sh5-CUNT5jgB305YbOp_FDp2P70ZPw5iwoextxcdWmoc4wgm_K0pAUZEvTtCpvCk_-EwtaqaRS0act1BM-Bz-id4NOwo-OVZES5q9fLQIqk5_typL767idkb4jdzrrwNLD3h7iaaoleKGQpaW5kzI_oHEtibBRY2uWhU07oiwDHOAwb-cQ-kIA4nJIay7wVoL7QRx8nECpb56s68lMWhrdbqKj6uRQwsAILY7eoV-sDCbWWjnr98ja_olixhlV95txiV-oCR2qW6GKn4TVKl2raGbwjWRdS2GACP0fm7RUI_glPl7q65Erhrcr7Z2uF2SRF46VI5vfXkjXxv58e0x6SSRmKXF397e4QpPM6RyopmgDa9sSWAbkBxC86O9b30l47GX9w98trc76jsfU-UcdqK-Vu7Qy3-j8ehYMDpNvZRFNX64fUrsfusLJcHnhAPqUgCbvjfmEa605GkH7amlP2j23vprb94auzCVk8rgVkrSrBMek6YlWA0nzXtSjq8mVAvFE-n6x3ByLdt68Ldgc602FsFqifuzUm7CnjapIIwSAat_TXQCs-mjXyB983AEw__RwiXN Then you will get the following message as response { \"DecodedMessage\" : \"{\\\"allowed\\\":false,\\\"explicitDeny\\\":false,\\\"matchedStatements\\\":{\\\"items\\\":[]},\\\"failures\\\":{\\\"items\\\":[]},\\\"context\\\":{\\\"principal\\\":{\\\"id\\\":\\\"AROA27VDEGQLGDB4JOSOI:1f708920-18a6-11ed-8dd4-f162dca8fc19\\\",\\\"arn\\\":\\\"arn:aws:sts::xxxxx:assumed-role/cross-acount-role/1f708920-18a6-11ed-8dd4-f162dca8fc19\\\"},\\\"action\\\":\\\"ec2:CreateVpc\\\",\\\"resource\\\":\\\"arn:aws:ec2:us-east-2:xxxxx:vpc/*\\\",\\\"conditions\\\":{\\\"items\\\":[{\\\"key\\\":\\\"aws:Region\\\",\\\"values\\\":{\\\"items\\\":[{\\\"value\\\":\\\"us-east-2\\\"}]}},{\\\"key\\\":\\\"aws:Service\\\",\\\"values\\\":{\\\"items\\\":[{\\\"value\\\":\\\"ec2\\\"}]}},{\\\"key\\\":\\\"aws:Resource\\\",\\\"values\\\":{\\\"items\\\":[{\\\"value\\\":\\\"vpc/*\\\"}]}},{\\\"key\\\":\\\"aws:Type\\\",\\\"values\\\":{\\\"items\\\":[{\\\"value\\\":\\\"vpc\\\"}]}},{\\\"key\\\":\\\"aws:Account\\\",\\\"values\\\":{\\\"items\\\":[{\\\"value\\\":\\\"xxxxxx\\\"}]}},{\\\"key\\\":\\\"ec2:VpcID\\\",\\\"values\\\":{\\\"items\\\":[{\\\"value\\\":\\\"*\\\"}]}},{\\\"key\\\":\\\"aws:ARN\\\",\\\"values\\\":{\\\"items\\\":[{\\\"value\\\":\\\"arn:aws:ec2:us-east-2:xxxx:vpc/*\\\"}]}}]}}}\" } From the above response we can see that the cross-account role is missing the ec2:CreateVpc permission. The solution is to terminate the cluster in error and update cross account role setup with the missing permission(s) and then try to create a new cluster. Missing permissions error during cluster creation # If you encounter the following error right after creating your cluster, then the issue is with the instance profile permissions. Missing permissions error during cluster creation This issue could be caused by one of the following: The instance profile that you have chosen during cluster creation is actually missing the permissions stated in the error on your chosen S3 bucket . Then in that case, update your instance profile accordingly and then click Retry to retry cluster creation operation. Your AWS organization is using SCPs policy that disallow policy simulation. In that case, you could do a simple test to confirm the issue by using the AWS PolicySim on AWS console . If policy simulation is disallowed, you can configure managed.hopsworks.ai to skip the policy simulation step by removing the iam:SimulatePrincipalPolicy permission from your cross account role , by navigating to the AWS Roles console , search for your cross account role name and click on it, on the permissions tab click edit on hopsworks inline policy, choose JSON tab, remove iam:SimulatePrincipalPolicy , click Review Policy , and then click Save Changes , and finally navigate back to managed.hopsworks.ai and click Retry to retry the cluster creation.","title":"Troubleshooting"},{"location":"setup_installation/aws/troubleshooting/#troubleshooting","text":"A list of common problems that you might encounter during cluster creation and how to solve them.","title":"Troubleshooting"},{"location":"setup_installation/aws/troubleshooting/#unauthorized-error-during-cluster-creation","text":"If you encounter the following error right after creating your cluster, then it is likely that you have either missed or misconfigured one of the permissions in the cross account role setup . Unauthorized error during cluster creation In order to get more details regarding the authorization error above, you need to use the AWS Command Line Interface (AWS CLI) to decode the message using the aws sts decode-authorization-message --encoded-message command. $ aws sts decode-authorization-message --encoded-message hg-Sh5-CUNT5jgB305YbOp_FDp2P70ZPw5iwoextxcdWmoc4wgm_K0pAUZEvTtCpvCk_-EwtaqaRS0act1BM-Bz-id4NOwo-OVZES5q9fLQIqk5_typL767idkb4jdzrrwNLD3h7iaaoleKGQpaW5kzI_oHEtibBRY2uWhU07oiwDHOAwb-cQ-kIA4nJIay7wVoL7QRx8nECpb56s68lMWhrdbqKj6uRQwsAILY7eoV-sDCbWWjnr98ja_olixhlV95txiV-oCR2qW6GKn4TVKl2raGbwjWRdS2GACP0fm7RUI_glPl7q65Erhrcr7Z2uF2SRF46VI5vfXkjXxv58e0x6SSRmKXF397e4QpPM6RyopmgDa9sSWAbkBxC86O9b30l47GX9w98trc76jsfU-UcdqK-Vu7Qy3-j8ehYMDpNvZRFNX64fUrsfusLJcHnhAPqUgCbvjfmEa605GkH7amlP2j23vprb94auzCVk8rgVkrSrBMek6YlWA0nzXtSjq8mVAvFE-n6x3ByLdt68Ldgc602FsFqifuzUm7CnjapIIwSAat_TXQCs-mjXyB983AEw__RwiXN Then you will get the following message as response { \"DecodedMessage\" : \"{\\\"allowed\\\":false,\\\"explicitDeny\\\":false,\\\"matchedStatements\\\":{\\\"items\\\":[]},\\\"failures\\\":{\\\"items\\\":[]},\\\"context\\\":{\\\"principal\\\":{\\\"id\\\":\\\"AROA27VDEGQLGDB4JOSOI:1f708920-18a6-11ed-8dd4-f162dca8fc19\\\",\\\"arn\\\":\\\"arn:aws:sts::xxxxx:assumed-role/cross-acount-role/1f708920-18a6-11ed-8dd4-f162dca8fc19\\\"},\\\"action\\\":\\\"ec2:CreateVpc\\\",\\\"resource\\\":\\\"arn:aws:ec2:us-east-2:xxxxx:vpc/*\\\",\\\"conditions\\\":{\\\"items\\\":[{\\\"key\\\":\\\"aws:Region\\\",\\\"values\\\":{\\\"items\\\":[{\\\"value\\\":\\\"us-east-2\\\"}]}},{\\\"key\\\":\\\"aws:Service\\\",\\\"values\\\":{\\\"items\\\":[{\\\"value\\\":\\\"ec2\\\"}]}},{\\\"key\\\":\\\"aws:Resource\\\",\\\"values\\\":{\\\"items\\\":[{\\\"value\\\":\\\"vpc/*\\\"}]}},{\\\"key\\\":\\\"aws:Type\\\",\\\"values\\\":{\\\"items\\\":[{\\\"value\\\":\\\"vpc\\\"}]}},{\\\"key\\\":\\\"aws:Account\\\",\\\"values\\\":{\\\"items\\\":[{\\\"value\\\":\\\"xxxxxx\\\"}]}},{\\\"key\\\":\\\"ec2:VpcID\\\",\\\"values\\\":{\\\"items\\\":[{\\\"value\\\":\\\"*\\\"}]}},{\\\"key\\\":\\\"aws:ARN\\\",\\\"values\\\":{\\\"items\\\":[{\\\"value\\\":\\\"arn:aws:ec2:us-east-2:xxxx:vpc/*\\\"}]}}]}}}\" } From the above response we can see that the cross-account role is missing the ec2:CreateVpc permission. The solution is to terminate the cluster in error and update cross account role setup with the missing permission(s) and then try to create a new cluster.","title":"Unauthorized error during cluster creation"},{"location":"setup_installation/aws/troubleshooting/#missing-permissions-error-during-cluster-creation","text":"If you encounter the following error right after creating your cluster, then the issue is with the instance profile permissions. Missing permissions error during cluster creation This issue could be caused by one of the following: The instance profile that you have chosen during cluster creation is actually missing the permissions stated in the error on your chosen S3 bucket . Then in that case, update your instance profile accordingly and then click Retry to retry cluster creation operation. Your AWS organization is using SCPs policy that disallow policy simulation. In that case, you could do a simple test to confirm the issue by using the AWS PolicySim on AWS console . If policy simulation is disallowed, you can configure managed.hopsworks.ai to skip the policy simulation step by removing the iam:SimulatePrincipalPolicy permission from your cross account role , by navigating to the AWS Roles console , search for your cross account role name and click on it, on the permissions tab click edit on hopsworks inline policy, choose JSON tab, remove iam:SimulatePrincipalPolicy , click Review Policy , and then click Save Changes , and finally navigate back to managed.hopsworks.ai and click Retry to retry the cluster creation.","title":"Missing permissions error during cluster creation"},{"location":"setup_installation/aws/upgrade/","text":"Upgrade existing clusters on managed.hopsworks.ai from version 2.2 or older (AWS) # This guide shows you how to upgrade your existing Hopsworks cluster to a newer version of Hopsworks. Step 1: Make sure your cluster is running # It is important that your cluster is Running . Otherwise you will not be able to upgrade. As soon as a new version is available an upgrade notification will appear. You can proceed by clicking the Upgrade button. A new Hopsworks version is available Step 2: Add upgrade permissions to your instance profile # Note You can skip this step if you already have the following permissions in your instance profile: [ \"ec2:DescribeVolumes\" , \"ec2:DetachVolume\" , \"ec2:AttachVolume\" , \"ec2:ModifyInstanceAttribute\" ] We require extra permissions to be added to the instance profile attached to your cluster to proceed with the upgrade. First to get the name of your instance profile, click on the Details tab as shown below: Getting the name of your instance profile Once you get your instance profile name, navigate to AWS management console , then click on Roles and then search for your role name and click on it. Go to the Permissions tab, click on Add inline policy , and then go to the JSON tab. Paste the following snippet, click on Review policy , name it, and click Create policy . Note You can restrict the upgrade permissions given to your instance profile. Refer to this guide for more information. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"UpgradePermissions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:DescribeVolumes\" , \"ec2:DetachVolume\" , \"ec2:AttachVolume\" , \"ec2:ModifyInstanceAttribute\" ], \"Resource\" : \"*\" } ] } Step 3: Run the upgrade process # You need to click on Upgrade to start the upgrade process. You will be prompted with the screen shown below to confirm your intention to upgrade: Note No need to worry about the following message since this is done already in Step 2 Make sure that your instance profile (hopsworks-doc) includes the following permissions: [ \"ec2:DetachVolume\", \"ec2:AttachVolume\", \"ec2:ModifyInstanceAttribute\" ] Upgrade confirmation Check the Yes, upgrade cluster checkbox to proceed, then the Upgrade button will be activated as shown below: Warning Currently, we only support upgrade for the head node and you will need to recreate your workers once the upgrade is successfully completed. Upgrade confirmation Depending on how big your current cluster is, the upgrade process may take from 1 hour to a few hours until completion. Note We don't delete your old cluster until the upgrade process is successfully completed. Upgrade is running Once the upgrade is completed, you can confirm that you have the new Hopsworks version by checking the Details tab of your cluster as below: Upgrade is complete Error handling # There are two categories of errors that you may encounter during an upgrade. First, a permission error due to missing permission or a misconfigured policy in your instance profile, see Error 1 . Second, an error during the upgrade process running on your cluster, see Error 2 . Error 1: Misconfigured upgrade permissions # During the upgrade process, managed.hopsworks.ai starts by validating your instance profile permissions to ensure that it includes the required upgrade permissions. If one or more permissions are missing, or if the resource is not set correctly, you will be notified with an error message and a Retry button will appear as shown below: Upgrade permissions are missing Update you instance profile accordingly, then click Retry Upgrade is running Error 2: Upgrade process error # If an error occurs during the upgrade process, you will have the option to rollback to your old cluster as shown below: Error occurred during upgrade Click on Rollback to recover your old cluster before upgrade. Upgrade rollback confirmation Check the Yes, rollback cluster checkbox to proceed, then the Rollback button will be activated as shown below: Upgrade rollback confirmation Once the rollback is completed, you will be able to continue working as normal with your old cluster. Note The old cluster will be stopped after the rollback. You have to click on the Start button.","title":"Version 2.2 or older"},{"location":"setup_installation/aws/upgrade/#upgrade-existing-clusters-on-managedhopsworksai-from-version-22-or-older-aws","text":"This guide shows you how to upgrade your existing Hopsworks cluster to a newer version of Hopsworks.","title":"Upgrade existing clusters on managed.hopsworks.ai from version 2.2 or older (AWS)"},{"location":"setup_installation/aws/upgrade/#step-1-make-sure-your-cluster-is-running","text":"It is important that your cluster is Running . Otherwise you will not be able to upgrade. As soon as a new version is available an upgrade notification will appear. You can proceed by clicking the Upgrade button. A new Hopsworks version is available","title":"Step 1: Make sure your cluster is running"},{"location":"setup_installation/aws/upgrade/#step-2-add-upgrade-permissions-to-your-instance-profile","text":"Note You can skip this step if you already have the following permissions in your instance profile: [ \"ec2:DescribeVolumes\" , \"ec2:DetachVolume\" , \"ec2:AttachVolume\" , \"ec2:ModifyInstanceAttribute\" ] We require extra permissions to be added to the instance profile attached to your cluster to proceed with the upgrade. First to get the name of your instance profile, click on the Details tab as shown below: Getting the name of your instance profile Once you get your instance profile name, navigate to AWS management console , then click on Roles and then search for your role name and click on it. Go to the Permissions tab, click on Add inline policy , and then go to the JSON tab. Paste the following snippet, click on Review policy , name it, and click Create policy . Note You can restrict the upgrade permissions given to your instance profile. Refer to this guide for more information. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"UpgradePermissions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:DescribeVolumes\" , \"ec2:DetachVolume\" , \"ec2:AttachVolume\" , \"ec2:ModifyInstanceAttribute\" ], \"Resource\" : \"*\" } ] }","title":"Step 2: Add upgrade permissions to your instance profile"},{"location":"setup_installation/aws/upgrade/#step-3-run-the-upgrade-process","text":"You need to click on Upgrade to start the upgrade process. You will be prompted with the screen shown below to confirm your intention to upgrade: Note No need to worry about the following message since this is done already in Step 2 Make sure that your instance profile (hopsworks-doc) includes the following permissions: [ \"ec2:DetachVolume\", \"ec2:AttachVolume\", \"ec2:ModifyInstanceAttribute\" ] Upgrade confirmation Check the Yes, upgrade cluster checkbox to proceed, then the Upgrade button will be activated as shown below: Warning Currently, we only support upgrade for the head node and you will need to recreate your workers once the upgrade is successfully completed. Upgrade confirmation Depending on how big your current cluster is, the upgrade process may take from 1 hour to a few hours until completion. Note We don't delete your old cluster until the upgrade process is successfully completed. Upgrade is running Once the upgrade is completed, you can confirm that you have the new Hopsworks version by checking the Details tab of your cluster as below: Upgrade is complete","title":"Step 3: Run the upgrade process"},{"location":"setup_installation/aws/upgrade/#error-handling","text":"There are two categories of errors that you may encounter during an upgrade. First, a permission error due to missing permission or a misconfigured policy in your instance profile, see Error 1 . Second, an error during the upgrade process running on your cluster, see Error 2 .","title":"Error handling"},{"location":"setup_installation/aws/upgrade/#error-1-misconfigured-upgrade-permissions","text":"During the upgrade process, managed.hopsworks.ai starts by validating your instance profile permissions to ensure that it includes the required upgrade permissions. If one or more permissions are missing, or if the resource is not set correctly, you will be notified with an error message and a Retry button will appear as shown below: Upgrade permissions are missing Update you instance profile accordingly, then click Retry Upgrade is running","title":"Error 1: Misconfigured upgrade permissions"},{"location":"setup_installation/aws/upgrade/#error-2-upgrade-process-error","text":"If an error occurs during the upgrade process, you will have the option to rollback to your old cluster as shown below: Error occurred during upgrade Click on Rollback to recover your old cluster before upgrade. Upgrade rollback confirmation Check the Yes, rollback cluster checkbox to proceed, then the Rollback button will be activated as shown below: Upgrade rollback confirmation Once the rollback is completed, you will be able to continue working as normal with your old cluster. Note The old cluster will be stopped after the rollback. You have to click on the Start button.","title":"Error 2: Upgrade process error"},{"location":"setup_installation/aws/upgrade_2.4/","text":"Upgrade existing clusters on managed.hopsworks.ai from version 2.4 or newer (AWS) # This guide shows you how to upgrade your existing Hopsworks cluster to a newer version of Hopsworks. Step 1: Make sure your cluster is running # It is important that your cluster is Running . Otherwise you will not be able to upgrade. As soon as a new version is available an upgrade notification will appear. You can proceed by clicking the Upgrade button. A new Hopsworks version is available Step 2: Add backup permissions to your cross account role # Note You can skip this step if you already have the following permissions in your cross account role: [ \"ec2:RegisterImage\" , \"ec2:DeregisterImage\" , \"ec2:DescribeImages\" , \"ec2:CreateSnapshot\" , \"ec2:DeleteSnapshot\" , \"ec2:DescribeSnapshots\" ] We require some extra permissions to be added to the role you have created when connecting your AWS account as described in getting started guide . These permissions are required to create a snapshot of your cluster before proceeding with the upgrade. First, check which role or access key you have added to managed.hopsworks.ai, you can go to the Settings tab, and then click Edit next to the AWS cloud account Cloud Accounts Once you have clicked on Edit , you will be able to see the current assigned role AWS Cross-Account Role Once you get your role name, navigate to AWS management console , then click on Roles and then search for your role name and click on it. Go to the Permissions tab, click on Add inline policy , and then go to the JSON tab. Paste the following snippet, click on Review policy , name it, and click Create policy . { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"HopsworksAIBackup\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:RegisterImage\" , \"ec2:DeregisterImage\" , \"ec2:DescribeImages\" , \"ec2:CreateSnapshot\" , \"ec2:DeleteSnapshot\" , \"ec2:DescribeSnapshots\" ], \"Resource\" : \"*\" } ] } Step 3: Run the upgrade process # You need to click on Upgrade to start the upgrade process. You will be prompted with the screen shown below to confirm your intention to upgrade: Note No need to worry about the following message since this is done already in Step 2 Make sure that your cross-account role which you have connected to managed.hopsworks.ai has the following permissions: [ \"ec2:RegisterImage\", \"ec2:DeregisterImage\", \"ec2:DescribeImages\", \"ec2:CreateSnapshot\", \"ec2:DeleteSnapshot\", \"ec2:DescribeSnapshots\"] Upgrade confirmation Check the Yes, upgrade cluster checkbox to proceed, then the Upgrade button will be activated as shown below: Warning Currently, we only support upgrade for the head node and you will need to recreate your workers once the upgrade is successfully completed. Upgrade confirmation Depending on how big your current cluster is, the upgrade process may take from 1 hour to a few hours until completion. Note We don't delete your old cluster until the upgrade process is successfully completed. Upgrade is running Once the upgrade is completed, you can confirm that you have the new Hopsworks version by checking the version on the Details tab of your cluster. Error handling # There are two categories of errors that you may encounter during an upgrade. First, a permission error due to missing permission or a misconfigured policy in your cross-account role, see Error 1 . Second, an error during the upgrade process running on your cluster, see Error 2 . Error 1: Missing backup permissions # If one or more backup permissions are missing, or if the resource is not set correctly, you will be notified with an error message as shown below: Upgrade permissions are missing Update you cross account role as described in Step 2 , then click Start . Once the cluster is up and running, you can try running the upgrade again. Error 2: Upgrade process error # If an error occurs during the upgrade process, you will have the option to rollback to your old cluster as shown below: Error occurred during upgrade Click on Rollback to recover your old cluster before upgrade. Upgrade rollback confirmation Check the Yes, rollback cluster checkbox to proceed, then the Rollback button will be activated as shown below: Upgrade rollback confirmation Once the rollback is completed, you will be able to continue working as normal with your old cluster. Note The old cluster will be stopped after the rollback. You have to click on the Start button.","title":"Version 2.4 or newer"},{"location":"setup_installation/aws/upgrade_2.4/#upgrade-existing-clusters-on-managedhopsworksai-from-version-24-or-newer-aws","text":"This guide shows you how to upgrade your existing Hopsworks cluster to a newer version of Hopsworks.","title":"Upgrade existing clusters on managed.hopsworks.ai from version 2.4 or newer (AWS)"},{"location":"setup_installation/aws/upgrade_2.4/#step-1-make-sure-your-cluster-is-running","text":"It is important that your cluster is Running . Otherwise you will not be able to upgrade. As soon as a new version is available an upgrade notification will appear. You can proceed by clicking the Upgrade button. A new Hopsworks version is available","title":"Step 1: Make sure your cluster is running"},{"location":"setup_installation/aws/upgrade_2.4/#step-2-add-backup-permissions-to-your-cross-account-role","text":"Note You can skip this step if you already have the following permissions in your cross account role: [ \"ec2:RegisterImage\" , \"ec2:DeregisterImage\" , \"ec2:DescribeImages\" , \"ec2:CreateSnapshot\" , \"ec2:DeleteSnapshot\" , \"ec2:DescribeSnapshots\" ] We require some extra permissions to be added to the role you have created when connecting your AWS account as described in getting started guide . These permissions are required to create a snapshot of your cluster before proceeding with the upgrade. First, check which role or access key you have added to managed.hopsworks.ai, you can go to the Settings tab, and then click Edit next to the AWS cloud account Cloud Accounts Once you have clicked on Edit , you will be able to see the current assigned role AWS Cross-Account Role Once you get your role name, navigate to AWS management console , then click on Roles and then search for your role name and click on it. Go to the Permissions tab, click on Add inline policy , and then go to the JSON tab. Paste the following snippet, click on Review policy , name it, and click Create policy . { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"HopsworksAIBackup\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:RegisterImage\" , \"ec2:DeregisterImage\" , \"ec2:DescribeImages\" , \"ec2:CreateSnapshot\" , \"ec2:DeleteSnapshot\" , \"ec2:DescribeSnapshots\" ], \"Resource\" : \"*\" } ] }","title":"Step 2: Add backup permissions to your cross account role"},{"location":"setup_installation/aws/upgrade_2.4/#step-3-run-the-upgrade-process","text":"You need to click on Upgrade to start the upgrade process. You will be prompted with the screen shown below to confirm your intention to upgrade: Note No need to worry about the following message since this is done already in Step 2 Make sure that your cross-account role which you have connected to managed.hopsworks.ai has the following permissions: [ \"ec2:RegisterImage\", \"ec2:DeregisterImage\", \"ec2:DescribeImages\", \"ec2:CreateSnapshot\", \"ec2:DeleteSnapshot\", \"ec2:DescribeSnapshots\"] Upgrade confirmation Check the Yes, upgrade cluster checkbox to proceed, then the Upgrade button will be activated as shown below: Warning Currently, we only support upgrade for the head node and you will need to recreate your workers once the upgrade is successfully completed. Upgrade confirmation Depending on how big your current cluster is, the upgrade process may take from 1 hour to a few hours until completion. Note We don't delete your old cluster until the upgrade process is successfully completed. Upgrade is running Once the upgrade is completed, you can confirm that you have the new Hopsworks version by checking the version on the Details tab of your cluster.","title":"Step 3: Run the upgrade process"},{"location":"setup_installation/aws/upgrade_2.4/#error-handling","text":"There are two categories of errors that you may encounter during an upgrade. First, a permission error due to missing permission or a misconfigured policy in your cross-account role, see Error 1 . Second, an error during the upgrade process running on your cluster, see Error 2 .","title":"Error handling"},{"location":"setup_installation/aws/upgrade_2.4/#error-1-missing-backup-permissions","text":"If one or more backup permissions are missing, or if the resource is not set correctly, you will be notified with an error message as shown below: Upgrade permissions are missing Update you cross account role as described in Step 2 , then click Start . Once the cluster is up and running, you can try running the upgrade again.","title":"Error 1: Missing backup permissions"},{"location":"setup_installation/aws/upgrade_2.4/#error-2-upgrade-process-error","text":"If an error occurs during the upgrade process, you will have the option to rollback to your old cluster as shown below: Error occurred during upgrade Click on Rollback to recover your old cluster before upgrade. Upgrade rollback confirmation Check the Yes, rollback cluster checkbox to proceed, then the Rollback button will be activated as shown below: Upgrade rollback confirmation Once the rollback is completed, you will be able to continue working as normal with your old cluster. Note The old cluster will be stopped after the rollback. You have to click on the Start button.","title":"Error 2: Upgrade process error"},{"location":"setup_installation/aws/upgrade_3.0/","text":"Upgrade existing clusters on managed.hopsworks.ai from version 3.0 or newer (AWS) # This guide shows you how to upgrade your existing Hopsworks cluster to a newer version of Hopsworks. Step 1: Make sure your cluster is running # It is important that your cluster is Running . Otherwise you will not be able to upgrade. As soon as a new version is available an upgrade notification will appear. You can proceed by clicking the Upgrade button. A new Hopsworks version is available Step 2: Add backup permissions to your cross account role # Note You can skip this step if you already have the following permissions in your cross account role: [ \"ec2:RegisterImage\" , \"ec2:DeregisterImage\" , \"ec2:DescribeImages\" , \"ec2:CreateSnapshot\" , \"ec2:DeleteSnapshot\" , \"ec2:DescribeSnapshots\" ] We require some extra permissions to be added to the role you have created when connecting your AWS account as described in getting started guide . These permissions are required to create a snapshot of your cluster before proceeding with the upgrade. First, check which role or access key you have added to managed.hopsworks.ai, you can go to the Settings tab, and then click Edit next to the AWS cloud account Cloud Accounts Once you have clicked on Edit , you will be able to see the current assigned role AWS Cross-Account Role Once you get your role name, navigate to AWS management console , then click on Roles and then search for your role name and click on it. Go to the Permissions tab, click on Add inline policy , and then go to the JSON tab. Paste the following snippet, click on Review policy , name it, and click Create policy . { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"HopsworksAIBackup\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:RegisterImage\" , \"ec2:DeregisterImage\" , \"ec2:DescribeImages\" , \"ec2:CreateSnapshot\" , \"ec2:DeleteSnapshot\" , \"ec2:DescribeSnapshots\" ], \"Resource\" : \"*\" } ] } Step 3: Update the instance profile permissions # We have enforced using managed docker registry (ECR) starting from Hopsworks version 3.1.0, so you need to update your instance profile to include extra permissions to allow access to ECR. First, get the instance profile of your cluster by clicking on the Details tab and check the IAM role ARN shown in front of IAM Instance Profile . Once you get your role name, navigate to AWS management console , then click on Roles and then search for your role name and click on it. Go to the Permissions tab, click on Add inline policy , and then go to the JSON tab. Paste the following snippet, click on Review policy , name it, and click Create policy . { \"Version\" : \"2012-10-17\" , \"Statement\" :[ { \"Sid\" : \"AllowPullImagesFromHopsworkAi\" , \"Effect\" : \"Allow\" , \"Action\" :[ \"ecr:GetDownloadUrlForLayer\" , \"ecr:BatchGetImage\" ], \"Resource\" :[ \"arn:aws:ecr:*:822623301872:repository/filebeat\" , \"arn:aws:ecr:*:822623301872:repository/base\" , \"arn:aws:ecr:*:822623301872:repository/onlinefs\" , \"arn:aws:ecr:*:822623301872:repository/airflow\" , \"arn:aws:ecr:*:822623301872:repository/git\" ] }, { \"Sid\" : \"AllowCreateRespositry\" , \"Effect\" : \"Allow\" , \"Action\" : \"ecr:CreateRepository\" , \"Resource\" : \"*\" }, { \"Sid\" : \"AllowPushandPullImagesToUserRepo\" , \"Effect\" : \"Allow\" , \"Action\" :[ \"ecr:GetDownloadUrlForLayer\" , \"ecr:BatchGetImage\" , \"ecr:CompleteLayerUpload\" , \"ecr:UploadLayerPart\" , \"ecr:InitiateLayerUpload\" , \"ecr:BatchCheckLayerAvailability\" , \"ecr:PutImage\" , \"ecr:ListImages\" , \"ecr:BatchDeleteImage\" , \"ecr:GetLifecyclePolicy\" , \"ecr:PutLifecyclePolicy\" , \"ecr:TagResource\" ], \"Resource\" :[ \"arn:aws:ecr:*:*:repository/*/filebeat\" , \"arn:aws:ecr:*:*:repository/*/base\" , \"arn:aws:ecr:*:*:repository/*/onlinefs\" , \"arn:aws:ecr:*:*:repository/*/airflow\" , \"arn:aws:ecr:*:*:repository/*/git\" ] }, { \"Sid\" : \"AllowGetAuthToken\" , \"Effect\" : \"Allow\" , \"Action\" : \"ecr:GetAuthorizationToken\" , \"Resource\" : \"*\" } ] } Step 4: Run the upgrade process # You need to click on Upgrade to start the upgrade process. You will be prompted with the screen shown below to confirm your intention to upgrade: Note No need to worry about the steps shown below if you have already completed Step 2 and Step 3 Upgrade confirmation Check the Yes, upgrade cluster checkbox to proceed, then the Upgrade button will be activated as shown below: Warning Currently, we only support upgrade for the head node and you will need to recreate your workers once the upgrade is successfully completed. Upgrade confirmation Depending on how big your current cluster is, the upgrade process may take from 1 hour to a few hours until completion. Note We don't delete your old cluster until the upgrade process is successfully completed. Upgrade is running Once the upgrade is completed, you can confirm that you have the new Hopsworks version by checking the version on the Details tab of your cluster. For more details about error handling check this guide","title":"Version 3.0 or newer"},{"location":"setup_installation/aws/upgrade_3.0/#upgrade-existing-clusters-on-managedhopsworksai-from-version-30-or-newer-aws","text":"This guide shows you how to upgrade your existing Hopsworks cluster to a newer version of Hopsworks.","title":"Upgrade existing clusters on managed.hopsworks.ai from version 3.0 or newer (AWS)"},{"location":"setup_installation/aws/upgrade_3.0/#step-1-make-sure-your-cluster-is-running","text":"It is important that your cluster is Running . Otherwise you will not be able to upgrade. As soon as a new version is available an upgrade notification will appear. You can proceed by clicking the Upgrade button. A new Hopsworks version is available","title":"Step 1: Make sure your cluster is running"},{"location":"setup_installation/aws/upgrade_3.0/#step-2-add-backup-permissions-to-your-cross-account-role","text":"Note You can skip this step if you already have the following permissions in your cross account role: [ \"ec2:RegisterImage\" , \"ec2:DeregisterImage\" , \"ec2:DescribeImages\" , \"ec2:CreateSnapshot\" , \"ec2:DeleteSnapshot\" , \"ec2:DescribeSnapshots\" ] We require some extra permissions to be added to the role you have created when connecting your AWS account as described in getting started guide . These permissions are required to create a snapshot of your cluster before proceeding with the upgrade. First, check which role or access key you have added to managed.hopsworks.ai, you can go to the Settings tab, and then click Edit next to the AWS cloud account Cloud Accounts Once you have clicked on Edit , you will be able to see the current assigned role AWS Cross-Account Role Once you get your role name, navigate to AWS management console , then click on Roles and then search for your role name and click on it. Go to the Permissions tab, click on Add inline policy , and then go to the JSON tab. Paste the following snippet, click on Review policy , name it, and click Create policy . { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"HopsworksAIBackup\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:RegisterImage\" , \"ec2:DeregisterImage\" , \"ec2:DescribeImages\" , \"ec2:CreateSnapshot\" , \"ec2:DeleteSnapshot\" , \"ec2:DescribeSnapshots\" ], \"Resource\" : \"*\" } ] }","title":"Step 2: Add backup permissions to your cross account role"},{"location":"setup_installation/aws/upgrade_3.0/#step-3-update-the-instance-profile-permissions","text":"We have enforced using managed docker registry (ECR) starting from Hopsworks version 3.1.0, so you need to update your instance profile to include extra permissions to allow access to ECR. First, get the instance profile of your cluster by clicking on the Details tab and check the IAM role ARN shown in front of IAM Instance Profile . Once you get your role name, navigate to AWS management console , then click on Roles and then search for your role name and click on it. Go to the Permissions tab, click on Add inline policy , and then go to the JSON tab. Paste the following snippet, click on Review policy , name it, and click Create policy . { \"Version\" : \"2012-10-17\" , \"Statement\" :[ { \"Sid\" : \"AllowPullImagesFromHopsworkAi\" , \"Effect\" : \"Allow\" , \"Action\" :[ \"ecr:GetDownloadUrlForLayer\" , \"ecr:BatchGetImage\" ], \"Resource\" :[ \"arn:aws:ecr:*:822623301872:repository/filebeat\" , \"arn:aws:ecr:*:822623301872:repository/base\" , \"arn:aws:ecr:*:822623301872:repository/onlinefs\" , \"arn:aws:ecr:*:822623301872:repository/airflow\" , \"arn:aws:ecr:*:822623301872:repository/git\" ] }, { \"Sid\" : \"AllowCreateRespositry\" , \"Effect\" : \"Allow\" , \"Action\" : \"ecr:CreateRepository\" , \"Resource\" : \"*\" }, { \"Sid\" : \"AllowPushandPullImagesToUserRepo\" , \"Effect\" : \"Allow\" , \"Action\" :[ \"ecr:GetDownloadUrlForLayer\" , \"ecr:BatchGetImage\" , \"ecr:CompleteLayerUpload\" , \"ecr:UploadLayerPart\" , \"ecr:InitiateLayerUpload\" , \"ecr:BatchCheckLayerAvailability\" , \"ecr:PutImage\" , \"ecr:ListImages\" , \"ecr:BatchDeleteImage\" , \"ecr:GetLifecyclePolicy\" , \"ecr:PutLifecyclePolicy\" , \"ecr:TagResource\" ], \"Resource\" :[ \"arn:aws:ecr:*:*:repository/*/filebeat\" , \"arn:aws:ecr:*:*:repository/*/base\" , \"arn:aws:ecr:*:*:repository/*/onlinefs\" , \"arn:aws:ecr:*:*:repository/*/airflow\" , \"arn:aws:ecr:*:*:repository/*/git\" ] }, { \"Sid\" : \"AllowGetAuthToken\" , \"Effect\" : \"Allow\" , \"Action\" : \"ecr:GetAuthorizationToken\" , \"Resource\" : \"*\" } ] }","title":"Step 3: Update the instance profile permissions"},{"location":"setup_installation/aws/upgrade_3.0/#step-4-run-the-upgrade-process","text":"You need to click on Upgrade to start the upgrade process. You will be prompted with the screen shown below to confirm your intention to upgrade: Note No need to worry about the steps shown below if you have already completed Step 2 and Step 3 Upgrade confirmation Check the Yes, upgrade cluster checkbox to proceed, then the Upgrade button will be activated as shown below: Warning Currently, we only support upgrade for the head node and you will need to recreate your workers once the upgrade is successfully completed. Upgrade confirmation Depending on how big your current cluster is, the upgrade process may take from 1 hour to a few hours until completion. Note We don't delete your old cluster until the upgrade process is successfully completed. Upgrade is running Once the upgrade is completed, you can confirm that you have the new Hopsworks version by checking the version on the Details tab of your cluster. For more details about error handling check this guide","title":"Step 4: Run the upgrade process"},{"location":"setup_installation/azure/aks_acr_integration/","text":"Integration with Azure AKS # This guide shows how to create a cluster in managed.hopsworks.ai with integrated support for Azure Kubernetes Service (AKS). This enables Hopsworks to launch Python jobs, Jupyter servers, and serve models on top of AKS. This guide provides an example setup with a private AKS cluster. Note If you prefer to use Terraform over command lines you can refer to our Terraform example here . Step 1: Create a Virtual network and a subnet # First, you need to create the virtual network and the subnet in which Hopsworks and the AKS nodes will run. To do this run the following commands, replacing $RESOURCE_GROUP with the resource group in which you will run your cluster. az network vnet create --resource-group $RESOURCE_GROUP --name hopsworks-vnet --address-prefixes 172 .18.0.0/16 az network vnet subnet create --resource-group $RESOURCE_GROUP --name hopsworks-subnet --vnet-name hopsworks-vnet --address-prefixes 172 .18.0.0/24 Step 2: Create the AKS cluster. # Run the following command to create the AKS cluster. Replace $RESOURCE_GROUP with the resource group in which you will run your cluster and $SUBSCRIPTION_ID . aksidentity = $( az aks create --resource-group $RESOURCE_GROUP --name hopsworks-aks --network-plugin azure --enable-private-cluster --enable-managed-identity --vnet-subnet-id /subscriptions/ $SUBSCRIPTION_ID /resourceGroups/ $RESOURCE_GROUP /providers/Microsoft.Network/virtualNetworks/hopsworks-vnet/subnets/hopsworks-subnet --query identityProfile.kubeletidentity.objectId -o tsv ) Step 3: Add permissions to the managed identity # You need to add permission to the managed identity you will assign to your Hopsworks cluster to access the AKS cluster. To do it run the following command, replacing $RESOURCE_GROUP with the resource group in which you will run your cluster and $identityId with the id of the identity you will assign to your Hopsworks cluster. az role assignment create --resource-group $RESOURCE_GROUP --role \"Azure Kubernetes Service Cluster User Role\" --assignee $identityId You also need to grant permission to pull images from the ACR to the AKS nodes. To do it run the following command, replacing $RESOURCE_GROUP with the resource group in which you will run your cluster az role assignment create --resource-group $RESOURCE_GROUP --role \"AcrPull\" --assignee $aksidentity Step 4: Create the Hopsworks cluster # Create the hopsworks cluster by following the same steps as in the getting started until the backup tab. Then click on next to get to the Managed containers tab. Set Use Azure AKS as enabled. One new field will pop up. Fill it with the name of the AKS you created above (hopsworks-aks). Click next and in the Virtual Network tab select the virtual network we created above (hopsworks-vnet). Finally, click next and select the subnet we created above (hopsworks-subnet) in the Subnet tab. You can now click on Review and Submit and create your cluster. Hopsworks AKS configuration Going further # You can also deploy the AKS cluster and the Hopsworks cluster in two different virtual networks by using network peering. For this we recommend to use our terraform provider and to refer to our example","title":"AKS integration"},{"location":"setup_installation/azure/aks_acr_integration/#integration-with-azure-aks","text":"This guide shows how to create a cluster in managed.hopsworks.ai with integrated support for Azure Kubernetes Service (AKS). This enables Hopsworks to launch Python jobs, Jupyter servers, and serve models on top of AKS. This guide provides an example setup with a private AKS cluster. Note If you prefer to use Terraform over command lines you can refer to our Terraform example here .","title":"Integration with Azure AKS"},{"location":"setup_installation/azure/aks_acr_integration/#step-1-create-a-virtual-network-and-a-subnet","text":"First, you need to create the virtual network and the subnet in which Hopsworks and the AKS nodes will run. To do this run the following commands, replacing $RESOURCE_GROUP with the resource group in which you will run your cluster. az network vnet create --resource-group $RESOURCE_GROUP --name hopsworks-vnet --address-prefixes 172 .18.0.0/16 az network vnet subnet create --resource-group $RESOURCE_GROUP --name hopsworks-subnet --vnet-name hopsworks-vnet --address-prefixes 172 .18.0.0/24","title":"Step 1: Create a Virtual network and a subnet"},{"location":"setup_installation/azure/aks_acr_integration/#step-2-create-the-aks-cluster","text":"Run the following command to create the AKS cluster. Replace $RESOURCE_GROUP with the resource group in which you will run your cluster and $SUBSCRIPTION_ID . aksidentity = $( az aks create --resource-group $RESOURCE_GROUP --name hopsworks-aks --network-plugin azure --enable-private-cluster --enable-managed-identity --vnet-subnet-id /subscriptions/ $SUBSCRIPTION_ID /resourceGroups/ $RESOURCE_GROUP /providers/Microsoft.Network/virtualNetworks/hopsworks-vnet/subnets/hopsworks-subnet --query identityProfile.kubeletidentity.objectId -o tsv )","title":"Step 2: Create the AKS cluster."},{"location":"setup_installation/azure/aks_acr_integration/#step-3-add-permissions-to-the-managed-identity","text":"You need to add permission to the managed identity you will assign to your Hopsworks cluster to access the AKS cluster. To do it run the following command, replacing $RESOURCE_GROUP with the resource group in which you will run your cluster and $identityId with the id of the identity you will assign to your Hopsworks cluster. az role assignment create --resource-group $RESOURCE_GROUP --role \"Azure Kubernetes Service Cluster User Role\" --assignee $identityId You also need to grant permission to pull images from the ACR to the AKS nodes. To do it run the following command, replacing $RESOURCE_GROUP with the resource group in which you will run your cluster az role assignment create --resource-group $RESOURCE_GROUP --role \"AcrPull\" --assignee $aksidentity","title":"Step 3: Add permissions to the managed identity"},{"location":"setup_installation/azure/aks_acr_integration/#step-4-create-the-hopsworks-cluster","text":"Create the hopsworks cluster by following the same steps as in the getting started until the backup tab. Then click on next to get to the Managed containers tab. Set Use Azure AKS as enabled. One new field will pop up. Fill it with the name of the AKS you created above (hopsworks-aks). Click next and in the Virtual Network tab select the virtual network we created above (hopsworks-vnet). Finally, click next and select the subnet we created above (hopsworks-subnet) in the Subnet tab. You can now click on Review and Submit and create your cluster. Hopsworks AKS configuration","title":"Step 4: Create the Hopsworks cluster"},{"location":"setup_installation/azure/aks_acr_integration/#going-further","text":"You can also deploy the AKS cluster and the Hopsworks cluster in two different virtual networks by using network peering. For this we recommend to use our terraform provider and to refer to our example","title":"Going further"},{"location":"setup_installation/azure/cluster_creation/","text":"Getting started with managed.hopsworks.ai (Azure) # This guide goes into detail for each of the steps of the cluster creation in managed.hopsworks.ai Step 1 starting to create a cluster # In managed.hopsworks.ai , select Create cluster : Create a Hopsworks cluster Step 2 setting the General information # Select the Resource Group (1) you want to use. Note If the Resource Group does not appear in the drop-down, make sure that you properly created and set the custom role for this resource group. Name your cluster (2). Your cluster will be deployed in the Location of your Resource Group (3). Select the Instance type (4) and Local storage (5) size for the cluster Head node . Optional (6): Enable customer-managed encryption keys and specify a disk encryption set to be used for encryption of local storage. The disk encryption set has to be specified using the format: /subscriptions/[SUBSCRIPTION_ID]/resourceGroups/[RESOURCE_GROUP]/providers/Microsoft.Compute/diskEncryptionSets/[DISK_ENCRYPTION_SET] . Note that you have to grant the service principal of managed.hopsworks.ai Reader access to the disk encryption set and Key Vault Reader and Key Vault Secrets User on the key vault used with the disk encryption set. Refer to the Azure documentation for more details: Server-side encryption of Azure Disk Storage . To provide the capacity of adding and removing workers on demand, the Hopsworks clusters deployed by managed.hopsworks.ai store their data in an Azure storage container. In this step, you select which storage account and container to use for this purpose. Select the storage account (7) you want to use in Azure Storage account name . The name of the container in which the data will be stored is displayed in Azure Container name (8). You can change this name. For more details on how to create and configure a storage in Azure refer to Creating and configuring a storage Note You can choose to use a container already existing in your storage account by using the name of this container, but you need to first make sure that this container is empty. Enter the Azure container registry name (9) to be used as the managed docker registry for the cluster. General configuration On this page, you can also choose to opt out of managed.hopsworks.ai log collection. Managed.hopsworks.ai collects logs about the services running in your cluster to help us improve our system and to provide support. If you choose to opt-out from the log collection and need support you will have to provide us the log by yourself, which will slow down the support process. Step 3 workers configuration # In this step, you configure the workers. There are two possible setups static or autoscaling. In the static setup, the cluster has a fixed number of workers that you decide. You can then add and remove workers manually, for more details: the documentation . In the autoscaling setup, you configure conditions to add and remove workers and the cluster will automatically add and remove workers depending on the demand. Static workers configuration # You can set the static configuration by selecting Disabled in the first drop-down (1). Then you select the number of workers you want to start the cluster with (2). And, select the Instance type (3) and Local storage size (4) for the worker nodes . Create a Hopsworks cluster, static workers configuration Autoscaling workers configuration # You can set the autoscaling configuration by selecting enabled in the first drop-down (1). You can configure: The instance type you want to use. The size of the instances' disk. The minimum number of workers. The maximum number of workers. The targeted number of standby workers. Setting some resources in standby ensures that there are always some free resources in your cluster. This ensures that requests for new resources are fulfilled promptly. You configure the standby by setting the amount of workers you want to be in standby. For example, if you set a value of 0.5 the system will start a new worker every time the aggregated free cluster resources drop below 50% of a worker's resources. If you set this value to 0 new workers will only be started when a job or notebook request the resources. The time to wait before removing unused resources. One often starts a new computation shortly after finishing the previous one. To avoid having to wait for workers to stop and start between each computation it is recommended to wait before shutting down workers. Here you set the amount of time in seconds resources need to be unused before they get removed from the system. Note The standby will not be taken into account if you set the minimum number of workers to 0 and no resources are used in the cluster. This ensures that the number of nodes can fall to 0 when no resources are used. The standby will start to take effect as soon as you start using resources. Create a Hopsworks cluster, autoscale workers configuration Step 4 select a SSH key # When deploying clusters, managed.hopsworks.ai installs a ssh key on the cluster's instances so that you can access them if necessary. Select the SSH key that you want to use to access cluster instances. For more detail on how to add a shh key in Azure refer to Adding a ssh key to your resource group Choose SSH key Step 5 select the User assigned managed identity: # In order to let the cluster instances access to the Azure storage we need to attach a User assigned managed identity to the virtual machines. In this step you choose which identity to use. This identity need to have access right to the storage account you selected in Step 2 . For more information about how to create this identity and give it access to the storage account refer to Creating and configuring a storage : Choose the User assigned managed identity Step 6 set the backup retention policy: # To backup the Azure blob storage data when taking a cluster backups we need to set a retention policy for the blob storage. In this step, you choose the retention period in days. You can deactivate the retention policy by setting this value to 0 but this will block you from taking any backup of your cluster. Choose the backup retention policy Step 7 Virtual network selection # In this step, you can select the virtual network which will be used by the Hopsworks cluster. You can either select an existing virtual network or let managed.hopsworks.ai create one for you. If you decide to let managed.hopsworks.ai create the virtual network for you, you can choose the CIDR block for this virtual network. Refer to Create a virtual network and subnet for more details on how to create your own virtual network in Azure. Choose virtual network Step 8 Subnet selection # If you selected an existing virtual network in the previous step, this step lets you select which subnet of this virtual network to use. For more information about creating your own subnet refer to Create a virtual network and subnet . If you did not select an existing virtual network in the previous step managed.hopsworks.ai will create the subnet for you. You can choose the CIDR block this subnet will use. Choose subnet Step 9 Network Security group selection # In this step, you can select the network security group you want to use to manage the inbound and outbound network rules. You can either let managed.hopsworks.ai create a network security group for you or select an existing security group. For more information about how to create your own network security group in Azure refer to Create a network security group . Note Managed.hopsworks.ai require some rules for inbound and outbound traffic in your security group, for more details refer to inbound traffic rules and outbound traffic rules . Note Managed.hopsworks.ai attaches a public ip to your cluster by default. However, you can disable this behavior by unchecking the Attach Public IP checkbox. Choose security group Limiting outbound traffic to managed.hopsworks.ai # Clusters created on managed.hopsworks.ai need to be able to send http requests to api.hopsworks.ai. If you have strict regulation regarding outbound traffic, you can enable the Use static IPs to communicate with managed.hopsworks.ai checkbox to get the list of IPs to be allowed as shown below: Enable static IPs Step 10 User management selection # In this step, you can choose which user management system to use. You have four choices: Managed : managed.hopsworks.ai automatically adds and removes users from the Hopsworks cluster when you add and remove users from your organization (more details here ). OAuth2 : integrate the cluster with your organization's OAuth2 identity provider. See Use OAuth2 for user management for more detail. LDAP : integrate the cluster with your organization's LDAP/ActiveDirectory server. See Use LDAP for user management for more detail. Disabled : let you manage users manually from within Hopsworks. Choose user management type Step 12 Managed RonDB # Hopsworks uses RonDB as a database engine for its online Feature Store. By default database will run on its own VM. Premium users can scale-out database services to multiple VMs to handle increased workload. For details on how to configure RonDB check our guide here . Configure RonDB If you need to deploy a RonDB cluster instead of a single node please contact us . Step 13 add tags to your instances. # In this step, you can define tags that will be added to the cluster virtual machines. Add tags Step 14 add an init script to your instances. # In this step, you can enter an initialization script that will be run at startup on every instance. Note this init script must be a bash script starting with #!/usr/bin/env bash Add initialization script Step 15 Review and create # Review all information and select Create : Review cluster information The cluster will start. This will take a few minutes: Booting Hopsworks cluster As soon as the cluster has started, you will be able to log in to your new Hopsworks cluster. You will also be able to stop, restart, or terminate the cluster. Running Hopsworks cluster","title":"Cluster Creation"},{"location":"setup_installation/azure/cluster_creation/#getting-started-with-managedhopsworksai-azure","text":"This guide goes into detail for each of the steps of the cluster creation in managed.hopsworks.ai","title":"Getting started with managed.hopsworks.ai (Azure)"},{"location":"setup_installation/azure/cluster_creation/#step-1-starting-to-create-a-cluster","text":"In managed.hopsworks.ai , select Create cluster : Create a Hopsworks cluster","title":"Step 1 starting to create a cluster"},{"location":"setup_installation/azure/cluster_creation/#step-2-setting-the-general-information","text":"Select the Resource Group (1) you want to use. Note If the Resource Group does not appear in the drop-down, make sure that you properly created and set the custom role for this resource group. Name your cluster (2). Your cluster will be deployed in the Location of your Resource Group (3). Select the Instance type (4) and Local storage (5) size for the cluster Head node . Optional (6): Enable customer-managed encryption keys and specify a disk encryption set to be used for encryption of local storage. The disk encryption set has to be specified using the format: /subscriptions/[SUBSCRIPTION_ID]/resourceGroups/[RESOURCE_GROUP]/providers/Microsoft.Compute/diskEncryptionSets/[DISK_ENCRYPTION_SET] . Note that you have to grant the service principal of managed.hopsworks.ai Reader access to the disk encryption set and Key Vault Reader and Key Vault Secrets User on the key vault used with the disk encryption set. Refer to the Azure documentation for more details: Server-side encryption of Azure Disk Storage . To provide the capacity of adding and removing workers on demand, the Hopsworks clusters deployed by managed.hopsworks.ai store their data in an Azure storage container. In this step, you select which storage account and container to use for this purpose. Select the storage account (7) you want to use in Azure Storage account name . The name of the container in which the data will be stored is displayed in Azure Container name (8). You can change this name. For more details on how to create and configure a storage in Azure refer to Creating and configuring a storage Note You can choose to use a container already existing in your storage account by using the name of this container, but you need to first make sure that this container is empty. Enter the Azure container registry name (9) to be used as the managed docker registry for the cluster. General configuration On this page, you can also choose to opt out of managed.hopsworks.ai log collection. Managed.hopsworks.ai collects logs about the services running in your cluster to help us improve our system and to provide support. If you choose to opt-out from the log collection and need support you will have to provide us the log by yourself, which will slow down the support process.","title":"Step 2 setting the General information"},{"location":"setup_installation/azure/cluster_creation/#step-3-workers-configuration","text":"In this step, you configure the workers. There are two possible setups static or autoscaling. In the static setup, the cluster has a fixed number of workers that you decide. You can then add and remove workers manually, for more details: the documentation . In the autoscaling setup, you configure conditions to add and remove workers and the cluster will automatically add and remove workers depending on the demand.","title":"Step 3 workers configuration"},{"location":"setup_installation/azure/cluster_creation/#static-workers-configuration","text":"You can set the static configuration by selecting Disabled in the first drop-down (1). Then you select the number of workers you want to start the cluster with (2). And, select the Instance type (3) and Local storage size (4) for the worker nodes . Create a Hopsworks cluster, static workers configuration","title":"Static workers configuration"},{"location":"setup_installation/azure/cluster_creation/#autoscaling-workers-configuration","text":"You can set the autoscaling configuration by selecting enabled in the first drop-down (1). You can configure: The instance type you want to use. The size of the instances' disk. The minimum number of workers. The maximum number of workers. The targeted number of standby workers. Setting some resources in standby ensures that there are always some free resources in your cluster. This ensures that requests for new resources are fulfilled promptly. You configure the standby by setting the amount of workers you want to be in standby. For example, if you set a value of 0.5 the system will start a new worker every time the aggregated free cluster resources drop below 50% of a worker's resources. If you set this value to 0 new workers will only be started when a job or notebook request the resources. The time to wait before removing unused resources. One often starts a new computation shortly after finishing the previous one. To avoid having to wait for workers to stop and start between each computation it is recommended to wait before shutting down workers. Here you set the amount of time in seconds resources need to be unused before they get removed from the system. Note The standby will not be taken into account if you set the minimum number of workers to 0 and no resources are used in the cluster. This ensures that the number of nodes can fall to 0 when no resources are used. The standby will start to take effect as soon as you start using resources. Create a Hopsworks cluster, autoscale workers configuration","title":"Autoscaling workers configuration"},{"location":"setup_installation/azure/cluster_creation/#step-4-select-a-ssh-key","text":"When deploying clusters, managed.hopsworks.ai installs a ssh key on the cluster's instances so that you can access them if necessary. Select the SSH key that you want to use to access cluster instances. For more detail on how to add a shh key in Azure refer to Adding a ssh key to your resource group Choose SSH key","title":"Step 4 select a SSH key"},{"location":"setup_installation/azure/cluster_creation/#step-5-select-the-user-assigned-managed-identity","text":"In order to let the cluster instances access to the Azure storage we need to attach a User assigned managed identity to the virtual machines. In this step you choose which identity to use. This identity need to have access right to the storage account you selected in Step 2 . For more information about how to create this identity and give it access to the storage account refer to Creating and configuring a storage : Choose the User assigned managed identity","title":"Step 5 select the User assigned managed identity:"},{"location":"setup_installation/azure/cluster_creation/#step-6-set-the-backup-retention-policy","text":"To backup the Azure blob storage data when taking a cluster backups we need to set a retention policy for the blob storage. In this step, you choose the retention period in days. You can deactivate the retention policy by setting this value to 0 but this will block you from taking any backup of your cluster. Choose the backup retention policy","title":"Step 6 set the backup retention policy:"},{"location":"setup_installation/azure/cluster_creation/#step-7-virtual-network-selection","text":"In this step, you can select the virtual network which will be used by the Hopsworks cluster. You can either select an existing virtual network or let managed.hopsworks.ai create one for you. If you decide to let managed.hopsworks.ai create the virtual network for you, you can choose the CIDR block for this virtual network. Refer to Create a virtual network and subnet for more details on how to create your own virtual network in Azure. Choose virtual network","title":"Step 7 Virtual network selection"},{"location":"setup_installation/azure/cluster_creation/#step-8-subnet-selection","text":"If you selected an existing virtual network in the previous step, this step lets you select which subnet of this virtual network to use. For more information about creating your own subnet refer to Create a virtual network and subnet . If you did not select an existing virtual network in the previous step managed.hopsworks.ai will create the subnet for you. You can choose the CIDR block this subnet will use. Choose subnet","title":"Step 8 Subnet selection"},{"location":"setup_installation/azure/cluster_creation/#step-9-network-security-group-selection","text":"In this step, you can select the network security group you want to use to manage the inbound and outbound network rules. You can either let managed.hopsworks.ai create a network security group for you or select an existing security group. For more information about how to create your own network security group in Azure refer to Create a network security group . Note Managed.hopsworks.ai require some rules for inbound and outbound traffic in your security group, for more details refer to inbound traffic rules and outbound traffic rules . Note Managed.hopsworks.ai attaches a public ip to your cluster by default. However, you can disable this behavior by unchecking the Attach Public IP checkbox. Choose security group","title":"Step 9 Network Security group selection"},{"location":"setup_installation/azure/cluster_creation/#limiting-outbound-traffic-to-managedhopsworksai","text":"Clusters created on managed.hopsworks.ai need to be able to send http requests to api.hopsworks.ai. If you have strict regulation regarding outbound traffic, you can enable the Use static IPs to communicate with managed.hopsworks.ai checkbox to get the list of IPs to be allowed as shown below: Enable static IPs","title":"Limiting outbound traffic to managed.hopsworks.ai"},{"location":"setup_installation/azure/cluster_creation/#step-10-user-management-selection","text":"In this step, you can choose which user management system to use. You have four choices: Managed : managed.hopsworks.ai automatically adds and removes users from the Hopsworks cluster when you add and remove users from your organization (more details here ). OAuth2 : integrate the cluster with your organization's OAuth2 identity provider. See Use OAuth2 for user management for more detail. LDAP : integrate the cluster with your organization's LDAP/ActiveDirectory server. See Use LDAP for user management for more detail. Disabled : let you manage users manually from within Hopsworks. Choose user management type","title":"Step 10 User management selection"},{"location":"setup_installation/azure/cluster_creation/#step-12-managed-rondb","text":"Hopsworks uses RonDB as a database engine for its online Feature Store. By default database will run on its own VM. Premium users can scale-out database services to multiple VMs to handle increased workload. For details on how to configure RonDB check our guide here . Configure RonDB If you need to deploy a RonDB cluster instead of a single node please contact us .","title":"Step 12 Managed RonDB"},{"location":"setup_installation/azure/cluster_creation/#step-13-add-tags-to-your-instances","text":"In this step, you can define tags that will be added to the cluster virtual machines. Add tags","title":"Step 13 add tags to your instances."},{"location":"setup_installation/azure/cluster_creation/#step-14-add-an-init-script-to-your-instances","text":"In this step, you can enter an initialization script that will be run at startup on every instance. Note this init script must be a bash script starting with #!/usr/bin/env bash Add initialization script","title":"Step 14 add an init script to your instances."},{"location":"setup_installation/azure/cluster_creation/#step-15-review-and-create","text":"Review all information and select Create : Review cluster information The cluster will start. This will take a few minutes: Booting Hopsworks cluster As soon as the cluster has started, you will be able to log in to your new Hopsworks cluster. You will also be able to stop, restart, or terminate the cluster. Running Hopsworks cluster","title":"Step 15 Review and create"},{"location":"setup_installation/azure/getting_started/","text":"Getting started with managed.hopsworks.ai (Azure) # Managed.hopsworks.ai is our managed platform for running Hopsworks and the Feature Store in the cloud. It integrates seamlessly with third-party platforms such as Databricks, SageMaker and KubeFlow. This guide shows how to set up managed.hopsworks.ai with your organization's Azure account. Prerequisites # To follow the instruction on this page you will need the following: An Azure resource group in which the Hopsworks cluster will be deployed. The azure CLI installed and logged in . Permissions # To run all the commands on this page the user needs to have at least the following permissions on the Azure resource group: Microso ft .Au t horiza t io n /roleDe f i n i t io ns /wri te Microso ft .Au t horiza t io n /roleAssig n me nts /wri te Microso ft .Compu te /sshPublicKeys/ge nerate KeyPair/ac t io n Microso ft .Compu te /sshPublicKeys/read Microso ft .Compu te /sshPublicKeys/wri te Microso ft .Co nta i ner Regis tr y/regis tr ies/opera t io n S tatuses /read Microso ft .Co nta i ner Regis tr y/regis tr ies/read Microso ft .Co nta i ner Regis tr y/regis tr ies/wri te Microso ft .Ma na gedIde nt i t y/userAssig ne dIde nt i t ies/wri te Microso ft .Resources/subscrip t io ns /resourcegroups/read Microso ft .S t orage/s t orageAccou nts /wri te You will also need to have a role such as Application Administrator on the Azure Active Directory to be able to create the hopsworks.ai service principal. Resource providers # For managed.hopsworks.ai to deploy a cluster the following resource providers need to be registered on your Azure subscription. Microso ft .Ne t work Microso ft .Compu te Microso ft .S t orage Microso ft .Ma na gedIde nt i t y Microso ft .Co nta i ner Regis tr y This can be done by running the following commands: Note To run these commands you need to have the following permission on your subscription: Microsoft.Network/register/action az provider register --namespace 'Microsoft.Network' az provider register --namespace 'Microsoft.Compute' az provider register --namespace 'Microsoft.Storage' az provider register --namespace 'Microsoft.ManagedIdentity' az provider register --namespace 'Microsoft.ContainerRegistry' Other # All the commands have been written for a Unix system. These commands will need to be adapted to your terminal if it is not directly compatible. All the commands use your default location. Add the --location parameter if you want to run your cluster in another location. Make sure to create the resources in the same location as you are going to run your cluster. Step 1: Connect your Azure account # Managed.hopsworks.ai deploys Hopsworks clusters to your Azure account. To enable this, you have to create a service principal and a custom role for managed.hopsworks.ai granting access to your resource group. Step 1.1: Connect your Azure account # In managed.hopsworks.ai click on Connect to Azure or go to Settings and click on Configure next to Azure . This will direct you to a page with the instructions needed to create the service principal and set up the connection. Follow the instructions. Note it is possible to limit the permissions that are set up during this phase. For more details see restrictive-permissions . Cloud account settings Step 2: Create a storage # Note If you prefer using terraform, you can skip this step and the remaining steps, and instead, follow this guide . The Hopsworks clusters deployed by managed.hopsworks.ai store their data in a storage container in your Azure account. To enable this you need to create a storage account. This is done by running the following command, replacing $RESOURCE_GROUP with the name of your resource group. az storage account create --resource-group $RESOURCE_GROUP --name hopsworksstorage $RANDOM Step 3: Create an ACR Container Registry # The Hopsworks clusters deployed by managed.hopsworks.ai store their docker images in a container registry in your Azure account. To create this storage account run the following command, replacing $RESOURCE_GROUP with the name of your resource group. az acr create --resource-group $RESOURCE_GROUP --name hopsworksecr --sku Premium To prevent the registry from filling up with unnecessary images and artifacts you can enable a retention policy. A retention policy will automatically remove untagged manifests after a specified number of days. To enable a retention policy, run the following command, replacing $RESOURCE_GROUP with the name of your resource group. az acr config retention update --resource-group $RESOURCE_GROUP --registry hopsworksecr --status Enabled --days 7 --type UntaggedManifests Step 4: Create a managed identity # To allow the hopsworks cluster instances to access the storage account and the container registry, managed.hopsworks.ai assigns a managed identity to the cluster nodes. To enable this you need to: Create a managed identity Create a role with appropriate permission and assign it to the managed identity Step 4.1: Create a managed identity # You create a managed identity by running the following command, replacing $RESOURCE_GROUP with the name of your resource group. identityId = $( az identity create --name hopsworks-instance --resource-group $RESOURCE_GROUP --query principalId -o tsv ) Step 4.2: Create a role for the managed identity # To create a new role for the managed identity, first, create a file called instance-role.json with the following content. Replace SUBSCRIPTION_ID by your subscription id and RESOURCE_GROUP by your resource group { \"Name\" : \"hopsworks-instance\" , \"IsCustom\" : true , \"Description\" : \"Allow the hopsworks instance to access the storage and the docker repository\" , \"Actions\" : [ \"Microsoft.Storage/storageAccounts/blobServices/containers/write\" , \"Microsoft.Storage/storageAccounts/blobServices/containers/read\" , \"Microsoft.Storage/storageAccounts/blobServices/write\" , \"Microsoft.Storage/storageAccounts/blobServices/read\" , \"Microsoft.Storage/storageAccounts/listKeys/action\" , \"Microsoft.ContainerRegistry/registries/artifacts/delete\" , \"Microsoft.ContainerRegistry/registries/pull/read\" , \"Microsoft.ContainerRegistry/registries/push/write\" ], \"NotActions\" : [ ], \"DataActions\" : [ \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/delete\" , \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/read\" , \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/move/action\" , \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/write\" ], \"AssignableScopes\" : [ \"/subscriptions/SUBSCRIPTION_ID/resourceGroups/RESOURCE_GROUP\" ] } Then run the following command, to create the new role. az role definition create --role-definition instance-role.json Finally assign the role to the managed identity by running the following command, replacing $RESOURCE_GROUP with the name of your resource group. az role assignment create --resource-group $RESOURCE_GROUP --role hopsworks-instance --assignee $identityId Note It takes several minutes between the time you create the managed identity and the time a role can be assigned to it. So if we get an error message starting by the following wait and retry: Cannot find user or service principal in graph database Step 5: Add an ssh key to your resource group # When deploying clusters, managed.hopsworks.ai installs an ssh key on the cluster's instances so that you can access them if necessary. For this purpose, you need to add an ssh key to your resource group. To create an ssh key in your resource group run the following command, replacing $RESOURCE_GROUP with the name of your resource group. az sshkey create --resource-group $RESOURCE_GROUP --name hopsworksKey Note the command returns the path to the private and public keys associated with this ssh key. You can also create a key from an existing public key as indicated in the Azure documentation Step 6: Deploy a Hopsworks cluster # In managed.hopsworks.ai , select Create cluster : Create a Hopsworks cluster Select the Resource Group (1) in which you created your storage account and managed identity (see above). Note If the Resource Group does not appear in the drop-down, make sure that the custom role you created in step 1.1 has the Microsoft.Resources/subscriptions/resourceGroups/read permission and is assigned to the hopsworks.ai user. Name your cluster (2). Your cluster will be deployed in the Location of your Resource Group (3). Select the Instance type (4) and Local storage (5) size for the cluster Head node . Check if you want to Use customer-managed encryption key (6) Select the storage account (7) you created above in Azure Storage account name . The name of the container in which the data will be stored is displayed in Azure Container name (8), you can modify it if needed. Note You can choose to use a container already existing in your storage account by using the name of this container, but you need to first make sure that this container is empty. Enter the Azure container registry name (9) of the ACR registry created in Step 3.1 Press Next : General configuration Select the number of workers you want to start the cluster with (2). Select the Instance type (3) and Local storage size (4) for the worker nodes . Note It is possible to add or remove workers or to enable autoscaling once the cluster is running. Press Next : Create a Hopsworks cluster, static workers configuration Select the SSH key that you want to use to access cluster instances: Choose SSH key Select the User assigned managed identity that you created above: Choose the User assigned managed identity To backup the Azure blob storage data when taking a cluster backup we need to set a retention policy for the blob storage. You can deactivate the retention policy by setting this value to 0 but this will block you from taking any backup of your cluster. Choose the retention period in days and click on Review and submit . Choose the backup retention policy Review all information and select Create : Review cluster information Note We skipped cluster creation steps that are not mandatory. You can find more details about these steps here The cluster will start. This will take a few minutes: Booting Hopsworks cluster As soon as the cluster has started, you will be able to log in to your new Hopsworks cluster. You will also be able to stop, restart or terminate the cluster. Running Hopsworks cluster Step 7: Next steps # Check out our other guides for how to get started with Hopsworks and the Feature Store: Make Hopsworks services accessible from outside services Get started with the Hopsworks Feature Store Follow one of our tutorials Follow one of our Guide Code examples and notebooks: hops-examples","title":"Getting Started"},{"location":"setup_installation/azure/getting_started/#getting-started-with-managedhopsworksai-azure","text":"Managed.hopsworks.ai is our managed platform for running Hopsworks and the Feature Store in the cloud. It integrates seamlessly with third-party platforms such as Databricks, SageMaker and KubeFlow. This guide shows how to set up managed.hopsworks.ai with your organization's Azure account.","title":"Getting started with managed.hopsworks.ai (Azure)"},{"location":"setup_installation/azure/getting_started/#prerequisites","text":"To follow the instruction on this page you will need the following: An Azure resource group in which the Hopsworks cluster will be deployed. The azure CLI installed and logged in .","title":"Prerequisites"},{"location":"setup_installation/azure/getting_started/#permissions","text":"To run all the commands on this page the user needs to have at least the following permissions on the Azure resource group: Microso ft .Au t horiza t io n /roleDe f i n i t io ns /wri te Microso ft .Au t horiza t io n /roleAssig n me nts /wri te Microso ft .Compu te /sshPublicKeys/ge nerate KeyPair/ac t io n Microso ft .Compu te /sshPublicKeys/read Microso ft .Compu te /sshPublicKeys/wri te Microso ft .Co nta i ner Regis tr y/regis tr ies/opera t io n S tatuses /read Microso ft .Co nta i ner Regis tr y/regis tr ies/read Microso ft .Co nta i ner Regis tr y/regis tr ies/wri te Microso ft .Ma na gedIde nt i t y/userAssig ne dIde nt i t ies/wri te Microso ft .Resources/subscrip t io ns /resourcegroups/read Microso ft .S t orage/s t orageAccou nts /wri te You will also need to have a role such as Application Administrator on the Azure Active Directory to be able to create the hopsworks.ai service principal.","title":"Permissions"},{"location":"setup_installation/azure/getting_started/#resource-providers","text":"For managed.hopsworks.ai to deploy a cluster the following resource providers need to be registered on your Azure subscription. Microso ft .Ne t work Microso ft .Compu te Microso ft .S t orage Microso ft .Ma na gedIde nt i t y Microso ft .Co nta i ner Regis tr y This can be done by running the following commands: Note To run these commands you need to have the following permission on your subscription: Microsoft.Network/register/action az provider register --namespace 'Microsoft.Network' az provider register --namespace 'Microsoft.Compute' az provider register --namespace 'Microsoft.Storage' az provider register --namespace 'Microsoft.ManagedIdentity' az provider register --namespace 'Microsoft.ContainerRegistry'","title":"Resource providers"},{"location":"setup_installation/azure/getting_started/#other","text":"All the commands have been written for a Unix system. These commands will need to be adapted to your terminal if it is not directly compatible. All the commands use your default location. Add the --location parameter if you want to run your cluster in another location. Make sure to create the resources in the same location as you are going to run your cluster.","title":"Other"},{"location":"setup_installation/azure/getting_started/#step-1-connect-your-azure-account","text":"Managed.hopsworks.ai deploys Hopsworks clusters to your Azure account. To enable this, you have to create a service principal and a custom role for managed.hopsworks.ai granting access to your resource group.","title":"Step 1: Connect your Azure account"},{"location":"setup_installation/azure/getting_started/#step-11-connect-your-azure-account","text":"In managed.hopsworks.ai click on Connect to Azure or go to Settings and click on Configure next to Azure . This will direct you to a page with the instructions needed to create the service principal and set up the connection. Follow the instructions. Note it is possible to limit the permissions that are set up during this phase. For more details see restrictive-permissions . Cloud account settings","title":"Step 1.1: Connect your Azure account"},{"location":"setup_installation/azure/getting_started/#step-2-create-a-storage","text":"Note If you prefer using terraform, you can skip this step and the remaining steps, and instead, follow this guide . The Hopsworks clusters deployed by managed.hopsworks.ai store their data in a storage container in your Azure account. To enable this you need to create a storage account. This is done by running the following command, replacing $RESOURCE_GROUP with the name of your resource group. az storage account create --resource-group $RESOURCE_GROUP --name hopsworksstorage $RANDOM","title":"Step 2: Create a storage"},{"location":"setup_installation/azure/getting_started/#step-3-create-an-acr-container-registry","text":"The Hopsworks clusters deployed by managed.hopsworks.ai store their docker images in a container registry in your Azure account. To create this storage account run the following command, replacing $RESOURCE_GROUP with the name of your resource group. az acr create --resource-group $RESOURCE_GROUP --name hopsworksecr --sku Premium To prevent the registry from filling up with unnecessary images and artifacts you can enable a retention policy. A retention policy will automatically remove untagged manifests after a specified number of days. To enable a retention policy, run the following command, replacing $RESOURCE_GROUP with the name of your resource group. az acr config retention update --resource-group $RESOURCE_GROUP --registry hopsworksecr --status Enabled --days 7 --type UntaggedManifests","title":"Step 3: Create an ACR Container Registry"},{"location":"setup_installation/azure/getting_started/#step-4-create-a-managed-identity","text":"To allow the hopsworks cluster instances to access the storage account and the container registry, managed.hopsworks.ai assigns a managed identity to the cluster nodes. To enable this you need to: Create a managed identity Create a role with appropriate permission and assign it to the managed identity","title":"Step 4: Create a managed identity"},{"location":"setup_installation/azure/getting_started/#step-41-create-a-managed-identity","text":"You create a managed identity by running the following command, replacing $RESOURCE_GROUP with the name of your resource group. identityId = $( az identity create --name hopsworks-instance --resource-group $RESOURCE_GROUP --query principalId -o tsv )","title":"Step 4.1: Create a managed identity"},{"location":"setup_installation/azure/getting_started/#step-42-create-a-role-for-the-managed-identity","text":"To create a new role for the managed identity, first, create a file called instance-role.json with the following content. Replace SUBSCRIPTION_ID by your subscription id and RESOURCE_GROUP by your resource group { \"Name\" : \"hopsworks-instance\" , \"IsCustom\" : true , \"Description\" : \"Allow the hopsworks instance to access the storage and the docker repository\" , \"Actions\" : [ \"Microsoft.Storage/storageAccounts/blobServices/containers/write\" , \"Microsoft.Storage/storageAccounts/blobServices/containers/read\" , \"Microsoft.Storage/storageAccounts/blobServices/write\" , \"Microsoft.Storage/storageAccounts/blobServices/read\" , \"Microsoft.Storage/storageAccounts/listKeys/action\" , \"Microsoft.ContainerRegistry/registries/artifacts/delete\" , \"Microsoft.ContainerRegistry/registries/pull/read\" , \"Microsoft.ContainerRegistry/registries/push/write\" ], \"NotActions\" : [ ], \"DataActions\" : [ \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/delete\" , \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/read\" , \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/move/action\" , \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/write\" ], \"AssignableScopes\" : [ \"/subscriptions/SUBSCRIPTION_ID/resourceGroups/RESOURCE_GROUP\" ] } Then run the following command, to create the new role. az role definition create --role-definition instance-role.json Finally assign the role to the managed identity by running the following command, replacing $RESOURCE_GROUP with the name of your resource group. az role assignment create --resource-group $RESOURCE_GROUP --role hopsworks-instance --assignee $identityId Note It takes several minutes between the time you create the managed identity and the time a role can be assigned to it. So if we get an error message starting by the following wait and retry: Cannot find user or service principal in graph database","title":"Step 4.2: Create a role for the managed identity"},{"location":"setup_installation/azure/getting_started/#step-5-add-an-ssh-key-to-your-resource-group","text":"When deploying clusters, managed.hopsworks.ai installs an ssh key on the cluster's instances so that you can access them if necessary. For this purpose, you need to add an ssh key to your resource group. To create an ssh key in your resource group run the following command, replacing $RESOURCE_GROUP with the name of your resource group. az sshkey create --resource-group $RESOURCE_GROUP --name hopsworksKey Note the command returns the path to the private and public keys associated with this ssh key. You can also create a key from an existing public key as indicated in the Azure documentation","title":"Step 5: Add an ssh key to your resource group"},{"location":"setup_installation/azure/getting_started/#step-6-deploy-a-hopsworks-cluster","text":"In managed.hopsworks.ai , select Create cluster : Create a Hopsworks cluster Select the Resource Group (1) in which you created your storage account and managed identity (see above). Note If the Resource Group does not appear in the drop-down, make sure that the custom role you created in step 1.1 has the Microsoft.Resources/subscriptions/resourceGroups/read permission and is assigned to the hopsworks.ai user. Name your cluster (2). Your cluster will be deployed in the Location of your Resource Group (3). Select the Instance type (4) and Local storage (5) size for the cluster Head node . Check if you want to Use customer-managed encryption key (6) Select the storage account (7) you created above in Azure Storage account name . The name of the container in which the data will be stored is displayed in Azure Container name (8), you can modify it if needed. Note You can choose to use a container already existing in your storage account by using the name of this container, but you need to first make sure that this container is empty. Enter the Azure container registry name (9) of the ACR registry created in Step 3.1 Press Next : General configuration Select the number of workers you want to start the cluster with (2). Select the Instance type (3) and Local storage size (4) for the worker nodes . Note It is possible to add or remove workers or to enable autoscaling once the cluster is running. Press Next : Create a Hopsworks cluster, static workers configuration Select the SSH key that you want to use to access cluster instances: Choose SSH key Select the User assigned managed identity that you created above: Choose the User assigned managed identity To backup the Azure blob storage data when taking a cluster backup we need to set a retention policy for the blob storage. You can deactivate the retention policy by setting this value to 0 but this will block you from taking any backup of your cluster. Choose the retention period in days and click on Review and submit . Choose the backup retention policy Review all information and select Create : Review cluster information Note We skipped cluster creation steps that are not mandatory. You can find more details about these steps here The cluster will start. This will take a few minutes: Booting Hopsworks cluster As soon as the cluster has started, you will be able to log in to your new Hopsworks cluster. You will also be able to stop, restart or terminate the cluster. Running Hopsworks cluster","title":"Step 6: Deploy a Hopsworks cluster"},{"location":"setup_installation/azure/getting_started/#step-7-next-steps","text":"Check out our other guides for how to get started with Hopsworks and the Feature Store: Make Hopsworks services accessible from outside services Get started with the Hopsworks Feature Store Follow one of our tutorials Follow one of our Guide Code examples and notebooks: hops-examples","title":"Step 7: Next steps"},{"location":"setup_installation/azure/restrictive_permissions/","text":"Limiting Azure permissions # Managed.hopsworks.ai requires a set of permissions to be able to manage resources in the user\u2019s Azure resource group. By default, these permissions are set to easily allow a wide range of different configurations and allow us to automate as many steps as possible. While we ensure to never access resources we shouldn\u2019t, we do understand that this might not be enough for your organization or security policy. This guide explains how to lock down access permissions following the IT security policy principle of least privilege. Default permissions # This is the list of default permissions that are required by managed.hopsworks.ai . If you prefer to limit these permissions, then proceed to the next section . { \"Name\" : \"hopsworks.ai\" , \"IsCustom\" : true , \"Description\" : \"Allows hopsworks.ai to start and manage clusters\" , \"Actions\" : [ \"Microsoft.Compute/virtualMachines/write\" , \"Microsoft.Compute/virtualMachines/start/action\" , \"Microsoft.Compute/virtualMachines/delete\" , \"Microsoft.Compute/virtualMachines/read\" , \"Microsoft.Compute/virtualMachines/deallocate/action\" , \"Microsoft.Compute/disks/write\" , \"Microsoft.Compute/disks/delete\" , \"Microsoft.Network/networkInterfaces/read\" , \"Microsoft.Network/networkInterfaces/join/action\" , \"Microsoft.Network/networkInterfaces/write\" , \"Microsoft.Network/networkInterfaces/delete\" , \"Microsoft.Network/networkSecurityGroups/read\" , \"Microsoft.Network/networkSecurityGroups/join/action\" , \"Microsoft.Network/networkSecurityGroups/write\" , \"Microsoft.Network/networkSecurityGroups/delete\" , \"Microsoft.Network/publicIPAddresses/join/action\" , \"Microsoft.Network/publicIPAddresses/read\" , \"Microsoft.Network/publicIPAddresses/write\" , \"Microsoft.Network/publicIPAddresses/delete\" , \"Microsoft.Network/virtualNetworks/write\" , \"Microsoft.Network/virtualNetworks/delete\" , \"Microsoft.Network/virtualNetworks/read\" , \"Microsoft.Network/virtualNetworks/subnets/read\" , \"Microsoft.Network/virtualNetworks/subnets/join/action\" , \"Microsoft.Resources/subscriptions/resourceGroups/read\" , \"Microsoft.Compute/sshPublicKeys/read\" , \"Microsoft.ManagedIdentity/userAssignedIdentities/assign/action\" , \"Microsoft.ManagedIdentity/userAssignedIdentities/read\" , \"Microsoft.Storage/storageAccounts/read\" , \"Microsoft.Compute/snapshots/write\" , \"Microsoft.Compute/snapshots/read\" , \"Microsoft.Compute/snapshots/delete\" , \"Microsoft.Compute/disks/beginGetAccess/action\" , \"Microsoft.Compute/disks/read\" , \"Microsoft.Authorization/roleAssignments/read\" ], \"NotActions\" : [], \"AssignableScopes\" : [ \"/subscriptions/SUBSCRIPTION_ID/resourceGroups/RESOURCE_GROUP\" ] } Limiting the cross-account role permissions # Step 1: Create a virtual network and subnet # To restrict managed.hopsworks.ai from having write and delete access on virtual networks and subnet you need to create them manually. This can be achieved in the Azure portal following this guide: Create a virtual network . Make sure to use the resource group and location in which you intend to deploy your Hopsworks cluster. For the remaining of the configuration, the default options proposed by the portal should work out of the box. Note the names of the virtual network and subnet you want to use for the following steps. Step 2: Create a network security group # To restrict managed.hopsworks.ai from having write and delete access on network security groups you need to create it manually. This can be achieved in the Azure portal following this guide: Create a network security group . Make sure to use the resource group and location in which you intend to deploy your Hopsworks cluster. Inbound traffic # For managed.hopsworks.ai to create the SSL certificates the network security group needs to allow inbound traffic on port 80. For this, you need to add an inbound security rule to your network security group. This can be achieved in the Azure portal following this guide: Create a security rule . Setting the destination port ranges to 80 and letting the default values for the other fields should work out of the box. Note If you intend to use the managed users option on your Hopsworks cluster you should also add a rule to open port 443. Outbound traffic # Clusters created on managed.hopsworks.ai need to be able to send http requests to api.hopsworks.ai . The api.hopsworks.ai domain use a content delivery network for better performance. This result in the impossibility to predict which IP the request will be sent to. If you require a list of static IPs to allow outbound traffic from your security group, use the static IPs option during cluster creation . Note If you intend to use the managed users option on your Hopsworks cluster you should also allow outbound traffic to cognito-idp.us-east-2.amazonaws.com and managedhopsworks-prod.auth.us-east-2.amazoncognito.com . Step 3: Set permissions of the cross-account role # During the account setup for managed.hopsworks.ai , you were asked to create create a custom role for your resource group. Edit this role in the Azure portal by going to your resource group, clicking on Access control (IAM) , opening the tab Roles , searching for the role you created, clicking on the three dots at the end of the role line and clicking on edit. You can then navigate to the JSON tab and overwrite the \"action\" field with the following: \"actions\" : [ \"Microsoft.Compute/virtualMachines/write\" , \"Microsoft.Compute/virtualMachines/start/action\" , \"Microsoft.Compute/virtualMachines/delete\" , \"Microsoft.Compute/virtualMachines/read\" , \"Microsoft.Compute/virtualMachines/deallocate/action\" , \"Microsoft.Compute/disks/write\" , \"Microsoft.Compute/disks/delete\" , \"Microsoft.Network/networkInterfaces/read\" , \"Microsoft.Network/networkInterfaces/join/action\" , \"Microsoft.Network/networkInterfaces/write\" , \"Microsoft.Network/networkInterfaces/delete\" , \"Microsoft.Network/networkSecurityGroups/read\" , \"Microsoft.Network/networkSecurityGroups/join/action\" , \"Microsoft.Network/publicIPAddresses/join/action\" , \"Microsoft.Network/publicIPAddresses/read\" , \"Microsoft.Network/publicIPAddresses/write\" , \"Microsoft.Network/publicIPAddresses/delete\" , \"Microsoft.Network/virtualNetworks/read\" , \"Microsoft.Network/virtualNetworks/subnets/read\" , \"Microsoft.Network/virtualNetworks/subnets/join/action\" , \"Microsoft.Resources/subscriptions/resourceGroups/read\" , \"Microsoft.Compute/sshPublicKeys/read\" , \"Microsoft.ManagedIdentity/userAssignedIdentities/assign/action\" , \"Microsoft.ManagedIdentity/userAssignedIdentities/read\" , \"Microsoft.Storage/storageAccounts/read\" , \"Microsoft.Compute/snapshots/write\" , \"Microsoft.Compute/snapshots/read\" , \"Microsoft.Compute/snapshots/delete\" , \"Microsoft.Compute/disks/beginGetAccess/action\" , \"Microsoft.Compute/disks/read\" ] Step 4: Create your Hopsworks instance # You can now create a new Hopsworks instance in managed.hopsworks.ai by selecting the virtual network, subnet, and network security group during the instance configuration. Backup permissions # The following permissions are only needed for the backup feature: \"actions\" : [ \"Microsoft.Compute/snapshots/write\" , \"Microsoft.Compute/snapshots/read\" , \"Microsoft.Compute/snapshots/delete\" , \"Microsoft.Compute/disks/beginGetAccess/action\" , ] If you are not going to create backups or if you do not have access to this Enterprise feature, you can further limit the permission of the cross-account role to the following: \"actions\" : [ \"Microsoft.Compute/virtualMachines/write\" , \"Microsoft.Compute/virtualMachines/start/action\" , \"Microsoft.Compute/virtualMachines/delete\" , \"Microsoft.Compute/virtualMachines/read\" , \"Microsoft.Compute/virtualMachines/deallocate/action\" , \"Microsoft.Compute/disks/write\" , \"Microsoft.Compute/disks/delete\" , \"Microsoft.Network/networkInterfaces/read\" , \"Microsoft.Network/networkInterfaces/join/action\" , \"Microsoft.Network/networkInterfaces/write\" , \"Microsoft.Network/networkInterfaces/delete\" , \"Microsoft.Network/networkSecurityGroups/read\" , \"Microsoft.Network/networkSecurityGroups/join/action\" , \"Microsoft.Network/publicIPAddresses/join/action\" , \"Microsoft.Network/publicIPAddresses/read\" , \"Microsoft.Network/publicIPAddresses/write\" , \"Microsoft.Network/publicIPAddresses/delete\" , \"Microsoft.Network/virtualNetworks/read\" , \"Microsoft.Network/virtualNetworks/subnets/read\" , \"Microsoft.Network/virtualNetworks/subnets/join/action\" , \"Microsoft.Resources/subscriptions/resourceGroups/read\" , \"Microsoft.Compute/sshPublicKeys/read\" , \"Microsoft.ManagedIdentity/userAssignedIdentities/assign/action\" , \"Microsoft.ManagedIdentity/userAssignedIdentities/read\" , \"Microsoft.Storage/storageAccounts/read\" , ] Public IP Addresses permissions # The following permissions are used to create and attach a public IP Address to the head node. If you do not want to use a public IP Address for the head node, you can remove them: \"actions\" : [ \"Microsoft.Network/publicIPAddresses/join/action\" , \"Microsoft.Network/publicIPAddresses/read\" , \"Microsoft.Network/publicIPAddresses/write\" , \"Microsoft.Network/publicIPAddresses/delete\" , ] You then have to make sure that you uncheck the Attach Public IP check box in the Security Group section of the cluster creation: Attach Public IP Other removable permissions # The following permission is only needed to select the Azure Storage account through a drop-down during cluster creation. You can remove it from the cross-account role and enter the value manually \"actions\" : [ \"Microsoft.Storage/storageAccounts/read\" , ] The following permission is only needed, during cluster creation, to check that the managed identity has the proper permission. If you remove it, this check will not be done and the deployment may fail later if the managed identity does not have the proper permissions \"actions\" : [ \"Microsoft.Authorization/roleAssignments/read\" ] Limiting the User Assigned Managed Identity permissions # Backups # If you do not intend to take backups or if you do not have access to this Enterprise feature you can remove the permissions that are only used by the backup feature when configuring your managed identity storage permissions. For this remove the following actions from your user assigned managed identity : \"actions\" : [ \"Microsoft.Storage/storageAccounts/blobServices/write\" , \"Microsoft.Storage/storageAccounts/listKeys/action\" ]","title":"Limiting Permissions"},{"location":"setup_installation/azure/restrictive_permissions/#limiting-azure-permissions","text":"Managed.hopsworks.ai requires a set of permissions to be able to manage resources in the user\u2019s Azure resource group. By default, these permissions are set to easily allow a wide range of different configurations and allow us to automate as many steps as possible. While we ensure to never access resources we shouldn\u2019t, we do understand that this might not be enough for your organization or security policy. This guide explains how to lock down access permissions following the IT security policy principle of least privilege.","title":"Limiting Azure permissions"},{"location":"setup_installation/azure/restrictive_permissions/#default-permissions","text":"This is the list of default permissions that are required by managed.hopsworks.ai . If you prefer to limit these permissions, then proceed to the next section . { \"Name\" : \"hopsworks.ai\" , \"IsCustom\" : true , \"Description\" : \"Allows hopsworks.ai to start and manage clusters\" , \"Actions\" : [ \"Microsoft.Compute/virtualMachines/write\" , \"Microsoft.Compute/virtualMachines/start/action\" , \"Microsoft.Compute/virtualMachines/delete\" , \"Microsoft.Compute/virtualMachines/read\" , \"Microsoft.Compute/virtualMachines/deallocate/action\" , \"Microsoft.Compute/disks/write\" , \"Microsoft.Compute/disks/delete\" , \"Microsoft.Network/networkInterfaces/read\" , \"Microsoft.Network/networkInterfaces/join/action\" , \"Microsoft.Network/networkInterfaces/write\" , \"Microsoft.Network/networkInterfaces/delete\" , \"Microsoft.Network/networkSecurityGroups/read\" , \"Microsoft.Network/networkSecurityGroups/join/action\" , \"Microsoft.Network/networkSecurityGroups/write\" , \"Microsoft.Network/networkSecurityGroups/delete\" , \"Microsoft.Network/publicIPAddresses/join/action\" , \"Microsoft.Network/publicIPAddresses/read\" , \"Microsoft.Network/publicIPAddresses/write\" , \"Microsoft.Network/publicIPAddresses/delete\" , \"Microsoft.Network/virtualNetworks/write\" , \"Microsoft.Network/virtualNetworks/delete\" , \"Microsoft.Network/virtualNetworks/read\" , \"Microsoft.Network/virtualNetworks/subnets/read\" , \"Microsoft.Network/virtualNetworks/subnets/join/action\" , \"Microsoft.Resources/subscriptions/resourceGroups/read\" , \"Microsoft.Compute/sshPublicKeys/read\" , \"Microsoft.ManagedIdentity/userAssignedIdentities/assign/action\" , \"Microsoft.ManagedIdentity/userAssignedIdentities/read\" , \"Microsoft.Storage/storageAccounts/read\" , \"Microsoft.Compute/snapshots/write\" , \"Microsoft.Compute/snapshots/read\" , \"Microsoft.Compute/snapshots/delete\" , \"Microsoft.Compute/disks/beginGetAccess/action\" , \"Microsoft.Compute/disks/read\" , \"Microsoft.Authorization/roleAssignments/read\" ], \"NotActions\" : [], \"AssignableScopes\" : [ \"/subscriptions/SUBSCRIPTION_ID/resourceGroups/RESOURCE_GROUP\" ] }","title":"Default permissions"},{"location":"setup_installation/azure/restrictive_permissions/#limiting-the-cross-account-role-permissions","text":"","title":"Limiting the cross-account role permissions"},{"location":"setup_installation/azure/restrictive_permissions/#step-1-create-a-virtual-network-and-subnet","text":"To restrict managed.hopsworks.ai from having write and delete access on virtual networks and subnet you need to create them manually. This can be achieved in the Azure portal following this guide: Create a virtual network . Make sure to use the resource group and location in which you intend to deploy your Hopsworks cluster. For the remaining of the configuration, the default options proposed by the portal should work out of the box. Note the names of the virtual network and subnet you want to use for the following steps.","title":"Step 1: Create a virtual network and subnet"},{"location":"setup_installation/azure/restrictive_permissions/#step-2-create-a-network-security-group","text":"To restrict managed.hopsworks.ai from having write and delete access on network security groups you need to create it manually. This can be achieved in the Azure portal following this guide: Create a network security group . Make sure to use the resource group and location in which you intend to deploy your Hopsworks cluster.","title":"Step 2: Create a network security group"},{"location":"setup_installation/azure/restrictive_permissions/#inbound-traffic","text":"For managed.hopsworks.ai to create the SSL certificates the network security group needs to allow inbound traffic on port 80. For this, you need to add an inbound security rule to your network security group. This can be achieved in the Azure portal following this guide: Create a security rule . Setting the destination port ranges to 80 and letting the default values for the other fields should work out of the box. Note If you intend to use the managed users option on your Hopsworks cluster you should also add a rule to open port 443.","title":"Inbound traffic"},{"location":"setup_installation/azure/restrictive_permissions/#outbound-traffic","text":"Clusters created on managed.hopsworks.ai need to be able to send http requests to api.hopsworks.ai . The api.hopsworks.ai domain use a content delivery network for better performance. This result in the impossibility to predict which IP the request will be sent to. If you require a list of static IPs to allow outbound traffic from your security group, use the static IPs option during cluster creation . Note If you intend to use the managed users option on your Hopsworks cluster you should also allow outbound traffic to cognito-idp.us-east-2.amazonaws.com and managedhopsworks-prod.auth.us-east-2.amazoncognito.com .","title":"Outbound traffic"},{"location":"setup_installation/azure/restrictive_permissions/#step-3-set-permissions-of-the-cross-account-role","text":"During the account setup for managed.hopsworks.ai , you were asked to create create a custom role for your resource group. Edit this role in the Azure portal by going to your resource group, clicking on Access control (IAM) , opening the tab Roles , searching for the role you created, clicking on the three dots at the end of the role line and clicking on edit. You can then navigate to the JSON tab and overwrite the \"action\" field with the following: \"actions\" : [ \"Microsoft.Compute/virtualMachines/write\" , \"Microsoft.Compute/virtualMachines/start/action\" , \"Microsoft.Compute/virtualMachines/delete\" , \"Microsoft.Compute/virtualMachines/read\" , \"Microsoft.Compute/virtualMachines/deallocate/action\" , \"Microsoft.Compute/disks/write\" , \"Microsoft.Compute/disks/delete\" , \"Microsoft.Network/networkInterfaces/read\" , \"Microsoft.Network/networkInterfaces/join/action\" , \"Microsoft.Network/networkInterfaces/write\" , \"Microsoft.Network/networkInterfaces/delete\" , \"Microsoft.Network/networkSecurityGroups/read\" , \"Microsoft.Network/networkSecurityGroups/join/action\" , \"Microsoft.Network/publicIPAddresses/join/action\" , \"Microsoft.Network/publicIPAddresses/read\" , \"Microsoft.Network/publicIPAddresses/write\" , \"Microsoft.Network/publicIPAddresses/delete\" , \"Microsoft.Network/virtualNetworks/read\" , \"Microsoft.Network/virtualNetworks/subnets/read\" , \"Microsoft.Network/virtualNetworks/subnets/join/action\" , \"Microsoft.Resources/subscriptions/resourceGroups/read\" , \"Microsoft.Compute/sshPublicKeys/read\" , \"Microsoft.ManagedIdentity/userAssignedIdentities/assign/action\" , \"Microsoft.ManagedIdentity/userAssignedIdentities/read\" , \"Microsoft.Storage/storageAccounts/read\" , \"Microsoft.Compute/snapshots/write\" , \"Microsoft.Compute/snapshots/read\" , \"Microsoft.Compute/snapshots/delete\" , \"Microsoft.Compute/disks/beginGetAccess/action\" , \"Microsoft.Compute/disks/read\" ]","title":"Step 3: Set permissions of the cross-account role"},{"location":"setup_installation/azure/restrictive_permissions/#step-4-create-your-hopsworks-instance","text":"You can now create a new Hopsworks instance in managed.hopsworks.ai by selecting the virtual network, subnet, and network security group during the instance configuration.","title":"Step 4: Create your Hopsworks instance"},{"location":"setup_installation/azure/restrictive_permissions/#backup-permissions","text":"The following permissions are only needed for the backup feature: \"actions\" : [ \"Microsoft.Compute/snapshots/write\" , \"Microsoft.Compute/snapshots/read\" , \"Microsoft.Compute/snapshots/delete\" , \"Microsoft.Compute/disks/beginGetAccess/action\" , ] If you are not going to create backups or if you do not have access to this Enterprise feature, you can further limit the permission of the cross-account role to the following: \"actions\" : [ \"Microsoft.Compute/virtualMachines/write\" , \"Microsoft.Compute/virtualMachines/start/action\" , \"Microsoft.Compute/virtualMachines/delete\" , \"Microsoft.Compute/virtualMachines/read\" , \"Microsoft.Compute/virtualMachines/deallocate/action\" , \"Microsoft.Compute/disks/write\" , \"Microsoft.Compute/disks/delete\" , \"Microsoft.Network/networkInterfaces/read\" , \"Microsoft.Network/networkInterfaces/join/action\" , \"Microsoft.Network/networkInterfaces/write\" , \"Microsoft.Network/networkInterfaces/delete\" , \"Microsoft.Network/networkSecurityGroups/read\" , \"Microsoft.Network/networkSecurityGroups/join/action\" , \"Microsoft.Network/publicIPAddresses/join/action\" , \"Microsoft.Network/publicIPAddresses/read\" , \"Microsoft.Network/publicIPAddresses/write\" , \"Microsoft.Network/publicIPAddresses/delete\" , \"Microsoft.Network/virtualNetworks/read\" , \"Microsoft.Network/virtualNetworks/subnets/read\" , \"Microsoft.Network/virtualNetworks/subnets/join/action\" , \"Microsoft.Resources/subscriptions/resourceGroups/read\" , \"Microsoft.Compute/sshPublicKeys/read\" , \"Microsoft.ManagedIdentity/userAssignedIdentities/assign/action\" , \"Microsoft.ManagedIdentity/userAssignedIdentities/read\" , \"Microsoft.Storage/storageAccounts/read\" , ]","title":"Backup permissions"},{"location":"setup_installation/azure/restrictive_permissions/#public-ip-addresses-permissions","text":"The following permissions are used to create and attach a public IP Address to the head node. If you do not want to use a public IP Address for the head node, you can remove them: \"actions\" : [ \"Microsoft.Network/publicIPAddresses/join/action\" , \"Microsoft.Network/publicIPAddresses/read\" , \"Microsoft.Network/publicIPAddresses/write\" , \"Microsoft.Network/publicIPAddresses/delete\" , ] You then have to make sure that you uncheck the Attach Public IP check box in the Security Group section of the cluster creation: Attach Public IP","title":"Public IP Addresses permissions"},{"location":"setup_installation/azure/restrictive_permissions/#other-removable-permissions","text":"The following permission is only needed to select the Azure Storage account through a drop-down during cluster creation. You can remove it from the cross-account role and enter the value manually \"actions\" : [ \"Microsoft.Storage/storageAccounts/read\" , ] The following permission is only needed, during cluster creation, to check that the managed identity has the proper permission. If you remove it, this check will not be done and the deployment may fail later if the managed identity does not have the proper permissions \"actions\" : [ \"Microsoft.Authorization/roleAssignments/read\" ]","title":"Other removable permissions"},{"location":"setup_installation/azure/restrictive_permissions/#limiting-the-user-assigned-managed-identity-permissions","text":"","title":"Limiting the User Assigned Managed Identity permissions"},{"location":"setup_installation/azure/restrictive_permissions/#backups","text":"If you do not intend to take backups or if you do not have access to this Enterprise feature you can remove the permissions that are only used by the backup feature when configuring your managed identity storage permissions. For this remove the following actions from your user assigned managed identity : \"actions\" : [ \"Microsoft.Storage/storageAccounts/blobServices/write\" , \"Microsoft.Storage/storageAccounts/listKeys/action\" ]","title":"Backups"},{"location":"setup_installation/azure/upgrade/","text":"Upgrade existing clusters on managed.hopsworks.ai from version 2.2 or older (Azure) # This guide shows you how to upgrade your existing Hopsworks cluster to a newer version of Hopsworks. Step 1: Make sure your cluster is running # It is important that your cluster is Running . Otherwise you will not be able to upgrade. As soon as a new version is available an upgrade notification will appear: A new Hopsworks version is available Step 2: Add upgrade permissions to your user assigned managed identity # Note You can skip this step if you already have the following permissions in your user assigned managed identity : [ \"Microsoft.Compute/virtualMachines/read\" , \"Microsoft.Compute/virtualMachines/write\" , \"Microsoft.Compute/disks/read\" , \"Microsoft.Compute/disks/write\" , \"Microsoft.Storage/storageAccounts/listKeys/action\" ] Make sure that the scope of these permissions is your resource group. We require extra permissions to be added to the user assigned managed identity attached to your cluster to proceed with the upgrade. First to get the name of your user assigned managed identity and the resource group of your cluster, click on the Details tab as shown below: Get the resource group name (1) and the user assigned managed identity (2) of your cluster Step 2.1: Add custom role for upgrade permissions # Once you get the names of the resource group and user-assigned managed identity, navigate to Azure portal , then click on Resource groups and then search for your resource group and click on it. Go to the Access control (IAM) tab, select Add , and click on Add custom role Add a custom role for upgrade Name the custom role and then click on next till you reach the JSON tab. Name the custom role for upgrade Once you reach the JSON tab, click on Edit to edit the role permissions: Edit the JSON permissions for the custom role for upgrade Once you have clicked on Edit , replace the permissions array with the following snippet: \"permissions\" : [ { \"actions\" : [ \"Microsoft.Compute/virtualMachines/read\" , \"Microsoft.Compute/virtualMachines/write\" , \"Microsoft.Compute/disks/read\" , \"Microsoft.Compute/disks/write\" , \"Microsoft.Storage/storageAccounts/listKeys/action\" ], \"notActions\" : [], \"dataActions\" : [], \"notDataActions\" : [] } ] Then, click on Save to save the updated permissions Save permissions for the custom role for upgrade Click on Review and create and then click on Create to create the custom role: Save permissions for the custom role for upgrade Step 2.2: Assign the custom role to your user-assigned managed identity # Navigate back to the your Resource group home page at Azure portal , click on Add and then click on Add role assignment Assign upgrade role to your user assigned managed identity (1) choose the upgrade role that you have just created in Step 2.1 , (2) choose User Assigned Managed Identity , (3) search for the user assigned managed identity attached to your cluster and select it. Finally, (4) click on Save to save the role assignment. Assign upgrade role to your user assigned managed identity Warning When you assign roles or remove role assignments, it can take up to 30 minutes for changes to take effect. Step 3: Add disk read permissions to your role connected to managed.hopsworks.ai # We require extra permission (\"Microsoft.Compute/disks/read\") to be added to the role you used to connect to managed.hopsworks.ai , the one that you have created when connecting your azure account . If you don't remember the name of the role that you have created when connecting your azure account , you can navigate to your Resource group, (1) click on Access Control , (2) navigate to the Check Access tab, (3) search for hopsworks.ai , (4) click on it, (5) now you have the name of your custom role used to connect to managed.hopsworks.ai . Get your role connected to hopswork.ai To edit the permissions associated with your role, stay on the same Access Control page, (1) click on the Roles tab, (2) search for your role name (the one you obtained above), (3) click on ... , (4) click on Edit . Edit your role connected to hopswork.ai You will arrive at the Update a custom role page as shown below: Edit your role connected to hopswork.ai Navigate to the JSON tab, then click on Edit , as shown below: Edit your role connected to hopswork.ai Now, add the missing permission \"Microsoft.Compute/disks/read\" to the list of actions, then click on Save , click on Review + update , and finally click on Update . Add missing permissions to your role connected to hopswork.ai Step 4: Run the upgrade process # You need to click on Upgrade to start the upgrade process. You will be prompted with the screen shown below to confirm your intention to upgrade: Note No need to worry about the following message since this is done already in Step 2 Make sure that your user assigned managed identity (hopsworks-doc-identity) includes the following permissions: [ \"Microsoft.Compute/virtualMachines/read\", \"Microsoft.Compute/virtualMachines/write\", \"Microsoft.Compute/disks/read\", \"Microsoft.Compute/disks/write\", \"Microsoft.Storage/storageAccounts/listKeys/action\" ] Upgrade confirmation Check the Yes, upgrade cluster checkbox to proceed, then the Upgrade button will be activated as shown below: Warning Currently, we only support upgrade for the head node and you will need to recreate your workers once the upgrade is successfully completed. Upgrade confirmation Depending on how big your current cluster is, the upgrade process may take from 1 hour to a few hours until completion. Note We don't delete your old cluster until the upgrade process is successfully completed. Upgrade is running Once the upgrade is completed, you can confirm that you have the new Hopsworks version by checking the Details tab of your cluster as below: Upgrade is complete Error handling # There are two categories of errors that you may encounter during an upgrade. First, a permission error due to a missing permission in your role connected to managed.hopsworks.ai , see Error 1 . Second, an error during the upgrade process running on your cluster, see Error 2 . Error 1: Missing permissions error # If you encounter the following permission error right after starting an upgrade, then you need to make sure that the role you used to connect to managed.hopsworks.ai , the one that you have created when connecting your azure account , have permissions to read and write disks (\"Microsoft.Compute/disks/read\", \"Microsoft.Compute/disks/write\") . Missing permission error If you don't remember the name of the role that you have created when connecting your azure account , you can navigate to your Resource group, (1) click on Access Control , (2) navigate to the Check Access tab, (3) search for hopsworks.ai , (4) click on it, (5) now you have the name of your custom role used to connect to managed.hopsworks.ai . Get your role connected to hopswork.ai To edit the permissions associated with your role, stay on the same Access Control page, (1) click on the Roles tab, (2) search for your role name (the one you obtained above), (3) click on ... , (4) click on Edit . Edit your role connected to hopswork.ai You will arrive at the Update a custom role page as shown below: Edit your role connected to hopswork.ai Navigate to the JSON tab, then click on Edit , as shown below: Edit your role connected to hopswork.ai In our example, we were missing only the read permission (\"Microsoft.Compute/disks/read\"). First, add the missing permission, then click on Save , click on Review + update , and finally click on Update . Add missing permissions to your role connected to hopswork.ai Once you have updated your role, click on Retry to retry the upgrade process. Retry the upgrade process Error 2: Upgrade process error # If an error occurs during the upgrade process, you will have the option to rollback to your old cluster as shown below: Error occurred during upgrade Click on Rollback to recover your old cluster before upgrade. Upgrade rollback confirmation Check the Yes, rollback cluster checkbox to proceed, then the Rollback button will be activated as shown below: Upgrade rollback confirmation Once the rollback is completed, you will be able to continue working as normal with your old cluster. Note The old cluster will be stopped after the rollback. You have to click on the Start button.","title":"Version 2.2 or older"},{"location":"setup_installation/azure/upgrade/#upgrade-existing-clusters-on-managedhopsworksai-from-version-22-or-older-azure","text":"This guide shows you how to upgrade your existing Hopsworks cluster to a newer version of Hopsworks.","title":"Upgrade existing clusters on managed.hopsworks.ai from version 2.2 or older (Azure)"},{"location":"setup_installation/azure/upgrade/#step-1-make-sure-your-cluster-is-running","text":"It is important that your cluster is Running . Otherwise you will not be able to upgrade. As soon as a new version is available an upgrade notification will appear: A new Hopsworks version is available","title":"Step 1: Make sure your cluster is running"},{"location":"setup_installation/azure/upgrade/#step-2-add-upgrade-permissions-to-your-user-assigned-managed-identity","text":"Note You can skip this step if you already have the following permissions in your user assigned managed identity : [ \"Microsoft.Compute/virtualMachines/read\" , \"Microsoft.Compute/virtualMachines/write\" , \"Microsoft.Compute/disks/read\" , \"Microsoft.Compute/disks/write\" , \"Microsoft.Storage/storageAccounts/listKeys/action\" ] Make sure that the scope of these permissions is your resource group. We require extra permissions to be added to the user assigned managed identity attached to your cluster to proceed with the upgrade. First to get the name of your user assigned managed identity and the resource group of your cluster, click on the Details tab as shown below: Get the resource group name (1) and the user assigned managed identity (2) of your cluster","title":"Step 2: Add upgrade permissions to your user assigned managed identity"},{"location":"setup_installation/azure/upgrade/#step-21-add-custom-role-for-upgrade-permissions","text":"Once you get the names of the resource group and user-assigned managed identity, navigate to Azure portal , then click on Resource groups and then search for your resource group and click on it. Go to the Access control (IAM) tab, select Add , and click on Add custom role Add a custom role for upgrade Name the custom role and then click on next till you reach the JSON tab. Name the custom role for upgrade Once you reach the JSON tab, click on Edit to edit the role permissions: Edit the JSON permissions for the custom role for upgrade Once you have clicked on Edit , replace the permissions array with the following snippet: \"permissions\" : [ { \"actions\" : [ \"Microsoft.Compute/virtualMachines/read\" , \"Microsoft.Compute/virtualMachines/write\" , \"Microsoft.Compute/disks/read\" , \"Microsoft.Compute/disks/write\" , \"Microsoft.Storage/storageAccounts/listKeys/action\" ], \"notActions\" : [], \"dataActions\" : [], \"notDataActions\" : [] } ] Then, click on Save to save the updated permissions Save permissions for the custom role for upgrade Click on Review and create and then click on Create to create the custom role: Save permissions for the custom role for upgrade","title":"Step 2.1: Add custom role for upgrade permissions"},{"location":"setup_installation/azure/upgrade/#step-22-assign-the-custom-role-to-your-user-assigned-managed-identity","text":"Navigate back to the your Resource group home page at Azure portal , click on Add and then click on Add role assignment Assign upgrade role to your user assigned managed identity (1) choose the upgrade role that you have just created in Step 2.1 , (2) choose User Assigned Managed Identity , (3) search for the user assigned managed identity attached to your cluster and select it. Finally, (4) click on Save to save the role assignment. Assign upgrade role to your user assigned managed identity Warning When you assign roles or remove role assignments, it can take up to 30 minutes for changes to take effect.","title":"Step 2.2: Assign the custom role to your user-assigned managed identity"},{"location":"setup_installation/azure/upgrade/#step-3-add-disk-read-permissions-to-your-role-connected-to-managedhopsworksai","text":"We require extra permission (\"Microsoft.Compute/disks/read\") to be added to the role you used to connect to managed.hopsworks.ai , the one that you have created when connecting your azure account . If you don't remember the name of the role that you have created when connecting your azure account , you can navigate to your Resource group, (1) click on Access Control , (2) navigate to the Check Access tab, (3) search for hopsworks.ai , (4) click on it, (5) now you have the name of your custom role used to connect to managed.hopsworks.ai . Get your role connected to hopswork.ai To edit the permissions associated with your role, stay on the same Access Control page, (1) click on the Roles tab, (2) search for your role name (the one you obtained above), (3) click on ... , (4) click on Edit . Edit your role connected to hopswork.ai You will arrive at the Update a custom role page as shown below: Edit your role connected to hopswork.ai Navigate to the JSON tab, then click on Edit , as shown below: Edit your role connected to hopswork.ai Now, add the missing permission \"Microsoft.Compute/disks/read\" to the list of actions, then click on Save , click on Review + update , and finally click on Update . Add missing permissions to your role connected to hopswork.ai","title":"Step 3: Add disk read permissions to your role connected to managed.hopsworks.ai"},{"location":"setup_installation/azure/upgrade/#step-4-run-the-upgrade-process","text":"You need to click on Upgrade to start the upgrade process. You will be prompted with the screen shown below to confirm your intention to upgrade: Note No need to worry about the following message since this is done already in Step 2 Make sure that your user assigned managed identity (hopsworks-doc-identity) includes the following permissions: [ \"Microsoft.Compute/virtualMachines/read\", \"Microsoft.Compute/virtualMachines/write\", \"Microsoft.Compute/disks/read\", \"Microsoft.Compute/disks/write\", \"Microsoft.Storage/storageAccounts/listKeys/action\" ] Upgrade confirmation Check the Yes, upgrade cluster checkbox to proceed, then the Upgrade button will be activated as shown below: Warning Currently, we only support upgrade for the head node and you will need to recreate your workers once the upgrade is successfully completed. Upgrade confirmation Depending on how big your current cluster is, the upgrade process may take from 1 hour to a few hours until completion. Note We don't delete your old cluster until the upgrade process is successfully completed. Upgrade is running Once the upgrade is completed, you can confirm that you have the new Hopsworks version by checking the Details tab of your cluster as below: Upgrade is complete","title":"Step 4: Run the upgrade process"},{"location":"setup_installation/azure/upgrade/#error-handling","text":"There are two categories of errors that you may encounter during an upgrade. First, a permission error due to a missing permission in your role connected to managed.hopsworks.ai , see Error 1 . Second, an error during the upgrade process running on your cluster, see Error 2 .","title":"Error handling"},{"location":"setup_installation/azure/upgrade/#error-1-missing-permissions-error","text":"If you encounter the following permission error right after starting an upgrade, then you need to make sure that the role you used to connect to managed.hopsworks.ai , the one that you have created when connecting your azure account , have permissions to read and write disks (\"Microsoft.Compute/disks/read\", \"Microsoft.Compute/disks/write\") . Missing permission error If you don't remember the name of the role that you have created when connecting your azure account , you can navigate to your Resource group, (1) click on Access Control , (2) navigate to the Check Access tab, (3) search for hopsworks.ai , (4) click on it, (5) now you have the name of your custom role used to connect to managed.hopsworks.ai . Get your role connected to hopswork.ai To edit the permissions associated with your role, stay on the same Access Control page, (1) click on the Roles tab, (2) search for your role name (the one you obtained above), (3) click on ... , (4) click on Edit . Edit your role connected to hopswork.ai You will arrive at the Update a custom role page as shown below: Edit your role connected to hopswork.ai Navigate to the JSON tab, then click on Edit , as shown below: Edit your role connected to hopswork.ai In our example, we were missing only the read permission (\"Microsoft.Compute/disks/read\"). First, add the missing permission, then click on Save , click on Review + update , and finally click on Update . Add missing permissions to your role connected to hopswork.ai Once you have updated your role, click on Retry to retry the upgrade process. Retry the upgrade process","title":"Error 1: Missing permissions error"},{"location":"setup_installation/azure/upgrade/#error-2-upgrade-process-error","text":"If an error occurs during the upgrade process, you will have the option to rollback to your old cluster as shown below: Error occurred during upgrade Click on Rollback to recover your old cluster before upgrade. Upgrade rollback confirmation Check the Yes, rollback cluster checkbox to proceed, then the Rollback button will be activated as shown below: Upgrade rollback confirmation Once the rollback is completed, you will be able to continue working as normal with your old cluster. Note The old cluster will be stopped after the rollback. You have to click on the Start button.","title":"Error 2: Upgrade process error"},{"location":"setup_installation/azure/upgrade_2.4/","text":"Upgrade existing clusters on managed.hopsworks.ai from version 2.4 or newer (Azure) # This guide shows you how to upgrade your existing Hopsworks cluster to a newer version of Hopsworks. Step 1: Make sure your cluster is running # It is important that your cluster is Running . Otherwise you will not be able to upgrade. As soon as a new version is available an upgrade notification will appear: A new Hopsworks version is available Step 2: Add backup permissions to your role connected to managed.hopsworks.ai # We require extra permission to be added to the role you used to connect to managed.hopsworks.ai , the one that you have created when connecting your Azure account . These permissions are required to create a snapshot of your cluster before proceeding with the upgrade. \"actions\" : [ \"Microsoft.Compute/snapshots/write\" , \"Microsoft.Compute/snapshots/read\" , \"Microsoft.Compute/snapshots/delete\" , \"Microsoft.Compute/disks/beginGetAccess/action\" , ] If you don't remember the name of the role that you have created when connecting your Azure account , you can navigate to your Resource group, (1) click on Access Control , (2) navigate to the Check Access tab, (3) search for hopsworks.ai , (4) click on it, (5) now you have the name of your custom role used to connect to managed.hopsworks.ai . Get your role connected to managed.hopsworks.ai To edit the permissions associated with your role, stay on the same Access Control page, (1) click on the Roles tab, (2) search for your role name (the one you obtained above), (3) click on ... , (4) click on Edit . Edit your role connected to managed.hopsworks.ai You will arrive at the Update a custom role page as shown below: Edit your role connected to managed.hopsworks.ai Navigate to the JSON tab, then click on Edit , as shown below: Edit your role connected to managed.hopsworks.ai Now, add the following permissions to the list of actions, then click on Save , click on Review + update , and finally click on Update . \"Microsoft.Compute/snapshots/write\" , \"Microsoft.Compute/snapshots/read\" , \"Microsoft.Compute/snapshots/delete\" , \"Microsoft.Compute/disks/beginGetAccess/action\" , Add missing permissions to your role connected to managed.hopsworks.ai Step 3: Run the upgrade process # You need to click on Upgrade to start the upgrade process. You will be prompted with the screen shown below to confirm your intention to upgrade: Note No need to worry about the following message since this is done already in Step 2 Make sure that your custom role which you have connected to managed.hopsworks.ai has the following permissions: [ \"Microsoft.Compute/snapshots/write\", \"Microsoft.Compute/snapshots/read\", \"Microsoft.Compute/snapshots/delete\", \"Microsoft.Compute/disks/beginGetAccess/action\", ] Upgrade confirmation Check the Yes, upgrade cluster checkbox to proceed, then the Upgrade button will be activated as shown below: Warning Currently, we only support upgrade for the head node and you will need to recreate your workers once the upgrade is successfully completed. Upgrade confirmation Depending on how big your current cluster is, the upgrade process may take from 1 hour to a few hours until completion. Note We don't delete your old cluster until the upgrade process is successfully completed. Upgrade is running Once the upgrade is completed, you can confirm that you have the new Hopsworks version by checking the version number on the Details tab of your cluster. Error handling # There are two categories of errors that you may encounter during an upgrade. First, a permission error due to a missing permission in your role connected to managed.hopsworks.ai , see Error 1 . Second, an error during the upgrade process running on your cluster, see Error 2 . Error 1: Missing permissions error # If one or more backup permissions are missing, or if the resource is not set correctly, you will be notified with an error message as shown below: Missing permission error Update your cross custom role as described in Step 2 , then click Start . Once the cluster is up and running, you can try running the upgrade again. Error 2: Upgrade process error # If an error occurs during the upgrade process, you will have the option to rollback to your old cluster as shown below: Error occurred during upgrade Click on Rollback to recover your old cluster before upgrade. Upgrade rollback confirmation Check the Yes, rollback cluster checkbox to proceed, then the Rollback button will be activated as shown below: Upgrade rollback confirmation Once the rollback is completed, you will be able to continue working as normal with your old cluster. Note The old cluster will be stopped after the rollback. You have to click on the Start button.","title":"Version 2.4 or newer"},{"location":"setup_installation/azure/upgrade_2.4/#upgrade-existing-clusters-on-managedhopsworksai-from-version-24-or-newer-azure","text":"This guide shows you how to upgrade your existing Hopsworks cluster to a newer version of Hopsworks.","title":"Upgrade existing clusters on managed.hopsworks.ai from version 2.4 or newer (Azure)"},{"location":"setup_installation/azure/upgrade_2.4/#step-1-make-sure-your-cluster-is-running","text":"It is important that your cluster is Running . Otherwise you will not be able to upgrade. As soon as a new version is available an upgrade notification will appear: A new Hopsworks version is available","title":"Step 1: Make sure your cluster is running"},{"location":"setup_installation/azure/upgrade_2.4/#step-2-add-backup-permissions-to-your-role-connected-to-managedhopsworksai","text":"We require extra permission to be added to the role you used to connect to managed.hopsworks.ai , the one that you have created when connecting your Azure account . These permissions are required to create a snapshot of your cluster before proceeding with the upgrade. \"actions\" : [ \"Microsoft.Compute/snapshots/write\" , \"Microsoft.Compute/snapshots/read\" , \"Microsoft.Compute/snapshots/delete\" , \"Microsoft.Compute/disks/beginGetAccess/action\" , ] If you don't remember the name of the role that you have created when connecting your Azure account , you can navigate to your Resource group, (1) click on Access Control , (2) navigate to the Check Access tab, (3) search for hopsworks.ai , (4) click on it, (5) now you have the name of your custom role used to connect to managed.hopsworks.ai . Get your role connected to managed.hopsworks.ai To edit the permissions associated with your role, stay on the same Access Control page, (1) click on the Roles tab, (2) search for your role name (the one you obtained above), (3) click on ... , (4) click on Edit . Edit your role connected to managed.hopsworks.ai You will arrive at the Update a custom role page as shown below: Edit your role connected to managed.hopsworks.ai Navigate to the JSON tab, then click on Edit , as shown below: Edit your role connected to managed.hopsworks.ai Now, add the following permissions to the list of actions, then click on Save , click on Review + update , and finally click on Update . \"Microsoft.Compute/snapshots/write\" , \"Microsoft.Compute/snapshots/read\" , \"Microsoft.Compute/snapshots/delete\" , \"Microsoft.Compute/disks/beginGetAccess/action\" , Add missing permissions to your role connected to managed.hopsworks.ai","title":"Step 2: Add backup permissions to your role connected to managed.hopsworks.ai"},{"location":"setup_installation/azure/upgrade_2.4/#step-3-run-the-upgrade-process","text":"You need to click on Upgrade to start the upgrade process. You will be prompted with the screen shown below to confirm your intention to upgrade: Note No need to worry about the following message since this is done already in Step 2 Make sure that your custom role which you have connected to managed.hopsworks.ai has the following permissions: [ \"Microsoft.Compute/snapshots/write\", \"Microsoft.Compute/snapshots/read\", \"Microsoft.Compute/snapshots/delete\", \"Microsoft.Compute/disks/beginGetAccess/action\", ] Upgrade confirmation Check the Yes, upgrade cluster checkbox to proceed, then the Upgrade button will be activated as shown below: Warning Currently, we only support upgrade for the head node and you will need to recreate your workers once the upgrade is successfully completed. Upgrade confirmation Depending on how big your current cluster is, the upgrade process may take from 1 hour to a few hours until completion. Note We don't delete your old cluster until the upgrade process is successfully completed. Upgrade is running Once the upgrade is completed, you can confirm that you have the new Hopsworks version by checking the version number on the Details tab of your cluster.","title":"Step 3: Run the upgrade process"},{"location":"setup_installation/azure/upgrade_2.4/#error-handling","text":"There are two categories of errors that you may encounter during an upgrade. First, a permission error due to a missing permission in your role connected to managed.hopsworks.ai , see Error 1 . Second, an error during the upgrade process running on your cluster, see Error 2 .","title":"Error handling"},{"location":"setup_installation/azure/upgrade_2.4/#error-1-missing-permissions-error","text":"If one or more backup permissions are missing, or if the resource is not set correctly, you will be notified with an error message as shown below: Missing permission error Update your cross custom role as described in Step 2 , then click Start . Once the cluster is up and running, you can try running the upgrade again.","title":"Error 1: Missing permissions error"},{"location":"setup_installation/azure/upgrade_2.4/#error-2-upgrade-process-error","text":"If an error occurs during the upgrade process, you will have the option to rollback to your old cluster as shown below: Error occurred during upgrade Click on Rollback to recover your old cluster before upgrade. Upgrade rollback confirmation Check the Yes, rollback cluster checkbox to proceed, then the Rollback button will be activated as shown below: Upgrade rollback confirmation Once the rollback is completed, you will be able to continue working as normal with your old cluster. Note The old cluster will be stopped after the rollback. You have to click on the Start button.","title":"Error 2: Upgrade process error"},{"location":"setup_installation/azure/upgrade_3.0/","text":"Upgrade existing clusters on managed.hopsworks.ai from version 3.0 or newer (Azure) # This guide shows you how to upgrade your existing Hopsworks cluster to a newer version of Hopsworks. Step 1: Make sure your cluster is running # It is important that your cluster is Running . Otherwise you will not be able to upgrade. As soon as a new version is available an upgrade notification will appear: A new Hopsworks version is available Step 2: Add backup permissions to your role connected to managed.hopsworks.ai # We require extra permission to be added to the role you used to connect to managed.hopsworks.ai , the one that you have created when connecting your Azure account . These permissions are required to create a snapshot of your cluster before proceeding with the upgrade. \"actions\" : [ \"Microsoft.Compute/snapshots/write\" , \"Microsoft.Compute/snapshots/read\" , \"Microsoft.Compute/snapshots/delete\" , \"Microsoft.Compute/disks/beginGetAccess/action\" , ] If you don't remember the name of the role that you have created when connecting your Azure account , you can navigate to your Resource group, (1) click on Access Control , (2) navigate to the Check Access tab, (3) search for hopsworks.ai , (4) click on it, (5) now you have the name of your custom role used to connect to managed.hopsworks.ai . Get your role connected to managed.hopsworks.ai To edit the permissions associated with your role, stay on the same Access Control page, (1) click on the Roles tab, (2) search for your role name (the one you obtained above), (3) click on ... , (4) click on Edit . Edit your role connected to managed.hopsworks.ai You will arrive at the Update a custom role page as shown below: Edit your role connected to managed.hopsworks.ai Navigate to the JSON tab, then click on Edit , as shown below: Edit your role connected to managed.hopsworks.ai Now, add the following permissions to the list of actions, then click on Save , click on Review + update , and finally click on Update . \"Microsoft.Compute/snapshots/write\" , \"Microsoft.Compute/snapshots/read\" , \"Microsoft.Compute/snapshots/delete\" , \"Microsoft.Compute/disks/beginGetAccess/action\" , Add missing permissions to your role connected to managed.hopsworks.ai Step 3: Create an ACR Container Registry # We have enforced using managed docker registry (ACR) starting from Hopsworks version 3.1.0, so you need to create an ACR container registry and configure your managed identity to allow access to the container registry. First, get the name of the managed identity used in your cluster by clicking on the Details tab and check the name shown infront of Managed Identity . Then, follow this guide to create and configure an ACR container registry. Step 4: Run the upgrade process # You need to click on Upgrade to start the upgrade process. You will be prompted with the screen shown below to confirm your intention to upgrade: Note No need to worry about the steps shown below if you have already completed Step 2 and Step 3 Upgrade confirmation Enter the name of your ACR container registry that you have created in Step 3 and check the Yes, upgrade cluster checkbox to proceed, then the Upgrade button will be activated as shown below: Warning Currently, we only support upgrade for the head node and you will need to recreate your workers once the upgrade is successfully completed. Upgrade confirmation Depending on how big your current cluster is, the upgrade process may take from 1 hour to a few hours until completion. Note We don't delete your old cluster until the upgrade process is successfully completed. Upgrade is running Once the upgrade is completed, you can confirm that you have the new Hopsworks version by checking the version number on the Details tab of your cluster. For more details about error handling check this guide","title":"Version 3.0 or newer"},{"location":"setup_installation/azure/upgrade_3.0/#upgrade-existing-clusters-on-managedhopsworksai-from-version-30-or-newer-azure","text":"This guide shows you how to upgrade your existing Hopsworks cluster to a newer version of Hopsworks.","title":"Upgrade existing clusters on managed.hopsworks.ai from version 3.0 or newer (Azure)"},{"location":"setup_installation/azure/upgrade_3.0/#step-1-make-sure-your-cluster-is-running","text":"It is important that your cluster is Running . Otherwise you will not be able to upgrade. As soon as a new version is available an upgrade notification will appear: A new Hopsworks version is available","title":"Step 1: Make sure your cluster is running"},{"location":"setup_installation/azure/upgrade_3.0/#step-2-add-backup-permissions-to-your-role-connected-to-managedhopsworksai","text":"We require extra permission to be added to the role you used to connect to managed.hopsworks.ai , the one that you have created when connecting your Azure account . These permissions are required to create a snapshot of your cluster before proceeding with the upgrade. \"actions\" : [ \"Microsoft.Compute/snapshots/write\" , \"Microsoft.Compute/snapshots/read\" , \"Microsoft.Compute/snapshots/delete\" , \"Microsoft.Compute/disks/beginGetAccess/action\" , ] If you don't remember the name of the role that you have created when connecting your Azure account , you can navigate to your Resource group, (1) click on Access Control , (2) navigate to the Check Access tab, (3) search for hopsworks.ai , (4) click on it, (5) now you have the name of your custom role used to connect to managed.hopsworks.ai . Get your role connected to managed.hopsworks.ai To edit the permissions associated with your role, stay on the same Access Control page, (1) click on the Roles tab, (2) search for your role name (the one you obtained above), (3) click on ... , (4) click on Edit . Edit your role connected to managed.hopsworks.ai You will arrive at the Update a custom role page as shown below: Edit your role connected to managed.hopsworks.ai Navigate to the JSON tab, then click on Edit , as shown below: Edit your role connected to managed.hopsworks.ai Now, add the following permissions to the list of actions, then click on Save , click on Review + update , and finally click on Update . \"Microsoft.Compute/snapshots/write\" , \"Microsoft.Compute/snapshots/read\" , \"Microsoft.Compute/snapshots/delete\" , \"Microsoft.Compute/disks/beginGetAccess/action\" , Add missing permissions to your role connected to managed.hopsworks.ai","title":"Step 2: Add backup permissions to your role connected to managed.hopsworks.ai"},{"location":"setup_installation/azure/upgrade_3.0/#step-3-create-an-acr-container-registry","text":"We have enforced using managed docker registry (ACR) starting from Hopsworks version 3.1.0, so you need to create an ACR container registry and configure your managed identity to allow access to the container registry. First, get the name of the managed identity used in your cluster by clicking on the Details tab and check the name shown infront of Managed Identity . Then, follow this guide to create and configure an ACR container registry.","title":"Step 3: Create an ACR Container Registry"},{"location":"setup_installation/azure/upgrade_3.0/#step-4-run-the-upgrade-process","text":"You need to click on Upgrade to start the upgrade process. You will be prompted with the screen shown below to confirm your intention to upgrade: Note No need to worry about the steps shown below if you have already completed Step 2 and Step 3 Upgrade confirmation Enter the name of your ACR container registry that you have created in Step 3 and check the Yes, upgrade cluster checkbox to proceed, then the Upgrade button will be activated as shown below: Warning Currently, we only support upgrade for the head node and you will need to recreate your workers once the upgrade is successfully completed. Upgrade confirmation Depending on how big your current cluster is, the upgrade process may take from 1 hour to a few hours until completion. Note We don't delete your old cluster until the upgrade process is successfully completed. Upgrade is running Once the upgrade is completed, you can confirm that you have the new Hopsworks version by checking the version number on the Details tab of your cluster. For more details about error handling check this guide","title":"Step 4: Run the upgrade process"},{"location":"setup_installation/common/adding_removing_workers/","text":"Adding and removing workers # Once you have started a Hopsworks cluster you can add and remove workers from the cluster to accommodate your workload. Adding workers # If the computation you are running is using all the resources of your Hopsworks cluster you can add workers to your cluster. To add workers to a cluster, go to the Details tab of this cluster and click on Add workers . Add worker Select the number of workers you want to add (1). Select the type of instance you want the workers to run on (2). Select the local storage size for the workers (3). Click on Next . Add workers Review your request and click Add . Add workers Managed.hopsworks.ai will start the new workers and you will be able to use them in your cluster as soon as they have finished starting. Removing workers # If the load on your Hopsworks cluster is low, you can decide to remove worker nodes from your cluster. Warning When removing workers managed.hopsworks.ai will try to select workers that can be removed while interfering as little as possible with any ongoing computation. It will also wait for the workers to be done with their computation before stopping them. But, if this computation lasts too long, the worker may get stopped before the computation properly finish. This could interfere with your ongoing computation. Note You can remove all the workers of your cluster. If you do so the cluster will be able to store data but not run any computations. This may affect feature store functionality. To remove workers from a cluster, go to the Details tab of this cluster and click on Remove workers Remove workers For each of the types of instances existing in your cluster select the number of workers you want to remove and click on Next . Remove workers Review your request and click Remove . Remove workers Managed.hopsworks.ai will select the workers corresponding to your criteria which can be stopped with as little interferences as possible with any ongoing computation. It will set them to decommission and stop them when they have finished decommissioning.","title":"Adding and Removing workers"},{"location":"setup_installation/common/adding_removing_workers/#adding-and-removing-workers","text":"Once you have started a Hopsworks cluster you can add and remove workers from the cluster to accommodate your workload.","title":"Adding and removing workers"},{"location":"setup_installation/common/adding_removing_workers/#adding-workers","text":"If the computation you are running is using all the resources of your Hopsworks cluster you can add workers to your cluster. To add workers to a cluster, go to the Details tab of this cluster and click on Add workers . Add worker Select the number of workers you want to add (1). Select the type of instance you want the workers to run on (2). Select the local storage size for the workers (3). Click on Next . Add workers Review your request and click Add . Add workers Managed.hopsworks.ai will start the new workers and you will be able to use them in your cluster as soon as they have finished starting.","title":"Adding workers"},{"location":"setup_installation/common/adding_removing_workers/#removing-workers","text":"If the load on your Hopsworks cluster is low, you can decide to remove worker nodes from your cluster. Warning When removing workers managed.hopsworks.ai will try to select workers that can be removed while interfering as little as possible with any ongoing computation. It will also wait for the workers to be done with their computation before stopping them. But, if this computation lasts too long, the worker may get stopped before the computation properly finish. This could interfere with your ongoing computation. Note You can remove all the workers of your cluster. If you do so the cluster will be able to store data but not run any computations. This may affect feature store functionality. To remove workers from a cluster, go to the Details tab of this cluster and click on Remove workers Remove workers For each of the types of instances existing in your cluster select the number of workers you want to remove and click on Next . Remove workers Review your request and click Remove . Remove workers Managed.hopsworks.ai will select the workers corresponding to your criteria which can be stopped with as little interferences as possible with any ongoing computation. It will set them to decommission and stop them when they have finished decommissioning.","title":"Removing workers"},{"location":"setup_installation/common/api_key/","text":"Managed.hopsworks.ai API Key # Managed.hopsworks.ai allows users to generate an API Key that can be used to authenticate and access the managed.hopsworks.ai REST APIs. Generate an API Key # First, login to your managed.hopsworks.ai account, then click on the Settings tab as shown below: Click on the Settings tab Click on the API Key tab, and then click on the Generate API Key button: Generate an API Key Copy the generated API Key and store it in a secure location. Warning Make sure to copy your API Key now. You won\u2019t be able to see it again. However, you can always delete it and generate a new one. Copy the generated API Key Use the API Key # To access the managed.hopsworks.ai REST APIs, you should pass the API key as a header x-api-key when executing requests on managed.hopsworks.ai as shown below: curl -XGET -H \"x-api-key: <YOUR API KEY>\" https://api.hopsworks.ai/api/clusters Alternatively, you can use your API Key with the Hopsworks.ai terraform provider to manage your Hopsworks clusters using terraform . Delete your API Key # First, login to your managed.hopsworks.ai account, click on the Settings tab, then click on the API Key tab, and finally click on Delete API Key as shown below: Delete your API Key","title":"API Key"},{"location":"setup_installation/common/api_key/#managedhopsworksai-api-key","text":"Managed.hopsworks.ai allows users to generate an API Key that can be used to authenticate and access the managed.hopsworks.ai REST APIs.","title":"Managed.hopsworks.ai API Key"},{"location":"setup_installation/common/api_key/#generate-an-api-key","text":"First, login to your managed.hopsworks.ai account, then click on the Settings tab as shown below: Click on the Settings tab Click on the API Key tab, and then click on the Generate API Key button: Generate an API Key Copy the generated API Key and store it in a secure location. Warning Make sure to copy your API Key now. You won\u2019t be able to see it again. However, you can always delete it and generate a new one. Copy the generated API Key","title":"Generate an API Key"},{"location":"setup_installation/common/api_key/#use-the-api-key","text":"To access the managed.hopsworks.ai REST APIs, you should pass the API key as a header x-api-key when executing requests on managed.hopsworks.ai as shown below: curl -XGET -H \"x-api-key: <YOUR API KEY>\" https://api.hopsworks.ai/api/clusters Alternatively, you can use your API Key with the Hopsworks.ai terraform provider to manage your Hopsworks clusters using terraform .","title":"Use the API Key"},{"location":"setup_installation/common/api_key/#delete-your-api-key","text":"First, login to your managed.hopsworks.ai account, click on the Settings tab, then click on the API Key tab, and finally click on Delete API Key as shown below: Delete your API Key","title":"Delete your API Key"},{"location":"setup_installation/common/arrow_flight_duckdb/","text":"ArrowFlight Server with DuckDB # By default, Hopsworks uses big data technologies (Spark or Hive) to create training data and read data for Python clients. This is great for large datasets, but for small or moderately sized datasets (think of the size of data that would fit in a Pandas DataFrame in your local Python environment), the overhead of starting a Spark or Hive job and doing distributed data processing can be significant. ArrowFlight Server with DuckDB significantly reduces the time that Python clients need to read feature groups and batch inference data from the Feature Store, as well as creating moderately-sized in-memory training datasets. When the service is enabled, clients will automatically use it for the following operations: reading Feature Groups reading Queries reading Training Datasets creating In-Memory Training Datasets reading Batch Inference Data For larger datasets, clients can still make use of the Spark/Hive backend by explicitly setting read_options={\"use_hive\": True} . Service configuration # Note Supported only on AWS at the moment. Note Make sure that your cross account role has the load balancer permissions as described in here , otherwise you have to create and manage the load balancer yourself. The ArrowFlight Server is co-located with RonDB in the Hopsworks cluster. If the ArrowFlight Server is activated, RonDB and ArrowFlight Server can each use up to 50% of the available resources on the node, so they can co-exist without impacting each other. Just like RonDB, the ArrowFlight Server can be replicated across multiple nodes to serve more clients at lower latency. To guarantee high performance, each individual ArrowFlight Server instance processes client requests sequentially. Requests will be queued for up to 10 minutes before they are rejected. Activate ArrowFlight Server with DuckDB on a RonDB cluster To deploy ArrowFlight Server on a cluster: Select \"RonDB cluster\" Select an instance type with at least 16GB of memory and 4 cores. (*) Tick the checkbox Enable ArrowFlight Server . (*) The service should have at least the 2x the amount of memory available that a typical Python client would have. Because RonDB and ArrowFlight Server share the same node we recommend selecting an instance type with at least 4x the client memory. For example, if the service serves Python clients with typically 4GB of memory, an instance with at least 16GB of memory should be selected. An instance with 16GB of memory will be able to read feature groups and training datasets of up to 10-100M rows, depending on the number of columns and size of the features (~2GB in parquet). The same instance will be able to create point-in-time correct training datasets with 1-10M rows, also depending on the number and the size of the features. Larger instances are able to handle larger datasets. The numbers scale roughly linearly with the instance size.","title":"ArrowFlight Server with DuckDB"},{"location":"setup_installation/common/arrow_flight_duckdb/#arrowflight-server-with-duckdb","text":"By default, Hopsworks uses big data technologies (Spark or Hive) to create training data and read data for Python clients. This is great for large datasets, but for small or moderately sized datasets (think of the size of data that would fit in a Pandas DataFrame in your local Python environment), the overhead of starting a Spark or Hive job and doing distributed data processing can be significant. ArrowFlight Server with DuckDB significantly reduces the time that Python clients need to read feature groups and batch inference data from the Feature Store, as well as creating moderately-sized in-memory training datasets. When the service is enabled, clients will automatically use it for the following operations: reading Feature Groups reading Queries reading Training Datasets creating In-Memory Training Datasets reading Batch Inference Data For larger datasets, clients can still make use of the Spark/Hive backend by explicitly setting read_options={\"use_hive\": True} .","title":"ArrowFlight Server with DuckDB"},{"location":"setup_installation/common/arrow_flight_duckdb/#service-configuration","text":"Note Supported only on AWS at the moment. Note Make sure that your cross account role has the load balancer permissions as described in here , otherwise you have to create and manage the load balancer yourself. The ArrowFlight Server is co-located with RonDB in the Hopsworks cluster. If the ArrowFlight Server is activated, RonDB and ArrowFlight Server can each use up to 50% of the available resources on the node, so they can co-exist without impacting each other. Just like RonDB, the ArrowFlight Server can be replicated across multiple nodes to serve more clients at lower latency. To guarantee high performance, each individual ArrowFlight Server instance processes client requests sequentially. Requests will be queued for up to 10 minutes before they are rejected. Activate ArrowFlight Server with DuckDB on a RonDB cluster To deploy ArrowFlight Server on a cluster: Select \"RonDB cluster\" Select an instance type with at least 16GB of memory and 4 cores. (*) Tick the checkbox Enable ArrowFlight Server . (*) The service should have at least the 2x the amount of memory available that a typical Python client would have. Because RonDB and ArrowFlight Server share the same node we recommend selecting an instance type with at least 4x the client memory. For example, if the service serves Python clients with typically 4GB of memory, an instance with at least 16GB of memory should be selected. An instance with 16GB of memory will be able to read feature groups and training datasets of up to 10-100M rows, depending on the number of columns and size of the features (~2GB in parquet). The same instance will be able to create point-in-time correct training datasets with 1-10M rows, also depending on the number and the size of the features. Larger instances are able to handle larger datasets. The numbers scale roughly linearly with the instance size.","title":"Service configuration"},{"location":"setup_installation/common/autoscaling/","text":"Autoscaling # If you run a Hopsworks cluster version 2.2 or above you can enable autoscaling to let managed.hopsworks.ai start and stop workers depending on the demand. Enabling and configuring the autoscaling # Once you have created a cluster you can enable autoscaling by going to the Details tab and clicking on Configure autoscale . You can also set up autoscaling during the cluster creation. For more details about this see the cluster creation documentation ( AWS , AZURE ). Configure autoscale Once you have clicked on Configure autoscale you will access a form allowing you to configure the autoscaling. You can configure the following: The instance type you want to use. The size of the instances' disk. The minimum number of workers. The maximum number of workers. The targeted number of standby workers. Setting some resources in standby ensures that there are always some free resources in your cluster. This ensures that requests for new resources are fulfilled promptly. You configure the standby by setting the amount of workers you want to be in standby. For example, if you set a value of 0.5 the system will start a new worker every time the aggregated free cluster resources drop below 50% of a worker's resources. If you set this value to 0 new workers will only be started when a job or notebook request the resources. The time to wait before removing unused resources. One often starts a new computation shortly after finishing the previous one. To avoid having to wait for workers to stop and start between each computation it is recommended to wait before shutting down workers. Here you set the amount of time in seconds resources need to be unused before they get removed from the system. Note The standby will not be taken into account if you set the minimum number of workers to 0 and no resources are used in the cluster. This ensures that the number of nodes can fall to 0 when no resources are used. The standby will start to take effect as soon as you start using resources. Configure autoscale details Once you have set your configuration you can review it and enable the autoscaling. Note There are two scenarios if you already have workers in your cluster when enabling the autoscaling: The preexisting workers have the same instance type as the one you set up in the autoscaling. In this case, the autoscaling system will manage these workers and start or stop them automatically. The preexisting workers have a different instance type from the one you set up in the autoscaling. In this case, the autoscaling will not manage these nodes but you will still be able to remove them manually. Configure autoscale review Modifying the autoscaling configuration # You can update the autoscale configuration by going to the Details tab of the cluster and clicking on Configure autoscale . You will then go through the same steps as above. Note that if you change the instance type , nodes that currently exist in the cluster with a different instance type will not be managed by the autoscale system anymore and you will have to remove them manually. Disabling the autoscaling # To disable the autoscaling go to the Details tab, click on Disable autoscale and confirm your action. When you disable autoscaling the nodes that are currently running will keep running. You will need to stop them manually. Disable autoscale","title":"Autoscaling"},{"location":"setup_installation/common/autoscaling/#autoscaling","text":"If you run a Hopsworks cluster version 2.2 or above you can enable autoscaling to let managed.hopsworks.ai start and stop workers depending on the demand.","title":"Autoscaling"},{"location":"setup_installation/common/autoscaling/#enabling-and-configuring-the-autoscaling","text":"Once you have created a cluster you can enable autoscaling by going to the Details tab and clicking on Configure autoscale . You can also set up autoscaling during the cluster creation. For more details about this see the cluster creation documentation ( AWS , AZURE ). Configure autoscale Once you have clicked on Configure autoscale you will access a form allowing you to configure the autoscaling. You can configure the following: The instance type you want to use. The size of the instances' disk. The minimum number of workers. The maximum number of workers. The targeted number of standby workers. Setting some resources in standby ensures that there are always some free resources in your cluster. This ensures that requests for new resources are fulfilled promptly. You configure the standby by setting the amount of workers you want to be in standby. For example, if you set a value of 0.5 the system will start a new worker every time the aggregated free cluster resources drop below 50% of a worker's resources. If you set this value to 0 new workers will only be started when a job or notebook request the resources. The time to wait before removing unused resources. One often starts a new computation shortly after finishing the previous one. To avoid having to wait for workers to stop and start between each computation it is recommended to wait before shutting down workers. Here you set the amount of time in seconds resources need to be unused before they get removed from the system. Note The standby will not be taken into account if you set the minimum number of workers to 0 and no resources are used in the cluster. This ensures that the number of nodes can fall to 0 when no resources are used. The standby will start to take effect as soon as you start using resources. Configure autoscale details Once you have set your configuration you can review it and enable the autoscaling. Note There are two scenarios if you already have workers in your cluster when enabling the autoscaling: The preexisting workers have the same instance type as the one you set up in the autoscaling. In this case, the autoscaling system will manage these workers and start or stop them automatically. The preexisting workers have a different instance type from the one you set up in the autoscaling. In this case, the autoscaling will not manage these nodes but you will still be able to remove them manually. Configure autoscale review","title":"Enabling and configuring the autoscaling"},{"location":"setup_installation/common/autoscaling/#modifying-the-autoscaling-configuration","text":"You can update the autoscale configuration by going to the Details tab of the cluster and clicking on Configure autoscale . You will then go through the same steps as above. Note that if you change the instance type , nodes that currently exist in the cluster with a different instance type will not be managed by the autoscale system anymore and you will have to remove them manually.","title":"Modifying the autoscaling configuration"},{"location":"setup_installation/common/autoscaling/#disabling-the-autoscaling","text":"To disable the autoscaling go to the Details tab, click on Disable autoscale and confirm your action. When you disable autoscaling the nodes that are currently running will keep running. You will need to stop them manually. Disable autoscale","title":"Disabling the autoscaling"},{"location":"setup_installation/common/backup/","text":"How to take, restore and manage backups in managed.hopsworks.ai # Introduction # Managed.hopsworks.ai is our managed platform for running Hopsworks and the Feature Store in the cloud. When managing a cluster it is important to be able to take and restore backups to handle any failure eventuality. In this tutorial you will learn how to take , restore and manage backups in managed.hopsworks.ai Prerequisites # To follow this tutorial you need to create a cluster in Managed.hopsworks.ai . During the cluster creation you need to set a positive number for the maximum retention period for your backups. This is done in the backups step of the cluster creation by setting the wanted retention period in Validity of cluster backup images . This step is needed because the backup process relies on the cloud bucket retention policy and needs to configure it before any backup is taken. Warning This value cannot be edited later on so make sure to set the proper one. Note To be able to take backup the cluster instances need special permissions. These permissions are cloud provider specific and indicated on the cluster creation page. Choose the backup retention policy Taking a backup # To take a backup go to the backup tab of your cluster (1) and click on Create backup (2). If you wish to give a name to your backup edit the value in New backup name (3) before clicking on Create backup . Note The cluster needs to be running to take a backup. Create a backup Taking a backup takes time and necessitates restarting the cluster. To avoid any risk of accidental restart you will be asked to confirm that you want to take a backup. To confirm check the check box and click on the Backup button. Warning This will interrupt any operation currently running on the cluster. Make sure to stop them properly before taking a backup. Confirm backup creation You can then wait until the backup is complete. The backup process being underway is indicated next to the cluster status. Backup ongoing Once the backup is taken your cluster will be back up and running and ready to use. Restoring a backup # Go to the Backup tab of the dashboard (left menu (1)) to the list of all the backups. This list is organized by cluster. For each of the clusters, you can see the state of the cluster (2) and the list of backups for this cluster (3). To be able to restore a backup the corresponding cluster needs to be terminated . If your cluster is not terminated go and terminate it. Once the cluster is terminated you restore a backup by clicking on the Restore button (4) of the backup you want to restore. List of backups Warning Restoring a backup put back the bucket in the state it was at the time of the backup. This makes it impossible to then restore a backup taken more recently. If you try to restore a backup that is not the latest backup you will be asked to confirm that you want to restore and thus delete any more recent backup. Delete succeeding backups Once you have clicked on Restore you will be brought to the cluster creation menu. All the entries should be prefilled with the values corresponding to your cluster configuration. You can go through all the cluster configurations to verify (recommended) or directly click on Review in the left menu and click on the Create button. Review and restore backup A new cluster will be created and set in the state your cluster was at the time of the backup. Note Restoring a backup does not recreate the workers for this cluster. You need to add the workers back once the cluster is created. Managing your backups # To manage your backups either go to the Backup tab of your cluster (1) or go to the backup tab of the dashboard (2). Backup tabs If you go to the backup tab of your cluster you will see the list of backups associated with this cluster. For each of the backups, you will see their name (1), id (2), date of creation (3), and status (4). The status can be: Completed : the backup has been created and is ready to be restored. Expired : the backup is older than the maximum retention time set during cluster creation , it will not be possible to restore it. Failed : the backup failed during its creation. Running : the backup is currently being created. Deleting : the backup is being deleted. Backup info If you go to the dashboard backup tab you will get a view of all the backups of all the clusters. For each of the backups, you get the same information as above. To delete a backup click on the Delete button on the same line as the backup name. Delete backup Once you have clicked on the Delete button, you will be asked to confirm that you want to delete it. Check the check box and click Delete to confirm. Confirm backup deletion The backup will then be deleted. Conclusion # During this tutorial, you have created a backup, restored a cluster from this backup, checked the information about this backup, and finally deleted the backup. Now that you have restored a cluster you can add workers or set up autoscale on it.","title":"Backup"},{"location":"setup_installation/common/backup/#how-to-take-restore-and-manage-backups-in-managedhopsworksai","text":"","title":"How to take, restore and manage backups in managed.hopsworks.ai"},{"location":"setup_installation/common/backup/#introduction","text":"Managed.hopsworks.ai is our managed platform for running Hopsworks and the Feature Store in the cloud. When managing a cluster it is important to be able to take and restore backups to handle any failure eventuality. In this tutorial you will learn how to take , restore and manage backups in managed.hopsworks.ai","title":"Introduction"},{"location":"setup_installation/common/backup/#prerequisites","text":"To follow this tutorial you need to create a cluster in Managed.hopsworks.ai . During the cluster creation you need to set a positive number for the maximum retention period for your backups. This is done in the backups step of the cluster creation by setting the wanted retention period in Validity of cluster backup images . This step is needed because the backup process relies on the cloud bucket retention policy and needs to configure it before any backup is taken. Warning This value cannot be edited later on so make sure to set the proper one. Note To be able to take backup the cluster instances need special permissions. These permissions are cloud provider specific and indicated on the cluster creation page. Choose the backup retention policy","title":"Prerequisites"},{"location":"setup_installation/common/backup/#taking-a-backup","text":"To take a backup go to the backup tab of your cluster (1) and click on Create backup (2). If you wish to give a name to your backup edit the value in New backup name (3) before clicking on Create backup . Note The cluster needs to be running to take a backup. Create a backup Taking a backup takes time and necessitates restarting the cluster. To avoid any risk of accidental restart you will be asked to confirm that you want to take a backup. To confirm check the check box and click on the Backup button. Warning This will interrupt any operation currently running on the cluster. Make sure to stop them properly before taking a backup. Confirm backup creation You can then wait until the backup is complete. The backup process being underway is indicated next to the cluster status. Backup ongoing Once the backup is taken your cluster will be back up and running and ready to use.","title":"Taking a backup"},{"location":"setup_installation/common/backup/#restoring-a-backup","text":"Go to the Backup tab of the dashboard (left menu (1)) to the list of all the backups. This list is organized by cluster. For each of the clusters, you can see the state of the cluster (2) and the list of backups for this cluster (3). To be able to restore a backup the corresponding cluster needs to be terminated . If your cluster is not terminated go and terminate it. Once the cluster is terminated you restore a backup by clicking on the Restore button (4) of the backup you want to restore. List of backups Warning Restoring a backup put back the bucket in the state it was at the time of the backup. This makes it impossible to then restore a backup taken more recently. If you try to restore a backup that is not the latest backup you will be asked to confirm that you want to restore and thus delete any more recent backup. Delete succeeding backups Once you have clicked on Restore you will be brought to the cluster creation menu. All the entries should be prefilled with the values corresponding to your cluster configuration. You can go through all the cluster configurations to verify (recommended) or directly click on Review in the left menu and click on the Create button. Review and restore backup A new cluster will be created and set in the state your cluster was at the time of the backup. Note Restoring a backup does not recreate the workers for this cluster. You need to add the workers back once the cluster is created.","title":"Restoring a backup"},{"location":"setup_installation/common/backup/#managing-your-backups","text":"To manage your backups either go to the Backup tab of your cluster (1) or go to the backup tab of the dashboard (2). Backup tabs If you go to the backup tab of your cluster you will see the list of backups associated with this cluster. For each of the backups, you will see their name (1), id (2), date of creation (3), and status (4). The status can be: Completed : the backup has been created and is ready to be restored. Expired : the backup is older than the maximum retention time set during cluster creation , it will not be possible to restore it. Failed : the backup failed during its creation. Running : the backup is currently being created. Deleting : the backup is being deleted. Backup info If you go to the dashboard backup tab you will get a view of all the backups of all the clusters. For each of the backups, you get the same information as above. To delete a backup click on the Delete button on the same line as the backup name. Delete backup Once you have clicked on the Delete button, you will be asked to confirm that you want to delete it. Check the check box and click Delete to confirm. Confirm backup deletion The backup will then be deleted.","title":"Managing your backups"},{"location":"setup_installation/common/backup/#conclusion","text":"During this tutorial, you have created a backup, restored a cluster from this backup, checked the information about this backup, and finally deleted the backup. Now that you have restored a cluster you can add workers or set up autoscale on it.","title":"Conclusion"},{"location":"setup_installation/common/dashboard/","text":"How to manage your clusters in managed.hopsworks.ai # Introduction # Managed.hopsworks.ai is our managed platform for running Hopsworks and the Feature Store in the cloud. On this page, you will get an overview of the different functionalities of the managed.hopsworks.ai dashboard. Prerequisites # If you want to navigate the to the different tabs presented in this document you will need to connect managed.hopsworks.ai and create a cluster. Instructions about this process can be found in the getting started pages ( AWS , Azure , GCP ) Dashboard overview # The landing page of managed.hopsworks.ai can be seen in the picture below. It is composed of three main parts. At the top, you have a menu bar (1) allowing you to navigate between the dashboard and the settings . Bellow, you have a menu column (2) allowing you to navigate between different functionalities of the dashboard. And finally, in the middle, you find pannels representing your different clusters (3) and a button to create new clusters (4). Dashboard overview The different functionalities of the dashboard are: Clusters : the landing page which we will detail below. Members : the place to manage the members of your organization ( doc ). Backup : the place to manage your clusters' backup ( doc ). Usage : the place to get your credits consumption. Managing your clusters in the cluster panel # The cluster panels contain the name of the cluster (1), its state (2) a button to terminate the cluster (3), a button to stop the cluster (4) and different tabs to manage the cluster (5). You will now learn more details about these actions and tabs. Cluster pannel structure Stop the cluster # To stop a cluster click on the Stop button on the top right part of the cluster panel. Stopping the cluster stop in instances it is running on and delete the workers nodes. This allows you to save credits in managed.hopsworks.ai and in your cloud provider. Stop a cluster Terminate the cluster # To terminate a cluster click on the Terminate button on the top right part of the cluster panel. Terminate a cluster Terminating the cluster will destroy it and delete all the resources that were automatically created during the cluster creation. To be sure that you are not terminating a cluster by accident you will be asked to confirm that you want to terminate the cluster. To confirm the termination, check the check box and click on Terminate . Note Terminating a cluster does not delete or empty the bucket associated with the cluster. This is because this bucket is needed to restore a backup. You can find more information about backups in the backup documentation . Confirm a cluster Termination The general tab # The General tab gives you the basic information about your cluster. If you have created a cluster with managed users it will only give you the URL of the cluster. If you have created a cluster without managed cluster it will also give you the user name and password that were set for the admin user at cluster creation. General tab Manage the services in the services tab # The services tab shows which service ports are open to the internet on your cluster. More details can be found in the services documentation . Services tab Get information about your cluster state in the Console tab # The console tab display more detailed information about the current state of your cluster. If your cluster is running and everything is as planned it will only say \"everything is ok\". But, if something failed, this is where you will find more details about the error. Console tab Manage your backups in the Backups tab # The backups tab is where you create and manage backups for your cluster. You can find more details about the backups, in the backups documentation . Backups tab Get more details and manage your workers in the Details tab # The Details tab provides you with details about your cluster setup. It is also where you can add and remove workers or configure the autoscaling . Details tab Get more details about your cluster RonDB in the RonDB tab # The RonDB tab provides you with details about the instances running RonDB in your cluster. This is also where you can scale up Rondb if needed. RonDB tab Conclusion # You now have an overview of where your different cluster information can be found and how you can manage your cluster. To go further you can learn how to add and remove workers or configure the autoscaling on your cluster or how to take and restore backups .","title":"The dashboard"},{"location":"setup_installation/common/dashboard/#how-to-manage-your-clusters-in-managedhopsworksai","text":"","title":"How to manage your clusters in managed.hopsworks.ai"},{"location":"setup_installation/common/dashboard/#introduction","text":"Managed.hopsworks.ai is our managed platform for running Hopsworks and the Feature Store in the cloud. On this page, you will get an overview of the different functionalities of the managed.hopsworks.ai dashboard.","title":"Introduction"},{"location":"setup_installation/common/dashboard/#prerequisites","text":"If you want to navigate the to the different tabs presented in this document you will need to connect managed.hopsworks.ai and create a cluster. Instructions about this process can be found in the getting started pages ( AWS , Azure , GCP )","title":"Prerequisites"},{"location":"setup_installation/common/dashboard/#dashboard-overview","text":"The landing page of managed.hopsworks.ai can be seen in the picture below. It is composed of three main parts. At the top, you have a menu bar (1) allowing you to navigate between the dashboard and the settings . Bellow, you have a menu column (2) allowing you to navigate between different functionalities of the dashboard. And finally, in the middle, you find pannels representing your different clusters (3) and a button to create new clusters (4). Dashboard overview The different functionalities of the dashboard are: Clusters : the landing page which we will detail below. Members : the place to manage the members of your organization ( doc ). Backup : the place to manage your clusters' backup ( doc ). Usage : the place to get your credits consumption.","title":"Dashboard overview"},{"location":"setup_installation/common/dashboard/#managing-your-clusters-in-the-cluster-panel","text":"The cluster panels contain the name of the cluster (1), its state (2) a button to terminate the cluster (3), a button to stop the cluster (4) and different tabs to manage the cluster (5). You will now learn more details about these actions and tabs. Cluster pannel structure","title":"Managing your clusters in the cluster panel"},{"location":"setup_installation/common/dashboard/#stop-the-cluster","text":"To stop a cluster click on the Stop button on the top right part of the cluster panel. Stopping the cluster stop in instances it is running on and delete the workers nodes. This allows you to save credits in managed.hopsworks.ai and in your cloud provider. Stop a cluster","title":"Stop the cluster"},{"location":"setup_installation/common/dashboard/#terminate-the-cluster","text":"To terminate a cluster click on the Terminate button on the top right part of the cluster panel. Terminate a cluster Terminating the cluster will destroy it and delete all the resources that were automatically created during the cluster creation. To be sure that you are not terminating a cluster by accident you will be asked to confirm that you want to terminate the cluster. To confirm the termination, check the check box and click on Terminate . Note Terminating a cluster does not delete or empty the bucket associated with the cluster. This is because this bucket is needed to restore a backup. You can find more information about backups in the backup documentation . Confirm a cluster Termination","title":"Terminate the cluster"},{"location":"setup_installation/common/dashboard/#the-general-tab","text":"The General tab gives you the basic information about your cluster. If you have created a cluster with managed users it will only give you the URL of the cluster. If you have created a cluster without managed cluster it will also give you the user name and password that were set for the admin user at cluster creation. General tab","title":"The general tab"},{"location":"setup_installation/common/dashboard/#manage-the-services-in-the-services-tab","text":"The services tab shows which service ports are open to the internet on your cluster. More details can be found in the services documentation . Services tab","title":"Manage the services in the services tab"},{"location":"setup_installation/common/dashboard/#get-information-about-your-cluster-state-in-the-console-tab","text":"The console tab display more detailed information about the current state of your cluster. If your cluster is running and everything is as planned it will only say \"everything is ok\". But, if something failed, this is where you will find more details about the error. Console tab","title":"Get information about your cluster state in the Console tab"},{"location":"setup_installation/common/dashboard/#manage-your-backups-in-the-backups-tab","text":"The backups tab is where you create and manage backups for your cluster. You can find more details about the backups, in the backups documentation . Backups tab","title":"Manage your backups in the Backups tab"},{"location":"setup_installation/common/dashboard/#get-more-details-and-manage-your-workers-in-the-details-tab","text":"The Details tab provides you with details about your cluster setup. It is also where you can add and remove workers or configure the autoscaling . Details tab","title":"Get more details and manage your workers in the Details tab"},{"location":"setup_installation/common/dashboard/#get-more-details-about-your-cluster-rondb-in-the-rondb-tab","text":"The RonDB tab provides you with details about the instances running RonDB in your cluster. This is also where you can scale up Rondb if needed. RonDB tab","title":"Get more details about your cluster RonDB in the RonDB tab"},{"location":"setup_installation/common/dashboard/#conclusion","text":"You now have an overview of where your different cluster information can be found and how you can manage your cluster. To go further you can learn how to add and remove workers or configure the autoscaling on your cluster or how to take and restore backups .","title":"Conclusion"},{"location":"setup_installation/common/rondb/","text":"Managed RonDB # For applications where Feature Store's performance and scalability is paramount we give users the option to create clusters with Managed RonDB . You don't need to worry about configuration as managed.hopsworks.ai will automatically pick the best options for your setup. To start a new Hopsworks Cluster with a Managed RonDB installation one first goes to the Dashboard view seen below. Dashboard After clicking on Create cluster we can select the Cloud to use, AWS, GCP or Azure. Choose cloud menu The configuration of your RonDB cluster is done in the RonDB database tab at the left in your Create cluster menu . Single node RonDB # The minimum setup for a Hopsworks cluster is to run all database services on their own virtual machine additionally to the Head node. This way the RonDB database service can scale independently of other cluster services. You can pick the VM type of this VM in the scroll bar RonDB database instance type . It is also possible to change the size of the local storage in the VM through the scroll bar Local storage size . Normally it isn't necessary to change this setting. It is possible to reconfigure from a Single Node RonDB to a RonDB cluster. It is not possible to reconfigure from a RonDB cluster to a Single Node RonDB. When reconfiguring from a Single Node RonDB to RonDB cluster the Single Node VM is converted into the Management Server VM. The management server VM only needs to use 2 VCPUs in a RonDB cluster. The Single Node RonDB is mainly intended for experiments and Proof of Concept installations. For production usage it is better to select a RonDB cluster and use a single replica if a really small cluster is desired. Scaling from a small RonDB cluster to a very large RonDB cluster can be done as online operations. Configure RonDB Note For cluster versions <= 2.5.0 the database services run on Head node RonDB cluster # To setup a cluster with multiple RonDB nodes, select RonDB cluster during cluster creation. If this option is not available contact us . General # If you enable Managed RonDB you will see a basic configuration page where you can configure the database nodes. RonDB basic configuration Data node # First, you need to select the instance type and local storage size for the data nodes. These are the database nodes that will store data. RonDB in Hopsworks is an in-memory database, so in order to fit more data you need to choose an instance type with more memory. Local storage is used for offline storage of recovery data. RonDB does support on-disk columns that can be used for on-disk features, this will be accessible from Hopsworks in some future version. Since the data node is an in-memory database we only provide options with high memory compared to the CPUs. Most VMs have 8 GByte of memory per VCPU. The VM type is changed with the scroll bar RonDB Data node instance type . Local storage size should be sufficient to use the default. Number of replicas # Next you need to select the number of replicas . This is the number of copies the cluster will maintain of your data. Choosing 1 replica is the cheapest option since it requires the lowest number of nodes, but this way you don't have High Availability. Whenever any of the RonDB data nodes in the cluster fails, the cluster will also fail, so only choose 1 replica if you are willing to accept cluster failure. The default and recommended is to use 2 replicas, which allows the cluster to continue operating after any one data node fails. With 3 replicas, the cluster can continue operating after any two data node failures that happen after each other. If you want to try out RonDB with a cheap setup it is possible to select 1 replica when you create the cluster and later reconfigure the cluster with 2 or 3 replicas. One can also decrease the number of replicas through online reconfiguration. MySQLd nodes # Next you can configure the number of MySQLd nodes. These are dedicated nodes for performing SQL queries against your RonDB cluster. The Feature Store will use all the MySQL Servers with load balancing. The load balancing is implemented using Consul. This means that if a MySQL Server is stopped the application will discover a failure and will reconnect and choose one of the MySQL Servers that are still up. Similarly when a MySQL Server is started it will be used in the selection of which MySQL Server to use for a new connection setup. Selection of VM type for the MySQL Servers is done in the scroll bar MySQLd instance type . As usual the default for local storage size should be sufficient. Feature Stores is a read-heavy application. In such environment it is normal that optimal throughput is to have 50% to 100% more CPUs in the MySQL Servers compared to the Data nodes. MySQL Servers are most efficient up to 32 VCPUs. Scaling to more MySQL Servers is efficient, thus it is best to first use enough MySQL Servers for high availability, next to scale them to 32 VCPUs. If more CPUs are needed then scale with even more MySQL Servers. MySQL Servers are only clients to the data nodes in RonDB. Thus it will not use a lot of memory, we mainly provide options with high CPU compared to the memory. Advanced # The advanced tab offers less common options. One node group will serve most use cases, it is mainly useful to have several node groups if the database doesn't fit in a single data node VM. API nodes is currently mostly useful for benchmarking, but is intended also for custom applications with extreme requirements on low latency. We recommend keeping the defaults unless you know what you are doing. RonDB advanced configuration RonDB Data node groups # You can choose the number of node groups, also known as database shards. The default is 1 node group, which means that all nodes will have a complete copy of all data. Increasing the number of node groups will split the data evenly. This way, you can create a cluster with higher capacity than a single node. For use cases where it is possible, we recommend using 1 node group and choose an instance type with enough memory to fit all data. Below the number of node groups, you will see a summary of cluster resources. The number of data nodes is an important consideration for the cost of the cluster. It is calculated as the number of node groups multiplied by the number of replicas. The memory available to the cluster is calculated as the number of node groups multiplied by the memory per node. Note that the number replicas does not affect the available memory. The CPUs available to the cluster is calculated as the number of node groups multiplied by the number of CPUs per node. Note that the number replicas does not affect the available CPUs. API nodes # API nodes are specialized nodes which can run user code connecting directly to RonDB datanodes for increased performance. You can choose the number of nodes, the instance type and local storage size. There is also a checkbox to grant access to benchmark tools. This will let a benchmark user access specific database tables, so that you can benchmark RonDB safely. For more information on how to benchmark RonDB, see the RonDB documentation . Online Reconfiguration of RonDB # After creating a RonDB cluster it is possible to resize the RonDB cluster through an online reconfiguration. This means that both reads and writes of tables and feature groups can continue while the change is ongoing without any downtime. During reconfiguration of data nodes there could be periods where no new tables can be added and tables cannot be dropped. Reads and writes to all existing tables will however always be possible. The online reconfiguration supports increasing and decreasing size of VMs, local storage of MySQL Servers. For data nodes we support changing to larger VMs and changing numbers of replicas to 1,2 and 3 and changing the local storage size to a larger size. Why Reconfiguring Storage Client Layer (MySQL Servers) # Reconfiguration of MySQL Servers can be done several times per day to meet current demands by the application. During a reconfiguration new MySQL Servers are added to the Consul address onlinefs.mysql.service.consul. Thus new connections will be quickly using the new MySQL Servers. MySQL Servers that are stopped will be removed from Consul a few seconds before the MySQL Server is stopped. Thus an application using the MySQL Server can avoid most, if not all temporary errors due to reconfiguration by reconnecting to the MySQL Server after using a connection for about 3 seconds. However even without this the impact on applications due to the reconfiguration will be very small. An example of an application could be an online retailer. During nights the activity is low, so the need of the MySQL Servers is low and one could use 2 x 4 VCPUs. In the morning business picks up and one can increase the use to 2 x 16 VCPUs. In the evening the business is heavier and one needs 2 x 32 VCPUs to meet the demand. Finally weekend rushes is handled with 3 x 32 VCPUs. Why Reconfiguring Storage Layer (RonDB Data nodes) # Reconfiguration of the RonDB Data nodes is a bit more rare since the database resides in memory. At the moment we only support increasing the size of the RonDB data nodes to ensure that we always have room for the database. Storing the database in-memory provides much lower latency to online applications. Thus reconfiguring of the Storage Layer is mainly intended to meet long-term needs of the business in terms of CPU and memory. In addition one can change the number of replicas to improve the availability of the RonDB cluster. Reconfiguration of Storage Client Layer # To start a reconfiguration of the MySQL Servers choose the RonDB tab in the Cluster you want to change. In the Configuration Tab you select the MySQLd instance type scroll bar and set the new VM type you want to use. In the figure above we selected e2-highcpu-16 (previously used e2-highcpu-8), thus doubling the VCPUs used by the Storage Client Layer. Reconfiguration of MySQL Server When you made this choice the Changes to be submitted is listed above the Submit button. If you decide to skip the current change you can push the Reset button to return to the original settings and start again the reconfiguration change. After clicking the Submit button, the below pop-up window appears. You need to check the Yes, reconfigure RonDB cluster button to proceeed. Accept Reconfiguration After clicking this button you now need to push Reconfigure button in the below pop-up window. Final Accept Reconfiguration After clicking the Reconfigure the reconfiguration and you will see the below. Reconfiguration Pending And shortly thereafter you will see the state change to only reconfiguring. Reconfiguration Ongoing The process to reconfigure a few MySQL Servers will take a few minutes where a major portion is spent on creating the new MySQL Server VMs. During this process we can follow the process by clicking at the Console where the state of the Reconfiguration will be presented in more details. The Reconfiguration process # The first step in the reconfiguration process is to create the new VMs. During this step the we see the below message in the Console. The steps below isn't all the states shown and could change in future versions. So this part is mostly to give an idea of what is happening during the reconfiguration process. Reconfiguration Console Wait For VMs The next step is to initialise the VMs and the below message is shown while this step is ongoing. Reconfiguration Console Wait For Initialisation of VMs When all VMs have been created and initialised we see the below message in the Console. Reconfiguration Console All VMs created With all the new MySQL Server VMs created and initialised we are ready to start the first MySQL Server. During this we see the following message for a short time. Reconfiguration Console Start MySQL Server After starting the new MySQL Server we need to insert the MySQL Server into Consul and other post init activities. During this step we see the message below. Already here the new MySQL Server can start serving queries in the cluster. Reconfiguration Console Post Init MySQL Server At some point the old MySQL Servers need to be stopped and removed from the cluster. During this step we see this message. After this step the old MySQL Servers are no longer serving queries in the cluster. The order of starting MySQL Server and stopping them can change, but we always ensure that we never decrease the number of MySQL Servers until the final step if at all. Reconfiguration Console Deactivate MySQL Server After starting all the MySQL Servers we have some cleanup steps to go through such as deleting the old VMs. However during this step the cluster is already reconfigured. After all cleanup steps are completed the final message arrives. As you can see the state is changed to running . This means that a new reconfiguration can be started again as well. Only one reconfiguration at a time is allowed. Reconfiguration Console Final state Reconfiguration of the Storage Layer (RonDB Data nodes) # When reconfiguring the Storage Layer we can change 3 things. We can change the VM type to choose a VM type with more memory and more CPUs. We can change the number of replicas and finally we can change the local storage size. We can change all three parameters at once. The below shows an example of how to do this. Reconfiguration Data nodes It is currently not possible to reconfigure API nodes. The VM type differs from cloud to cloud. The start of a reconfiguration uses the same pop-up windows as when reconfiguring the Storage Client Layer. The steps it goes through is slightly different but shares many similarities. However starting a RonDB data node will take longer time, the time it takes is dependent on the database size. Combined reconfiguration of all Layers # It is possible to reconfigure both the Storage Layer and the Storage Client Layer simultaneously. The process is the same, but will obviously take a bit more time since more changes are required. RonDB details # Once the cluster is created you can view some details by clicking on the RonDB tab, followed by clicking on the Nodes tab as shown in the picture below. RonDB cluster details","title":"Managed RonDB"},{"location":"setup_installation/common/rondb/#managed-rondb","text":"For applications where Feature Store's performance and scalability is paramount we give users the option to create clusters with Managed RonDB . You don't need to worry about configuration as managed.hopsworks.ai will automatically pick the best options for your setup. To start a new Hopsworks Cluster with a Managed RonDB installation one first goes to the Dashboard view seen below. Dashboard After clicking on Create cluster we can select the Cloud to use, AWS, GCP or Azure. Choose cloud menu The configuration of your RonDB cluster is done in the RonDB database tab at the left in your Create cluster menu .","title":"Managed RonDB"},{"location":"setup_installation/common/rondb/#single-node-rondb","text":"The minimum setup for a Hopsworks cluster is to run all database services on their own virtual machine additionally to the Head node. This way the RonDB database service can scale independently of other cluster services. You can pick the VM type of this VM in the scroll bar RonDB database instance type . It is also possible to change the size of the local storage in the VM through the scroll bar Local storage size . Normally it isn't necessary to change this setting. It is possible to reconfigure from a Single Node RonDB to a RonDB cluster. It is not possible to reconfigure from a RonDB cluster to a Single Node RonDB. When reconfiguring from a Single Node RonDB to RonDB cluster the Single Node VM is converted into the Management Server VM. The management server VM only needs to use 2 VCPUs in a RonDB cluster. The Single Node RonDB is mainly intended for experiments and Proof of Concept installations. For production usage it is better to select a RonDB cluster and use a single replica if a really small cluster is desired. Scaling from a small RonDB cluster to a very large RonDB cluster can be done as online operations. Configure RonDB Note For cluster versions <= 2.5.0 the database services run on Head node","title":"Single node RonDB"},{"location":"setup_installation/common/rondb/#rondb-cluster","text":"To setup a cluster with multiple RonDB nodes, select RonDB cluster during cluster creation. If this option is not available contact us .","title":"RonDB cluster"},{"location":"setup_installation/common/rondb/#general","text":"If you enable Managed RonDB you will see a basic configuration page where you can configure the database nodes. RonDB basic configuration","title":"General"},{"location":"setup_installation/common/rondb/#data-node","text":"First, you need to select the instance type and local storage size for the data nodes. These are the database nodes that will store data. RonDB in Hopsworks is an in-memory database, so in order to fit more data you need to choose an instance type with more memory. Local storage is used for offline storage of recovery data. RonDB does support on-disk columns that can be used for on-disk features, this will be accessible from Hopsworks in some future version. Since the data node is an in-memory database we only provide options with high memory compared to the CPUs. Most VMs have 8 GByte of memory per VCPU. The VM type is changed with the scroll bar RonDB Data node instance type . Local storage size should be sufficient to use the default.","title":"Data node"},{"location":"setup_installation/common/rondb/#number-of-replicas","text":"Next you need to select the number of replicas . This is the number of copies the cluster will maintain of your data. Choosing 1 replica is the cheapest option since it requires the lowest number of nodes, but this way you don't have High Availability. Whenever any of the RonDB data nodes in the cluster fails, the cluster will also fail, so only choose 1 replica if you are willing to accept cluster failure. The default and recommended is to use 2 replicas, which allows the cluster to continue operating after any one data node fails. With 3 replicas, the cluster can continue operating after any two data node failures that happen after each other. If you want to try out RonDB with a cheap setup it is possible to select 1 replica when you create the cluster and later reconfigure the cluster with 2 or 3 replicas. One can also decrease the number of replicas through online reconfiguration.","title":"Number of replicas"},{"location":"setup_installation/common/rondb/#mysqld-nodes","text":"Next you can configure the number of MySQLd nodes. These are dedicated nodes for performing SQL queries against your RonDB cluster. The Feature Store will use all the MySQL Servers with load balancing. The load balancing is implemented using Consul. This means that if a MySQL Server is stopped the application will discover a failure and will reconnect and choose one of the MySQL Servers that are still up. Similarly when a MySQL Server is started it will be used in the selection of which MySQL Server to use for a new connection setup. Selection of VM type for the MySQL Servers is done in the scroll bar MySQLd instance type . As usual the default for local storage size should be sufficient. Feature Stores is a read-heavy application. In such environment it is normal that optimal throughput is to have 50% to 100% more CPUs in the MySQL Servers compared to the Data nodes. MySQL Servers are most efficient up to 32 VCPUs. Scaling to more MySQL Servers is efficient, thus it is best to first use enough MySQL Servers for high availability, next to scale them to 32 VCPUs. If more CPUs are needed then scale with even more MySQL Servers. MySQL Servers are only clients to the data nodes in RonDB. Thus it will not use a lot of memory, we mainly provide options with high CPU compared to the memory.","title":"MySQLd nodes"},{"location":"setup_installation/common/rondb/#advanced","text":"The advanced tab offers less common options. One node group will serve most use cases, it is mainly useful to have several node groups if the database doesn't fit in a single data node VM. API nodes is currently mostly useful for benchmarking, but is intended also for custom applications with extreme requirements on low latency. We recommend keeping the defaults unless you know what you are doing. RonDB advanced configuration","title":"Advanced"},{"location":"setup_installation/common/rondb/#rondb-data-node-groups","text":"You can choose the number of node groups, also known as database shards. The default is 1 node group, which means that all nodes will have a complete copy of all data. Increasing the number of node groups will split the data evenly. This way, you can create a cluster with higher capacity than a single node. For use cases where it is possible, we recommend using 1 node group and choose an instance type with enough memory to fit all data. Below the number of node groups, you will see a summary of cluster resources. The number of data nodes is an important consideration for the cost of the cluster. It is calculated as the number of node groups multiplied by the number of replicas. The memory available to the cluster is calculated as the number of node groups multiplied by the memory per node. Note that the number replicas does not affect the available memory. The CPUs available to the cluster is calculated as the number of node groups multiplied by the number of CPUs per node. Note that the number replicas does not affect the available CPUs.","title":"RonDB Data node groups"},{"location":"setup_installation/common/rondb/#api-nodes","text":"API nodes are specialized nodes which can run user code connecting directly to RonDB datanodes for increased performance. You can choose the number of nodes, the instance type and local storage size. There is also a checkbox to grant access to benchmark tools. This will let a benchmark user access specific database tables, so that you can benchmark RonDB safely. For more information on how to benchmark RonDB, see the RonDB documentation .","title":"API nodes"},{"location":"setup_installation/common/rondb/#online-reconfiguration-of-rondb","text":"After creating a RonDB cluster it is possible to resize the RonDB cluster through an online reconfiguration. This means that both reads and writes of tables and feature groups can continue while the change is ongoing without any downtime. During reconfiguration of data nodes there could be periods where no new tables can be added and tables cannot be dropped. Reads and writes to all existing tables will however always be possible. The online reconfiguration supports increasing and decreasing size of VMs, local storage of MySQL Servers. For data nodes we support changing to larger VMs and changing numbers of replicas to 1,2 and 3 and changing the local storage size to a larger size.","title":"Online Reconfiguration of RonDB"},{"location":"setup_installation/common/rondb/#why-reconfiguring-storage-client-layer-mysql-servers","text":"Reconfiguration of MySQL Servers can be done several times per day to meet current demands by the application. During a reconfiguration new MySQL Servers are added to the Consul address onlinefs.mysql.service.consul. Thus new connections will be quickly using the new MySQL Servers. MySQL Servers that are stopped will be removed from Consul a few seconds before the MySQL Server is stopped. Thus an application using the MySQL Server can avoid most, if not all temporary errors due to reconfiguration by reconnecting to the MySQL Server after using a connection for about 3 seconds. However even without this the impact on applications due to the reconfiguration will be very small. An example of an application could be an online retailer. During nights the activity is low, so the need of the MySQL Servers is low and one could use 2 x 4 VCPUs. In the morning business picks up and one can increase the use to 2 x 16 VCPUs. In the evening the business is heavier and one needs 2 x 32 VCPUs to meet the demand. Finally weekend rushes is handled with 3 x 32 VCPUs.","title":"Why Reconfiguring Storage Client Layer (MySQL Servers)"},{"location":"setup_installation/common/rondb/#why-reconfiguring-storage-layer-rondb-data-nodes","text":"Reconfiguration of the RonDB Data nodes is a bit more rare since the database resides in memory. At the moment we only support increasing the size of the RonDB data nodes to ensure that we always have room for the database. Storing the database in-memory provides much lower latency to online applications. Thus reconfiguring of the Storage Layer is mainly intended to meet long-term needs of the business in terms of CPU and memory. In addition one can change the number of replicas to improve the availability of the RonDB cluster.","title":"Why Reconfiguring Storage Layer (RonDB Data nodes)"},{"location":"setup_installation/common/rondb/#reconfiguration-of-storage-client-layer","text":"To start a reconfiguration of the MySQL Servers choose the RonDB tab in the Cluster you want to change. In the Configuration Tab you select the MySQLd instance type scroll bar and set the new VM type you want to use. In the figure above we selected e2-highcpu-16 (previously used e2-highcpu-8), thus doubling the VCPUs used by the Storage Client Layer. Reconfiguration of MySQL Server When you made this choice the Changes to be submitted is listed above the Submit button. If you decide to skip the current change you can push the Reset button to return to the original settings and start again the reconfiguration change. After clicking the Submit button, the below pop-up window appears. You need to check the Yes, reconfigure RonDB cluster button to proceeed. Accept Reconfiguration After clicking this button you now need to push Reconfigure button in the below pop-up window. Final Accept Reconfiguration After clicking the Reconfigure the reconfiguration and you will see the below. Reconfiguration Pending And shortly thereafter you will see the state change to only reconfiguring. Reconfiguration Ongoing The process to reconfigure a few MySQL Servers will take a few minutes where a major portion is spent on creating the new MySQL Server VMs. During this process we can follow the process by clicking at the Console where the state of the Reconfiguration will be presented in more details.","title":"Reconfiguration of Storage Client Layer"},{"location":"setup_installation/common/rondb/#the-reconfiguration-process","text":"The first step in the reconfiguration process is to create the new VMs. During this step the we see the below message in the Console. The steps below isn't all the states shown and could change in future versions. So this part is mostly to give an idea of what is happening during the reconfiguration process. Reconfiguration Console Wait For VMs The next step is to initialise the VMs and the below message is shown while this step is ongoing. Reconfiguration Console Wait For Initialisation of VMs When all VMs have been created and initialised we see the below message in the Console. Reconfiguration Console All VMs created With all the new MySQL Server VMs created and initialised we are ready to start the first MySQL Server. During this we see the following message for a short time. Reconfiguration Console Start MySQL Server After starting the new MySQL Server we need to insert the MySQL Server into Consul and other post init activities. During this step we see the message below. Already here the new MySQL Server can start serving queries in the cluster. Reconfiguration Console Post Init MySQL Server At some point the old MySQL Servers need to be stopped and removed from the cluster. During this step we see this message. After this step the old MySQL Servers are no longer serving queries in the cluster. The order of starting MySQL Server and stopping them can change, but we always ensure that we never decrease the number of MySQL Servers until the final step if at all. Reconfiguration Console Deactivate MySQL Server After starting all the MySQL Servers we have some cleanup steps to go through such as deleting the old VMs. However during this step the cluster is already reconfigured. After all cleanup steps are completed the final message arrives. As you can see the state is changed to running . This means that a new reconfiguration can be started again as well. Only one reconfiguration at a time is allowed. Reconfiguration Console Final state","title":"The Reconfiguration process"},{"location":"setup_installation/common/rondb/#reconfiguration-of-the-storage-layer-rondb-data-nodes","text":"When reconfiguring the Storage Layer we can change 3 things. We can change the VM type to choose a VM type with more memory and more CPUs. We can change the number of replicas and finally we can change the local storage size. We can change all three parameters at once. The below shows an example of how to do this. Reconfiguration Data nodes It is currently not possible to reconfigure API nodes. The VM type differs from cloud to cloud. The start of a reconfiguration uses the same pop-up windows as when reconfiguring the Storage Client Layer. The steps it goes through is slightly different but shares many similarities. However starting a RonDB data node will take longer time, the time it takes is dependent on the database size.","title":"Reconfiguration of the Storage Layer (RonDB Data nodes)"},{"location":"setup_installation/common/rondb/#combined-reconfiguration-of-all-layers","text":"It is possible to reconfigure both the Storage Layer and the Storage Client Layer simultaneously. The process is the same, but will obviously take a bit more time since more changes are required.","title":"Combined reconfiguration of all Layers"},{"location":"setup_installation/common/rondb/#rondb-details","text":"Once the cluster is created you can view some details by clicking on the RonDB tab, followed by clicking on the Nodes tab as shown in the picture below. RonDB cluster details","title":"RonDB details"},{"location":"setup_installation/common/scalingup/","text":"Scaling up # If you run into limitations due to the instance types you chose during a cluster creation it is possible to scale up the instances to overcome these limitations. Scaling up the workers # If spark jobs are not starting in your cluster it may come from the fact that you don't have worker resources to run them. As workers are stateless the best way to solve this problem is to add new workers with enough resources to handle your job. Or to configure autoscalling to automatically add the workers when needed. Scaling up the head node # You may run into the need to scale up the head node for different reasons. For example: You are running a cluster without dedicated RonDB nodes and have a workload with a high demand on the online feature store. You are running a cluster without managed containers and want to run an important number of jupyter notebooks simultaneously. While we are working on implementing a solution to add these features to an existing cluster you can use the following approach to run your head node on an instance with more vcores and memory to handle more load. To scale up the head node you first have to stop your cluster. Stop the cluster Once the cluster is stopped you can go to the Details tab and click on the head node instance type . Go to details tab an click on the head node instance type This will open a new window. Select the type of instance you want to change to and click on Review and submit Select the new instance type for the heade node Verify your choice and click on Modify Note If you set up your account with AWS in a period predating the introduction of this feature you may need to add the indicated permission to your managed.hopsworks.ai role. Validate your choice You can now start your cluster. The head node will be started on an instance type of the new type you chose. Scaling up the RonDB nodes # If you are running a cluster with dedicated RonDB nodes and have a workload with a high demand on the online feature store you may need to scale up the RonDB Datanodes and MySQLd nodes. For this stop the cluster. Stop the cluster Once the cluster is stopped you can go to the RonDB tab. To scale MySQLd or API nodes, click on the instance type for the node you want to scale up. To scale all datanodes, click on the Change button over their instance types. Datanodes cannot be scaled individually. Go to RonDB tab and click on the instance type you want to change or, for datanodes, click on the Change button This will open a new window. Select the type of instance you want to change to and click on Review and submit Select the new instance type for the heade node Verify your choice and click on Modify Note If you set up your account with AWS in a period predating the introduction of this feature you may need to add the indicated permission to your managed.hopsworks.ai role. Validate your choice You can now start your cluster. The nodes will be started on an instance type of the new type you chose.","title":"Scaling up"},{"location":"setup_installation/common/scalingup/#scaling-up","text":"If you run into limitations due to the instance types you chose during a cluster creation it is possible to scale up the instances to overcome these limitations.","title":"Scaling up"},{"location":"setup_installation/common/scalingup/#scaling-up-the-workers","text":"If spark jobs are not starting in your cluster it may come from the fact that you don't have worker resources to run them. As workers are stateless the best way to solve this problem is to add new workers with enough resources to handle your job. Or to configure autoscalling to automatically add the workers when needed.","title":"Scaling up the workers"},{"location":"setup_installation/common/scalingup/#scaling-up-the-head-node","text":"You may run into the need to scale up the head node for different reasons. For example: You are running a cluster without dedicated RonDB nodes and have a workload with a high demand on the online feature store. You are running a cluster without managed containers and want to run an important number of jupyter notebooks simultaneously. While we are working on implementing a solution to add these features to an existing cluster you can use the following approach to run your head node on an instance with more vcores and memory to handle more load. To scale up the head node you first have to stop your cluster. Stop the cluster Once the cluster is stopped you can go to the Details tab and click on the head node instance type . Go to details tab an click on the head node instance type This will open a new window. Select the type of instance you want to change to and click on Review and submit Select the new instance type for the heade node Verify your choice and click on Modify Note If you set up your account with AWS in a period predating the introduction of this feature you may need to add the indicated permission to your managed.hopsworks.ai role. Validate your choice You can now start your cluster. The head node will be started on an instance type of the new type you chose.","title":"Scaling up the head node"},{"location":"setup_installation/common/scalingup/#scaling-up-the-rondb-nodes","text":"If you are running a cluster with dedicated RonDB nodes and have a workload with a high demand on the online feature store you may need to scale up the RonDB Datanodes and MySQLd nodes. For this stop the cluster. Stop the cluster Once the cluster is stopped you can go to the RonDB tab. To scale MySQLd or API nodes, click on the instance type for the node you want to scale up. To scale all datanodes, click on the Change button over their instance types. Datanodes cannot be scaled individually. Go to RonDB tab and click on the instance type you want to change or, for datanodes, click on the Change button This will open a new window. Select the type of instance you want to change to and click on Review and submit Select the new instance type for the heade node Verify your choice and click on Modify Note If you set up your account with AWS in a period predating the introduction of this feature you may need to add the indicated permission to your managed.hopsworks.ai role. Validate your choice You can now start your cluster. The nodes will be started on an instance type of the new type you chose.","title":"Scaling up the RonDB nodes"},{"location":"setup_installation/common/services/","text":"Services # Hopsworks clusters provide several services that can be accessed from outside Hopsworks. In this documentation, we first show how to make these services accessible to external networks. We will then go through the different services to give a short introduction and link to the associated documentation. Outside Access to the Feature Store # By default, only the Hopsworks UI is made available to clients on external networks, like the Internet. To integrate with external platforms and access APIs for services such as the Feature Store, you have to open the service's ports. Open ports by going to the Services tab, selecting a service, and pressing Update . This will update the Security Group attached to the Hopsworks cluster to allow incoming traffic on the relevant ports. Outside Access to the Feature Store If you do not want the ports to be open to the internet you can set up VPC peering between the Hopsworks VPC and your client VPC. You then need to make sure that the ports associated with the services you want to use are open between the two VPCs. The ports associated with each of the services are indicated in the descriptions of the services below. Feature store # The Feature Store is a data management system for managing machine learning features, including the feature engineering code and the feature data. The Feature Store helps ensure that features used during training and serving are consistent and that features are documented and reused within enterprises. You can find more about the feature store here and information about how to connect to the Feature Store from different external services here Ports: 8020, 30010, 9083 and 9085 Online Feature store # The online Feature store is required for online applications, where the goal is to retrieve a single feature vector with low latency and the same logic as was applied to generate the training dataset, such that the vector can subsequently be passed to a machine learning model in production to compute a prediction. You can find a more detailed explanation of the difference between Online and Offline Feature Store here . Once you have opened the ports, the Online Feature store can be used with the same library as the offline feature store. You can find more in the user guildes . Port: 3306 Kafka # Hopsworks provides Kafka-as-a-Service for streaming applications and to ingest data. You can find more information about how to use Kafka in Hopsworks in this documentation Port: 9092 SSH # If you want to be able to SSH into the virtual machines running the Hopsworks cluster, you can open the ports using the Services tab. You can then SSH into the machine using your cluster operation system ( ubuntu or centos ) as the user name and the ssh key you selected during the cluster creation. Port: 22. ArrowFlight with DuckDB # Hopsworks provides ArrowFlight Server(s) with DuckDB to significantly reduce the time that Python clients need to read feature groups and batch inference data from the Feature Store, as well as creating moderately-sized in-memory training datasets. Port: 5005 Limiting outbound traffic to managed.hopsworks.ai # If you have enabled the use of static IPs to communicate with managed.hopsworks.ai as described in AWS and AZURE , you need to ensure that your security group allow outbound traffic to the two IPs indicated in the service page. Limiting outbound traffic to managed.hopsworks.ai","title":"Services"},{"location":"setup_installation/common/services/#services","text":"Hopsworks clusters provide several services that can be accessed from outside Hopsworks. In this documentation, we first show how to make these services accessible to external networks. We will then go through the different services to give a short introduction and link to the associated documentation.","title":"Services"},{"location":"setup_installation/common/services/#outside-access-to-the-feature-store","text":"By default, only the Hopsworks UI is made available to clients on external networks, like the Internet. To integrate with external platforms and access APIs for services such as the Feature Store, you have to open the service's ports. Open ports by going to the Services tab, selecting a service, and pressing Update . This will update the Security Group attached to the Hopsworks cluster to allow incoming traffic on the relevant ports. Outside Access to the Feature Store If you do not want the ports to be open to the internet you can set up VPC peering between the Hopsworks VPC and your client VPC. You then need to make sure that the ports associated with the services you want to use are open between the two VPCs. The ports associated with each of the services are indicated in the descriptions of the services below.","title":"Outside Access to the Feature Store"},{"location":"setup_installation/common/services/#feature-store","text":"The Feature Store is a data management system for managing machine learning features, including the feature engineering code and the feature data. The Feature Store helps ensure that features used during training and serving are consistent and that features are documented and reused within enterprises. You can find more about the feature store here and information about how to connect to the Feature Store from different external services here Ports: 8020, 30010, 9083 and 9085","title":"Feature store"},{"location":"setup_installation/common/services/#online-feature-store","text":"The online Feature store is required for online applications, where the goal is to retrieve a single feature vector with low latency and the same logic as was applied to generate the training dataset, such that the vector can subsequently be passed to a machine learning model in production to compute a prediction. You can find a more detailed explanation of the difference between Online and Offline Feature Store here . Once you have opened the ports, the Online Feature store can be used with the same library as the offline feature store. You can find more in the user guildes . Port: 3306","title":"Online Feature store"},{"location":"setup_installation/common/services/#kafka","text":"Hopsworks provides Kafka-as-a-Service for streaming applications and to ingest data. You can find more information about how to use Kafka in Hopsworks in this documentation Port: 9092","title":"Kafka"},{"location":"setup_installation/common/services/#ssh","text":"If you want to be able to SSH into the virtual machines running the Hopsworks cluster, you can open the ports using the Services tab. You can then SSH into the machine using your cluster operation system ( ubuntu or centos ) as the user name and the ssh key you selected during the cluster creation. Port: 22.","title":"SSH"},{"location":"setup_installation/common/services/#arrowflight-with-duckdb","text":"Hopsworks provides ArrowFlight Server(s) with DuckDB to significantly reduce the time that Python clients need to read feature groups and batch inference data from the Feature Store, as well as creating moderately-sized in-memory training datasets. Port: 5005","title":"ArrowFlight with DuckDB"},{"location":"setup_installation/common/services/#limiting-outbound-traffic-to-managedhopsworksai","text":"If you have enabled the use of static IPs to communicate with managed.hopsworks.ai as described in AWS and AZURE , you need to ensure that your security group allow outbound traffic to the two IPs indicated in the service page. Limiting outbound traffic to managed.hopsworks.ai","title":"Limiting outbound traffic to managed.hopsworks.ai"},{"location":"setup_installation/common/settings/","text":"How to manage your managed.hopsworks.ai account # Introduction # Managed.hopsworks.ai is our managed platform for running Hopsworks and the Feature Store in the cloud. On this page, you will get an overview of the managed.hopsworks.ai page. How to get to the settings page and what does it look like. # From the managed.hopsworks.ai landing page , you can access the setting page by clicking on Settings on the top left. The settings page contains a menu on the left. The remaining of the page display information depending on the section you have selected in the menu. Getting to the settings page Manage the connection to your cloud accounts under Cloud Accounts # The landing section of the settings page is the Cloud Accounts section. On this page you can edit the link between managed.hopsworks.ai and your cloud provider by clicking on the Edit button (1). You can delete the link and remove any access from managed.hopsworks.ai to your cloud manager by clicking on the Delete button (2). Or, you can configure a new connection with a cloud provider by clicking on the Configure button (3). For more details about setting up a connection with a cloud provider see the getting started pages for: AWS Azure GCP Cloud accounts management Manage your personal information under Profile # The Profile section is where you can edit your personal information such as name, phone number, company, etc. Profile Change your password and configure multi-factor authentication under the Security section # Change your password # To change your password, go to the security section, enter your current password in the Current password field (1), enter the new password in the New password field (2), and click on Save (3). Change password Set up multi-factor authentication. # To set up multi-factor authentication, go to the security section, scan the QR code (1) with your authenticator app (example: Google Authenticator ). Then enter the security code provided by the authenticator app in the Security code field (2) and click on Enable TOTP (3). Enable MFA Create and manage API keys under the API keys section # The API key section is where you create or delete your managed.hopsworks.ai API keys. More details about the API keys can be found in the API keys documentation Generate an API Key Conclusion # You now know where and how to update your profile, cloud accounts, and API keys. To keep familiarizing with managed.hopsworks.ai check the dashboard documentation","title":"Settings"},{"location":"setup_installation/common/settings/#how-to-manage-your-managedhopsworksai-account","text":"","title":"How to manage your managed.hopsworks.ai account"},{"location":"setup_installation/common/settings/#introduction","text":"Managed.hopsworks.ai is our managed platform for running Hopsworks and the Feature Store in the cloud. On this page, you will get an overview of the managed.hopsworks.ai page.","title":"Introduction"},{"location":"setup_installation/common/settings/#how-to-get-to-the-settings-page-and-what-does-it-look-like","text":"From the managed.hopsworks.ai landing page , you can access the setting page by clicking on Settings on the top left. The settings page contains a menu on the left. The remaining of the page display information depending on the section you have selected in the menu. Getting to the settings page","title":"How to get to the settings page and what does it look like."},{"location":"setup_installation/common/settings/#manage-the-connection-to-your-cloud-accounts-under-cloud-accounts","text":"The landing section of the settings page is the Cloud Accounts section. On this page you can edit the link between managed.hopsworks.ai and your cloud provider by clicking on the Edit button (1). You can delete the link and remove any access from managed.hopsworks.ai to your cloud manager by clicking on the Delete button (2). Or, you can configure a new connection with a cloud provider by clicking on the Configure button (3). For more details about setting up a connection with a cloud provider see the getting started pages for: AWS Azure GCP Cloud accounts management","title":"Manage the connection to your cloud accounts under Cloud Accounts"},{"location":"setup_installation/common/settings/#manage-your-personal-information-under-profile","text":"The Profile section is where you can edit your personal information such as name, phone number, company, etc. Profile","title":"Manage your personal information under Profile"},{"location":"setup_installation/common/settings/#change-your-password-and-configure-multi-factor-authentication-under-the-security-section","text":"","title":"Change your password and configure multi-factor authentication under the Security section"},{"location":"setup_installation/common/settings/#change-your-password","text":"To change your password, go to the security section, enter your current password in the Current password field (1), enter the new password in the New password field (2), and click on Save (3). Change password","title":"Change your password"},{"location":"setup_installation/common/settings/#set-up-multi-factor-authentication","text":"To set up multi-factor authentication, go to the security section, scan the QR code (1) with your authenticator app (example: Google Authenticator ). Then enter the security code provided by the authenticator app in the Security code field (2) and click on Enable TOTP (3). Enable MFA","title":"Set up multi-factor authentication."},{"location":"setup_installation/common/settings/#create-and-manage-api-keys-under-the-api-keys-section","text":"The API key section is where you create or delete your managed.hopsworks.ai API keys. More details about the API keys can be found in the API keys documentation Generate an API Key","title":"Create and manage API keys under the API keys section"},{"location":"setup_installation/common/settings/#conclusion","text":"You now know where and how to update your profile, cloud accounts, and API keys. To keep familiarizing with managed.hopsworks.ai check the dashboard documentation","title":"Conclusion"},{"location":"setup_installation/common/terraform/","text":"Hopsworks.ai Terraform Provider # Managed.hopsworks.ai allows users to create and manage their clusters using the Hopsworks.ai terraform provider . In this guide, we first provide brief description on how to get started on AWS and AZURE, then we show how to import an existing cluster to be managed by terraform. Getting Started with AWS # Complete the following steps to start using Hopsworks.ai Terraform Provider on AWS. Create a managed.hopsworks.ai API KEY as described in details here , and export the API KEY as follows export HOPSWORKSAI_API_KEY = <YOUR_API_KEY> Download the proper Terraform CLI for your os from here . Install the AWS CLI and run aws configurre to configure your AWS credentials. Example # In this section, we provide a simple example to create a Hopsworks cluster on AWS along with all its required resources (ssh key, S3 bucket, and instance profile with the required permissions). In your terminal, run the following to create a demo directory and cd to it mkdir demo cd demo In this empty directory, create an empty file main.tf . Open the file and paste the following configurations to it then save it. terraform { required_providers { aws = { source = \"hashicorp/aws\" version = \"4.16.0\" } hopsworksai = { source = \"logicalclocks/hopsworksai\" } } } variable \"region\" { type = string default = \"us-east-2\" } provider \"aws\" { region = var.region } provider \"hopsworksai\" { } # Create the required aws resources, an ssh key, an s3 bucket, and an instance profile with the required Hopsworks permissions module \"aws\" { source = \"logicalclocks/helpers/hopsworksai//modules/aws\" region = var.region version = \"2.3.0\" } # Create a cluster with no workers resource \"hopsworksai_cluster\" \"cluster\" { name = \"tf-hopsworks-cluster\" ssh_key = module.aws.ssh_key_pair_name head { instance_type = \"m5.2xlarge\" } aws_attributes { region = var.region instance_profile_arn = module.aws.instance_profile_arn bucket { name = module.aws.bucket_name } } rondb { single_node { instance_type = \"t3a.xlarge\" } } autoscale { non_gpu_workers { instance_type = \"m5.2xlarge\" disk_size = 256 min_workers = 1 max_workers = 5 standby_workers = 0.5 downscale_wait_time = 300 } } open_ports { ssh = true } } output \"hopsworks_cluster_url\" { value = hopsworksai_cluster.cluster.url } Initialize the terraform directory by running the following command terraform init Now you can apply the changes to create all required resources terraform apply Once terraform finishes creating the resources, it will output the url to the newly created cluster. Notice that for now, you have to navigate to your managed.hopsworks.ai dashboard to get your login credentials. After you finish working with the cluster, you can terminate it along with the other AWS resources using the following command terraform destroy Getting Started with AZURE # Complete the following steps to start using Hopsworks.ai Terraform Provider on AZURE. Create a managed.hopsworks.ai API KEY as described in details here , and export the API KEY as follows export HOPSWORKSAI_API_KEY = <YOUR_API_KEY> Download the proper Terraform CLI for your os from here . Install the AZURE CLI and run az login to configure your AZURE credentials. Example # In this section, we provide a simple example to create a Hopsworks cluster on AZURE along with all its required resources (ssh key, storage account, acr registry, and user assigned managed identity with the required permissions). In your terminal, run the following to create a demo directory and cd to it mkdir demo cd demo In this empty directory, create an empty file main.tf . Open the file and paste the following configurations to it then save it. terraform { required_providers { azurerm = { source = \"hashicorp/azurerm\" version = \"3.8.0\" } hopsworksai = { source = \"logicalclocks/hopsworksai\" } } } variable \"resource_group\" { type = string } provider \"azurerm\" { features {} skip_provider_registration = true } provider \"hopsworksai\" { } data \"azurerm_resource_group\" \"rg\" { name = var.resource_group } # Create the required azure resources, an ssh key, a storage account, and an user assigned managed identity with the required Hopsworks permissions module \"azure\" { source = \"logicalclocks/helpers/hopsworksai//modules/azure\" resource_group = var.resource_group version = \"2.3.0\" } # Create an ACR registry resource \"azurerm_container_registry\" \"acr\" { name = replace ( module.azure.storage_account_name , \"storageaccount\", \"acr\" ) resource_group_name = module.azure.resource_group location = module.azure.location sku = \"Premium\" admin_enabled = false retention_policy { enabled = true days = 7 } } # Create a cluster with no workers resource \"hopsworksai_cluster\" \"cluster\" { name = \"tf-hopsworks-cluster\" ssh_key = module.azure.ssh_key_pair_name head { instance_type = \"Standard_D8_v3\" } azure_attributes { location = module.azure.location resource_group = module.azure.resource_group user_assigned_managed_identity = module.azure.user_assigned_identity_name container { storage_account = module.azure.storage_account_name } acr_registry_name = azurerm_container_registry.acr.name } rondb { single_node { instance_type = \"Standard_D4s_v4\" } } autoscale { non_gpu_workers { instance_type = \"Standard_D8_v3\" disk_size = 256 min_workers = 1 max_workers = 5 standby_workers = 0.5 downscale_wait_time = 300 } } open_ports { ssh = true } } output \"hopsworks_cluster_url\" { value = hopsworksai_cluster.cluster.url } Initialize the terraform directory by running the following command terraform init Now you can apply the changes to create all required resources. Replace the placeholders with your Azure resource group terraform apply -var = \"resource_group=<YOUR_RESOURCE_GROUP>\" Once terraform finishes creating the resources, it will output the url to the newly created cluster. Notice that for now, you have to navigate to your managed.hopsworks.ai dashboard to get your login credentials. After you finish working with the cluster, you can terminate it along with the other AZURE resources using the following command terraform destroy -var = \"resource_group=<YOUR_RESOURCE_GROUP>\" Importing an existing cluster to terraform # In this section, we show how to use terraform import to manage your existing Hopsworks cluster. Step 1 : In your managed.hopsworks.ai dashboard , choose the cluster you want to import to terraform, then go to the Details tab and copy the Id as shown in the figure below Click on the Details tab and copy the Id Step 2 : In your terminal, create an empty directory and cd to it. mkdir import-demo cd import-demo Step 3 : In this empty directory, create an empty file versions.tf . Open the file and paste the following configurations. Note Notice that you need to change these configurations depending on your cluster, in this example, the Hopsworks cluster reside in region us-east-2 on AWS. terraform { required_providers { aws = { source = \"hashicorp/aws\" version = \"4.16.0\" } hopsworksai = { source = \"logicalclocks/hopsworksai\" } } } provider \"aws\" { region = us-east- 2 } provider \"hopsworksai\" { } Step 4 : Initialize the terraform directory by running the following command terraform init Step 5 : Create another file main.tf . Open the file and paste the following configuration. resource \"hopsworksai_cluster\" \"cluster\" { } Step 6 : Import the cluster state using terraform import , in this step you need the cluster id from Step 1 (33ae7ae0-d03c-11eb-84e2-af555fb63565). terraform import hopsworksai_cluster.cluster 33ae7ae0-d03c-11eb-84e2-af555fb63565 The output should be similar to the following snippet hopsworksai_cluster.cluster: Importing from ID \"33ae7ae0-d03c-11eb-84e2-af555fb63565\" ... hopsworksai_cluster.cluster: Import prepared! Prepared hopsworksai_cluster for import hopsworksai_cluster.cluster: Refreshing state... [ id = 33ae7ae0-d03c-11eb-84e2-af555fb63565 ] Import successful! The resources that were imported are shown above. These resources are now in your Terraform state and will henceforth be managed by Terraform. Step 7 : At that moment the local terraform state is updated, however, if we try to run terraform plan or terraform apply it will complain about missing configurations. The reason is that our local resource configuration in main.tf is empty, we should populate it using the terraform state commands as shown below: terraform show -no-color > main.tf Step 8 : If you try to run terraform plan again, the command will complain that the read-only attributes are set (Computed attributes) as shown below. The solution is to remove these attributes from the main.tf and retry again until you have no errors. Error: Computed attributes cannot be set on main.tf line 3 , in resource \"hopsworksai_cluster\" \"cluster\" : 3 : activation_state = \"stoppable\" Computed attributes cannot be set, but a value was set for \"activation_state\" . Error: Computed attributes cannot be set on main.tf line 6 , in resource \"hopsworksai_cluster\" \"cluster\" : 6 : cluster_id = \"33ae7ae0-d03c-11eb-84e2-af555fb63565\" Computed attributes cannot be set, but a value was set for \"cluster_id\" . Error: Computed attributes cannot be set on main.tf line 7 , in resource \"hopsworksai_cluster\" \"cluster\" : 7 : creation_date = \"2021-06-18T15:51:07+02:00\" Computed attributes cannot be set, but a value was set for \"creation_date\" . Error: Invalid or unknown key on main.tf line 8 , in resource \"hopsworksai_cluster\" \"cluster\" : 8 : id = \"33ae7ae0-d03c-11eb-84e2-af555fb63565\" Error: Computed attributes cannot be set on main.tf line 13 , in resource \"hopsworksai_cluster\" \"cluster\" : 13 : start_date = \"2021-06-18T15:51:07+02:00\" Computed attributes cannot be set, but a value was set for \"start_date\" . Error: Computed attributes cannot be set on main.tf line 14 , in resource \"hopsworksai_cluster\" \"cluster\" : 14 : state = \"running\" Computed attributes cannot be set, but a value was set for \"state\" . Error: Computed attributes cannot be set on main.tf line 17 , in resource \"hopsworksai_cluster\" \"cluster\" : 17 : url = \"https://33ae7ae0-d03c-11eb-84e2-af555fb63565.dev-cloud.hopsworks.ai/hopsworks/#!/\" Computed attributes cannot be set, but a value was set for \"url\" . Step 9 : Once you have fixed all the errors, you should get the following output when running terraform plan . With that, you can proceed as normal to manage this cluster locally using terraform. hopsworksai_cluster.cluster: Refreshing state... [ id = 33ae7ae0-d03c-11eb-84e2-af555fb63565 ] No changes. Infrastructure is up-to-date. This means that Terraform did not detect any differences between your configuration and real physical resources that exist. As a result, no actions need to be performed. Next Steps # Check the Hopsworks.ai terraform provider documentation for more details about the different resources and data sources supported by the provider and a description of their attributes. Check the Hopsworks.ai terraform AWS examples , each example contains a README file describing how to run it and more details about configuring it. Check the Hopsworks.ai terraform AZURE examples , each example contains a README file describing how to run it and more details about configuring it.","title":"Terraform"},{"location":"setup_installation/common/terraform/#hopsworksai-terraform-provider","text":"Managed.hopsworks.ai allows users to create and manage their clusters using the Hopsworks.ai terraform provider . In this guide, we first provide brief description on how to get started on AWS and AZURE, then we show how to import an existing cluster to be managed by terraform.","title":"Hopsworks.ai Terraform Provider"},{"location":"setup_installation/common/terraform/#getting-started-with-aws","text":"Complete the following steps to start using Hopsworks.ai Terraform Provider on AWS. Create a managed.hopsworks.ai API KEY as described in details here , and export the API KEY as follows export HOPSWORKSAI_API_KEY = <YOUR_API_KEY> Download the proper Terraform CLI for your os from here . Install the AWS CLI and run aws configurre to configure your AWS credentials.","title":"Getting Started with AWS"},{"location":"setup_installation/common/terraform/#example","text":"In this section, we provide a simple example to create a Hopsworks cluster on AWS along with all its required resources (ssh key, S3 bucket, and instance profile with the required permissions). In your terminal, run the following to create a demo directory and cd to it mkdir demo cd demo In this empty directory, create an empty file main.tf . Open the file and paste the following configurations to it then save it. terraform { required_providers { aws = { source = \"hashicorp/aws\" version = \"4.16.0\" } hopsworksai = { source = \"logicalclocks/hopsworksai\" } } } variable \"region\" { type = string default = \"us-east-2\" } provider \"aws\" { region = var.region } provider \"hopsworksai\" { } # Create the required aws resources, an ssh key, an s3 bucket, and an instance profile with the required Hopsworks permissions module \"aws\" { source = \"logicalclocks/helpers/hopsworksai//modules/aws\" region = var.region version = \"2.3.0\" } # Create a cluster with no workers resource \"hopsworksai_cluster\" \"cluster\" { name = \"tf-hopsworks-cluster\" ssh_key = module.aws.ssh_key_pair_name head { instance_type = \"m5.2xlarge\" } aws_attributes { region = var.region instance_profile_arn = module.aws.instance_profile_arn bucket { name = module.aws.bucket_name } } rondb { single_node { instance_type = \"t3a.xlarge\" } } autoscale { non_gpu_workers { instance_type = \"m5.2xlarge\" disk_size = 256 min_workers = 1 max_workers = 5 standby_workers = 0.5 downscale_wait_time = 300 } } open_ports { ssh = true } } output \"hopsworks_cluster_url\" { value = hopsworksai_cluster.cluster.url } Initialize the terraform directory by running the following command terraform init Now you can apply the changes to create all required resources terraform apply Once terraform finishes creating the resources, it will output the url to the newly created cluster. Notice that for now, you have to navigate to your managed.hopsworks.ai dashboard to get your login credentials. After you finish working with the cluster, you can terminate it along with the other AWS resources using the following command terraform destroy","title":"Example"},{"location":"setup_installation/common/terraform/#getting-started-with-azure","text":"Complete the following steps to start using Hopsworks.ai Terraform Provider on AZURE. Create a managed.hopsworks.ai API KEY as described in details here , and export the API KEY as follows export HOPSWORKSAI_API_KEY = <YOUR_API_KEY> Download the proper Terraform CLI for your os from here . Install the AZURE CLI and run az login to configure your AZURE credentials.","title":"Getting Started with AZURE"},{"location":"setup_installation/common/terraform/#example_1","text":"In this section, we provide a simple example to create a Hopsworks cluster on AZURE along with all its required resources (ssh key, storage account, acr registry, and user assigned managed identity with the required permissions). In your terminal, run the following to create a demo directory and cd to it mkdir demo cd demo In this empty directory, create an empty file main.tf . Open the file and paste the following configurations to it then save it. terraform { required_providers { azurerm = { source = \"hashicorp/azurerm\" version = \"3.8.0\" } hopsworksai = { source = \"logicalclocks/hopsworksai\" } } } variable \"resource_group\" { type = string } provider \"azurerm\" { features {} skip_provider_registration = true } provider \"hopsworksai\" { } data \"azurerm_resource_group\" \"rg\" { name = var.resource_group } # Create the required azure resources, an ssh key, a storage account, and an user assigned managed identity with the required Hopsworks permissions module \"azure\" { source = \"logicalclocks/helpers/hopsworksai//modules/azure\" resource_group = var.resource_group version = \"2.3.0\" } # Create an ACR registry resource \"azurerm_container_registry\" \"acr\" { name = replace ( module.azure.storage_account_name , \"storageaccount\", \"acr\" ) resource_group_name = module.azure.resource_group location = module.azure.location sku = \"Premium\" admin_enabled = false retention_policy { enabled = true days = 7 } } # Create a cluster with no workers resource \"hopsworksai_cluster\" \"cluster\" { name = \"tf-hopsworks-cluster\" ssh_key = module.azure.ssh_key_pair_name head { instance_type = \"Standard_D8_v3\" } azure_attributes { location = module.azure.location resource_group = module.azure.resource_group user_assigned_managed_identity = module.azure.user_assigned_identity_name container { storage_account = module.azure.storage_account_name } acr_registry_name = azurerm_container_registry.acr.name } rondb { single_node { instance_type = \"Standard_D4s_v4\" } } autoscale { non_gpu_workers { instance_type = \"Standard_D8_v3\" disk_size = 256 min_workers = 1 max_workers = 5 standby_workers = 0.5 downscale_wait_time = 300 } } open_ports { ssh = true } } output \"hopsworks_cluster_url\" { value = hopsworksai_cluster.cluster.url } Initialize the terraform directory by running the following command terraform init Now you can apply the changes to create all required resources. Replace the placeholders with your Azure resource group terraform apply -var = \"resource_group=<YOUR_RESOURCE_GROUP>\" Once terraform finishes creating the resources, it will output the url to the newly created cluster. Notice that for now, you have to navigate to your managed.hopsworks.ai dashboard to get your login credentials. After you finish working with the cluster, you can terminate it along with the other AZURE resources using the following command terraform destroy -var = \"resource_group=<YOUR_RESOURCE_GROUP>\"","title":"Example"},{"location":"setup_installation/common/terraform/#importing-an-existing-cluster-to-terraform","text":"In this section, we show how to use terraform import to manage your existing Hopsworks cluster. Step 1 : In your managed.hopsworks.ai dashboard , choose the cluster you want to import to terraform, then go to the Details tab and copy the Id as shown in the figure below Click on the Details tab and copy the Id Step 2 : In your terminal, create an empty directory and cd to it. mkdir import-demo cd import-demo Step 3 : In this empty directory, create an empty file versions.tf . Open the file and paste the following configurations. Note Notice that you need to change these configurations depending on your cluster, in this example, the Hopsworks cluster reside in region us-east-2 on AWS. terraform { required_providers { aws = { source = \"hashicorp/aws\" version = \"4.16.0\" } hopsworksai = { source = \"logicalclocks/hopsworksai\" } } } provider \"aws\" { region = us-east- 2 } provider \"hopsworksai\" { } Step 4 : Initialize the terraform directory by running the following command terraform init Step 5 : Create another file main.tf . Open the file and paste the following configuration. resource \"hopsworksai_cluster\" \"cluster\" { } Step 6 : Import the cluster state using terraform import , in this step you need the cluster id from Step 1 (33ae7ae0-d03c-11eb-84e2-af555fb63565). terraform import hopsworksai_cluster.cluster 33ae7ae0-d03c-11eb-84e2-af555fb63565 The output should be similar to the following snippet hopsworksai_cluster.cluster: Importing from ID \"33ae7ae0-d03c-11eb-84e2-af555fb63565\" ... hopsworksai_cluster.cluster: Import prepared! Prepared hopsworksai_cluster for import hopsworksai_cluster.cluster: Refreshing state... [ id = 33ae7ae0-d03c-11eb-84e2-af555fb63565 ] Import successful! The resources that were imported are shown above. These resources are now in your Terraform state and will henceforth be managed by Terraform. Step 7 : At that moment the local terraform state is updated, however, if we try to run terraform plan or terraform apply it will complain about missing configurations. The reason is that our local resource configuration in main.tf is empty, we should populate it using the terraform state commands as shown below: terraform show -no-color > main.tf Step 8 : If you try to run terraform plan again, the command will complain that the read-only attributes are set (Computed attributes) as shown below. The solution is to remove these attributes from the main.tf and retry again until you have no errors. Error: Computed attributes cannot be set on main.tf line 3 , in resource \"hopsworksai_cluster\" \"cluster\" : 3 : activation_state = \"stoppable\" Computed attributes cannot be set, but a value was set for \"activation_state\" . Error: Computed attributes cannot be set on main.tf line 6 , in resource \"hopsworksai_cluster\" \"cluster\" : 6 : cluster_id = \"33ae7ae0-d03c-11eb-84e2-af555fb63565\" Computed attributes cannot be set, but a value was set for \"cluster_id\" . Error: Computed attributes cannot be set on main.tf line 7 , in resource \"hopsworksai_cluster\" \"cluster\" : 7 : creation_date = \"2021-06-18T15:51:07+02:00\" Computed attributes cannot be set, but a value was set for \"creation_date\" . Error: Invalid or unknown key on main.tf line 8 , in resource \"hopsworksai_cluster\" \"cluster\" : 8 : id = \"33ae7ae0-d03c-11eb-84e2-af555fb63565\" Error: Computed attributes cannot be set on main.tf line 13 , in resource \"hopsworksai_cluster\" \"cluster\" : 13 : start_date = \"2021-06-18T15:51:07+02:00\" Computed attributes cannot be set, but a value was set for \"start_date\" . Error: Computed attributes cannot be set on main.tf line 14 , in resource \"hopsworksai_cluster\" \"cluster\" : 14 : state = \"running\" Computed attributes cannot be set, but a value was set for \"state\" . Error: Computed attributes cannot be set on main.tf line 17 , in resource \"hopsworksai_cluster\" \"cluster\" : 17 : url = \"https://33ae7ae0-d03c-11eb-84e2-af555fb63565.dev-cloud.hopsworks.ai/hopsworks/#!/\" Computed attributes cannot be set, but a value was set for \"url\" . Step 9 : Once you have fixed all the errors, you should get the following output when running terraform plan . With that, you can proceed as normal to manage this cluster locally using terraform. hopsworksai_cluster.cluster: Refreshing state... [ id = 33ae7ae0-d03c-11eb-84e2-af555fb63565 ] No changes. Infrastructure is up-to-date. This means that Terraform did not detect any differences between your configuration and real physical resources that exist. As a result, no actions need to be performed.","title":"Importing an existing cluster to terraform"},{"location":"setup_installation/common/terraform/#next-steps","text":"Check the Hopsworks.ai terraform provider documentation for more details about the different resources and data sources supported by the provider and a description of their attributes. Check the Hopsworks.ai terraform AWS examples , each example contains a README file describing how to run it and more details about configuring it. Check the Hopsworks.ai terraform AZURE examples , each example contains a README file describing how to run it and more details about configuring it.","title":"Next Steps"},{"location":"setup_installation/common/user_management/","text":"User management # In managed.hopsworks.ai users can be grouped into organizations to access the same resources. When a new user registers with managed.hopsworks.ai a new organization is created. This user later on can invite other registered users to their organization so they can share access to the same clusters. Cloud Accounts configuration is also shared among users of the same organization. So if user Alice has configured her account with her credentials, all member of her organization will automatically deploy clusters in her cloud account. Credits and cluster usage are also grouped to ease reporting. Adding members to an organization # Organization membership can be edited by clicking Members on the left of managed.hopsworks.ai Dashboard page. Organization membership To add a new member to your organization add the user's email and click Add . The invited user will receive an email with the invitation. You can set the user as administrator by checking the Admin checkbox. More details about organization administrators can be found [here].(#administrator-role) An invited user must accept the invitation to be part of the organization. An invitation will show up on the invitee's Dashboard. The invitee may have to close the Welcome splash screen to be able to see the invitation. In this example, Alice has invited Bob to her organization, but Bob hasn't accepted the invitation yet. Alice has sent the invitation Bob's dashboard Sharing resources # Once Bob has accepted the invitation he does not have to configure his account, they share the same configuration. Also, he will be able to view the same Dashboard as Alice, so he can start, stop or terminate clusters in the organization. Alice's dashboard Bob's dashboard If Alice had existing clusters running and she had selected Managed user management during cluster creation, an account will be automatically created for Bob on these clusters. Removing members from an organization # To remove a member from your organization simply go to Members page and click the Remove button next to the user you want to remove. You will stop sharing any resource and the user will be blocked from any shared cluster. Delete organization member Organization permissions # The owner and the administrators of an organization can set permissions at the organization level. For this got to the members tab, check the checkboxes in the Member permissions section and click on Update . The supported permissions are: Non admin members can invite new members to the organization . If this permission is enabled, any member of the organization will be able to invite other members to the organization. Note that only the owner and the administrators will be able to invite new members as administrators. If this permission is not enabled only the owner and the administrators can invite new members to the organization. Non admin members can create and terminate clusters . If this permission is enabled, any member of the organization will be able to create and terminate clusters. If it is not enabled, only the owner and the administrators will be able to create and terminate clusters. Non admin members can open clusters ports . If this permission is enabled, any member of the organization can open and close services ports on organization's clusters. If it is not enabled, only the organization owner and administrators will be able to do so. Modify permissions Administrator role # Members of an organization can be set as administrators. This can be done by checking the admin checkbox at the time of invitation or by checking the admin checkbox then clicking the Update button next to a member email. Administrators can do all the actions described in the Organization permissions section of this documentation. They can also update the configuration of these permissions and set other users as administrators. Finally, administrators are automatically set as administrators on all the clusters of the organization that have Managed user enabled and are version 2.6.0 or above. Set a member as admin","title":"User management"},{"location":"setup_installation/common/user_management/#user-management","text":"In managed.hopsworks.ai users can be grouped into organizations to access the same resources. When a new user registers with managed.hopsworks.ai a new organization is created. This user later on can invite other registered users to their organization so they can share access to the same clusters. Cloud Accounts configuration is also shared among users of the same organization. So if user Alice has configured her account with her credentials, all member of her organization will automatically deploy clusters in her cloud account. Credits and cluster usage are also grouped to ease reporting.","title":"User management"},{"location":"setup_installation/common/user_management/#adding-members-to-an-organization","text":"Organization membership can be edited by clicking Members on the left of managed.hopsworks.ai Dashboard page. Organization membership To add a new member to your organization add the user's email and click Add . The invited user will receive an email with the invitation. You can set the user as administrator by checking the Admin checkbox. More details about organization administrators can be found [here].(#administrator-role) An invited user must accept the invitation to be part of the organization. An invitation will show up on the invitee's Dashboard. The invitee may have to close the Welcome splash screen to be able to see the invitation. In this example, Alice has invited Bob to her organization, but Bob hasn't accepted the invitation yet. Alice has sent the invitation Bob's dashboard","title":"Adding members to an organization"},{"location":"setup_installation/common/user_management/#sharing-resources","text":"Once Bob has accepted the invitation he does not have to configure his account, they share the same configuration. Also, he will be able to view the same Dashboard as Alice, so he can start, stop or terminate clusters in the organization. Alice's dashboard Bob's dashboard If Alice had existing clusters running and she had selected Managed user management during cluster creation, an account will be automatically created for Bob on these clusters.","title":"Sharing resources"},{"location":"setup_installation/common/user_management/#removing-members-from-an-organization","text":"To remove a member from your organization simply go to Members page and click the Remove button next to the user you want to remove. You will stop sharing any resource and the user will be blocked from any shared cluster. Delete organization member","title":"Removing members from an organization"},{"location":"setup_installation/common/user_management/#organization-permissions","text":"The owner and the administrators of an organization can set permissions at the organization level. For this got to the members tab, check the checkboxes in the Member permissions section and click on Update . The supported permissions are: Non admin members can invite new members to the organization . If this permission is enabled, any member of the organization will be able to invite other members to the organization. Note that only the owner and the administrators will be able to invite new members as administrators. If this permission is not enabled only the owner and the administrators can invite new members to the organization. Non admin members can create and terminate clusters . If this permission is enabled, any member of the organization will be able to create and terminate clusters. If it is not enabled, only the owner and the administrators will be able to create and terminate clusters. Non admin members can open clusters ports . If this permission is enabled, any member of the organization can open and close services ports on organization's clusters. If it is not enabled, only the organization owner and administrators will be able to do so. Modify permissions","title":"Organization permissions"},{"location":"setup_installation/common/user_management/#administrator-role","text":"Members of an organization can be set as administrators. This can be done by checking the admin checkbox at the time of invitation or by checking the admin checkbox then clicking the Update button next to a member email. Administrators can do all the actions described in the Organization permissions section of this documentation. They can also update the configuration of these permissions and set other users as administrators. Finally, administrators are automatically set as administrators on all the clusters of the organization that have Managed user enabled and are version 2.6.0 or above. Set a member as admin","title":"Administrator role"},{"location":"setup_installation/common/sso/ldap/","text":"Configure your Hopsworks cluster to use LDAP for user management. # If you want to use your organization's LDAP as an identity provider to manage users in your Hopsworks cluster this document will guide you through the necessary steps to configure managed.hopsworks.ai to use LDAP. The LDAP attributes below are used to configure JNDI resources in the Hopsworks server. The JNDI resource will communicate with your LDAP server to perform the authentication. Setup LDAP jndilookupname : should contain the LDAP domain. hopsworks.ldap.basedn : the LDAP domain, it should be the same as jndilookupname java.naming.provider.url : url of your LDAP server with port. java.naming.ldap.attributes.binary : is the binary unique identifier that will be used in subsequent logins to identify the user. java.naming.security.authentication : how to authenticate to the LDAP server. java.naming.security.principal : contains the username of the user that will be used to query LDAP. java.naming.security.credentials : contains the password of the user that will be used to query LDAP. java.naming.referral : whether to follow or ignore an alternate location in which an LDAP Request may be processed. After configuring LDAP and creating your cluster you can log into your Hopsworks cluster and edit the LDAP attributes to field names to match your server. By default all attributes to field names are set to the values in OpenLDAP . See Configure LDAP on how to edit the LDAP default configurations. Note A default admin user that can log in with username and password will be created for the user that is creating the cluster. This user can be removed after making sure users can log in using LDAP.","title":"LDAP"},{"location":"setup_installation/common/sso/ldap/#configure-your-hopsworks-cluster-to-use-ldap-for-user-management","text":"If you want to use your organization's LDAP as an identity provider to manage users in your Hopsworks cluster this document will guide you through the necessary steps to configure managed.hopsworks.ai to use LDAP. The LDAP attributes below are used to configure JNDI resources in the Hopsworks server. The JNDI resource will communicate with your LDAP server to perform the authentication. Setup LDAP jndilookupname : should contain the LDAP domain. hopsworks.ldap.basedn : the LDAP domain, it should be the same as jndilookupname java.naming.provider.url : url of your LDAP server with port. java.naming.ldap.attributes.binary : is the binary unique identifier that will be used in subsequent logins to identify the user. java.naming.security.authentication : how to authenticate to the LDAP server. java.naming.security.principal : contains the username of the user that will be used to query LDAP. java.naming.security.credentials : contains the password of the user that will be used to query LDAP. java.naming.referral : whether to follow or ignore an alternate location in which an LDAP Request may be processed. After configuring LDAP and creating your cluster you can log into your Hopsworks cluster and edit the LDAP attributes to field names to match your server. By default all attributes to field names are set to the values in OpenLDAP . See Configure LDAP on how to edit the LDAP default configurations. Note A default admin user that can log in with username and password will be created for the user that is creating the cluster. This user can be removed after making sure users can log in using LDAP.","title":"Configure your Hopsworks cluster to use LDAP for user management."},{"location":"setup_installation/common/sso/oauth/","text":"Configure your Hopsworks cluster to use OAuth2 for user management. # If you want to use your organization's OAuth 2.0 identity provider to manage users in your Hopsworks cluster this document will guide you through the necessary steps to register your identity provider in managed.hopsworks.ai . Before registering your identity provider in Hopsworks you need to create a client application in your identity provider and acquire a client id and a client secret . Examples on how to create a client using Okta and Azure Active Directory identity providers can be found here and here respectively. In the User management step of cluster creation ( AWS , Azure ) you can choose which user management system to use. Select OAuth2 (OpenId) from the dropdown and configure your identity provider. Setup OAuth Register your identity provider by setting the following fields: Create Administrator password user : if checked an administrator that can log in to the Hopsworks cluster, with email and password, will be created for the user creating the cluster. If Not checked a group mapping that maps at least one group in the identity provider to HOPS_ADMIN is required. ClientId : the client id generated when registering Hopsworks in your identity provider. Client Secret : the client secret generated when registering Hopsworks in your identity provider. Provider URI : is the base uri of the identity provider (URI should contain scheme http:// or https://). Provider Name : a unique name to identify the identity provider in your Hopsworks cluster. This name will be used in the login page as an alternative login method if Provider DisplayName is not set. Optionally you can also set: Provider DisplayName : the name to display for the alternative login method (if not set Provider Name will be used) Provider Logo URI : a logo URL to an image. The logo will be shown on the login page with the provider name. Code Challenge Method : if your identity provider requires a code challenge for authorization request check the code challenge check box. This will allow you to choose a code challenge method that can be either plain or S256. Group Mapping : will allow you to map groups in your identity provider to groups in hopsworks. You can choose to map all users to HOPS_USER or HOPS_ADMIN. Alternatively you can add mappings as in the example below. IT->HOPS_ADMIN;DATA_SCIENCE->HOPS_USER This will map users in the IT group in your identity provider to HOPS_ADMIN and users in the DATA_SCIENCE group to HOPS_USER. Verify Email : if checked only users with verified email address (in the identity provider) can log in to Hopsworks. Activate user : if not checked an administrator in Hopsworks needs to activate users before they can login. Need consent : if checked, users will be asked for consent when logging in for the first time. Disable registration : if unchecked users will have the possibility to create accounts in the Hopsworks cluster using user name and password instead of OAuth. Provider Metadata Endpoint Supported : if your provider defines a discovery mechanism, called OpenID Connect Discovery, where it publishes its metadata at a well-known URL, typically https://server.com/.well-known/openid-configuration you can check this and the metadata will be discovered by hopsworks. If your provider does not publish its metadata you need to supply these values manually. Setup Provider Authorization Endpoint : the authorization endpoint of your identity provider, typically https://server.com/oauth2/authorize End Session Endpoint : the logout endpoint of your identity provider, typically https://server.com/oauth2/logout Token Endpoint : the token endpoint of your identity provider, typically https://server.com/oauth2/token UserInfo Endpoint : the user info endpoint of your identity provider, typically https://server.com/oauth2/userinfo JWKS URI : the JSON Web Key Set endpoint of your identity provider, typically https://server.com/oauth2/keys After configuring OAuth2 you can click on Next to configure the rest of your cluster. You can also configure OAuth2 once you have created a Hopsworks cluster. For instructions on how to configure OAUth2 on Hopsworks see Authentication Methods .","title":"OAuth2"},{"location":"setup_installation/common/sso/oauth/#configure-your-hopsworks-cluster-to-use-oauth2-for-user-management","text":"If you want to use your organization's OAuth 2.0 identity provider to manage users in your Hopsworks cluster this document will guide you through the necessary steps to register your identity provider in managed.hopsworks.ai . Before registering your identity provider in Hopsworks you need to create a client application in your identity provider and acquire a client id and a client secret . Examples on how to create a client using Okta and Azure Active Directory identity providers can be found here and here respectively. In the User management step of cluster creation ( AWS , Azure ) you can choose which user management system to use. Select OAuth2 (OpenId) from the dropdown and configure your identity provider. Setup OAuth Register your identity provider by setting the following fields: Create Administrator password user : if checked an administrator that can log in to the Hopsworks cluster, with email and password, will be created for the user creating the cluster. If Not checked a group mapping that maps at least one group in the identity provider to HOPS_ADMIN is required. ClientId : the client id generated when registering Hopsworks in your identity provider. Client Secret : the client secret generated when registering Hopsworks in your identity provider. Provider URI : is the base uri of the identity provider (URI should contain scheme http:// or https://). Provider Name : a unique name to identify the identity provider in your Hopsworks cluster. This name will be used in the login page as an alternative login method if Provider DisplayName is not set. Optionally you can also set: Provider DisplayName : the name to display for the alternative login method (if not set Provider Name will be used) Provider Logo URI : a logo URL to an image. The logo will be shown on the login page with the provider name. Code Challenge Method : if your identity provider requires a code challenge for authorization request check the code challenge check box. This will allow you to choose a code challenge method that can be either plain or S256. Group Mapping : will allow you to map groups in your identity provider to groups in hopsworks. You can choose to map all users to HOPS_USER or HOPS_ADMIN. Alternatively you can add mappings as in the example below. IT->HOPS_ADMIN;DATA_SCIENCE->HOPS_USER This will map users in the IT group in your identity provider to HOPS_ADMIN and users in the DATA_SCIENCE group to HOPS_USER. Verify Email : if checked only users with verified email address (in the identity provider) can log in to Hopsworks. Activate user : if not checked an administrator in Hopsworks needs to activate users before they can login. Need consent : if checked, users will be asked for consent when logging in for the first time. Disable registration : if unchecked users will have the possibility to create accounts in the Hopsworks cluster using user name and password instead of OAuth. Provider Metadata Endpoint Supported : if your provider defines a discovery mechanism, called OpenID Connect Discovery, where it publishes its metadata at a well-known URL, typically https://server.com/.well-known/openid-configuration you can check this and the metadata will be discovered by hopsworks. If your provider does not publish its metadata you need to supply these values manually. Setup Provider Authorization Endpoint : the authorization endpoint of your identity provider, typically https://server.com/oauth2/authorize End Session Endpoint : the logout endpoint of your identity provider, typically https://server.com/oauth2/logout Token Endpoint : the token endpoint of your identity provider, typically https://server.com/oauth2/token UserInfo Endpoint : the user info endpoint of your identity provider, typically https://server.com/oauth2/userinfo JWKS URI : the JSON Web Key Set endpoint of your identity provider, typically https://server.com/oauth2/keys After configuring OAuth2 you can click on Next to configure the rest of your cluster. You can also configure OAuth2 once you have created a Hopsworks cluster. For instructions on how to configure OAUth2 on Hopsworks see Authentication Methods .","title":"Configure your Hopsworks cluster to use OAuth2 for user management."},{"location":"setup_installation/gcp/cluster_creation/","text":"Cluster creation in managed.hopsworks.ai (GCP) # This guide goes into detail for each of the steps of the cluster creation in managed.hopsworks.ai Step 1 starting to create a cluster # In managed.hopsworks.ai , select Create cluster : Create a Hopsworks cluster Step 2 setting the General information # Select the GCP Project (1) in which you want the cluster to run. Note If the Project does not appear in the drop-down, make sure that you properly Connected your GCP account for this project. Name your cluster (2). Choose the Region (3) and Zone (4) in which to deploy the cluster. Select the Instance type (5) and Local storage (6) size for the cluster Head node . Optional: Specify a customer-managed encryption key to be used for encryption of local storage. The key has to be specified using the format: projects/PROJECT_ID/locations/REGION/keyRings/KEY_RING/cryptoKeys/KEY . Note that your project needs to be configured to allow usage of the key. This can be achieved by executing the gcloud command below. Refer to the GCP documentation for more details: Protect resources by using Cloud KMS keys . gcloud projects add - iam - policy - binding KMS_PROJECT_ID \\ -- member serviceAccount : service - PROJECT_NUMBER @compute - system . iam . gserviceaccount . com \\ --role roles/cloudkms.cryptoKeyEncrypterDecrypter Enter the name of the bucket in which the Hopsworks cluster will store its data in Cloud Storage Bucket (8) Warning The bucket must be empty and must be in a region accessible from the region in which the cluster is deployed. General configuration Step 3 workers configuration # In this step, you configure the workers. There are two possible setups: static or autoscaling. In the static setup, the cluster has a fixed number of workers that you decide. You can then add and remove workers manually, for more details: documentation . In the autoscaling setup, you configure conditions to add and remove workers and the cluster will automatically add and remove workers depending on the demand, for more details: documentation . Static workers configuration # You can set the static configuration by selecting Disabled in the first drop-down (1). Then you select the number of workers you want to start the cluster with (2). And, select the Instance type (3) and Local storage size (4) for the worker nodes . Create a Hopsworks cluster, static workers configuration Autoscaling workers configuration # You can set the autoscaling configuration by selecting enabled in the first drop-down (1). You can configure: The instance type you want to use. The size of the instances' disk. The minimum number of workers. The maximum number of workers. The targeted number of standby workers. Setting some resources on standby ensures that there are always some free resources in your cluster. This ensures that requests for new resources are fulfilled promptly. You configure the standby by setting the number of workers you want to be on standby. For example, if you set a value of 0.5 the system will start a new worker every time the aggregated free cluster resources drop below 50% of a worker's resources. If you set this value to 0 new workers will only be started when a job or notebook requests the resources. The time to wait before removing unused resources. One often starts a new computation shortly after finishing the previous one. To avoid having to wait for workers to stop and start between each computation it is recommended to wait before shutting down workers. Here you set the amount of time in seconds resources need to be unused before they get removed from the system. Note The standby will not be taken into account if you set the minimum number of workers to 0 and no resources are used in the cluster. This ensures that the number of nodes can fall to 0 when no resources are used. The standby will start to take effect as soon as you start using resources. Create a Hopsworks cluster, autoscale workers configuration Step 4 select the Service Account # Hopsworks cluster store their data in a storage bucket. To let the cluster instances access the bucket we need to attach a Service Account to the virtual machines. In this step, you set which Service Account to use by entering its Email . This Service Account needs to have access right to the bucket you selected in Step 2 . For more details on how to create the Service Account and give it access to the bucket refer to Creating and configuring a storage Set the instance service account Step 5 set the backup retention policy # To backup the storage bucket data when taking a cluster backup we need to set a retention policy for the bucket. In this step, you choose the retention period in days. You can deactivate the retention policy by setting this value to 0 but this will block you from taking any backup of your cluster. Choose the backup retention policy Step 6 VPC and Subnet selection # You can select the VPC which will be used by the Hopsworks cluster. You can either select an existing VPC or let managed.hopsworks.ai create one for you. If you decide to use restricted managed.hopsworks.ai permissions (see restrictive-permissions for more details) you will need to select an existing VPC here. Select the vpc If you selected an existing VPC in the previous step, this step lets you select which subnet of this VPC to use. If you did not select an existing virtual network in the previous step managed.hopsworks.ai will create a subnet for you. You can choose the CIDR block this subnet will use. Select the Subnet to be used by your cluster and press Next . Select the subnet Step 7 User management selection # In this step, you can choose which user management system to use. You have three choices: Managed : managed.hopsworks.ai automatically adds and removes users from the Hopsworks cluster when you add and remove users from your organization (more details here ). OAuth2 : integrate the cluster with your organization's OAuth2 identity provider. See Use OAuth2 for user management for more detail. LDAP : integrate the cluster with your organization's LDAP/ActiveDirectory server. See Use LDAP for user management for more detail. Disabled : let you manage users manually from within Hopsworks. Choose user management type Step 8 Managed RonDB # Hopsworks uses RonDB as a database engine for its online Feature Store. By default database will run on its own VM. Premium users can scale-out database services to multiple VMs to handle increased workload. For details on how to configure RonDB check our guide here . Configure RonDB If you need to deploy a RonDB cluster instead of a single node please contact us . Step 9 add tags to your instances. # In this step, you can define tags that will be added to the cluster virtual machines. Add tags Step 10 add an init script to your instances. # In this step, you can enter an initialization script that will be run at startup on every instance. You can select whether this script will run before or after the VM configuration. Be cautious if you select to run it before the VM configuration as this might affect Cluster creation. Note this init script must be a bash script starting with #!/usr/bin/env bash Add initialization script Step 11 Review and create # Review all information and select Create : Review cluster information The cluster will start. This will take a few minutes: Booting Hopsworks cluster As soon as the cluster has started, you will be able to log in to your new Hopsworks cluster. You will also be able to stop, restart, or terminate the cluster. Running Hopsworks cluster","title":"Cluster Creation"},{"location":"setup_installation/gcp/cluster_creation/#cluster-creation-in-managedhopsworksai-gcp","text":"This guide goes into detail for each of the steps of the cluster creation in managed.hopsworks.ai","title":"Cluster creation in managed.hopsworks.ai (GCP)"},{"location":"setup_installation/gcp/cluster_creation/#step-1-starting-to-create-a-cluster","text":"In managed.hopsworks.ai , select Create cluster : Create a Hopsworks cluster","title":"Step 1 starting to create a cluster"},{"location":"setup_installation/gcp/cluster_creation/#step-2-setting-the-general-information","text":"Select the GCP Project (1) in which you want the cluster to run. Note If the Project does not appear in the drop-down, make sure that you properly Connected your GCP account for this project. Name your cluster (2). Choose the Region (3) and Zone (4) in which to deploy the cluster. Select the Instance type (5) and Local storage (6) size for the cluster Head node . Optional: Specify a customer-managed encryption key to be used for encryption of local storage. The key has to be specified using the format: projects/PROJECT_ID/locations/REGION/keyRings/KEY_RING/cryptoKeys/KEY . Note that your project needs to be configured to allow usage of the key. This can be achieved by executing the gcloud command below. Refer to the GCP documentation for more details: Protect resources by using Cloud KMS keys . gcloud projects add - iam - policy - binding KMS_PROJECT_ID \\ -- member serviceAccount : service - PROJECT_NUMBER @compute - system . iam . gserviceaccount . com \\ --role roles/cloudkms.cryptoKeyEncrypterDecrypter Enter the name of the bucket in which the Hopsworks cluster will store its data in Cloud Storage Bucket (8) Warning The bucket must be empty and must be in a region accessible from the region in which the cluster is deployed. General configuration","title":"Step 2 setting the General information"},{"location":"setup_installation/gcp/cluster_creation/#step-3-workers-configuration","text":"In this step, you configure the workers. There are two possible setups: static or autoscaling. In the static setup, the cluster has a fixed number of workers that you decide. You can then add and remove workers manually, for more details: documentation . In the autoscaling setup, you configure conditions to add and remove workers and the cluster will automatically add and remove workers depending on the demand, for more details: documentation .","title":"Step 3 workers configuration"},{"location":"setup_installation/gcp/cluster_creation/#static-workers-configuration","text":"You can set the static configuration by selecting Disabled in the first drop-down (1). Then you select the number of workers you want to start the cluster with (2). And, select the Instance type (3) and Local storage size (4) for the worker nodes . Create a Hopsworks cluster, static workers configuration","title":"Static workers configuration"},{"location":"setup_installation/gcp/cluster_creation/#autoscaling-workers-configuration","text":"You can set the autoscaling configuration by selecting enabled in the first drop-down (1). You can configure: The instance type you want to use. The size of the instances' disk. The minimum number of workers. The maximum number of workers. The targeted number of standby workers. Setting some resources on standby ensures that there are always some free resources in your cluster. This ensures that requests for new resources are fulfilled promptly. You configure the standby by setting the number of workers you want to be on standby. For example, if you set a value of 0.5 the system will start a new worker every time the aggregated free cluster resources drop below 50% of a worker's resources. If you set this value to 0 new workers will only be started when a job or notebook requests the resources. The time to wait before removing unused resources. One often starts a new computation shortly after finishing the previous one. To avoid having to wait for workers to stop and start between each computation it is recommended to wait before shutting down workers. Here you set the amount of time in seconds resources need to be unused before they get removed from the system. Note The standby will not be taken into account if you set the minimum number of workers to 0 and no resources are used in the cluster. This ensures that the number of nodes can fall to 0 when no resources are used. The standby will start to take effect as soon as you start using resources. Create a Hopsworks cluster, autoscale workers configuration","title":"Autoscaling workers configuration"},{"location":"setup_installation/gcp/cluster_creation/#step-4-select-the-service-account","text":"Hopsworks cluster store their data in a storage bucket. To let the cluster instances access the bucket we need to attach a Service Account to the virtual machines. In this step, you set which Service Account to use by entering its Email . This Service Account needs to have access right to the bucket you selected in Step 2 . For more details on how to create the Service Account and give it access to the bucket refer to Creating and configuring a storage Set the instance service account","title":"Step 4 select the Service Account"},{"location":"setup_installation/gcp/cluster_creation/#step-5-set-the-backup-retention-policy","text":"To backup the storage bucket data when taking a cluster backup we need to set a retention policy for the bucket. In this step, you choose the retention period in days. You can deactivate the retention policy by setting this value to 0 but this will block you from taking any backup of your cluster. Choose the backup retention policy","title":"Step 5 set the backup retention policy"},{"location":"setup_installation/gcp/cluster_creation/#step-6-vpc-and-subnet-selection","text":"You can select the VPC which will be used by the Hopsworks cluster. You can either select an existing VPC or let managed.hopsworks.ai create one for you. If you decide to use restricted managed.hopsworks.ai permissions (see restrictive-permissions for more details) you will need to select an existing VPC here. Select the vpc If you selected an existing VPC in the previous step, this step lets you select which subnet of this VPC to use. If you did not select an existing virtual network in the previous step managed.hopsworks.ai will create a subnet for you. You can choose the CIDR block this subnet will use. Select the Subnet to be used by your cluster and press Next . Select the subnet","title":"Step 6 VPC and Subnet selection"},{"location":"setup_installation/gcp/cluster_creation/#step-7-user-management-selection","text":"In this step, you can choose which user management system to use. You have three choices: Managed : managed.hopsworks.ai automatically adds and removes users from the Hopsworks cluster when you add and remove users from your organization (more details here ). OAuth2 : integrate the cluster with your organization's OAuth2 identity provider. See Use OAuth2 for user management for more detail. LDAP : integrate the cluster with your organization's LDAP/ActiveDirectory server. See Use LDAP for user management for more detail. Disabled : let you manage users manually from within Hopsworks. Choose user management type","title":"Step 7 User management selection"},{"location":"setup_installation/gcp/cluster_creation/#step-8-managed-rondb","text":"Hopsworks uses RonDB as a database engine for its online Feature Store. By default database will run on its own VM. Premium users can scale-out database services to multiple VMs to handle increased workload. For details on how to configure RonDB check our guide here . Configure RonDB If you need to deploy a RonDB cluster instead of a single node please contact us .","title":"Step 8 Managed RonDB"},{"location":"setup_installation/gcp/cluster_creation/#step-9-add-tags-to-your-instances","text":"In this step, you can define tags that will be added to the cluster virtual machines. Add tags","title":"Step 9 add tags to your instances."},{"location":"setup_installation/gcp/cluster_creation/#step-10-add-an-init-script-to-your-instances","text":"In this step, you can enter an initialization script that will be run at startup on every instance. You can select whether this script will run before or after the VM configuration. Be cautious if you select to run it before the VM configuration as this might affect Cluster creation. Note this init script must be a bash script starting with #!/usr/bin/env bash Add initialization script","title":"Step 10 add an init script to your instances."},{"location":"setup_installation/gcp/cluster_creation/#step-11-review-and-create","text":"Review all information and select Create : Review cluster information The cluster will start. This will take a few minutes: Booting Hopsworks cluster As soon as the cluster has started, you will be able to log in to your new Hopsworks cluster. You will also be able to stop, restart, or terminate the cluster. Running Hopsworks cluster","title":"Step 11 Review and create"},{"location":"setup_installation/gcp/getting_started/","text":"Getting started with managed.hopsworks.ai (Google Cloud Platform) # Managed.hopsworks.ai is our managed platform for running Hopsworks and the Feature Store in the cloud. It integrates seamlessly with third-party platforms such as Databricks, SageMaker and KubeFlow. This guide shows how to set up managed.hopsworks.ai with your organization's Google Cloud Platform's (GCP) account. Prerequisites # To follow the instruction of this page you will need the following: A GCP project in which the Hopsworks cluster will be deployed. The gcloud CLI The gsutil tool To run all the commands on this page the user needs to have at least the following permissions on the GCP project: iam.roles.create iam.roles.list iam.serviceAccountKeys.create iam.serviceAccounts.create resourcemanager.projects.getIamPolicy resourcemanager.projects.setIamPolicy serviceusage.services.enable storage.buckets.create Make sure to enable Compute Engine API , Cloud Resource Manager API , and Identity and Access Management (IAM) API on the GCP project. This can be done by running the following commands. Replacing $PROJECT_ID with the id of your GCP project. gcloud --project = $PROJECT_ID services enable compute.googleapis.com gcloud --project = $PROJECT_ID services enable cloudresourcemanager.googleapis.com gcloud --project = $PROJECT_ID services enable iam.googleapis.com You can find more information about GCP cloud APIs in the GCP documentation . Step 1: Connecting your GCP account # Managed.hopsworks.ai deploys Hopsworks clusters to a project in your GCP account. Managed.hopsworks.ai uses service account keys to connect to your GCP project. To enable this, you need to create a service account in your GCP project. Assign to the service account the required permissions. And, create a service account key JSON. For more details about creating and managing service accounts steps in GCP, see documentation . In managed.hopsworks.ai click on Connect to GCP or go to Settings and click on Configure next to GCP . This will direct you to a page with the instructions needed to create the service account and set up the connection. Follow the instructions. Note it is possible to limit the permissions that step up during this phase. For more details see restrictive-permissions . GCP configuration page Step 2: Creating storage # The Hopsworks clusters deployed by managed.hopsworks.ai store their data in a bucket in your GCP account. This bucket needs to be created before creating the Hopsworks cluster. Execute the following gsutil command to create a bucket. Replace all occurrences $PROJECT_ID with your GCP project id and $BUCKET_NAME with the name you want to give to your bucket. You can also replace US with another location if you are not going to run your cluster in this *Multi-Region (see note below for more details). gsutil mb -p $PROJECT_ID -l US gs://$BUCKET_NAME Note The Hopsworks cluster created by managed.hopsworks.ai must be in the same region as the bucket. The above command will create the bucket in the US so in the following steps, you must deploy your cluster in a US region. If you want to deploy your cluster in another part of the world us the -l option of gsutil mb . For more details about creating buckets with gsutil, see the google documentation Step 3: Creating a service account for your cluster instances # The cluster instances will need to be granted permission to access the storage bucket. You achieve this by creating a service account that will later be attached to the Hopsworks cluster instances. This service account should be different from the service account created in step 1, as it has only those permissions related to storing objects in a GCP bucket. Step 3.1: Creating a custom role for accessing storage # Create a file named hopsworksai_instances_role.yaml with the following content: title : Hopsworks AI Instances description : Role that allows Hopsworks AI Instances to access resources stage : GA includedPermissions : - storage.buckets.get - storage.buckets.update - storage.multipartUploads.abort - storage.multipartUploads.create - storage.multipartUploads.list - storage.multipartUploads.listParts - storage.objects.create - storage.objects.delete - storage.objects.get - storage.objects.list - storage.objects.update Note it is possible to limit the permissions that set up during this phase. For more details see restrictive-permissions . Execute the following gcloud command to create a custom role from the file. Replace $PROJECT_ID with your GCP project id: gcloud iam roles create hopsworksai_instances \\ --project=$PROJECT_ID \\ --file=hopsworksai_instances_role.yaml Step 3.2: Creating a service account # Execute the following gcloud command to create a service account for Hopsworks AI instances. Replace $PROJECT_ID with your GCP project id: gcloud iam service-accounts create hopsworks-ai-instances \\ --project=$PROJECT_ID \\ --description=\"Service account for Hopsworks AI instances\" \\ --display-name=\"Hopsworks AI instances\" Execute the following gcloud command to bind the custom role to the service account. Replace all occurrences $PROJECT_ID with your GCP project id: gcloud projects add-iam-policy-binding $PROJECT_ID \\ --member=\"serviceAccount:hopsworks-ai-instances@$PROJECT_ID.iam.gserviceaccount.com\" \\ --role=\"projects/$PROJECT_ID/roles/hopsworksai_instances\" Step 4: Deploying a Hopsworks cluster # In managed.hopsworks.ai , select Create cluster : Create a Hopsworks cluster Select the Project (1) in which you created your Bucket and Service Account (see above). Note If the Project does not appear in the drop-down, make sure that you properly Connected your GCP account for this project. Name your cluster (2). Choose the Region (3) and Zone (4) in which to deploy the cluster. Warning The cluster must be deployed in a region having access to the bucket you created above. Select the Instance type (5) and Local storage (6) size for the cluster Head node . Enter the name of the bucket you created above in Cloud Storage Bucket (7) Press Next : General configuration Select the number of workers you want to start the cluster with (2). Select the Instance type (3) and Local storage size (4) for the worker nodes . Note It is possible to add or remove workers or to enable autoscaling once the cluster is running. Press Next : Create a Hopsworks cluster, static workers configuration Enter Email of the instances service account that you created above . If you followed the instruction it should be hopsworks-ai-instances@$PROJECT_ID.iam.gserviceaccount.com with $PROJECT_ID the name of your project: Set the instance service account To backup the storage bucket data when taking a cluster backup we need to set a retention policy for the bucket. You can deactivate the retention policy by setting this value to 0 but this will block you from taking any backup of your cluster. Choose the retention period in days and click on Review and submit . Choose the backup retention policy Review all information and select Create : Review cluster information Note We skipped cluster creation steps that are not mandatory. The cluster will start. This will take a few minutes: Booting Hopsworks cluster As soon as the cluster has started, you will be able to log in to your new Hopsworks cluster. You will also be able to stop, restart or terminate the cluster. Running Hopsworks cluster Step 5: Next steps # Check out our other guides for how to get started with Hopsworks and the Feature Store: Make Hopsworks services accessible from outside services Get started with the Hopsworks Feature Store Follow one of our tutorials Follow one of our Guide Code examples and notebooks: hops-examples","title":"Getting Started"},{"location":"setup_installation/gcp/getting_started/#getting-started-with-managedhopsworksai-google-cloud-platform","text":"Managed.hopsworks.ai is our managed platform for running Hopsworks and the Feature Store in the cloud. It integrates seamlessly with third-party platforms such as Databricks, SageMaker and KubeFlow. This guide shows how to set up managed.hopsworks.ai with your organization's Google Cloud Platform's (GCP) account.","title":"Getting started with managed.hopsworks.ai (Google Cloud Platform)"},{"location":"setup_installation/gcp/getting_started/#prerequisites","text":"To follow the instruction of this page you will need the following: A GCP project in which the Hopsworks cluster will be deployed. The gcloud CLI The gsutil tool To run all the commands on this page the user needs to have at least the following permissions on the GCP project: iam.roles.create iam.roles.list iam.serviceAccountKeys.create iam.serviceAccounts.create resourcemanager.projects.getIamPolicy resourcemanager.projects.setIamPolicy serviceusage.services.enable storage.buckets.create Make sure to enable Compute Engine API , Cloud Resource Manager API , and Identity and Access Management (IAM) API on the GCP project. This can be done by running the following commands. Replacing $PROJECT_ID with the id of your GCP project. gcloud --project = $PROJECT_ID services enable compute.googleapis.com gcloud --project = $PROJECT_ID services enable cloudresourcemanager.googleapis.com gcloud --project = $PROJECT_ID services enable iam.googleapis.com You can find more information about GCP cloud APIs in the GCP documentation .","title":"Prerequisites"},{"location":"setup_installation/gcp/getting_started/#step-1-connecting-your-gcp-account","text":"Managed.hopsworks.ai deploys Hopsworks clusters to a project in your GCP account. Managed.hopsworks.ai uses service account keys to connect to your GCP project. To enable this, you need to create a service account in your GCP project. Assign to the service account the required permissions. And, create a service account key JSON. For more details about creating and managing service accounts steps in GCP, see documentation . In managed.hopsworks.ai click on Connect to GCP or go to Settings and click on Configure next to GCP . This will direct you to a page with the instructions needed to create the service account and set up the connection. Follow the instructions. Note it is possible to limit the permissions that step up during this phase. For more details see restrictive-permissions . GCP configuration page","title":"Step 1: Connecting your GCP account"},{"location":"setup_installation/gcp/getting_started/#step-2-creating-storage","text":"The Hopsworks clusters deployed by managed.hopsworks.ai store their data in a bucket in your GCP account. This bucket needs to be created before creating the Hopsworks cluster. Execute the following gsutil command to create a bucket. Replace all occurrences $PROJECT_ID with your GCP project id and $BUCKET_NAME with the name you want to give to your bucket. You can also replace US with another location if you are not going to run your cluster in this *Multi-Region (see note below for more details). gsutil mb -p $PROJECT_ID -l US gs://$BUCKET_NAME Note The Hopsworks cluster created by managed.hopsworks.ai must be in the same region as the bucket. The above command will create the bucket in the US so in the following steps, you must deploy your cluster in a US region. If you want to deploy your cluster in another part of the world us the -l option of gsutil mb . For more details about creating buckets with gsutil, see the google documentation","title":"Step 2: Creating storage"},{"location":"setup_installation/gcp/getting_started/#step-3-creating-a-service-account-for-your-cluster-instances","text":"The cluster instances will need to be granted permission to access the storage bucket. You achieve this by creating a service account that will later be attached to the Hopsworks cluster instances. This service account should be different from the service account created in step 1, as it has only those permissions related to storing objects in a GCP bucket.","title":"Step 3: Creating a service account for your cluster instances"},{"location":"setup_installation/gcp/getting_started/#step-31-creating-a-custom-role-for-accessing-storage","text":"Create a file named hopsworksai_instances_role.yaml with the following content: title : Hopsworks AI Instances description : Role that allows Hopsworks AI Instances to access resources stage : GA includedPermissions : - storage.buckets.get - storage.buckets.update - storage.multipartUploads.abort - storage.multipartUploads.create - storage.multipartUploads.list - storage.multipartUploads.listParts - storage.objects.create - storage.objects.delete - storage.objects.get - storage.objects.list - storage.objects.update Note it is possible to limit the permissions that set up during this phase. For more details see restrictive-permissions . Execute the following gcloud command to create a custom role from the file. Replace $PROJECT_ID with your GCP project id: gcloud iam roles create hopsworksai_instances \\ --project=$PROJECT_ID \\ --file=hopsworksai_instances_role.yaml","title":"Step 3.1: Creating a custom role for accessing storage"},{"location":"setup_installation/gcp/getting_started/#step-32-creating-a-service-account","text":"Execute the following gcloud command to create a service account for Hopsworks AI instances. Replace $PROJECT_ID with your GCP project id: gcloud iam service-accounts create hopsworks-ai-instances \\ --project=$PROJECT_ID \\ --description=\"Service account for Hopsworks AI instances\" \\ --display-name=\"Hopsworks AI instances\" Execute the following gcloud command to bind the custom role to the service account. Replace all occurrences $PROJECT_ID with your GCP project id: gcloud projects add-iam-policy-binding $PROJECT_ID \\ --member=\"serviceAccount:hopsworks-ai-instances@$PROJECT_ID.iam.gserviceaccount.com\" \\ --role=\"projects/$PROJECT_ID/roles/hopsworksai_instances\"","title":"Step 3.2: Creating a service account"},{"location":"setup_installation/gcp/getting_started/#step-4-deploying-a-hopsworks-cluster","text":"In managed.hopsworks.ai , select Create cluster : Create a Hopsworks cluster Select the Project (1) in which you created your Bucket and Service Account (see above). Note If the Project does not appear in the drop-down, make sure that you properly Connected your GCP account for this project. Name your cluster (2). Choose the Region (3) and Zone (4) in which to deploy the cluster. Warning The cluster must be deployed in a region having access to the bucket you created above. Select the Instance type (5) and Local storage (6) size for the cluster Head node . Enter the name of the bucket you created above in Cloud Storage Bucket (7) Press Next : General configuration Select the number of workers you want to start the cluster with (2). Select the Instance type (3) and Local storage size (4) for the worker nodes . Note It is possible to add or remove workers or to enable autoscaling once the cluster is running. Press Next : Create a Hopsworks cluster, static workers configuration Enter Email of the instances service account that you created above . If you followed the instruction it should be hopsworks-ai-instances@$PROJECT_ID.iam.gserviceaccount.com with $PROJECT_ID the name of your project: Set the instance service account To backup the storage bucket data when taking a cluster backup we need to set a retention policy for the bucket. You can deactivate the retention policy by setting this value to 0 but this will block you from taking any backup of your cluster. Choose the retention period in days and click on Review and submit . Choose the backup retention policy Review all information and select Create : Review cluster information Note We skipped cluster creation steps that are not mandatory. The cluster will start. This will take a few minutes: Booting Hopsworks cluster As soon as the cluster has started, you will be able to log in to your new Hopsworks cluster. You will also be able to stop, restart or terminate the cluster. Running Hopsworks cluster","title":"Step 4: Deploying a Hopsworks cluster"},{"location":"setup_installation/gcp/getting_started/#step-5-next-steps","text":"Check out our other guides for how to get started with Hopsworks and the Feature Store: Make Hopsworks services accessible from outside services Get started with the Hopsworks Feature Store Follow one of our tutorials Follow one of our Guide Code examples and notebooks: hops-examples","title":"Step 5: Next steps"},{"location":"setup_installation/gcp/restrictive_permissions/","text":"Limiting GCP permissions # Managed.hopsworks.ai requires a set of permissions to be able to manage resources in the user\u2019s GCP project. By default, these permissions are set to easily allow a wide range of different configurations and allow us to automate as many steps as possible. While we ensure to never access resources we shouldn\u2019t, we do understand that this might not be enough for your organization or security policy. This guide explains how to lock down access permissions following the IT security policy principle of least privilege. Default permissions # This is the list of default permissions that are required by managed.hopsworks.ai . If you prefer to limit these permissions, then proceed to the next section . title : Hopsworks AI description : Role that allows Hopsworks AI to manage resources stage : GA includedPermissions : - compute.disks.create - compute.disks.createSnapshot - compute.snapshots.create - compute.snapshots.delete - compute.snapshots.setLabels - compute.snapshots.get - compute.snapshots.useReadOnly - compute.firewalls.create - compute.firewalls.delete - compute.firewalls.get - compute.firewalls.list - compute.firewalls.update - compute.globalOperations.get - compute.instances.create - compute.instances.delete - compute.instances.get - compute.instances.setLabels - compute.instances.setMetadata - compute.instances.setServiceAccount - compute.instances.start - compute.instances.stop - compute.instances.setMachineType - compute.networks.create - compute.networks.delete - compute.networks.get - compute.networks.list - compute.networks.updatePolicy - compute.projects.get - compute.subnetworks.create - compute.subnetworks.delete - compute.subnetworks.get - compute.subnetworks.list - compute.subnetworks.use - compute.subnetworks.useExternalIp - compute.zoneOperations.get - iam.serviceAccounts.actAs - iam.serviceAccounts.list - resourcemanager.projects.get - iam.roles.get - resourcemanager.projects.getIamPolicy Limiting the Account Service Account permissions # Some of the permissions set up when connection your GCP account to managed.hopsworks.ai ( here ) can be removed under certain conditions. Backup permissions # The following permissions are only needed for the backup feature. If you are not going to create backups or if you do not have access to this Enterprise feature, you can limit the permission of the Service Account by removing them. - compute.disks.createSnapshot - compute.snapshots.create - compute.snapshots.delete - compute.snapshots.setLabels - compute.snapshots.get - compute.snapshots.useReadOnly Instance type modification permissions # The following permission is only needed to be able to change the head node and RonDB nodes instance type on an existing cluster ( documentation ). If you are not going to use this feature, you can limit the permission of the Service Account by removing it. - compute.instances.setMachineType Instance Service account check # The following permissions are only needed to check that the service account you select for your cluster has the proper permissions. If you remove them the check will be skipped. This may result in your cluster taking more time to detect the error if the service account does not have the proper permissions - iam.roles.get - resourcemanager.projects.getIamPolicy Create a VPC permissions # The following permissions are only needed if you want managed.hopsworks.ai to create VPC and subnet for you. - compute.networks.create - compute.networks.delete - compute.networks.get - compute.subnetworks.create - compute.subnetworks.delete - compute.subnetworks.get - compute.firewalls.create - compute.firewalls.delete You can remove these permissions by creating your own VPC, subnet, and firewalls and selecting them during cluster creation. For the VPC to be accepted you will need to associate it with firewall rules with the following constraints: One firewall rule associated with the VPC must allow ingress communication on all ports for communication between sources with the service account you will attach to the cluster nodes (source and target). To create such a rule, run the following command replacing $NETWORK with the name of your VPC, $SERVICE_ACCOUNT with the email of your service account, and $PROJECT_ID with the id of your project: gcloud compute firewall-rules create nodetonode --network = $NETWORK --allow = all --direction = INGRESS --target-service-accounts = $SERVICE_ACCOUNT --source-service-accounts = $SERVICE_ACCOUNT --project = $PROJECT_ID We recommend that you have a firewall rule associated with the VPC allowing TCP ingress communication to ports 443 and 80. If you don't have a rule opening port 443 the cluster will not be accessible from the internet. If you don't have a rule opening port 80 your cluster will be created with a self-signed certificate. You will have to acknowledge it by checking the Continue with self-signed certificate check box during subnet selection . Depending on your firewall rules setup at cluster creation managed.hopsworks.ai may not be able to manage services port at run time. In such a case you will have to open and close the ports yourself in the cluster provider. To create this rule, run the following command replacing $NETWORK with the name of your VPC, $SERVICE_ACCOUNT with the email of your service account and $PROJECT_ID with the id of your project: gcloud compute firewall-rules create inbound --network = $NETWORK --allow = all --direction = INGRESS --target-service-accounts = $SERVICE_ACCOUNT --allow = tcp:80,tcp:443 --source-ranges = \"0.0.0.0/0\" --project = $PROJECT_ID Update Firewall # The following permission is only needed to open and close service ports on the cluster. If you are not intending to open and close these ports from managed.hopsworks.ai you can remove the permission. - compute.firewalls.update Limiting the Instances Service Account permissions # Some of the permissions set up for the instances service account used during cluster creation ( here ) can be removed under certain conditions. Backups # If you do not intend to take backups or if you do not have access to this Enterprise feature you can remove the permissions that are only used by the backup feature when configuring your instances service account. For this remove the following permission from your instances service account : storage.buckets.update","title":"Limiting Permissions"},{"location":"setup_installation/gcp/restrictive_permissions/#limiting-gcp-permissions","text":"Managed.hopsworks.ai requires a set of permissions to be able to manage resources in the user\u2019s GCP project. By default, these permissions are set to easily allow a wide range of different configurations and allow us to automate as many steps as possible. While we ensure to never access resources we shouldn\u2019t, we do understand that this might not be enough for your organization or security policy. This guide explains how to lock down access permissions following the IT security policy principle of least privilege.","title":"Limiting GCP permissions"},{"location":"setup_installation/gcp/restrictive_permissions/#default-permissions","text":"This is the list of default permissions that are required by managed.hopsworks.ai . If you prefer to limit these permissions, then proceed to the next section . title : Hopsworks AI description : Role that allows Hopsworks AI to manage resources stage : GA includedPermissions : - compute.disks.create - compute.disks.createSnapshot - compute.snapshots.create - compute.snapshots.delete - compute.snapshots.setLabels - compute.snapshots.get - compute.snapshots.useReadOnly - compute.firewalls.create - compute.firewalls.delete - compute.firewalls.get - compute.firewalls.list - compute.firewalls.update - compute.globalOperations.get - compute.instances.create - compute.instances.delete - compute.instances.get - compute.instances.setLabels - compute.instances.setMetadata - compute.instances.setServiceAccount - compute.instances.start - compute.instances.stop - compute.instances.setMachineType - compute.networks.create - compute.networks.delete - compute.networks.get - compute.networks.list - compute.networks.updatePolicy - compute.projects.get - compute.subnetworks.create - compute.subnetworks.delete - compute.subnetworks.get - compute.subnetworks.list - compute.subnetworks.use - compute.subnetworks.useExternalIp - compute.zoneOperations.get - iam.serviceAccounts.actAs - iam.serviceAccounts.list - resourcemanager.projects.get - iam.roles.get - resourcemanager.projects.getIamPolicy","title":"Default permissions"},{"location":"setup_installation/gcp/restrictive_permissions/#limiting-the-account-service-account-permissions","text":"Some of the permissions set up when connection your GCP account to managed.hopsworks.ai ( here ) can be removed under certain conditions.","title":"Limiting the Account Service Account permissions"},{"location":"setup_installation/gcp/restrictive_permissions/#backup-permissions","text":"The following permissions are only needed for the backup feature. If you are not going to create backups or if you do not have access to this Enterprise feature, you can limit the permission of the Service Account by removing them. - compute.disks.createSnapshot - compute.snapshots.create - compute.snapshots.delete - compute.snapshots.setLabels - compute.snapshots.get - compute.snapshots.useReadOnly","title":"Backup permissions"},{"location":"setup_installation/gcp/restrictive_permissions/#instance-type-modification-permissions","text":"The following permission is only needed to be able to change the head node and RonDB nodes instance type on an existing cluster ( documentation ). If you are not going to use this feature, you can limit the permission of the Service Account by removing it. - compute.instances.setMachineType","title":"Instance type modification permissions"},{"location":"setup_installation/gcp/restrictive_permissions/#instance-service-account-check","text":"The following permissions are only needed to check that the service account you select for your cluster has the proper permissions. If you remove them the check will be skipped. This may result in your cluster taking more time to detect the error if the service account does not have the proper permissions - iam.roles.get - resourcemanager.projects.getIamPolicy","title":"Instance Service account check"},{"location":"setup_installation/gcp/restrictive_permissions/#create-a-vpc-permissions","text":"The following permissions are only needed if you want managed.hopsworks.ai to create VPC and subnet for you. - compute.networks.create - compute.networks.delete - compute.networks.get - compute.subnetworks.create - compute.subnetworks.delete - compute.subnetworks.get - compute.firewalls.create - compute.firewalls.delete You can remove these permissions by creating your own VPC, subnet, and firewalls and selecting them during cluster creation. For the VPC to be accepted you will need to associate it with firewall rules with the following constraints: One firewall rule associated with the VPC must allow ingress communication on all ports for communication between sources with the service account you will attach to the cluster nodes (source and target). To create such a rule, run the following command replacing $NETWORK with the name of your VPC, $SERVICE_ACCOUNT with the email of your service account, and $PROJECT_ID with the id of your project: gcloud compute firewall-rules create nodetonode --network = $NETWORK --allow = all --direction = INGRESS --target-service-accounts = $SERVICE_ACCOUNT --source-service-accounts = $SERVICE_ACCOUNT --project = $PROJECT_ID We recommend that you have a firewall rule associated with the VPC allowing TCP ingress communication to ports 443 and 80. If you don't have a rule opening port 443 the cluster will not be accessible from the internet. If you don't have a rule opening port 80 your cluster will be created with a self-signed certificate. You will have to acknowledge it by checking the Continue with self-signed certificate check box during subnet selection . Depending on your firewall rules setup at cluster creation managed.hopsworks.ai may not be able to manage services port at run time. In such a case you will have to open and close the ports yourself in the cluster provider. To create this rule, run the following command replacing $NETWORK with the name of your VPC, $SERVICE_ACCOUNT with the email of your service account and $PROJECT_ID with the id of your project: gcloud compute firewall-rules create inbound --network = $NETWORK --allow = all --direction = INGRESS --target-service-accounts = $SERVICE_ACCOUNT --allow = tcp:80,tcp:443 --source-ranges = \"0.0.0.0/0\" --project = $PROJECT_ID","title":"Create a VPC permissions"},{"location":"setup_installation/gcp/restrictive_permissions/#update-firewall","text":"The following permission is only needed to open and close service ports on the cluster. If you are not intending to open and close these ports from managed.hopsworks.ai you can remove the permission. - compute.firewalls.update","title":"Update Firewall"},{"location":"setup_installation/gcp/restrictive_permissions/#limiting-the-instances-service-account-permissions","text":"Some of the permissions set up for the instances service account used during cluster creation ( here ) can be removed under certain conditions.","title":"Limiting the Instances Service Account permissions"},{"location":"setup_installation/gcp/restrictive_permissions/#backups","text":"If you do not intend to take backups or if you do not have access to this Enterprise feature you can remove the permissions that are only used by the backup feature when configuring your instances service account. For this remove the following permission from your instances service account : storage.buckets.update","title":"Backups"},{"location":"setup_installation/on_prem/external_kafka_cluster/","text":"External Kafka cluster # Hopsworks uses Apache Kafka to ingest data to the feature store. Streaming applications and external clients send data to the Kafka cluster for ingestion to the online and offline feature store. By default, Hopsworks comes with an embedded Kafka cluster managed by Hopsworks itself, however, users can configure Hopsworks to leverage an existing external cluster. This guide will cover how to configure an Hopsworks cluster to leverage an external Kafka cluster. Configure the external Kafka cluster integration # To enable the integration with an external Kafka cluster, you should set the enable_bring_your_own_kafka configuration option to true . This can also be achieved in the cluster definition by setting the following attribute: hopsworks: enable_bring_your_own_kafka: \"true\" Online Feature Store service configuration # In addition to the configuration changes above, you should also configure the Online Feature Store service (OnlineFS in short) to connect to the external Kafka cluster. This can be achieved by provisioning the necessary credentials for OnlineFS to subscribe and consume messages from Kafka topics used by the Hopsworks feature store. OnlineFs can be configured to use these credentials by adding the following configurations to the cluster definition used to deploy Hopsworks: onlinefs: config_dir: \"/home/ubuntu/cluster-definitions/byok\" kafka_cosumers: topic_list: \"comma separated list of kafka topics to subscribe to\" In particular, the onlinefs/config_dir should contain the credentials necessary for the Kafka consumers to authenticate. Additionally the directory should contain a file name onlinefs-kafka.properties with the Kafka consumer configuration. The following is an example of the onlinefs-kafka.properties file: bootstrap.servers=cluster_identifier.us-east-2.aws.confluent.cloud:9092 security.protocol=SASL_SSL sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"username\" password=\"password\"; sasl.mechanism=PLAIN Hopsworks will not provision topics Please note that when using an external Kafka cluster, Hopsworks will not provision the topics for the different projects. Users are responsible for provisioning the necessary topics and configure the projects accordingly (see next section). Users should also specify the list of topics OnlineFS should subscribe to by providing the onlinefs/kafka_consumers/topic_list option in the cluster definition. Project configuration # Topic configuration # As mentioned above, when configuring Hopsworks to use an external Kafka cluster, Hopsworks will not provision the topics for the different projects. Instead, when creating a project, users will be aksed to provide the topic name to use for the feature store operations. Example project creation when using an external Kafka cluster Storage connector configuration # Users should create a Kafka storage connector named kafka_connector which is going to be used by the feature store clients to configure the necessary Kafka producers to send data. The configuration is done for each project to ensure its members have the necessary authentication/authorization. If the storage connector is not found in the project, default values referring to Hopsworks managed Kafka will be used.","title":"External Kafka cluster"},{"location":"setup_installation/on_prem/external_kafka_cluster/#external-kafka-cluster","text":"Hopsworks uses Apache Kafka to ingest data to the feature store. Streaming applications and external clients send data to the Kafka cluster for ingestion to the online and offline feature store. By default, Hopsworks comes with an embedded Kafka cluster managed by Hopsworks itself, however, users can configure Hopsworks to leverage an existing external cluster. This guide will cover how to configure an Hopsworks cluster to leverage an external Kafka cluster.","title":"External Kafka cluster"},{"location":"setup_installation/on_prem/external_kafka_cluster/#configure-the-external-kafka-cluster-integration","text":"To enable the integration with an external Kafka cluster, you should set the enable_bring_your_own_kafka configuration option to true . This can also be achieved in the cluster definition by setting the following attribute: hopsworks: enable_bring_your_own_kafka: \"true\"","title":"Configure the external Kafka cluster integration"},{"location":"setup_installation/on_prem/external_kafka_cluster/#online-feature-store-service-configuration","text":"In addition to the configuration changes above, you should also configure the Online Feature Store service (OnlineFS in short) to connect to the external Kafka cluster. This can be achieved by provisioning the necessary credentials for OnlineFS to subscribe and consume messages from Kafka topics used by the Hopsworks feature store. OnlineFs can be configured to use these credentials by adding the following configurations to the cluster definition used to deploy Hopsworks: onlinefs: config_dir: \"/home/ubuntu/cluster-definitions/byok\" kafka_cosumers: topic_list: \"comma separated list of kafka topics to subscribe to\" In particular, the onlinefs/config_dir should contain the credentials necessary for the Kafka consumers to authenticate. Additionally the directory should contain a file name onlinefs-kafka.properties with the Kafka consumer configuration. The following is an example of the onlinefs-kafka.properties file: bootstrap.servers=cluster_identifier.us-east-2.aws.confluent.cloud:9092 security.protocol=SASL_SSL sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"username\" password=\"password\"; sasl.mechanism=PLAIN Hopsworks will not provision topics Please note that when using an external Kafka cluster, Hopsworks will not provision the topics for the different projects. Users are responsible for provisioning the necessary topics and configure the projects accordingly (see next section). Users should also specify the list of topics OnlineFS should subscribe to by providing the onlinefs/kafka_consumers/topic_list option in the cluster definition.","title":"Online Feature Store service configuration"},{"location":"setup_installation/on_prem/external_kafka_cluster/#project-configuration","text":"","title":"Project configuration"},{"location":"setup_installation/on_prem/external_kafka_cluster/#topic-configuration","text":"As mentioned above, when configuring Hopsworks to use an external Kafka cluster, Hopsworks will not provision the topics for the different projects. Instead, when creating a project, users will be aksed to provide the topic name to use for the feature store operations. Example project creation when using an external Kafka cluster","title":"Topic configuration"},{"location":"setup_installation/on_prem/external_kafka_cluster/#storage-connector-configuration","text":"Users should create a Kafka storage connector named kafka_connector which is going to be used by the feature store clients to configure the necessary Kafka producers to send data. The configuration is done for each project to ensure its members have the necessary authentication/authorization. If the storage connector is not found in the project, default values referring to Hopsworks managed Kafka will be used.","title":"Storage connector configuration"},{"location":"setup_installation/on_prem/hopsworks_installer/","text":"Hopsworks On premises # It is possible to use Hopsworks on-premises, which means that companies can run their machine learning workloads on their own hardware and infrastructure, rather than relying on a cloud provider. This can provide greater flexibility, control, and cost savings, as well as enabling companies to meet specific compliance and security requirements. Working on-premises with Hopsworks typically involves collaboration with the Hopsworks engineering teams, as each infrastructure is unique and requires a tailored approach to deployment and configuration. The process begins with an assessment of the company's existing infrastructure and requirements, including network topology, security policies, and hardware specifications. For further details about on-premise installations; contact us . Minimum Requirements # You need at least one server or virtual machine on which Hopsworks will be installed with at least the following specification: Ubuntu 22.04, Centos or RHEL Linux (Version 8) supported, at least 32GB RAM, at least 8 CPUs, 100 GB of free hard-disk space, a UNIX user account with sudo privileges.","title":"Hopsworks Installer"},{"location":"setup_installation/on_prem/hopsworks_installer/#hopsworks-on-premises","text":"It is possible to use Hopsworks on-premises, which means that companies can run their machine learning workloads on their own hardware and infrastructure, rather than relying on a cloud provider. This can provide greater flexibility, control, and cost savings, as well as enabling companies to meet specific compliance and security requirements. Working on-premises with Hopsworks typically involves collaboration with the Hopsworks engineering teams, as each infrastructure is unique and requires a tailored approach to deployment and configuration. The process begins with an assessment of the company's existing infrastructure and requirements, including network topology, security policies, and hardware specifications. For further details about on-premise installations; contact us .","title":"Hopsworks On premises"},{"location":"setup_installation/on_prem/hopsworks_installer/#minimum-requirements","text":"You need at least one server or virtual machine on which Hopsworks will be installed with at least the following specification: Ubuntu 22.04, Centos or RHEL Linux (Version 8) supported, at least 32GB RAM, at least 8 CPUs, 100 GB of free hard-disk space, a UNIX user account with sudo privileges.","title":"Minimum Requirements"},{"location":"tutorials/","text":"Tutorials # We are happy to welcome you to our collection of tutorials dedicated to exploring the fundamentals of Hopsworks and Machine Learning development. In addition to offering different types of use cases and common subjects in the field, it facilitates navigation and use of models in a production environment using Hopsworks Feature Store. How to run the tutorials # In order to run the tutorials, you will need a Hopsworks account. To do so, go to app.hopsworks.ai and create one. With a managed account, just run the Jupyter notebook from within Hopsworks. Generally the notebooks contain the information you will need on how to interact with the Hopsworks Platform. The easiest way to get started is by using Google Colab to run the notebooks. However, you can also run them in your local Python environment with Jupyter. You can find the raw notebook files in our tutorials repository . Fraud Tutorial # This is a quick-start of the Hopsworks Feature Store; using a fraud use case we will load data into the feature store, create two feature groups from which we will make a training dataset and train a model. Batch # This is a batch use case variant of the fraud tutorial, it will give you a high level view on how to use our python APIs and the UI to navigate the feature groups. Notebooks 1. How to load, engineer and create feature groups 2. How to create training datasets 3. How to train a model from the feature store Online # This is a online use case variant of the fraud tutorial, it is similar to the batch use case, however, in this tutorial you will get introduced to the usage of Feature Groups which are kept in online storage, and how to access single feature vectors from the online storage at low latency. Additionally, the model will be deployed as a model serving instance, to provide a REST endpoint for real time serving. Notebooks 1. How to load, engineer and create feature groups 2. How to create training datasets 3. How to train a model from the feature store and deploying it as a serving instance together with the online feature store Churn Tutorial # This is a churn tutorial with the Hopsworks feature store and model serving to build a prediction service. In this tutorial you will get introduced to the usage of Feature Groups which are kept in online storage, and how to access single feature vectors from the online storage at low latency. Additionally, the model will be deployed as a model serving instance, to provide a REST endpoint for real time serving. Notebooks 1. How to load, engineer and create feature groups 2. How to create training datasets 3. How to train a model from the feature store and deploying it as a serving instance together with the online feature store Iris Tutorial # In this tutorial you will learn how to create an online prediction service for the Iris flower prediction problem. Notebooks 1. All-in-one notebook, showing how to create the needed feature groups, train the model and deploy it as a serving instance Integration Tutorials # Hopsworks is easily integrated with many tools, especially from the Python world. In this section you will find examples for some popular libraries and services. Great Expectations # Great Expectations is a library for data validation. You can use Great Expectations within Hopsworks to validate data which is to be inserted into the feature store, in order to ensure that only high-quality features end up in the feature store. Notebooks 1. A brief introduction to Great Expectations concepts which are relevant for integration with the Hopsworks MLOps platform 2. How to integrate Great Expectations seamlessly with your Hopsworks feature pipelines Weights and Biases # Weights and Biases is a developer tool for machine learning model training that with a couple of lines of code let you keep track of hyperparameters, system metrics, and outputs so you can compare experiments, and easily share your findings with colleagues. This tutorial is a variant of the batch fraud tutorial using Weights and Biases for model training, tracking and as model registry. Notebooks 1. How to load, engineer and create feature groups 2. How to create training datasets 3. How to train a model from the feature store and use Weights and Biases to track the process","title":"Tutorials"},{"location":"tutorials/#tutorials","text":"We are happy to welcome you to our collection of tutorials dedicated to exploring the fundamentals of Hopsworks and Machine Learning development. In addition to offering different types of use cases and common subjects in the field, it facilitates navigation and use of models in a production environment using Hopsworks Feature Store.","title":"Tutorials"},{"location":"tutorials/#how-to-run-the-tutorials","text":"In order to run the tutorials, you will need a Hopsworks account. To do so, go to app.hopsworks.ai and create one. With a managed account, just run the Jupyter notebook from within Hopsworks. Generally the notebooks contain the information you will need on how to interact with the Hopsworks Platform. The easiest way to get started is by using Google Colab to run the notebooks. However, you can also run them in your local Python environment with Jupyter. You can find the raw notebook files in our tutorials repository .","title":"How to run the tutorials"},{"location":"tutorials/#fraud-tutorial","text":"This is a quick-start of the Hopsworks Feature Store; using a fraud use case we will load data into the feature store, create two feature groups from which we will make a training dataset and train a model.","title":"Fraud Tutorial"},{"location":"tutorials/#batch","text":"This is a batch use case variant of the fraud tutorial, it will give you a high level view on how to use our python APIs and the UI to navigate the feature groups. Notebooks 1. How to load, engineer and create feature groups 2. How to create training datasets 3. How to train a model from the feature store","title":"Batch"},{"location":"tutorials/#online","text":"This is a online use case variant of the fraud tutorial, it is similar to the batch use case, however, in this tutorial you will get introduced to the usage of Feature Groups which are kept in online storage, and how to access single feature vectors from the online storage at low latency. Additionally, the model will be deployed as a model serving instance, to provide a REST endpoint for real time serving. Notebooks 1. How to load, engineer and create feature groups 2. How to create training datasets 3. How to train a model from the feature store and deploying it as a serving instance together with the online feature store","title":"Online"},{"location":"tutorials/#churn-tutorial","text":"This is a churn tutorial with the Hopsworks feature store and model serving to build a prediction service. In this tutorial you will get introduced to the usage of Feature Groups which are kept in online storage, and how to access single feature vectors from the online storage at low latency. Additionally, the model will be deployed as a model serving instance, to provide a REST endpoint for real time serving. Notebooks 1. How to load, engineer and create feature groups 2. How to create training datasets 3. How to train a model from the feature store and deploying it as a serving instance together with the online feature store","title":"Churn Tutorial"},{"location":"tutorials/#iris-tutorial","text":"In this tutorial you will learn how to create an online prediction service for the Iris flower prediction problem. Notebooks 1. All-in-one notebook, showing how to create the needed feature groups, train the model and deploy it as a serving instance","title":"Iris Tutorial"},{"location":"tutorials/#integration-tutorials","text":"Hopsworks is easily integrated with many tools, especially from the Python world. In this section you will find examples for some popular libraries and services.","title":"Integration Tutorials"},{"location":"tutorials/#great-expectations","text":"Great Expectations is a library for data validation. You can use Great Expectations within Hopsworks to validate data which is to be inserted into the feature store, in order to ensure that only high-quality features end up in the feature store. Notebooks 1. A brief introduction to Great Expectations concepts which are relevant for integration with the Hopsworks MLOps platform 2. How to integrate Great Expectations seamlessly with your Hopsworks feature pipelines","title":"Great Expectations"},{"location":"tutorials/#weights-and-biases","text":"Weights and Biases is a developer tool for machine learning model training that with a couple of lines of code let you keep track of hyperparameters, system metrics, and outputs so you can compare experiments, and easily share your findings with colleagues. This tutorial is a variant of the batch fraud tutorial using Weights and Biases for model training, tracking and as model registry. Notebooks 1. How to load, engineer and create feature groups 2. How to create training datasets 3. How to train a model from the feature store and use Weights and Biases to track the process","title":"Weights and Biases"},{"location":"user_guides/","text":"How-To Guides # This section serves to provide guides and examples for the common usage of abstractions and functionality of the Hopsworks platform through the Hopsworks UI and APIs. Client Installation : How to get started with the Hopsworks Client libraries. Feature Store : Learn about the common usage of the core Hopsworks Feature Store abstractions, such as Feature Groups, Feature Views, Data Validation and Storage Connectors. Also, learn from the Client Integrations guides how to connect to the Feature Store from external environments such as a local Python environment, Databricks, or AWS Sagemaker MLOps : Learn about the common usage of Hopsworks MLOps abstractions, such as the Model Registry or Model Serving. Projects : The core abstraction on Hopsworks are Projects . Learn in this section how to manage your projects and the services therein. Migration : Learn how to migrate to newer versions of Hopsworks.","title":"How-To Guides"},{"location":"user_guides/#how-to-guides","text":"This section serves to provide guides and examples for the common usage of abstractions and functionality of the Hopsworks platform through the Hopsworks UI and APIs. Client Installation : How to get started with the Hopsworks Client libraries. Feature Store : Learn about the common usage of the core Hopsworks Feature Store abstractions, such as Feature Groups, Feature Views, Data Validation and Storage Connectors. Also, learn from the Client Integrations guides how to connect to the Feature Store from external environments such as a local Python environment, Databricks, or AWS Sagemaker MLOps : Learn about the common usage of Hopsworks MLOps abstractions, such as the Model Registry or Model Serving. Projects : The core abstraction on Hopsworks are Projects . Learn in this section how to manage your projects and the services therein. Migration : Learn how to migrate to newer versions of Hopsworks.","title":"How-To Guides"},{"location":"user_guides/client_installation/","text":"Client Installation Guide # Hopsworks (including Feature Store and MLOps) # The Hopsworks client library is required to connect to the Hopsworks Feature Store and MLOps services from your local machine or any other Python environment such as Google Colab or AWS Sagemaker. Execute the following command to install the full Hopsworks client library in your Python environment: Virtual environment It is recommended to use a virtual python environment instead of the system environment used by your operating system, in order to avoid any side effects regarding interfering dependencies. pip install hopsworks Supported versions of Python: 3.7, 3.8, 3.9, 3.10, 3.11 ( PyPI \u2197 ) OSX Installation On OSX systems you might need to install librdkafka manually before installing hopsworks. You can verify if you have installed it previously using brew info librdkafka . Hopsworks requires librdkafka version to be lower than 2.0.0. If it is not installed yet, you can do so using brew install , however, you always need to set the C_INCLUDE_PATH and LIBRARY_PATH . If not installed yet, install librdkafka: curl -O https://raw.githubusercontent.com/Homebrew/homebrew-core/f7d0f40bbc4075177ecf16812fd95951a723a996/Formula/librdkafka.rb brew install --build-from-source librdkafka.rb If it is already installed, set the environment variables and proceed with installing the hopsworks client library: export C_INCLUDE_PATH=$(brew --prefix)/include export LIBRARY_PATH=$(brew --prefix)/lib pip install hopsworks Windows/Conda Installation On Windows systems you might need to install twofish manually before installing hopsworks, if you don't have the Microsoft Visual C++ Build Tools installed. In that case, it is recommended to use a conda environment and run the following commands: conda install twofish setx CONDA_DLL_SEARCH_MODIFICATION_ENABLE 1 pip install hopsworks Feature Store only # To only install the Hopsworks Feature Store client library, execute the following command: pip install hsfs[python] Supported versions of Python: 3.7, 3.8, 3.9, 3.10, 3.11 ( PyPI \u2197 ) OSX Installation On OSX systems you might need to install librdkafka manually before installing hopsworks. You can verify if you have installed it previously using brew info librdkafka . Hopsworks requires librdkafka version to be lower than 2.0.0. If it is not installed yet, you can do so using brew install , however, you always need to set the C_INCLUDE_PATH and LIBRARY_PATH . If not installed yet, install librdkafka: curl -O https://raw.githubusercontent.com/Homebrew/homebrew-core/f7d0f40bbc4075177ecf16812fd95951a723a996/Formula/librdkafka.rb brew install --build-from-source librdkafka.rb If it is already installed, set the environment variables and proceed with installing the hopsworks client library: export C_INCLUDE_PATH=$(brew --prefix)/include export LIBRARY_PATH=$(brew --prefix)/lib pip install hsfs[python] Windows/Conda Installation On Windows systems you might need to install twofish manually before installing hsfs, if you don't have the Microsoft Visual C++ Build Tools installed. In that case, it is recommended to use a conda environment and run the following commands: conda install twofish setx CONDA_DLL_SEARCH_MODIFICATION_ENABLE 1 pip install hsfs[python] Next Steps # If you are using a local python environment and want to connect to the Hopsworks Feature Store, you can follow the Python Guide section to create an API Key and to get started. Other environments # The Hopsworks Feature Store client libraries can also be installed in external environments, such as Databricks, AWS Sagemaker, or Azure Machine Learning. For more information, see Client Integrations .","title":"Client Installation Guide"},{"location":"user_guides/client_installation/#client-installation-guide","text":"","title":"Client Installation Guide"},{"location":"user_guides/client_installation/#hopsworks-including-feature-store-and-mlops","text":"The Hopsworks client library is required to connect to the Hopsworks Feature Store and MLOps services from your local machine or any other Python environment such as Google Colab or AWS Sagemaker. Execute the following command to install the full Hopsworks client library in your Python environment: Virtual environment It is recommended to use a virtual python environment instead of the system environment used by your operating system, in order to avoid any side effects regarding interfering dependencies. pip install hopsworks Supported versions of Python: 3.7, 3.8, 3.9, 3.10, 3.11 ( PyPI \u2197 ) OSX Installation On OSX systems you might need to install librdkafka manually before installing hopsworks. You can verify if you have installed it previously using brew info librdkafka . Hopsworks requires librdkafka version to be lower than 2.0.0. If it is not installed yet, you can do so using brew install , however, you always need to set the C_INCLUDE_PATH and LIBRARY_PATH . If not installed yet, install librdkafka: curl -O https://raw.githubusercontent.com/Homebrew/homebrew-core/f7d0f40bbc4075177ecf16812fd95951a723a996/Formula/librdkafka.rb brew install --build-from-source librdkafka.rb If it is already installed, set the environment variables and proceed with installing the hopsworks client library: export C_INCLUDE_PATH=$(brew --prefix)/include export LIBRARY_PATH=$(brew --prefix)/lib pip install hopsworks Windows/Conda Installation On Windows systems you might need to install twofish manually before installing hopsworks, if you don't have the Microsoft Visual C++ Build Tools installed. In that case, it is recommended to use a conda environment and run the following commands: conda install twofish setx CONDA_DLL_SEARCH_MODIFICATION_ENABLE 1 pip install hopsworks","title":"Hopsworks (including Feature Store and MLOps)"},{"location":"user_guides/client_installation/#feature-store-only","text":"To only install the Hopsworks Feature Store client library, execute the following command: pip install hsfs[python] Supported versions of Python: 3.7, 3.8, 3.9, 3.10, 3.11 ( PyPI \u2197 ) OSX Installation On OSX systems you might need to install librdkafka manually before installing hopsworks. You can verify if you have installed it previously using brew info librdkafka . Hopsworks requires librdkafka version to be lower than 2.0.0. If it is not installed yet, you can do so using brew install , however, you always need to set the C_INCLUDE_PATH and LIBRARY_PATH . If not installed yet, install librdkafka: curl -O https://raw.githubusercontent.com/Homebrew/homebrew-core/f7d0f40bbc4075177ecf16812fd95951a723a996/Formula/librdkafka.rb brew install --build-from-source librdkafka.rb If it is already installed, set the environment variables and proceed with installing the hopsworks client library: export C_INCLUDE_PATH=$(brew --prefix)/include export LIBRARY_PATH=$(brew --prefix)/lib pip install hsfs[python] Windows/Conda Installation On Windows systems you might need to install twofish manually before installing hsfs, if you don't have the Microsoft Visual C++ Build Tools installed. In that case, it is recommended to use a conda environment and run the following commands: conda install twofish setx CONDA_DLL_SEARCH_MODIFICATION_ENABLE 1 pip install hsfs[python]","title":"Feature Store only"},{"location":"user_guides/client_installation/#next-steps","text":"If you are using a local python environment and want to connect to the Hopsworks Feature Store, you can follow the Python Guide section to create an API Key and to get started.","title":"Next Steps"},{"location":"user_guides/client_installation/#other-environments","text":"The Hopsworks Feature Store client libraries can also be installed in external environments, such as Databricks, AWS Sagemaker, or Azure Machine Learning. For more information, see Client Integrations .","title":"Other environments"},{"location":"user_guides/fs/","text":"Feature Store Guides # This section serves to provide guides and examples for the common usage of abstractions and functionality of the Feature Store through the Hopsworks UI and APIs. Storage Connectors Feature Groups Feature Views Compute Engines Integrations","title":"Feature Store Guides"},{"location":"user_guides/fs/#feature-store-guides","text":"This section serves to provide guides and examples for the common usage of abstractions and functionality of the Feature Store through the Hopsworks UI and APIs. Storage Connectors Feature Groups Feature Views Compute Engines Integrations","title":"Feature Store Guides"},{"location":"user_guides/fs/compute_engines/","text":"Compute Engines # In order to execute a feature pipeline to write to the Feature Store, as well as to retrieve data from the Feature Store, you need a compute engine. Hopsworks Feature Store APIs are built around dataframes, that means feature data is inserted into the Feature Store from a Dataframe and likewise when reading data from the Feature Store, it is returned as a Dataframe. As such, Hopsworks supports three computational engines: Apache Spark : Spark Dataframes and Spark Structured Streaming Dataframes are supported, both from Python environments (PySpark) and from Scala environments. Pandas : For pure Python environments without dependencies on Spark, Hopsworks supports Pandas Dataframes . Apache Flink : Flink Data Streams are currently supported as an experimental feature from Java/Scala environments. Apache Beam experimental : Beam Data Streams are currently supported as an experimental feature from Java/Scala environments. Hopsworks supports running compute on the platform itself in the form of Jobs or in Jupyter Notebooks . Alternatlively, you can also connect to Hopsworks using Python or Spark from external environments , given that there is network connectivity. Functionality Support # Hopsworks is aiming to provide funtional parity between the computational engines, however, there are certain Hopsworks functionalities which are exclusive to the engines. Functionality Method Spark Python Flink Beam Comment Feature Group Creation from dataframes FeatureGroup.create_feature_group() - - Currently Flink/Beam doesn't support registering feature group metadata. Thus it needs to be pre-registered before you can write real time features computed by Flink/Beam. Training Dataset Creation from dataframes TrainingDataset.save() - - - Functionality was deprecated in version 3.0 Data validation using Great Expectations for streaming dataframes FeatureGroup.validate() FeatureGroup.insert_stream() - - - - insert_stream does not perform any data validation even when a expectation suite is attached. Stream ingestion FeatureGroup.insert_stream() - Python/Pandas has currently no notion of streaming. Reading from Streaming Storage Connectors KafkaConnector.read_stream() - - - Python/Pandas has currently no notion of streaming. For Flink/Beam only write operations are supported Reading training data from external storage other than S3 FeatureView.get_training_data() - - - Reading training data that was written to external storage using a Storage Connector other than S3 can currently not be read using HSFS APIs, instead you will have to use the storage's native client. Reading External Feature Groups into Dataframe ExternalFeatureGroup.read() - - - Reading an External Feature Group directly into a Pandas Dataframe is not supported, however, you can use the Query API to create Feature Views/Training Data containing External Feature Groups. Read Queries containing External Feature Groups into Dataframe Query.read() - - - Reading a Query containing an External Feature Group directly into a Pandas Dataframe is not supported, however, you can use the Query to create Feature Views/Training Data and write the data to a Storage Connector, from where you can read up the data into a Pandas Dataframe. Python # Inside Hopsworks # If you are using Spark or Python within Hopsworks, there is no further configuration required. Head over to the Getting Started Guide . Outside Hopsworks # Connecting to the Feature Store from any Python environment, such as your local environment or Google Colab, requires setting up an API Key and installing the HSFS Python client library. The Python integration guide explains step by step how to connect to the Feature Store from any Python environment. Spark # Inside Hopsworks # If you are using Spark or Python within Hopsworks, there is no further configuration required. Head over to the Getting Started Guide . Outside Hopsworks # Connecting to the Feature Store from an external Spark cluster, such as Cloudera or Databricks, requires configuring it with the Hopsworks client jars, configuration and certificates. The Spark integration guide explains step by step how to connect to the Feature Store from an external Spark cluster. Flink # Inside Hopsworks # If you are using Flink within Hopsworks, there is no further configuration required. For more details head over to the Getting Started Guide . Outside Hopsworks # Connecting to the Feature Store from an external Flink cluster, such as GCP DataProc or AWS EMR, requires configuring the Hopsworks certificates. The Flink integration guide explains step by step how to connect to the Feature Store from an external Flink cluster. Beam # Inside Hopsworks # Beam is only supported as an external client. Outside Hopsworks # Connecting to the Feature Store from Beam DataFlowRunner, requires configuring the Hopsworks certificates. The Beam integration guide explains step by step how to connect to the Feature Store from Beam Dataflow Runner. Warning Apache Beam integration with Hopsworks feature store was only tested using Dataflow Runner. For more details head over to the Getting Started Guide .","title":"Compute Engines"},{"location":"user_guides/fs/compute_engines/#compute-engines","text":"In order to execute a feature pipeline to write to the Feature Store, as well as to retrieve data from the Feature Store, you need a compute engine. Hopsworks Feature Store APIs are built around dataframes, that means feature data is inserted into the Feature Store from a Dataframe and likewise when reading data from the Feature Store, it is returned as a Dataframe. As such, Hopsworks supports three computational engines: Apache Spark : Spark Dataframes and Spark Structured Streaming Dataframes are supported, both from Python environments (PySpark) and from Scala environments. Pandas : For pure Python environments without dependencies on Spark, Hopsworks supports Pandas Dataframes . Apache Flink : Flink Data Streams are currently supported as an experimental feature from Java/Scala environments. Apache Beam experimental : Beam Data Streams are currently supported as an experimental feature from Java/Scala environments. Hopsworks supports running compute on the platform itself in the form of Jobs or in Jupyter Notebooks . Alternatlively, you can also connect to Hopsworks using Python or Spark from external environments , given that there is network connectivity.","title":"Compute Engines"},{"location":"user_guides/fs/compute_engines/#functionality-support","text":"Hopsworks is aiming to provide funtional parity between the computational engines, however, there are certain Hopsworks functionalities which are exclusive to the engines. Functionality Method Spark Python Flink Beam Comment Feature Group Creation from dataframes FeatureGroup.create_feature_group() - - Currently Flink/Beam doesn't support registering feature group metadata. Thus it needs to be pre-registered before you can write real time features computed by Flink/Beam. Training Dataset Creation from dataframes TrainingDataset.save() - - - Functionality was deprecated in version 3.0 Data validation using Great Expectations for streaming dataframes FeatureGroup.validate() FeatureGroup.insert_stream() - - - - insert_stream does not perform any data validation even when a expectation suite is attached. Stream ingestion FeatureGroup.insert_stream() - Python/Pandas has currently no notion of streaming. Reading from Streaming Storage Connectors KafkaConnector.read_stream() - - - Python/Pandas has currently no notion of streaming. For Flink/Beam only write operations are supported Reading training data from external storage other than S3 FeatureView.get_training_data() - - - Reading training data that was written to external storage using a Storage Connector other than S3 can currently not be read using HSFS APIs, instead you will have to use the storage's native client. Reading External Feature Groups into Dataframe ExternalFeatureGroup.read() - - - Reading an External Feature Group directly into a Pandas Dataframe is not supported, however, you can use the Query API to create Feature Views/Training Data containing External Feature Groups. Read Queries containing External Feature Groups into Dataframe Query.read() - - - Reading a Query containing an External Feature Group directly into a Pandas Dataframe is not supported, however, you can use the Query to create Feature Views/Training Data and write the data to a Storage Connector, from where you can read up the data into a Pandas Dataframe.","title":"Functionality Support"},{"location":"user_guides/fs/compute_engines/#python","text":"","title":"Python"},{"location":"user_guides/fs/compute_engines/#inside-hopsworks","text":"If you are using Spark or Python within Hopsworks, there is no further configuration required. Head over to the Getting Started Guide .","title":"Inside Hopsworks"},{"location":"user_guides/fs/compute_engines/#outside-hopsworks","text":"Connecting to the Feature Store from any Python environment, such as your local environment or Google Colab, requires setting up an API Key and installing the HSFS Python client library. The Python integration guide explains step by step how to connect to the Feature Store from any Python environment.","title":"Outside Hopsworks"},{"location":"user_guides/fs/compute_engines/#spark","text":"","title":"Spark"},{"location":"user_guides/fs/compute_engines/#inside-hopsworks_1","text":"If you are using Spark or Python within Hopsworks, there is no further configuration required. Head over to the Getting Started Guide .","title":"Inside Hopsworks"},{"location":"user_guides/fs/compute_engines/#outside-hopsworks_1","text":"Connecting to the Feature Store from an external Spark cluster, such as Cloudera or Databricks, requires configuring it with the Hopsworks client jars, configuration and certificates. The Spark integration guide explains step by step how to connect to the Feature Store from an external Spark cluster.","title":"Outside Hopsworks"},{"location":"user_guides/fs/compute_engines/#flink","text":"","title":"Flink"},{"location":"user_guides/fs/compute_engines/#inside-hopsworks_2","text":"If you are using Flink within Hopsworks, there is no further configuration required. For more details head over to the Getting Started Guide .","title":"Inside Hopsworks"},{"location":"user_guides/fs/compute_engines/#outside-hopsworks_2","text":"Connecting to the Feature Store from an external Flink cluster, such as GCP DataProc or AWS EMR, requires configuring the Hopsworks certificates. The Flink integration guide explains step by step how to connect to the Feature Store from an external Flink cluster.","title":"Outside Hopsworks"},{"location":"user_guides/fs/compute_engines/#beam","text":"","title":"Beam"},{"location":"user_guides/fs/compute_engines/#inside-hopsworks_3","text":"Beam is only supported as an external client.","title":"Inside Hopsworks"},{"location":"user_guides/fs/compute_engines/#outside-hopsworks_3","text":"Connecting to the Feature Store from Beam DataFlowRunner, requires configuring the Hopsworks certificates. The Beam integration guide explains step by step how to connect to the Feature Store from Beam Dataflow Runner. Warning Apache Beam integration with Hopsworks feature store was only tested using Dataflow Runner. For more details head over to the Getting Started Guide .","title":"Outside Hopsworks"},{"location":"user_guides/fs/feature_group/","text":"Feature Group User Guides # This section serves to provide guides and examples for the common usage of abstractions and functionality of the Feature Group through the Hopsworks UI and APIs. Create a Feature Group Create an external Feature Group Deprecating Feature Group Data Types and Schema management Statistics Data Validation","title":"Feature Group User Guides"},{"location":"user_guides/fs/feature_group/#feature-group-user-guides","text":"This section serves to provide guides and examples for the common usage of abstractions and functionality of the Feature Group through the Hopsworks UI and APIs. Create a Feature Group Create an external Feature Group Deprecating Feature Group Data Types and Schema management Statistics Data Validation","title":"Feature Group User Guides"},{"location":"user_guides/fs/feature_group/advanced_data_validation/","text":"Advanced Data Validation Options and Best Practices # The introduction to data vaildation guide can be found here . The notebook example to get started with Data Validation in Hopsworks can be found here . Data Validation Configuration Options in Hopsworks # Validation Ingestion Policy # Depending on your use case you can setup data validation as a monitoring or gatekeeping tool when trying to insert new data in your Feature Group. Switch behaviour by using the validation_ingestion_policy kwarg: \"ALWAYS\" is the default option and will attempt to insert the data regardless of the validation result. Hassle free, it is ideal to monitor data ingestion in a development setup. \"STRICT\" is the best option for production ready projects. This will prevent insertion of DataFrames which do not pass all data quality requirements. Ideal to avoid \"garbage-in, garbage-out\" scenarios, at the price of a potential loss of data. Check out the best practice section for more on that. In the UI # Go to the Feature Group edit page, in the Expectation section you can choose between the options above. In the python client # fg . expectation_suite . validation_ingestion_policy = \"ALWAYS\" # \"STRICT\" If your suite is registered with Hopsworks, it will persist the change to the server. Disable Data Validation # Should you wish to do so, you can disable data validation on a punctual basis or until further notice. In the UI # You can do it in the UI in the Expectation section of the Feature Group edit page. Simply tick or untick the enabled checkbox. This will be used as the default option but can be overriden via the API. In the python client # To disable data validation until further notice in the API, you can update the run_validation field of the expectation suite. If your suite is registered with Hopsworks, this will persist the change to the server. fg . expectation_suite . run_validation = False If you wish to override the default behaviour of the suite when inserting data in the Feature Group, you can do so via the validate_options kwarg. The example below will enable validation for this insertion only. fg . insert ( df_to_validate , validation_options = { \"run_validation\" : True }) We recommend to avoid using this option in scheduled job as it silently changes the expected behaviour that is displayed in the UI and prevents changes to the default behaviour to change the behaviour of the job. Edit Expectations # The one constant in life is change. If you need to add, remove or edit an expectation you can do it both in the UI or via the python client. Note that changing the expectation type or its corresponding feature will throw an error in order to preserve a meaningful validation history. In Hopworks UI # Go to the Feature Group edit page, in the expectation section. You can click on the expectation you want to edit and edit the json configuration. Check out Great Expectations documentation if you need more information on a particular expectation. In Hopsworks Python Client # There are several way to edit an Expectation in the python client. You can use Great Expectations API or directly go through Hopsworks. In the latter case, if you want to edit or remove an expectation, you will need the Hopsworks expectation ID. It can be found in the UI or in the meta field of an expectation. Note that you must have inserted data in the FG and attached the expectation suite to enable the Expectation API. Get an expectation with a given id: my_expectation = fg . expectation_suite . get_expectation ( expectation_id = my_expectation_id ) Add a new expectation: new_expectation = ge . core . ExpectationConfiguration ( expectation_type = \"expect_column_values_not_to_be_null\" , kwargs = { \"mostly\" : 1 } ) fg . expectation_suite . add_expectation ( new_expectation ) Edit expectation kwargs of an existing expectation : existing_expectation = fg . expectation_suite . get_expectation ( expectation_id = existing_expectation_id ) existing_expectation . kwargs [ \"mostly\" ] = 0.95 fg . expectation_suite . replace_expectation ( existing_expectation ) Remove an expectation: fg . expectation_suite . remove_expectation ( expectation_id = id_of_expectation_to_delete ) If you want to deal only with the Great Expectation API: my_suite = fg . get_expectation_suite () my_suite . add_expectation ( new_expectation ) fg . save_expectation_suite ( my_suite ) Save Validation Reports # When running validation using Great Expectations, a validation report is generated containing all validation results for the different expectations. Each result provides information about whether the provided DataFrame conforms to the corresponding expectation. These reports can be stored in Hopsworks to save a validation history for the data written to a particular Feature Group. The boilerplate of uploading report on insertion is taken care of by hopsworks, however for custom pipelines we provide an alternative method in the python client. The UI does not currently support upload of a validation report. In Hopsworks Python Client # fg . save_validation_report ( ge_report ) Monitor and Fetch Validation Reports # A summary of uploaded reports will then be available via an API call or in the Hopsworks UI enabling easy monitoring. For in-depth analysis, it is possible to download the complete report from the UI. In Hopsworks UI # Open the Feature Group overview page and go to the Expectations section. One tab allows you to check the report history with general information, while the other tab allows you to explore a summary of the result for individual expectations. In Hopsworks Python Client # # convenience method for rapid development ge_latest_report = fg . get_latest_validation_report () # fetching the latest summary prints a link to the UI # where you can download full report if summary is insufficient # or load multiple reports validation_history = fg . get_validation_reports () Validate your data manually # While Hopsworks provides automatic validation on insertion logic, we recognise that some use cases may require a more fine-grained control over the validation process. Therefore, Feature Group objects offers a convenience wrapper around Great Expectations to manually trigger validation using the registered Expectation Suite. In the UI # You can validate data already ingested in the Feature Group by going to the Feature Group overview page. In the top right corner is a button to trigger a validation. The button will lauch a job which will read the Feature Group data, run validation and persist the associated report. In the python client # ge_report = fg . validate ( df , ingestion_result = \"EXPERIMENT\" ) # set the save_report parameter to False to skip uploading the report to Hopsworks # ge_report = fg.validate(df, save_report=False) If you want to apply validation to the data already in the Feature Group you can call the .validate without providing data. It will read the data in the Feature Group. report = fg . validate () As validation objects returned by Hopsworks are native Great Expectation objects you can run validation using the usual Great Expectations syntax: ge_df = ge . from_pandas ( df , expectation_suite = fg . get_expectation_suite ()) ge_report = ge_df . validate () Note that you should always use an expectation suite that has been saved to Hopsworks if you intend to upload the associated validation report. Best Practices # Below is a set of recommendations and code snippets to help our users follow best practices when it comes to integrating a data validation step in your feature engineering pipelines. Rather than being prescriptive, we want to showcase how the API and configuration options can help adapt validation to your use-case. Development # Data validation is generally considered to be a production-only feature and as such is often only setup once a project has reached the end of the development phase. At Hopsworks, we think there is a lot of value in setting up validation during early development. That's why we made it quick to get started and ensured that by default data validation is never an obstacle to inserting data. Validate Early # As often with data validation, the best piece of advice is to set it up early in your development process. Use this phase to build a history you can then use when it becomes time to set quality requirements for a project in production. We made a code snippet to help you get started quickly: # Load sample data. Replace it with your own! my_data_df = pd . read_csv ( \"https://repo.hops.works/master/hopsworks-tutorials/data/card_fraud_data/credit_cards.csv\" ) # Use Great Expectation profiler (ignore deprecation warning) expectation_suite_profiled , validation_report = ge . from_pandas ( my_data_df ) . profile ( profiler = ge . profile . BasicSuiteBuilderProfiler ) # Create a Feature Group on hopsworks with an expectation suite attached. Don't forget to change the primary key! my_validated_data_fg = fs . get_or_create_feature_group ( name = \"my_validated_data_fg\" , version = 1 , description = \"My data\" , primary_key = [ 'cc_num' ], expectation_suite = expectation_suite_profiled ) Any data you insert in the Feature Group from now will be validated and a report will be uploaded to Hopsworks. # Insert and validate your data insert_job , validation_report = my_validated_data_fg . insert ( my_data_df ) Great Expectations profiler can inspect your data to build a standard Expectation Suite. You can attach this Expectation Suite directly when creating your Feature Group to make sure every piece of data finding its way in Hopsworks gets validated. Hopsworks will default to its \"ALWAYS\" ingestion policy, meaning data are ingested whether validation succeeds or not. This way data validation is not a barrier, just a monitoring tool. Identify Unreliable Features # Once you setup data validation, every insertion will upload a validation report to Hopsworks. Identifying Features which often have null values or wild statistical variations can help detecting unreliable Features that need refinements or should be avoided. Here are a few expectations you might find useful: expect_column_values_to_not_be_null expect_column_(min/max/mean/stdev)_to_be_between expect_column_values_to_be_unique Get the stakeholders involved # Hopsworks UI helps involve every project stakeholder by enabling both setting and monitoring of data quality requirements. No coding skills needed! You can monitor data quality requirements by checkint out the validation reports and results on the Feature Group page. If you need to set or edit the existing requirements, you can go on the Feature Group edit page. The Expectation suite section allows you to edit individual expectations and set success parameters that match ever changing business requirements. Production # Models in production require high-quality data to make accurate predictions for your customers. Hopsworks can use your Expectation Suite as a gatekeeper to make it simple to prevent low-quality data to make its way into production. Below are some simple tips and snippets to make the most of your data validation when your project is ready to enter its production phase. Be Strict in Production # Whether you use an existing or create a new (recommended) Feature Group for production, we recommend you set the validation ingestion policy of your Expectation Suite to \"STRICT\" . fg_prod . save_expectation_suite ( my_suite , validation_ingestion_policy = \"STRICT\" ) In this setup, Hopsworks will abort inserting a DataFrame that does not successfully fullfill all expectations in the attached Expectation Suite. This ensures data quality standards are upheld for every insertion and provide downstream users with strong guarantees. Avoid Data Loss on materialization jobs # Aborting insertions of DataFrames which do not satisfy the data quality standards can lead to data loss in your materialization job. To avoid such loss we recommend creating a duplicate Feature Group with the same Expectation Suite in \"ALWAYS\" mode which will hold the rejected data. job , report = fg_prod . insert ( df ) if report [ \"success\" ] is False : job , report = fg_rejected . insert ( df ) Take Advantage of the Validation History # You can easily retrieve the validation history of a specific expectation to export it to your favourite visualisation tool. You can filter on time and on whether insertion was successful or not validation_history = fg . get_validation_history ( expectation_id = my_id , filters = [ \"REJECTED\" , \"UNKNOWN\" ], ge_type = False ) timeseries = pd . DataFrame ( { \"observed_value\" : [ res . result [ \"observed_value\" ] for res in validation_histoy ]], \"validation_time\" : [ res . validation_time for res in validation_history ] } ) # export to your preferred Dashboard Setup Alerts # While checking your feature engineering pipeline executed properly in the morning can be good enough in the development phase, it won't make the cut for demanding production use-cases. In Hopsworks, you can setup alerts if ingestion fails or succeeds. First you will need to configure your preferred communication endpoint: slack, email or pagerduty. Check out this page for more information on how to set it up. A typical use-case would be to add an alert on ingestion success to a Feature Group you created to hold data that failed validation. Here is a quick walkthrough: Go the Feature Group page in the UI Scroll down and click on the Add an alert button. Choose the trigger, receiver and severity and click save. Conclusion # Hopsworks completes Great Expectation by automatically running the validation, persisting the reports along your data and allowing you to monitor data quality in its UI. How you decide to make use of these tools depends on your application and requirements. Whether in development or in production, real-time or batch, we think there is configuration that will work for your team. Check out our quick hands-on tutorial to start applying what you learned so far.","title":"Advanced Data Validation"},{"location":"user_guides/fs/feature_group/advanced_data_validation/#advanced-data-validation-options-and-best-practices","text":"The introduction to data vaildation guide can be found here . The notebook example to get started with Data Validation in Hopsworks can be found here .","title":"Advanced Data Validation Options and Best Practices"},{"location":"user_guides/fs/feature_group/advanced_data_validation/#data-validation-configuration-options-in-hopsworks","text":"","title":"Data Validation Configuration Options in Hopsworks"},{"location":"user_guides/fs/feature_group/advanced_data_validation/#validation-ingestion-policy","text":"Depending on your use case you can setup data validation as a monitoring or gatekeeping tool when trying to insert new data in your Feature Group. Switch behaviour by using the validation_ingestion_policy kwarg: \"ALWAYS\" is the default option and will attempt to insert the data regardless of the validation result. Hassle free, it is ideal to monitor data ingestion in a development setup. \"STRICT\" is the best option for production ready projects. This will prevent insertion of DataFrames which do not pass all data quality requirements. Ideal to avoid \"garbage-in, garbage-out\" scenarios, at the price of a potential loss of data. Check out the best practice section for more on that.","title":"Validation Ingestion Policy"},{"location":"user_guides/fs/feature_group/advanced_data_validation/#in-the-ui","text":"Go to the Feature Group edit page, in the Expectation section you can choose between the options above.","title":"In the UI"},{"location":"user_guides/fs/feature_group/advanced_data_validation/#in-the-python-client","text":"fg . expectation_suite . validation_ingestion_policy = \"ALWAYS\" # \"STRICT\" If your suite is registered with Hopsworks, it will persist the change to the server.","title":"In the python client"},{"location":"user_guides/fs/feature_group/advanced_data_validation/#disable-data-validation","text":"Should you wish to do so, you can disable data validation on a punctual basis or until further notice.","title":"Disable Data Validation"},{"location":"user_guides/fs/feature_group/advanced_data_validation/#in-the-ui_1","text":"You can do it in the UI in the Expectation section of the Feature Group edit page. Simply tick or untick the enabled checkbox. This will be used as the default option but can be overriden via the API.","title":"In the UI"},{"location":"user_guides/fs/feature_group/advanced_data_validation/#in-the-python-client_1","text":"To disable data validation until further notice in the API, you can update the run_validation field of the expectation suite. If your suite is registered with Hopsworks, this will persist the change to the server. fg . expectation_suite . run_validation = False If you wish to override the default behaviour of the suite when inserting data in the Feature Group, you can do so via the validate_options kwarg. The example below will enable validation for this insertion only. fg . insert ( df_to_validate , validation_options = { \"run_validation\" : True }) We recommend to avoid using this option in scheduled job as it silently changes the expected behaviour that is displayed in the UI and prevents changes to the default behaviour to change the behaviour of the job.","title":"In the python client"},{"location":"user_guides/fs/feature_group/advanced_data_validation/#edit-expectations","text":"The one constant in life is change. If you need to add, remove or edit an expectation you can do it both in the UI or via the python client. Note that changing the expectation type or its corresponding feature will throw an error in order to preserve a meaningful validation history.","title":"Edit Expectations"},{"location":"user_guides/fs/feature_group/advanced_data_validation/#in-hopworks-ui","text":"Go to the Feature Group edit page, in the expectation section. You can click on the expectation you want to edit and edit the json configuration. Check out Great Expectations documentation if you need more information on a particular expectation.","title":"In Hopworks UI"},{"location":"user_guides/fs/feature_group/advanced_data_validation/#in-hopsworks-python-client","text":"There are several way to edit an Expectation in the python client. You can use Great Expectations API or directly go through Hopsworks. In the latter case, if you want to edit or remove an expectation, you will need the Hopsworks expectation ID. It can be found in the UI or in the meta field of an expectation. Note that you must have inserted data in the FG and attached the expectation suite to enable the Expectation API. Get an expectation with a given id: my_expectation = fg . expectation_suite . get_expectation ( expectation_id = my_expectation_id ) Add a new expectation: new_expectation = ge . core . ExpectationConfiguration ( expectation_type = \"expect_column_values_not_to_be_null\" , kwargs = { \"mostly\" : 1 } ) fg . expectation_suite . add_expectation ( new_expectation ) Edit expectation kwargs of an existing expectation : existing_expectation = fg . expectation_suite . get_expectation ( expectation_id = existing_expectation_id ) existing_expectation . kwargs [ \"mostly\" ] = 0.95 fg . expectation_suite . replace_expectation ( existing_expectation ) Remove an expectation: fg . expectation_suite . remove_expectation ( expectation_id = id_of_expectation_to_delete ) If you want to deal only with the Great Expectation API: my_suite = fg . get_expectation_suite () my_suite . add_expectation ( new_expectation ) fg . save_expectation_suite ( my_suite )","title":"In Hopsworks Python Client"},{"location":"user_guides/fs/feature_group/advanced_data_validation/#save-validation-reports","text":"When running validation using Great Expectations, a validation report is generated containing all validation results for the different expectations. Each result provides information about whether the provided DataFrame conforms to the corresponding expectation. These reports can be stored in Hopsworks to save a validation history for the data written to a particular Feature Group. The boilerplate of uploading report on insertion is taken care of by hopsworks, however for custom pipelines we provide an alternative method in the python client. The UI does not currently support upload of a validation report.","title":"Save Validation Reports"},{"location":"user_guides/fs/feature_group/advanced_data_validation/#in-hopsworks-python-client_1","text":"fg . save_validation_report ( ge_report )","title":"In Hopsworks Python Client"},{"location":"user_guides/fs/feature_group/advanced_data_validation/#monitor-and-fetch-validation-reports","text":"A summary of uploaded reports will then be available via an API call or in the Hopsworks UI enabling easy monitoring. For in-depth analysis, it is possible to download the complete report from the UI.","title":"Monitor and Fetch Validation Reports"},{"location":"user_guides/fs/feature_group/advanced_data_validation/#in-hopsworks-ui","text":"Open the Feature Group overview page and go to the Expectations section. One tab allows you to check the report history with general information, while the other tab allows you to explore a summary of the result for individual expectations.","title":"In Hopsworks UI"},{"location":"user_guides/fs/feature_group/advanced_data_validation/#in-hopsworks-python-client_2","text":"# convenience method for rapid development ge_latest_report = fg . get_latest_validation_report () # fetching the latest summary prints a link to the UI # where you can download full report if summary is insufficient # or load multiple reports validation_history = fg . get_validation_reports ()","title":"In Hopsworks Python Client"},{"location":"user_guides/fs/feature_group/advanced_data_validation/#validate-your-data-manually","text":"While Hopsworks provides automatic validation on insertion logic, we recognise that some use cases may require a more fine-grained control over the validation process. Therefore, Feature Group objects offers a convenience wrapper around Great Expectations to manually trigger validation using the registered Expectation Suite.","title":"Validate your data manually"},{"location":"user_guides/fs/feature_group/advanced_data_validation/#in-the-ui_2","text":"You can validate data already ingested in the Feature Group by going to the Feature Group overview page. In the top right corner is a button to trigger a validation. The button will lauch a job which will read the Feature Group data, run validation and persist the associated report.","title":"In the UI"},{"location":"user_guides/fs/feature_group/advanced_data_validation/#in-the-python-client_2","text":"ge_report = fg . validate ( df , ingestion_result = \"EXPERIMENT\" ) # set the save_report parameter to False to skip uploading the report to Hopsworks # ge_report = fg.validate(df, save_report=False) If you want to apply validation to the data already in the Feature Group you can call the .validate without providing data. It will read the data in the Feature Group. report = fg . validate () As validation objects returned by Hopsworks are native Great Expectation objects you can run validation using the usual Great Expectations syntax: ge_df = ge . from_pandas ( df , expectation_suite = fg . get_expectation_suite ()) ge_report = ge_df . validate () Note that you should always use an expectation suite that has been saved to Hopsworks if you intend to upload the associated validation report.","title":"In the python client"},{"location":"user_guides/fs/feature_group/advanced_data_validation/#best-practices","text":"Below is a set of recommendations and code snippets to help our users follow best practices when it comes to integrating a data validation step in your feature engineering pipelines. Rather than being prescriptive, we want to showcase how the API and configuration options can help adapt validation to your use-case.","title":"Best Practices"},{"location":"user_guides/fs/feature_group/advanced_data_validation/#development","text":"Data validation is generally considered to be a production-only feature and as such is often only setup once a project has reached the end of the development phase. At Hopsworks, we think there is a lot of value in setting up validation during early development. That's why we made it quick to get started and ensured that by default data validation is never an obstacle to inserting data.","title":"Development"},{"location":"user_guides/fs/feature_group/advanced_data_validation/#validate-early","text":"As often with data validation, the best piece of advice is to set it up early in your development process. Use this phase to build a history you can then use when it becomes time to set quality requirements for a project in production. We made a code snippet to help you get started quickly: # Load sample data. Replace it with your own! my_data_df = pd . read_csv ( \"https://repo.hops.works/master/hopsworks-tutorials/data/card_fraud_data/credit_cards.csv\" ) # Use Great Expectation profiler (ignore deprecation warning) expectation_suite_profiled , validation_report = ge . from_pandas ( my_data_df ) . profile ( profiler = ge . profile . BasicSuiteBuilderProfiler ) # Create a Feature Group on hopsworks with an expectation suite attached. Don't forget to change the primary key! my_validated_data_fg = fs . get_or_create_feature_group ( name = \"my_validated_data_fg\" , version = 1 , description = \"My data\" , primary_key = [ 'cc_num' ], expectation_suite = expectation_suite_profiled ) Any data you insert in the Feature Group from now will be validated and a report will be uploaded to Hopsworks. # Insert and validate your data insert_job , validation_report = my_validated_data_fg . insert ( my_data_df ) Great Expectations profiler can inspect your data to build a standard Expectation Suite. You can attach this Expectation Suite directly when creating your Feature Group to make sure every piece of data finding its way in Hopsworks gets validated. Hopsworks will default to its \"ALWAYS\" ingestion policy, meaning data are ingested whether validation succeeds or not. This way data validation is not a barrier, just a monitoring tool.","title":"Validate Early"},{"location":"user_guides/fs/feature_group/advanced_data_validation/#identify-unreliable-features","text":"Once you setup data validation, every insertion will upload a validation report to Hopsworks. Identifying Features which often have null values or wild statistical variations can help detecting unreliable Features that need refinements or should be avoided. Here are a few expectations you might find useful: expect_column_values_to_not_be_null expect_column_(min/max/mean/stdev)_to_be_between expect_column_values_to_be_unique","title":"Identify Unreliable Features"},{"location":"user_guides/fs/feature_group/advanced_data_validation/#get-the-stakeholders-involved","text":"Hopsworks UI helps involve every project stakeholder by enabling both setting and monitoring of data quality requirements. No coding skills needed! You can monitor data quality requirements by checkint out the validation reports and results on the Feature Group page. If you need to set or edit the existing requirements, you can go on the Feature Group edit page. The Expectation suite section allows you to edit individual expectations and set success parameters that match ever changing business requirements.","title":"Get the stakeholders involved"},{"location":"user_guides/fs/feature_group/advanced_data_validation/#production","text":"Models in production require high-quality data to make accurate predictions for your customers. Hopsworks can use your Expectation Suite as a gatekeeper to make it simple to prevent low-quality data to make its way into production. Below are some simple tips and snippets to make the most of your data validation when your project is ready to enter its production phase.","title":"Production"},{"location":"user_guides/fs/feature_group/advanced_data_validation/#be-strict-in-production","text":"Whether you use an existing or create a new (recommended) Feature Group for production, we recommend you set the validation ingestion policy of your Expectation Suite to \"STRICT\" . fg_prod . save_expectation_suite ( my_suite , validation_ingestion_policy = \"STRICT\" ) In this setup, Hopsworks will abort inserting a DataFrame that does not successfully fullfill all expectations in the attached Expectation Suite. This ensures data quality standards are upheld for every insertion and provide downstream users with strong guarantees.","title":"Be Strict in Production"},{"location":"user_guides/fs/feature_group/advanced_data_validation/#avoid-data-loss-on-materialization-jobs","text":"Aborting insertions of DataFrames which do not satisfy the data quality standards can lead to data loss in your materialization job. To avoid such loss we recommend creating a duplicate Feature Group with the same Expectation Suite in \"ALWAYS\" mode which will hold the rejected data. job , report = fg_prod . insert ( df ) if report [ \"success\" ] is False : job , report = fg_rejected . insert ( df )","title":"Avoid Data Loss on materialization jobs"},{"location":"user_guides/fs/feature_group/advanced_data_validation/#take-advantage-of-the-validation-history","text":"You can easily retrieve the validation history of a specific expectation to export it to your favourite visualisation tool. You can filter on time and on whether insertion was successful or not validation_history = fg . get_validation_history ( expectation_id = my_id , filters = [ \"REJECTED\" , \"UNKNOWN\" ], ge_type = False ) timeseries = pd . DataFrame ( { \"observed_value\" : [ res . result [ \"observed_value\" ] for res in validation_histoy ]], \"validation_time\" : [ res . validation_time for res in validation_history ] } ) # export to your preferred Dashboard","title":"Take Advantage of the Validation History"},{"location":"user_guides/fs/feature_group/advanced_data_validation/#setup-alerts","text":"While checking your feature engineering pipeline executed properly in the morning can be good enough in the development phase, it won't make the cut for demanding production use-cases. In Hopsworks, you can setup alerts if ingestion fails or succeeds. First you will need to configure your preferred communication endpoint: slack, email or pagerduty. Check out this page for more information on how to set it up. A typical use-case would be to add an alert on ingestion success to a Feature Group you created to hold data that failed validation. Here is a quick walkthrough: Go the Feature Group page in the UI Scroll down and click on the Add an alert button. Choose the trigger, receiver and severity and click save.","title":"Setup Alerts"},{"location":"user_guides/fs/feature_group/advanced_data_validation/#conclusion","text":"Hopsworks completes Great Expectation by automatically running the validation, persisting the reports along your data and allowing you to monitor data quality in its UI. How you decide to make use of these tools depends on your application and requirements. Whether in development or in production, real-time or batch, we think there is configuration that will work for your team. Check out our quick hands-on tutorial to start applying what you learned so far.","title":"Conclusion"},{"location":"user_guides/fs/feature_group/create/","text":"How to create a Feature Group # Introduction # In this guide you will learn how to create and register a feature group with Hopsworks. This guide covers creating a feature group using the HSFS APIs as well as the user interface. Prerequisites # Before you begin this guide we suggest you read the Feature Group concept page to understand what a feature group is and how it fits in the ML pipeline. Create using the HSFS APIs # To create a feature group using the HSFS APIs, you need to provide a Pandas or Spark DataFrame. The DataFrame will contain all the features you want to register within the feature group, as well as the primary key, event time and partition key. Create a Feature Group # The first step to create a feature group is to create the API metadata object representing a feature group. Using the HSFS API you can execute: Batch Write API # PySpark fg = feature_store . create_feature_group ( name = \"weather\" , version = 1 , description = \"Weather Features\" , online_enabled = True , primary_key = [ 'location_id' ], partition_key = [ 'day' ], event_time = 'event_time' , ) The full method documentation is available here . name is the only mandatory parameter of the create_feature_group and represents the name of the feature group. In the example above we created the first version of a feature group named weather , we provide a description to make it searchable to the other project members, as well as making the feature group available online. Additionally we specify which columns of the DataFrame will be used as primary key, partition key and event time. Composite primary key and multi level partitioning is also supported. The version number is optional, if you don't specify the version number the APIs will create a new version by default with a version number equals to the highest existing version number plus one. The last parameter used in the examples above is stream . The stream parameter controls whether to enable the streaming write APIs to the online and offline feature store. When using the APIs in a Python environment this behavior is the default and it requires the time travel format to be set to 'HUDI'. Primary key # A primary key is required when using the default Hudi file format to store offline feature data. When inserting data in a feature group on the offline feature store, the DataFrame you are writing is checked against the existing data in the feature group. If a row with the same primary key is found in the feature group, the row will be updated. If the primary key is not found, the row is appended to the feature group. When writing data on the online feature store, existing rows with the same primary key will be overwritten by new rows with the same primary key. Event time # The event time column represents the time at which the event was generated. For example, with transaction data, the event time is the time at which a given transaction happened. In the context of feature pipelines, the event time is often also the end timestamp of the interval of events included in the feature computation. For example, computing the feature \"number of purchases by customer last week\", the event time should be the last day of this \"last week\" window. The event time is added to the primary key when writing to the offline feature store. This will make sure that the offline feature store has the entire history of feature values over time. As an example, if a user has made multiple purchases on a website, each of the purchases for a given user (identified by a user_id) will be saved in the feature group, with each purchase having a different event time (the combination of user_id and event_time makes up the primary key for the offline feature store). The event time is not part of the primary key when writing to the online feature store. This will ensure that the online feature store has the most recent version of the feature vector for each primary key. Event time data type restriction The supported data types for the event time column are: timestamp , date and bigint . Partition key # It is best practice to add a partition key. When you specify a partition key, the data in the feature group will be stored under multiple directories based on the value of the partition column(s). All the rows with a given value as partition key will be stored in the same directory. Choosing the correct partition key has significant impact on the query performance as the execution engine (Spark) will be able to skip listing and reading files belonging to partitions which are not included in the query. As an example, if you have partitioned your feature group by day and you are creating a training dataset that includes only the last year of data, Spark will read only 365 partitions and not the entire history of data. On the other hand, if the partition key is too fine grained (e.g. timestamp at millisecond resolution) - a large number of small partitions will be generated. This will slow down query execution as Spark will need to list and read a large amount of small directories/files. If you do not provide a partition key, all the feature data will be stored as files in a single directory. The system has a limit of 10240 direct children (files or other subdirectories) per directory. This means that, as you add new data to a non-partitioned feature group, new files will be created and you might reach the limit. If you do reach the limit, your feature engineering pipeline will fail with the following error: MaxDirectoryItemsExceededException - The directory item limit is exceeded: limit = 10240 items = 10240 By using partitioning the system will write the feature data in different subdirectories, thus allowing you to write 10240 files per partition. Streaming Write API # As explained above, the stream parameter controls whether to enable the streaming write APIs to the online and offline feature store. For Python environments, only the stream API is supported (stream=True). Python PySpark fg = feature_store . create_feature_group ( name = \"weather\" , version = 1 , description = \"Weather Features\" , online_enabled = True , primary_key = [ 'location_id' ], partition_key = [ 'day' ], event_time = 'event_time' ) fg = feature_store . create_feature_group ( name = \"weather\" , version = 1 , description = \"Weather Features\" , online_enabled = True , primary_key = [ 'location_id' ], partition_key = [ 'day' ], event_time = 'event_time' , stream = True ) When using the streaming API, the data will be written directly to the online storage (if online_enabled=True ). However, you can control when the sync to the offline storage is going to happen. You can do it synchronously after every call to fg.insert() , which is the default. Often, you defer writes to a later point in order to batch together multiple writes to the offline storage (useful to reduce the overhead of many small writes): # run multiple inserts without starting the offline materialization job job , _ = fg . insert ( df1 , write_options = { \"start_offline_materialization\" : False }) job , _ = fg . insert ( df2 , write_options = { \"start_offline_materialization\" : False }) job , _ = fg . insert ( df3 , write_options = { \"start_offline_materialization\" : False }) # start the materialization job for all three inserts # note the job object is always the same, you don't need to call it three times job . run () It is also possible to define the topics used for data ingestion, this can be done by setting the topic_name parameter with your preferred value. By default, feature groups in hopsworks will share a project-wide topic. Best Practices for Writing # When designing a feature group, it is worth taking a look at how this feature group will be queried in the future, in order to optimize it for those query patterns. At the same time, Spark and Hudi tend to overpartition writes, creatingtoo many small parquet files, which is inefficient and slowing down the write. But they also slow down queries, because file listings are taking more time, but also reading many small files is usually slower. The best practices described in this section hold both for the Streaming API and the Batch API. Four main considerations influence the write and the query performance: Partitioning on a feature group level Parquet file size within a feature group partition Backfilling of feature group partitions The choice of topic for data ingestion Partitioning on a feature group level # Partitioning on the feature group level allows Hopsworks and Hudi to push down filters to the filesystem during training dataset or batch data generation. In practice that means, less directories need to be listed and less files need to be read, speeding up queries. For example, most commonly, filtering is done on the event time column of a feature group when generating training data or batches of data: query = fg . select_all () # create a simple feature view fv = fs . create_feature_view ( name = 'transactions_view' , query = query ) # set up dates start_time = \"2022-01-01\" end_time = \"2022-06-30\" # create a training dataset version , job = feature_view . create_training_data ( start_time = start_time , end_time = end_time , description = 'Description of a dataset' , ) Assuming the feature group was partitioned by a daily event time column, for example, the features are updated with a daily batch job, the feature store will only have to list and read the files in the directories of those six months that are being queried. Too granular event time columns An event time column which is too granular, such as a timestamp, shouldn't be used as partition key. For example, a streaming pipeline generating features where the event time includes seconds, and therefore almost all event timestamps are unique can lead to many partition directories and small files, each of which contains only a few number of rows, which are inefficient to query even with pushed down filters. A good practice are partition keys with at most daily granularity, if they are based on time. Additionally, one can look at the size of a partition directory, which should be in the 100s of MB. Additionally, if you are commonly training models for different categories of your data, you can add another level of partitioning for this. That is, if the query contains an additional filter: query = fg . select_all () . filter ( fg . country_code == \"US\" ) The feature group can be created with the following partition key in order to push down filters also for the country_code category: fg = feature_store . create_feature_group ( ... partition_key = [ 'day' , 'country_code' ], event_time = 'day' , ) Parquet file size within a feature group partition # Once you have decided on the feature group level partitioning and you start inserting data to the feature group, there are multiple ways in order to influence how Hudi will split the data between parquet files within the feature group partitions . The two things that influence the number of parquet files per partition are The number of feature group partitions written in a single insert The shuffle parallelism used by Hudi In general, the inserted dataframe (unique combination of partition key values) will be parallised according to the following Hudi settings: Default Hudi partitioning write_options = { 'hoodie.bulkinsert.shuffle.parallelism' : 5 , 'hoodie.insert.shuffle.parallelism' : 5 , 'hoodie.upsert.shuffle.parallelism' : 5 } That means, using Spark, Hudi shuffles the data into five in-memory partitions, which each fill map to a task and finally a parquet file (see figure below). If the inserted Dataframe contains only a single feature group partition, this feature group partition will be written with five parquet files. If the inserted Dataframe contains multiple feature group partitions, the parquet files will be split among those partition, potentially more parquet files will be added. Mapping in-memory partitions to tasks, workers, executors and feature group partition files for a feature group insert Setting shuffle parallelism In practice that means the shuffle parallelism should be set equal to the number of feature group partitions in the inserted dataframe. This will create one parquet file per feature group partition, which in many cases is optimal. Theoretically, this rule holds up to a partition size of 2GB, which is the limit of Spark. However, one should bump this up accordingly already for smaller inputs. We recommend having shuffle parallelism hoodie.[insert|upsert|bulkinsert].shuffle.parallelism such that its at least input_data_size/500MB. You can change the write options on every insert, depending also on the size of the data you are writing: write_options = { 'hoodie.bulkinsert.shuffle.parallelism' : 5 , 'hoodie.insert.shuffle.parallelism' : 5 , 'hoodie.upsert.shuffle.parallelism' : 5 } fg . insert ( df , write_options = write_options ) Backfilling of feature group partitions # Hudi scales well with the number of partitions to write, when performing backfilling of old feature partitions, meaning moving backwards in time with the event-time, it makes sense to batch those feature group partitions together into a single fg.insert() call. As shown in the figure above, the number of utilised executors you choose for the insert depends highly on the number of partitions and shuffle parallelism you are writing. So by writing multiple feature group partitions in a single insert, you can scale up your Spark application and fully utilise the workers. In that case you can increase the Hudi shuffle parallelism accordingly. Concurrent feature group inserts Hopsworks 3.1 and earlier, currently does not support concurrent inserts to feature groups. This means that if your feature pipeline writes to one feature group partition at a time, you cannot run it multiple times in parallel for backfilling. The recommended approach is to unionise the dataframes and insert them with a single fg.insert() instead. For clients that write with the Stream API, it is enough to defer starting the backfill job until after multiple inserts, as described above . The choice of topic for data ingestion # When creating a feature group that uses streaming write APIs for data ingestion it is possible to define the Kafka topics that should be utilized. The default approach of using a project-wide topic functions great for use cases involving little to no overlap when producing data. However, concurrently inserting into multiple feature groups could cause read amplification for the Hudi delta streamer job. Therefore, it is advised to utilize separate topics when ingestions overlap or there is a large frequently running insertion into a specific feature group. Register the metadata and save the feature data # The snippet above only created the metadata object on the Python interpreter running the code. To register the feature group metadata and to save the feature data with Hopsworks, you should invoke the insert method: fg . insert ( df ) The save method takes in input a Pandas or Spark DataFrame. HSFS will use the DataFrame columns and types to determine the name and types of features, primary key, partition key and event time. The DataFrame must contain the columns specified as primary keys, partition key and event time in the create_feature_group call. If a feature group is online enabled, the insert method will store the feature data to both the online and offline storage. API Reference # FeatureGroup Create using the UI # You can also create a new feature group through the UI. For this, navigate to the Feature Groups section and press the Create button at the top-right corner. Subsequently, you will be able to define its properties (such as name, mode, features, and more). Refer to the documentation above for an explanation of the parameters available, they are the same as when you create a feature group using the SDK. Finally, complete the creation by clicking Create New Feature Group at the bottom of the page.","title":"Create"},{"location":"user_guides/fs/feature_group/create/#how-to-create-a-feature-group","text":"","title":"How to create a Feature Group"},{"location":"user_guides/fs/feature_group/create/#introduction","text":"In this guide you will learn how to create and register a feature group with Hopsworks. This guide covers creating a feature group using the HSFS APIs as well as the user interface.","title":"Introduction"},{"location":"user_guides/fs/feature_group/create/#prerequisites","text":"Before you begin this guide we suggest you read the Feature Group concept page to understand what a feature group is and how it fits in the ML pipeline.","title":"Prerequisites"},{"location":"user_guides/fs/feature_group/create/#create-using-the-hsfs-apis","text":"To create a feature group using the HSFS APIs, you need to provide a Pandas or Spark DataFrame. The DataFrame will contain all the features you want to register within the feature group, as well as the primary key, event time and partition key.","title":"Create using the HSFS APIs"},{"location":"user_guides/fs/feature_group/create/#create-a-feature-group","text":"The first step to create a feature group is to create the API metadata object representing a feature group. Using the HSFS API you can execute:","title":"Create a Feature Group"},{"location":"user_guides/fs/feature_group/create/#batch-write-api","text":"PySpark fg = feature_store . create_feature_group ( name = \"weather\" , version = 1 , description = \"Weather Features\" , online_enabled = True , primary_key = [ 'location_id' ], partition_key = [ 'day' ], event_time = 'event_time' , ) The full method documentation is available here . name is the only mandatory parameter of the create_feature_group and represents the name of the feature group. In the example above we created the first version of a feature group named weather , we provide a description to make it searchable to the other project members, as well as making the feature group available online. Additionally we specify which columns of the DataFrame will be used as primary key, partition key and event time. Composite primary key and multi level partitioning is also supported. The version number is optional, if you don't specify the version number the APIs will create a new version by default with a version number equals to the highest existing version number plus one. The last parameter used in the examples above is stream . The stream parameter controls whether to enable the streaming write APIs to the online and offline feature store. When using the APIs in a Python environment this behavior is the default and it requires the time travel format to be set to 'HUDI'.","title":"Batch Write API"},{"location":"user_guides/fs/feature_group/create/#primary-key","text":"A primary key is required when using the default Hudi file format to store offline feature data. When inserting data in a feature group on the offline feature store, the DataFrame you are writing is checked against the existing data in the feature group. If a row with the same primary key is found in the feature group, the row will be updated. If the primary key is not found, the row is appended to the feature group. When writing data on the online feature store, existing rows with the same primary key will be overwritten by new rows with the same primary key.","title":"Primary key"},{"location":"user_guides/fs/feature_group/create/#event-time","text":"The event time column represents the time at which the event was generated. For example, with transaction data, the event time is the time at which a given transaction happened. In the context of feature pipelines, the event time is often also the end timestamp of the interval of events included in the feature computation. For example, computing the feature \"number of purchases by customer last week\", the event time should be the last day of this \"last week\" window. The event time is added to the primary key when writing to the offline feature store. This will make sure that the offline feature store has the entire history of feature values over time. As an example, if a user has made multiple purchases on a website, each of the purchases for a given user (identified by a user_id) will be saved in the feature group, with each purchase having a different event time (the combination of user_id and event_time makes up the primary key for the offline feature store). The event time is not part of the primary key when writing to the online feature store. This will ensure that the online feature store has the most recent version of the feature vector for each primary key. Event time data type restriction The supported data types for the event time column are: timestamp , date and bigint .","title":"Event time"},{"location":"user_guides/fs/feature_group/create/#partition-key","text":"It is best practice to add a partition key. When you specify a partition key, the data in the feature group will be stored under multiple directories based on the value of the partition column(s). All the rows with a given value as partition key will be stored in the same directory. Choosing the correct partition key has significant impact on the query performance as the execution engine (Spark) will be able to skip listing and reading files belonging to partitions which are not included in the query. As an example, if you have partitioned your feature group by day and you are creating a training dataset that includes only the last year of data, Spark will read only 365 partitions and not the entire history of data. On the other hand, if the partition key is too fine grained (e.g. timestamp at millisecond resolution) - a large number of small partitions will be generated. This will slow down query execution as Spark will need to list and read a large amount of small directories/files. If you do not provide a partition key, all the feature data will be stored as files in a single directory. The system has a limit of 10240 direct children (files or other subdirectories) per directory. This means that, as you add new data to a non-partitioned feature group, new files will be created and you might reach the limit. If you do reach the limit, your feature engineering pipeline will fail with the following error: MaxDirectoryItemsExceededException - The directory item limit is exceeded: limit = 10240 items = 10240 By using partitioning the system will write the feature data in different subdirectories, thus allowing you to write 10240 files per partition.","title":"Partition key"},{"location":"user_guides/fs/feature_group/create/#streaming-write-api","text":"As explained above, the stream parameter controls whether to enable the streaming write APIs to the online and offline feature store. For Python environments, only the stream API is supported (stream=True). Python PySpark fg = feature_store . create_feature_group ( name = \"weather\" , version = 1 , description = \"Weather Features\" , online_enabled = True , primary_key = [ 'location_id' ], partition_key = [ 'day' ], event_time = 'event_time' ) fg = feature_store . create_feature_group ( name = \"weather\" , version = 1 , description = \"Weather Features\" , online_enabled = True , primary_key = [ 'location_id' ], partition_key = [ 'day' ], event_time = 'event_time' , stream = True ) When using the streaming API, the data will be written directly to the online storage (if online_enabled=True ). However, you can control when the sync to the offline storage is going to happen. You can do it synchronously after every call to fg.insert() , which is the default. Often, you defer writes to a later point in order to batch together multiple writes to the offline storage (useful to reduce the overhead of many small writes): # run multiple inserts without starting the offline materialization job job , _ = fg . insert ( df1 , write_options = { \"start_offline_materialization\" : False }) job , _ = fg . insert ( df2 , write_options = { \"start_offline_materialization\" : False }) job , _ = fg . insert ( df3 , write_options = { \"start_offline_materialization\" : False }) # start the materialization job for all three inserts # note the job object is always the same, you don't need to call it three times job . run () It is also possible to define the topics used for data ingestion, this can be done by setting the topic_name parameter with your preferred value. By default, feature groups in hopsworks will share a project-wide topic.","title":"Streaming Write API"},{"location":"user_guides/fs/feature_group/create/#best-practices-for-writing","text":"When designing a feature group, it is worth taking a look at how this feature group will be queried in the future, in order to optimize it for those query patterns. At the same time, Spark and Hudi tend to overpartition writes, creatingtoo many small parquet files, which is inefficient and slowing down the write. But they also slow down queries, because file listings are taking more time, but also reading many small files is usually slower. The best practices described in this section hold both for the Streaming API and the Batch API. Four main considerations influence the write and the query performance: Partitioning on a feature group level Parquet file size within a feature group partition Backfilling of feature group partitions The choice of topic for data ingestion","title":"Best Practices for Writing"},{"location":"user_guides/fs/feature_group/create/#partitioning-on-a-feature-group-level","text":"Partitioning on the feature group level allows Hopsworks and Hudi to push down filters to the filesystem during training dataset or batch data generation. In practice that means, less directories need to be listed and less files need to be read, speeding up queries. For example, most commonly, filtering is done on the event time column of a feature group when generating training data or batches of data: query = fg . select_all () # create a simple feature view fv = fs . create_feature_view ( name = 'transactions_view' , query = query ) # set up dates start_time = \"2022-01-01\" end_time = \"2022-06-30\" # create a training dataset version , job = feature_view . create_training_data ( start_time = start_time , end_time = end_time , description = 'Description of a dataset' , ) Assuming the feature group was partitioned by a daily event time column, for example, the features are updated with a daily batch job, the feature store will only have to list and read the files in the directories of those six months that are being queried. Too granular event time columns An event time column which is too granular, such as a timestamp, shouldn't be used as partition key. For example, a streaming pipeline generating features where the event time includes seconds, and therefore almost all event timestamps are unique can lead to many partition directories and small files, each of which contains only a few number of rows, which are inefficient to query even with pushed down filters. A good practice are partition keys with at most daily granularity, if they are based on time. Additionally, one can look at the size of a partition directory, which should be in the 100s of MB. Additionally, if you are commonly training models for different categories of your data, you can add another level of partitioning for this. That is, if the query contains an additional filter: query = fg . select_all () . filter ( fg . country_code == \"US\" ) The feature group can be created with the following partition key in order to push down filters also for the country_code category: fg = feature_store . create_feature_group ( ... partition_key = [ 'day' , 'country_code' ], event_time = 'day' , )","title":"Partitioning on a feature group level"},{"location":"user_guides/fs/feature_group/create/#parquet-file-size-within-a-feature-group-partition","text":"Once you have decided on the feature group level partitioning and you start inserting data to the feature group, there are multiple ways in order to influence how Hudi will split the data between parquet files within the feature group partitions . The two things that influence the number of parquet files per partition are The number of feature group partitions written in a single insert The shuffle parallelism used by Hudi In general, the inserted dataframe (unique combination of partition key values) will be parallised according to the following Hudi settings: Default Hudi partitioning write_options = { 'hoodie.bulkinsert.shuffle.parallelism' : 5 , 'hoodie.insert.shuffle.parallelism' : 5 , 'hoodie.upsert.shuffle.parallelism' : 5 } That means, using Spark, Hudi shuffles the data into five in-memory partitions, which each fill map to a task and finally a parquet file (see figure below). If the inserted Dataframe contains only a single feature group partition, this feature group partition will be written with five parquet files. If the inserted Dataframe contains multiple feature group partitions, the parquet files will be split among those partition, potentially more parquet files will be added. Mapping in-memory partitions to tasks, workers, executors and feature group partition files for a feature group insert Setting shuffle parallelism In practice that means the shuffle parallelism should be set equal to the number of feature group partitions in the inserted dataframe. This will create one parquet file per feature group partition, which in many cases is optimal. Theoretically, this rule holds up to a partition size of 2GB, which is the limit of Spark. However, one should bump this up accordingly already for smaller inputs. We recommend having shuffle parallelism hoodie.[insert|upsert|bulkinsert].shuffle.parallelism such that its at least input_data_size/500MB. You can change the write options on every insert, depending also on the size of the data you are writing: write_options = { 'hoodie.bulkinsert.shuffle.parallelism' : 5 , 'hoodie.insert.shuffle.parallelism' : 5 , 'hoodie.upsert.shuffle.parallelism' : 5 } fg . insert ( df , write_options = write_options )","title":"Parquet file size within a feature group partition"},{"location":"user_guides/fs/feature_group/create/#backfilling-of-feature-group-partitions","text":"Hudi scales well with the number of partitions to write, when performing backfilling of old feature partitions, meaning moving backwards in time with the event-time, it makes sense to batch those feature group partitions together into a single fg.insert() call. As shown in the figure above, the number of utilised executors you choose for the insert depends highly on the number of partitions and shuffle parallelism you are writing. So by writing multiple feature group partitions in a single insert, you can scale up your Spark application and fully utilise the workers. In that case you can increase the Hudi shuffle parallelism accordingly. Concurrent feature group inserts Hopsworks 3.1 and earlier, currently does not support concurrent inserts to feature groups. This means that if your feature pipeline writes to one feature group partition at a time, you cannot run it multiple times in parallel for backfilling. The recommended approach is to unionise the dataframes and insert them with a single fg.insert() instead. For clients that write with the Stream API, it is enough to defer starting the backfill job until after multiple inserts, as described above .","title":"Backfilling of feature group partitions"},{"location":"user_guides/fs/feature_group/create/#the-choice-of-topic-for-data-ingestion","text":"When creating a feature group that uses streaming write APIs for data ingestion it is possible to define the Kafka topics that should be utilized. The default approach of using a project-wide topic functions great for use cases involving little to no overlap when producing data. However, concurrently inserting into multiple feature groups could cause read amplification for the Hudi delta streamer job. Therefore, it is advised to utilize separate topics when ingestions overlap or there is a large frequently running insertion into a specific feature group.","title":"The choice of topic for data ingestion"},{"location":"user_guides/fs/feature_group/create/#register-the-metadata-and-save-the-feature-data","text":"The snippet above only created the metadata object on the Python interpreter running the code. To register the feature group metadata and to save the feature data with Hopsworks, you should invoke the insert method: fg . insert ( df ) The save method takes in input a Pandas or Spark DataFrame. HSFS will use the DataFrame columns and types to determine the name and types of features, primary key, partition key and event time. The DataFrame must contain the columns specified as primary keys, partition key and event time in the create_feature_group call. If a feature group is online enabled, the insert method will store the feature data to both the online and offline storage.","title":"Register the metadata and save the feature data"},{"location":"user_guides/fs/feature_group/create/#api-reference","text":"FeatureGroup","title":"API Reference"},{"location":"user_guides/fs/feature_group/create/#create-using-the-ui","text":"You can also create a new feature group through the UI. For this, navigate to the Feature Groups section and press the Create button at the top-right corner. Subsequently, you will be able to define its properties (such as name, mode, features, and more). Refer to the documentation above for an explanation of the parameters available, they are the same as when you create a feature group using the SDK. Finally, complete the creation by clicking Create New Feature Group at the bottom of the page.","title":"Create using the UI"},{"location":"user_guides/fs/feature_group/create_external/","text":"How to create an External Feature Group # Introduction # In this guide you will learn how to create and register an external feature group with Hopsworks. This guide covers creating an external feature group using the HSFS APIs as well as the user interface. Prerequisites # Before you begin this guide we suggest you read the External Feature Group concept page to understand what a feature group is and how it fits in the ML pipeline. Create using the HSFS APIs # Retrieve the storage connector # To create an external feature group using the HSFS APIs you need to provide an existing storage connector . Python connector = feature_store . get_storage_connector ( \"connector_name\" ) Create an External Feature Group # The first step is to instantiate the metadata through the create_external_feature_group method. Once you have defined the metadata, you can persist the metadata and create the feature group in Hopsworks by calling fg.save() . SQL based external feature group # Python query = \"\"\" SELECT TO_NUMERIC(ss_store_sk) AS ss_store_sk , AVG(ss_net_profit) AS avg_ss_net_profit , SUM(ss_net_profit) AS total_ss_net_profit , AVG(ss_list_price) AS avg_ss_list_price , AVG(ss_coupon_amt) AS avg_ss_coupon_amt , sale_date , ss_store_sk FROM STORE_SALES GROUP BY ss_store_sk, sales_date \"\"\" fg = feature_store . create_external_feature_group ( name = \"sales\" , version = 1 , description = \"Physical shop sales features\" , query = query , storage_connector = connector , primary_key = [ 'ss_store_sk' ], event_time = 'sale_date' ) fg . save () Data Lake based external feature group # Python fg = feature_store . create_external_feature_group ( name = \"sales\" , version = 1 , description = \"Physical shop sales features\" , data_format = \"parquet\" , storage_connector = connector , primary_key = [ 'ss_store_sk' ], event_time = 'sale_date' ) fg . save () The full method documentation is available here . name is a mandatory parameter of the create_external_feature_group and represents the name of the feature group. The version number is optional, if you don't specify the version number the APIs will create a new version by default with a version number equals to the highest existing version number plus one. If the storage connector is defined for a data warehouse (e.g. JDBC, Snowflake, Redshift) you need to provide a SQL statement that will be executed to compute the features. If the storage connector is defined for a data lake, the location of the data as well as the format need to be provided. Additionally we specify which columns of the DataFrame will be used as primary key, and event time. Composite primary keys are also supported. Register the metadata # In the snippet above it's important that the created metadata object gets registered in Hopsworks. To do so, you should invoke the save method: Python fg . save () Limitations # Hopsworks Feature Store does not support time-travel capabilities for external feature groups. Moreover, as the data resides on external systems, external feature groups cannot be made available online for low latency serving. To make data from an external feature group available online, users need to define an online enabled feature group and have a job that periodically reads data from the external feature group and writes in the online feature group. Python support Currently the HSFS library does not support calling the read() or show() methods on external feature groups. Likewise it is not possible to call the read() or show() methods on queries containing external feature groups. Nevertheless, external feature groups can be used from a Python engine to create training datasets. API Reference # External FeatureGroup Create using the UI # You can also create a new feature group through the UI. For this, navigate to the Feature Groups section and press the Create button at the top-right corner. Subsequently, you will be able to define its properties (such as name, mode, features, and more). Refer to the documentation above for an explanation of the parameters available, they are the same as when you create a feature group using the SDK. Finally, complete the creation by clicking Create New Feature Group at the bottom of the page.","title":"Create External"},{"location":"user_guides/fs/feature_group/create_external/#how-to-create-an-external-feature-group","text":"","title":"How to create an External Feature Group"},{"location":"user_guides/fs/feature_group/create_external/#introduction","text":"In this guide you will learn how to create and register an external feature group with Hopsworks. This guide covers creating an external feature group using the HSFS APIs as well as the user interface.","title":"Introduction"},{"location":"user_guides/fs/feature_group/create_external/#prerequisites","text":"Before you begin this guide we suggest you read the External Feature Group concept page to understand what a feature group is and how it fits in the ML pipeline.","title":"Prerequisites"},{"location":"user_guides/fs/feature_group/create_external/#create-using-the-hsfs-apis","text":"","title":"Create using the HSFS APIs"},{"location":"user_guides/fs/feature_group/create_external/#retrieve-the-storage-connector","text":"To create an external feature group using the HSFS APIs you need to provide an existing storage connector . Python connector = feature_store . get_storage_connector ( \"connector_name\" )","title":"Retrieve the storage connector"},{"location":"user_guides/fs/feature_group/create_external/#create-an-external-feature-group","text":"The first step is to instantiate the metadata through the create_external_feature_group method. Once you have defined the metadata, you can persist the metadata and create the feature group in Hopsworks by calling fg.save() .","title":"Create an External Feature Group"},{"location":"user_guides/fs/feature_group/create_external/#sql-based-external-feature-group","text":"Python query = \"\"\" SELECT TO_NUMERIC(ss_store_sk) AS ss_store_sk , AVG(ss_net_profit) AS avg_ss_net_profit , SUM(ss_net_profit) AS total_ss_net_profit , AVG(ss_list_price) AS avg_ss_list_price , AVG(ss_coupon_amt) AS avg_ss_coupon_amt , sale_date , ss_store_sk FROM STORE_SALES GROUP BY ss_store_sk, sales_date \"\"\" fg = feature_store . create_external_feature_group ( name = \"sales\" , version = 1 , description = \"Physical shop sales features\" , query = query , storage_connector = connector , primary_key = [ 'ss_store_sk' ], event_time = 'sale_date' ) fg . save ()","title":"SQL based external feature group"},{"location":"user_guides/fs/feature_group/create_external/#data-lake-based-external-feature-group","text":"Python fg = feature_store . create_external_feature_group ( name = \"sales\" , version = 1 , description = \"Physical shop sales features\" , data_format = \"parquet\" , storage_connector = connector , primary_key = [ 'ss_store_sk' ], event_time = 'sale_date' ) fg . save () The full method documentation is available here . name is a mandatory parameter of the create_external_feature_group and represents the name of the feature group. The version number is optional, if you don't specify the version number the APIs will create a new version by default with a version number equals to the highest existing version number plus one. If the storage connector is defined for a data warehouse (e.g. JDBC, Snowflake, Redshift) you need to provide a SQL statement that will be executed to compute the features. If the storage connector is defined for a data lake, the location of the data as well as the format need to be provided. Additionally we specify which columns of the DataFrame will be used as primary key, and event time. Composite primary keys are also supported.","title":"Data Lake based external feature group"},{"location":"user_guides/fs/feature_group/create_external/#register-the-metadata","text":"In the snippet above it's important that the created metadata object gets registered in Hopsworks. To do so, you should invoke the save method: Python fg . save ()","title":"Register the metadata"},{"location":"user_guides/fs/feature_group/create_external/#limitations","text":"Hopsworks Feature Store does not support time-travel capabilities for external feature groups. Moreover, as the data resides on external systems, external feature groups cannot be made available online for low latency serving. To make data from an external feature group available online, users need to define an online enabled feature group and have a job that periodically reads data from the external feature group and writes in the online feature group. Python support Currently the HSFS library does not support calling the read() or show() methods on external feature groups. Likewise it is not possible to call the read() or show() methods on queries containing external feature groups. Nevertheless, external feature groups can be used from a Python engine to create training datasets.","title":"Limitations"},{"location":"user_guides/fs/feature_group/create_external/#api-reference","text":"External FeatureGroup","title":"API Reference"},{"location":"user_guides/fs/feature_group/create_external/#create-using-the-ui","text":"You can also create a new feature group through the UI. For this, navigate to the Feature Groups section and press the Create button at the top-right corner. Subsequently, you will be able to define its properties (such as name, mode, features, and more). Refer to the documentation above for an explanation of the parameters available, they are the same as when you create a feature group using the SDK. Finally, complete the creation by clicking Create New Feature Group at the bottom of the page.","title":"Create using the UI"},{"location":"user_guides/fs/feature_group/create_spine/","text":"How to create Spine Group # Introduction # In this guide you will learn how to create and register a Spine Group with Hopsworks. Prerequisites # Before you begin this guide we suggest you read the Spine Group concept page to understand what a Spine Group is and how it fits in the ML pipeline. Create using the HSFS APIs # Create a Spine Group # Instead of using a feature group to save the label, you can also use a spine to use a Dataframe containing the labels on the fly. A spine is essentially a metadata object similar to a Feature Group, which tells the feature store the relevant event time column and primary key columns to perform point-in-time correct joins. Additionally, apart from primary key and event time information, a Spark dataframe is required in order to infer the schema of the group from. Python trans_spine = fs . get_or_create_spine_group ( name = \"spine_transactions\" , version = 1 , description = \"Transaction data\" , primary_key = [ 'cc_num' ], event_time = 'datetime' , dataframe = trans_df ) Once created, note that you can inspect the dataframe in the Spine Group: Python trans_spine . dataframe . show () And you can always also replace the dataframe contained within the Spine Group. You just need to make sure it has the same schema. Python trans_spine . dataframe = new_df Limitations # Python support Currently the HSFS library does not support usage of Spine Groups for training data creation or batch data retrieval in the Python engine. However, it is supported to create Spine Groups from the Python engine. API Reference # SpineGroup","title":"How to create Spine Group"},{"location":"user_guides/fs/feature_group/create_spine/#how-to-create-spine-group","text":"","title":"How to create Spine Group"},{"location":"user_guides/fs/feature_group/create_spine/#introduction","text":"In this guide you will learn how to create and register a Spine Group with Hopsworks.","title":"Introduction"},{"location":"user_guides/fs/feature_group/create_spine/#prerequisites","text":"Before you begin this guide we suggest you read the Spine Group concept page to understand what a Spine Group is and how it fits in the ML pipeline.","title":"Prerequisites"},{"location":"user_guides/fs/feature_group/create_spine/#create-using-the-hsfs-apis","text":"","title":"Create using the HSFS APIs"},{"location":"user_guides/fs/feature_group/create_spine/#create-a-spine-group","text":"Instead of using a feature group to save the label, you can also use a spine to use a Dataframe containing the labels on the fly. A spine is essentially a metadata object similar to a Feature Group, which tells the feature store the relevant event time column and primary key columns to perform point-in-time correct joins. Additionally, apart from primary key and event time information, a Spark dataframe is required in order to infer the schema of the group from. Python trans_spine = fs . get_or_create_spine_group ( name = \"spine_transactions\" , version = 1 , description = \"Transaction data\" , primary_key = [ 'cc_num' ], event_time = 'datetime' , dataframe = trans_df ) Once created, note that you can inspect the dataframe in the Spine Group: Python trans_spine . dataframe . show () And you can always also replace the dataframe contained within the Spine Group. You just need to make sure it has the same schema. Python trans_spine . dataframe = new_df","title":"Create a Spine Group"},{"location":"user_guides/fs/feature_group/create_spine/#limitations","text":"Python support Currently the HSFS library does not support usage of Spine Groups for training data creation or batch data retrieval in the Python engine. However, it is supported to create Spine Groups from the Python engine.","title":"Limitations"},{"location":"user_guides/fs/feature_group/create_spine/#api-reference","text":"SpineGroup","title":"API Reference"},{"location":"user_guides/fs/feature_group/data_types/","text":"How to manage schema and feature data types # Introduction # In this guide, you will learn how to manage the feature group schema and control the data type of the features in a feature group. Prerequisites # Before you begin this guide we suggest you read the Feature Group concept page to understand what a feature group is and how it fits in the ML pipeline. We also suggest you familiarize yourself with the APIs to create a feature group . Feature group schema # When a feature is stored in both the online and offline feature stores, it will be stored in a data type native to each store. Offline data type : The data type of the feature when stored on the offline feature store. The offline feature store is based on Apache Hudi and Hive Metastore, as such, Hive Data Types can be leveraged. Online data type : The data type of the feature when stored on the online feature store. The online storage is based on RonDB and hence, MySQL Data Types can be leveraged. The offline data type is always required, even if the feature group is stored only online. On the other hand, if the feature group is not online_enabled , its features will not have an online data type. The offline and online types for each feature are automatically inferred from the Spark or Pandas types of the input DataFrame as outlined in the following two sections. The default mapping, however, can be overwritten by using an explicit schema definition . Offline data types # When registering a Spark DataFrame in a PySpark environment (S), or a Pandas DataFrame in a Python-only environment (P) the following default mapping to offline feature types applies: Spark Type (S) Pandas Type (P) Offline Feature Type Remarks BooleanType bool, object(bool) BOOLEAN ByteType int8, Int8 TINYINT or INT INT when time_travel_type=\"HUDI\" ShortType uint8, int16, Int16 SMALLINT or INT INT when time_travel_type=\"HUDI\" IntegerType uint16, int32, Int32 INT LongType int, uint32, int64, Int64 BIGINT FloatType float, float16, float32 FLOAT DoubleType float64 DOUBLE DecimalType decimal.decimal DECIMAL(PREC, SCALE) Not supported in PO env. when time_travel_type=\"HUDI\" TimestampType datetime64[ns], datetime64[ns, tz] TIMESTAMP s. Timestamps and Timezones DateType object (datetime.date) DATE StringType object (str), object(np.unicode) STRING ArrayType object (list), object (np.ndarray) ARRAY<TYPE> StructType object (dict) STRUCT<NAME: TYPE, ...> BinaryType object (binary) BINARY MapType - MAP<String,TYPE> Only when time_travel_type!=\"HUDI\"; Only string keys permitted When registering a Pandas DataFrame in a PySpark environment (S) the Pandas DataFrame is first converted to a Spark DataFrame, using Spark's default conversion . It results in a less fine-grained mapping between Python and Spark types: Pandas Type (S) Spark Type Remarks bool BooleanType int8, uint8, int16, uint16, int32, int, uint32, int64 LongType float, float16, float32, float64 DoubleType object (decimal.decimal) DecimalType datetime64[ns], datetime64[ns, tz] TimestampType s. Timestamps and Timezones object (datetime.date) DateType object (str), object(np.unicode) StringType object (list), object (np.ndarray) - Not supported object (dict) StructType object (binary) BinaryType Online data types # The online data type is determined based on the offline type according to the following mapping, regardless of which environment the data originated from. Only a subset of the data types can be used as primary key, as indicated in the table as well: Offline Feature Type Online Feature Type Primary Key Remarks BOOLEAN TINYINT x TINYINT TINYINT x SMALLINT SMALLINT x INT INT x Also supports: TINYINT, SMALLINT BIGINT BIGINT x FLOAT FLOAT DOUBLE DOUBLE DECIMAL(PREC, SCALE) DECIMAL(PREC, SCALE) e.g. DECIMAL(38, 18) TIMESTAMP TIMESTAMP s. Timestamps and Timezones DATE DATE x STRING VARCHAR(100) x Also supports: TEXT ARRAY<TYPE> VARBINARY(100) x Also supports: BLOB STRUCT<NAME: TYPE, ...> VARBINARY(100) x Also supports: BLOB BINARY VARBINARY(100) x Also supports: BLOB MAP<String,TYPE> VARBINARY(100) x Also supports: BLOB More on how Hopsworks handles string types , complex data types and the online restrictions for primary keys and row size in the following sections. String online data types # String types are stored as VARCHAR(100) by default. This type is fixed-size, meaning it can only hold as many characters as specified in the argument (e.g. VARCHAR(100) can hold up to 100 unicode characters). The size should thus be within the maximum string length of the input data. Furthermore, the VARCHAR size has to be in line with the online restrictions for row size . If the string size exceeds 100 characters, a larger type (e.g. VARCHAR(500)) can be specified via an explicit schema definition . If the string size is unknown or if it exceeds the maximum row size, then the TEXT type can be used instead. String data that exceeds the specified VARCHAR size will lead to an error when data gets written to the online feature store. When in doubt, use the TEXT type instead, but note that it comes with a potential performance overhead. Complex online data types # Hopsworks allows users to store complex types (e.g. ARRAY ) in the online feature store. Hopsworks serializes the complex features transparently and stores them as VARBINARY in the online feature store. The serialization happens when calling the save() , insert() or insert_stream() methods. The deserialization will be executed when calling the get_serving_vector() method to retrieve data from the online feature store. If users query directly the online feature store, for instance using the fs.sql(\"SELECT ...\", online=True) statement, it will return a binary blob. On the feature store UI, the online feature type for complex features will be reported as VARBINARY . If the binary size exceeds 100 bytes, a larger type (e.g. VARBINARY(500)) can be specified via an explicit schema definition . If the binary size is unknown of if it exceeds the maximum row size, then the BLOB type can be used instead. Binary data that exceeds the specified VARBINARY size will lead to an error when data gets written to the online feature store. When in doubt, use the BLOB type instead, but note that it comes with a potential performance overhead. Online restrictions for primary key data types # When a feature is being used as a primary key, certain types are not allowed. Examples of such types are FLOAT , DOUBLE , TEXT and BLOB . Additionally, the size of the sum of the primary key online data types storage requirements should not exceed 4KB . Online restrictions for row size # The online feature store supports up to 500 columns and all column types combined should not exceed 30000 Bytes . The byte size of each column is determined by its data type and calculated as follows: Online Data Type Byte Size TINYINT 1 SMALLINT 2 INT 4 BIGINT 8 FLOAT 4 DOUBLE 8 DECIMAL(PREC, SCALE) 16 TIMESTAMP 8 DATE 8 VARCHAR(LENGTH) LENGTH * 4 VARCHAR(LENGTH) charset latin1; LENGTH * 1 TEXT 256 VARBINARY(LENGTH) LENGTH / 1.4 BLOB 256 other 8 Timestamps and Timezones # All timestamp features are stored in Hopsworks in UTC time. Also, all timestamp-based functions (such as point-in-time joins ) use UTC time. This ensures consistency of timestamp features across different client timezones and simplifies working with timestamp-based functions in general. When ingesting timestamp features, the Feature Store Write API will automatically handle the conversion to UTC, if necessary. The follwing table summarizes how different timestamp types are handled: Data Frame (Data Type) Environment Handling Pandas DataFrame (datetime64[ns]) Python-only and PySpark interpreted as UTC, independent of the client's timezone Pandas DataFrame (datetime64[ns, tz]) Python-only and PySpark timzone-sensitive conversion from 'tz' to UTC Spark (TimestampType) PySpark and Spark interpreted as UTC, independent of the client's timezone Timestamp features retrieved from the Feature Store, e.g. using the Feature Store Read API , use a timezone-unaware format: Data Frame (Data Type) Environment Timezone Pandas DataFrame (datetime64[ns]) Python-only timezone-unaware (UTC) Spark (TimestampType) PySpark and Spark timezone-unaware (UTC) Note that our PySpark/Spark client automatically sets the Spark SQL session's timezone to UTC. This ensures that Spark SQL will correctly interpret all timestamps as UTC. The setting will only apply to the client's session, and you don't have to worry about setting/unsetting the configuration yourself. Explicit schema definition # When creating a feature group it is possible for the user to control both the offline and online data type of each column. If users explicitly define the schema for the feature group, Hopsworks is going to use that schema to create the feature group, without performing any type mapping. You can explicitly define the feature group schema as follows: Python from hsfs.feature import Feature features = [ Feature ( name = \"id\" , type = \"int\" , online_type = \"int\" ), Feature ( name = \"name\" , type = \"string\" , online_type = \"varchar(20)\" ) ] fg = fs . create_feature_group ( name = \"fg_manual_schema\" , features = features , online_enabled = True ) fg . save ( features ) Append features to existing feature groups # Hopsworks supports appending additional features to an existing feature group. Adding additional features to an existing feature group is not considered a breaking change. Python from hsfs.feature import Feature features = [ Feature ( name = \"id\" , type = \"int\" , online_type = \"int\" ), Feature ( name = \"name\" , type = \"string\" , online_type = \"varchar(20)\" ) ] fg = fs . get_feature_group ( name = \"example\" , version = 1 ) fg . append_features ( features ) When adding additional features to a feature group, you can provide a default values for existing entries in the feature group. You can also backfill the new features for existing entries by running an insert() operation and update all existing combinations of primary key - event time .","title":"Data Types and Schema management"},{"location":"user_guides/fs/feature_group/data_types/#how-to-manage-schema-and-feature-data-types","text":"","title":"How to manage schema and feature data types"},{"location":"user_guides/fs/feature_group/data_types/#introduction","text":"In this guide, you will learn how to manage the feature group schema and control the data type of the features in a feature group.","title":"Introduction"},{"location":"user_guides/fs/feature_group/data_types/#prerequisites","text":"Before you begin this guide we suggest you read the Feature Group concept page to understand what a feature group is and how it fits in the ML pipeline. We also suggest you familiarize yourself with the APIs to create a feature group .","title":"Prerequisites"},{"location":"user_guides/fs/feature_group/data_types/#feature-group-schema","text":"When a feature is stored in both the online and offline feature stores, it will be stored in a data type native to each store. Offline data type : The data type of the feature when stored on the offline feature store. The offline feature store is based on Apache Hudi and Hive Metastore, as such, Hive Data Types can be leveraged. Online data type : The data type of the feature when stored on the online feature store. The online storage is based on RonDB and hence, MySQL Data Types can be leveraged. The offline data type is always required, even if the feature group is stored only online. On the other hand, if the feature group is not online_enabled , its features will not have an online data type. The offline and online types for each feature are automatically inferred from the Spark or Pandas types of the input DataFrame as outlined in the following two sections. The default mapping, however, can be overwritten by using an explicit schema definition .","title":"Feature group schema"},{"location":"user_guides/fs/feature_group/data_types/#offline-data-types","text":"When registering a Spark DataFrame in a PySpark environment (S), or a Pandas DataFrame in a Python-only environment (P) the following default mapping to offline feature types applies: Spark Type (S) Pandas Type (P) Offline Feature Type Remarks BooleanType bool, object(bool) BOOLEAN ByteType int8, Int8 TINYINT or INT INT when time_travel_type=\"HUDI\" ShortType uint8, int16, Int16 SMALLINT or INT INT when time_travel_type=\"HUDI\" IntegerType uint16, int32, Int32 INT LongType int, uint32, int64, Int64 BIGINT FloatType float, float16, float32 FLOAT DoubleType float64 DOUBLE DecimalType decimal.decimal DECIMAL(PREC, SCALE) Not supported in PO env. when time_travel_type=\"HUDI\" TimestampType datetime64[ns], datetime64[ns, tz] TIMESTAMP s. Timestamps and Timezones DateType object (datetime.date) DATE StringType object (str), object(np.unicode) STRING ArrayType object (list), object (np.ndarray) ARRAY<TYPE> StructType object (dict) STRUCT<NAME: TYPE, ...> BinaryType object (binary) BINARY MapType - MAP<String,TYPE> Only when time_travel_type!=\"HUDI\"; Only string keys permitted When registering a Pandas DataFrame in a PySpark environment (S) the Pandas DataFrame is first converted to a Spark DataFrame, using Spark's default conversion . It results in a less fine-grained mapping between Python and Spark types: Pandas Type (S) Spark Type Remarks bool BooleanType int8, uint8, int16, uint16, int32, int, uint32, int64 LongType float, float16, float32, float64 DoubleType object (decimal.decimal) DecimalType datetime64[ns], datetime64[ns, tz] TimestampType s. Timestamps and Timezones object (datetime.date) DateType object (str), object(np.unicode) StringType object (list), object (np.ndarray) - Not supported object (dict) StructType object (binary) BinaryType","title":"Offline data types"},{"location":"user_guides/fs/feature_group/data_types/#online-data-types","text":"The online data type is determined based on the offline type according to the following mapping, regardless of which environment the data originated from. Only a subset of the data types can be used as primary key, as indicated in the table as well: Offline Feature Type Online Feature Type Primary Key Remarks BOOLEAN TINYINT x TINYINT TINYINT x SMALLINT SMALLINT x INT INT x Also supports: TINYINT, SMALLINT BIGINT BIGINT x FLOAT FLOAT DOUBLE DOUBLE DECIMAL(PREC, SCALE) DECIMAL(PREC, SCALE) e.g. DECIMAL(38, 18) TIMESTAMP TIMESTAMP s. Timestamps and Timezones DATE DATE x STRING VARCHAR(100) x Also supports: TEXT ARRAY<TYPE> VARBINARY(100) x Also supports: BLOB STRUCT<NAME: TYPE, ...> VARBINARY(100) x Also supports: BLOB BINARY VARBINARY(100) x Also supports: BLOB MAP<String,TYPE> VARBINARY(100) x Also supports: BLOB More on how Hopsworks handles string types , complex data types and the online restrictions for primary keys and row size in the following sections.","title":"Online data types"},{"location":"user_guides/fs/feature_group/data_types/#string-online-data-types","text":"String types are stored as VARCHAR(100) by default. This type is fixed-size, meaning it can only hold as many characters as specified in the argument (e.g. VARCHAR(100) can hold up to 100 unicode characters). The size should thus be within the maximum string length of the input data. Furthermore, the VARCHAR size has to be in line with the online restrictions for row size . If the string size exceeds 100 characters, a larger type (e.g. VARCHAR(500)) can be specified via an explicit schema definition . If the string size is unknown or if it exceeds the maximum row size, then the TEXT type can be used instead. String data that exceeds the specified VARCHAR size will lead to an error when data gets written to the online feature store. When in doubt, use the TEXT type instead, but note that it comes with a potential performance overhead.","title":"String online data types"},{"location":"user_guides/fs/feature_group/data_types/#complex-online-data-types","text":"Hopsworks allows users to store complex types (e.g. ARRAY ) in the online feature store. Hopsworks serializes the complex features transparently and stores them as VARBINARY in the online feature store. The serialization happens when calling the save() , insert() or insert_stream() methods. The deserialization will be executed when calling the get_serving_vector() method to retrieve data from the online feature store. If users query directly the online feature store, for instance using the fs.sql(\"SELECT ...\", online=True) statement, it will return a binary blob. On the feature store UI, the online feature type for complex features will be reported as VARBINARY . If the binary size exceeds 100 bytes, a larger type (e.g. VARBINARY(500)) can be specified via an explicit schema definition . If the binary size is unknown of if it exceeds the maximum row size, then the BLOB type can be used instead. Binary data that exceeds the specified VARBINARY size will lead to an error when data gets written to the online feature store. When in doubt, use the BLOB type instead, but note that it comes with a potential performance overhead.","title":"Complex online data types"},{"location":"user_guides/fs/feature_group/data_types/#online-restrictions-for-primary-key-data-types","text":"When a feature is being used as a primary key, certain types are not allowed. Examples of such types are FLOAT , DOUBLE , TEXT and BLOB . Additionally, the size of the sum of the primary key online data types storage requirements should not exceed 4KB .","title":"Online restrictions for primary key data types"},{"location":"user_guides/fs/feature_group/data_types/#online-restrictions-for-row-size","text":"The online feature store supports up to 500 columns and all column types combined should not exceed 30000 Bytes . The byte size of each column is determined by its data type and calculated as follows: Online Data Type Byte Size TINYINT 1 SMALLINT 2 INT 4 BIGINT 8 FLOAT 4 DOUBLE 8 DECIMAL(PREC, SCALE) 16 TIMESTAMP 8 DATE 8 VARCHAR(LENGTH) LENGTH * 4 VARCHAR(LENGTH) charset latin1; LENGTH * 1 TEXT 256 VARBINARY(LENGTH) LENGTH / 1.4 BLOB 256 other 8","title":"Online restrictions for row size"},{"location":"user_guides/fs/feature_group/data_types/#timestamps-and-timezones","text":"All timestamp features are stored in Hopsworks in UTC time. Also, all timestamp-based functions (such as point-in-time joins ) use UTC time. This ensures consistency of timestamp features across different client timezones and simplifies working with timestamp-based functions in general. When ingesting timestamp features, the Feature Store Write API will automatically handle the conversion to UTC, if necessary. The follwing table summarizes how different timestamp types are handled: Data Frame (Data Type) Environment Handling Pandas DataFrame (datetime64[ns]) Python-only and PySpark interpreted as UTC, independent of the client's timezone Pandas DataFrame (datetime64[ns, tz]) Python-only and PySpark timzone-sensitive conversion from 'tz' to UTC Spark (TimestampType) PySpark and Spark interpreted as UTC, independent of the client's timezone Timestamp features retrieved from the Feature Store, e.g. using the Feature Store Read API , use a timezone-unaware format: Data Frame (Data Type) Environment Timezone Pandas DataFrame (datetime64[ns]) Python-only timezone-unaware (UTC) Spark (TimestampType) PySpark and Spark timezone-unaware (UTC) Note that our PySpark/Spark client automatically sets the Spark SQL session's timezone to UTC. This ensures that Spark SQL will correctly interpret all timestamps as UTC. The setting will only apply to the client's session, and you don't have to worry about setting/unsetting the configuration yourself.","title":"Timestamps and Timezones"},{"location":"user_guides/fs/feature_group/data_types/#explicit-schema-definition","text":"When creating a feature group it is possible for the user to control both the offline and online data type of each column. If users explicitly define the schema for the feature group, Hopsworks is going to use that schema to create the feature group, without performing any type mapping. You can explicitly define the feature group schema as follows: Python from hsfs.feature import Feature features = [ Feature ( name = \"id\" , type = \"int\" , online_type = \"int\" ), Feature ( name = \"name\" , type = \"string\" , online_type = \"varchar(20)\" ) ] fg = fs . create_feature_group ( name = \"fg_manual_schema\" , features = features , online_enabled = True ) fg . save ( features )","title":"Explicit schema definition"},{"location":"user_guides/fs/feature_group/data_types/#append-features-to-existing-feature-groups","text":"Hopsworks supports appending additional features to an existing feature group. Adding additional features to an existing feature group is not considered a breaking change. Python from hsfs.feature import Feature features = [ Feature ( name = \"id\" , type = \"int\" , online_type = \"int\" ), Feature ( name = \"name\" , type = \"string\" , online_type = \"varchar(20)\" ) ] fg = fs . get_feature_group ( name = \"example\" , version = 1 ) fg . append_features ( features ) When adding additional features to a feature group, you can provide a default values for existing entries in the feature group. You can also backfill the new features for existing entries by running an insert() operation and update all existing combinations of primary key - event time .","title":"Append features to existing feature groups"},{"location":"user_guides/fs/feature_group/data_validation/","text":"Data Validation # Validation on Insertion with Hopsworks and Great Expectations. Introduction # Clean, high quality feature data is of paramount importance to being able to train and serve high quality models. Hopsworks offers integration with Great Expectations to enable a smooth data validation workflow. This guide is designed to help you integrate a data validation step when inserting new DataFrames into a Feature Group. Note that validation is performed inline as part of your feature pipeline (on the client machine) - it is not executed by Hopsworks after writing features. UI # Create a Feature Group (Pre-requisite) # In the UI, you must create a Feature Group first before attaching an Expectation Suite. You can find out more information about creating a Feature Group here . You can attach at most one expectation suite to a Feature Group. Data validation is an optional step and is not required to write to a Feature Group. Step 1: Find and Edit Feature Group # Click on the Feature Group section in the navigation menu. Find your Feature Group in the list and click on its name to access the Feature Group page. Select edit in the top right corner or scroll to the Expectations section and click on Edit Expectation Suite . Step 2: Edit General Expectation Suite Settings # Scroll to the Expectation Suite section. Click add Expectation Suite and edit its metadata: Choose a name for your expectation suite. Checkbox enabled. This controls whether the Expectation Suite will be used to validate a Dataframe automatically upon insertion into a Feature Group. Note that validation is executed by the client. Disabling validation allows you to skip the validation step without deleting the Expectation Suite. 'ALWAYS' vs. 'STRICT' mode. This option controls what happens after validation. Hopsworks defaults to 'ALWAYS', where data is written to the Feature Group regardless of the validation result. This means that even if expectations are failing or throw an exception, Hopsworks will attempt to insert the data into the Feature Group. In 'STRICT' mode, Hopsworks will only write data to the Feature Group if each individual expectation has been successful. Step 3: Add new expectations # By clicking on Add expectation one can choose an expectation type from a searchable dropdown menu. Currently, only the built-in expectations from the Great Expectations framework are supported. For user-defined expectations, please use the Rest API or python client. All default kwargs associated to the selected expectation type are populated as a json below the dropdown menu. Edit the arguments in the json to configure the Expectation. In particular, arguments such as column , columnA , columnB , column_set and column_list require valid feature name(s). Click the tick button to save the expectation configuration and append it to the Expectation Suite locally. Info Click the Save feature group button to persist your changes! You can use the button Clear Expectation Suite to clean up before saving changes if you changed your mind. If the Expectation Suite is already registered, it will instead show a button to delete the Expectation Suite. Step 4: Save new data to a Feature Group # Use the python client to write a DataFrame to the Feature Group. Note that if an expectation suite is enabled for a Feature Group, calling the insert method will run validation and default to uploading the corresponding validation report to Hopsworks. The report is uploaded even if validation fails and 'STRICT' mode is selected. Step 5: Check Validation Results Summary # Hopsworks shows a visual summary of validation reports. To check it out, go to your Feature Group overview and scroll to the expectation section. Click on the Validation Results tab and check that all went according to plan. Each row corresponds to an expectation in the suite. Features can have several corresponding expectations and the same type of expectation can be applied to different features. You can navigate to older reports using the dropdown menu. Should you need more than the information displayed in the UI for e.g., debugging, the full report can be downloaded by clicking on the corresponding button. Step 6: Check Validation History # The Validation Reports tab in the Expectations section displays a brief history of recent validations. Each row corresponds to a validation report, with some summary information about the success of the validation step. You can download the full report by clicking the download icon button that appears at the end of the row. Code # Hopsworks python client interfaces with the Great Expectations library to enable you to add data validation to your feature engineering pipeline. In this section, we show you how in a single line you enable automatic validation on each insertion of new data into your Feature Group. Whether you have an existing Feature Group you want to add validation to or Follow the guide or get your hands dirty by running our tutorial data validation notebook in google colab. First checkout the pre-requisite and hospworks setup to follow the guide below. Create a project, install the hopsworks client and connect via the generated API key. You are ready to load your data in a DataFrame. The second step is a short introduction to the relevant Great Expectations API to build data validation suited to your data. Third and final step shows how to attach your Expectation Suite to the Feature Group to benefit from automatic validation on insertion capabilities. Step 1: Pre-requisite # In order to define and validate an expectation when writing to a Feature Group, you will need: A Hopsworks project. If you don't have a project yet you can go to managed.hopsworks.ai , signup with your email and create your first project. An API key, you can get one by following the instructions here The hopsworks python library installed in your client Connect your notebook to Hopsworks # Connect the client running your notebooks to Hopsworks. import hopsworks project = hopsworks . login () fs = project . get_feature_store () You will be prompt to paste your API key to connect the notebook to your project. The fs Feature Store entity is now ready to be used to insert or read data from Hopsworks. Import your data # Load your data in a DataFrame using the usual pandas API. import pandas as pd df = pd . read_csv ( \"https://repo.hops.works/master/hopsworks-tutorials/data/card_fraud_data/transactions.csv\" , parse_dates = [ \"datetime\" ]) df . head ( 3 ) Step 2: Great Expectation Introduction # To validate the data, we will use the Great Expectations library. Below is a short introduction on how to build an Expectation Suite to validate your data. Everything is done using the Great Expectations API so you can re-use any prior knowledge you may have of the library. Create an Expectation Suite # Create (or import an existing) expectation suite using the Great Expectations library. This suite will hold all the validation tests we want to perform on our data before inserting them into Hopsworks. import great_expectations as ge expectation_suite = ge . core . ExpectationSuite ( expectation_suite_name = \"validate_on_insert_suite\" ) Add Expectations in the Source Code # Add some expectation to your suite. Each expectation configuration corresponds to a validation test to be run against your data. expectation_suite . add_expectation ( ge . core . ExpectationConfiguration ( expectation_type = \"expect_column_min_to_be_between\" , kwargs = { \"column\" : \"foo_id\" , \"min_value\" : 0 , \"max_value\" : 1 } ) ) expectation_suite . add_expectation ( ge . core . ExpectationConfiguration ( expectation_type = \"expect_column_value_lengths_to_be_between\" , kwargs = { \"column\" : \"bar_name\" , \"min_value\" : 3 , \"max_value\" : 10 } ) ) Using Great Expectations Profiler # Building Expectation Suite by hand can be a major time commitment when you have dozens of Features. Great Expectations offers Profiler classes to inspect a sample of your data and infers a suitable Expectation Suite that you will be able to register with Hopsworks. ge_profiler = ge . profile . BasicSuiteBuilderProfiler () expectation_suite_profiler , _ = ge_profiler . profile ( ge . from_pandas ( df )) Once you have built an Expectation Suite you are satisfied with, it is time to create your first validation enabled Feature Group. Step 3: Attach an Expectation Suite to your Feature Group to enable Automatic Validation on Insertion. # Writing data in Hopsworks is done using Feature Groups. Once a Feature Group is registered in the Feature Store, you can use it to insert your pandas DataFrames. For more information see create Feature Group . To benefit from automatic validation on insertion, attach your newly created Expectation Suite when creating the Feature Group: fg = fs . create_feature_group ( \"fg_with_data_validation\" , version = 1 , description = \"Validated data\" , primary_key = [ 'foo_id' ], online_enabled = False expectation_suite = expectation_suite ) or, if the Feature Group already exist, you can simply run: fg . save_expectation_suite ( expectation_suite ) That is all there is to it. Hopsworks will now automatically use your suite to validate the DataFrames you want to write to the Feature Group. Try it out! job , validation_report = fg . insert ( df . head ( 5 )) As you can see, Hopsworks runs the validation in the client before attempting to insert the data. By default, Hopsworks will try to insert the data even if validation fails to prevent data loss. However it can be configured for production setup to be more restrictive, checkout the data validation advanced guide . Info Note that once the Expectation Suite is attached to the Feature Group, any subsequent attempt to insert to this Feature Group will apply the Data Validation step even from a different client or in a scheduled job. Step 4: Data Quality Monitoring # Upon running validation, Great Expectations generates a report to help you assess the quality of your data. Nothing to do here, Hopsworks client automatically uploads the validation report to the backend when ingesting new data. It enables you to monitor the quality of the inserted data in the Feature Group over time. You can checkout a summary of the reports in the UI on your Feature Group page. As you can see, your Feature Group conveniently gather all in one place: your data, the Expectation Suite and the reports generated each time you inserted data! Hopsworks client API allows you to retrieve validation reports for further analysis. # load multiple reports validation_reports = fg . get_validation_reports () # convenience method for rapid development ge_latest_report = fg . get_latest_validation_report () Similarly you can retrieve the historic of validation results for a particular expectation, e.g to plot a time-series of a given expectation observed value over time. validation_history = fg . get_validation_history ( expectationId = 1 ) You can find the expectationIds in the UI or using fg.get_expectation_suite and looking it up in the expectation's meta field. Info If Validation Reports or Results are too long, they can be truncated to fit in the database. A full version of the reports can be downloaded from the UI. Conclusion # The integration between Hopsworks and Great Expectations makes it simple to add a data validation step to your feature engineering pipeline. Build your Expectation Suite and attach it to your Feature Group with a single line of code. No need to add any code to your pipeline or job scripts, calling fg.insert will now automatically validate the data before inserting them in the Feature Group. The validation reports are stored along your data in Hopsworks allowing us to provide basic monitoring capabilities to quickly spot a data quality issue in the UI. Going further # If you wish to find out more about how to use the data validation API or best practices for development or production pipelines in Hopsworks, checkout the advanced guide .","title":"Data Validation"},{"location":"user_guides/fs/feature_group/data_validation/#data-validation","text":"Validation on Insertion with Hopsworks and Great Expectations.","title":"Data Validation"},{"location":"user_guides/fs/feature_group/data_validation/#introduction","text":"Clean, high quality feature data is of paramount importance to being able to train and serve high quality models. Hopsworks offers integration with Great Expectations to enable a smooth data validation workflow. This guide is designed to help you integrate a data validation step when inserting new DataFrames into a Feature Group. Note that validation is performed inline as part of your feature pipeline (on the client machine) - it is not executed by Hopsworks after writing features.","title":"Introduction"},{"location":"user_guides/fs/feature_group/data_validation/#ui","text":"","title":"UI"},{"location":"user_guides/fs/feature_group/data_validation/#create-a-feature-group-pre-requisite","text":"In the UI, you must create a Feature Group first before attaching an Expectation Suite. You can find out more information about creating a Feature Group here . You can attach at most one expectation suite to a Feature Group. Data validation is an optional step and is not required to write to a Feature Group.","title":"Create a Feature Group (Pre-requisite)"},{"location":"user_guides/fs/feature_group/data_validation/#step-1-find-and-edit-feature-group","text":"Click on the Feature Group section in the navigation menu. Find your Feature Group in the list and click on its name to access the Feature Group page. Select edit in the top right corner or scroll to the Expectations section and click on Edit Expectation Suite .","title":"Step 1: Find and Edit Feature Group"},{"location":"user_guides/fs/feature_group/data_validation/#step-2-edit-general-expectation-suite-settings","text":"Scroll to the Expectation Suite section. Click add Expectation Suite and edit its metadata: Choose a name for your expectation suite. Checkbox enabled. This controls whether the Expectation Suite will be used to validate a Dataframe automatically upon insertion into a Feature Group. Note that validation is executed by the client. Disabling validation allows you to skip the validation step without deleting the Expectation Suite. 'ALWAYS' vs. 'STRICT' mode. This option controls what happens after validation. Hopsworks defaults to 'ALWAYS', where data is written to the Feature Group regardless of the validation result. This means that even if expectations are failing or throw an exception, Hopsworks will attempt to insert the data into the Feature Group. In 'STRICT' mode, Hopsworks will only write data to the Feature Group if each individual expectation has been successful.","title":"Step 2: Edit General Expectation Suite Settings"},{"location":"user_guides/fs/feature_group/data_validation/#step-3-add-new-expectations","text":"By clicking on Add expectation one can choose an expectation type from a searchable dropdown menu. Currently, only the built-in expectations from the Great Expectations framework are supported. For user-defined expectations, please use the Rest API or python client. All default kwargs associated to the selected expectation type are populated as a json below the dropdown menu. Edit the arguments in the json to configure the Expectation. In particular, arguments such as column , columnA , columnB , column_set and column_list require valid feature name(s). Click the tick button to save the expectation configuration and append it to the Expectation Suite locally. Info Click the Save feature group button to persist your changes! You can use the button Clear Expectation Suite to clean up before saving changes if you changed your mind. If the Expectation Suite is already registered, it will instead show a button to delete the Expectation Suite.","title":"Step 3: Add new expectations"},{"location":"user_guides/fs/feature_group/data_validation/#step-4-save-new-data-to-a-feature-group","text":"Use the python client to write a DataFrame to the Feature Group. Note that if an expectation suite is enabled for a Feature Group, calling the insert method will run validation and default to uploading the corresponding validation report to Hopsworks. The report is uploaded even if validation fails and 'STRICT' mode is selected.","title":"Step 4: Save new data to a Feature Group"},{"location":"user_guides/fs/feature_group/data_validation/#step-5-check-validation-results-summary","text":"Hopsworks shows a visual summary of validation reports. To check it out, go to your Feature Group overview and scroll to the expectation section. Click on the Validation Results tab and check that all went according to plan. Each row corresponds to an expectation in the suite. Features can have several corresponding expectations and the same type of expectation can be applied to different features. You can navigate to older reports using the dropdown menu. Should you need more than the information displayed in the UI for e.g., debugging, the full report can be downloaded by clicking on the corresponding button.","title":"Step 5: Check Validation Results Summary"},{"location":"user_guides/fs/feature_group/data_validation/#step-6-check-validation-history","text":"The Validation Reports tab in the Expectations section displays a brief history of recent validations. Each row corresponds to a validation report, with some summary information about the success of the validation step. You can download the full report by clicking the download icon button that appears at the end of the row.","title":"Step 6: Check Validation History"},{"location":"user_guides/fs/feature_group/data_validation/#code","text":"Hopsworks python client interfaces with the Great Expectations library to enable you to add data validation to your feature engineering pipeline. In this section, we show you how in a single line you enable automatic validation on each insertion of new data into your Feature Group. Whether you have an existing Feature Group you want to add validation to or Follow the guide or get your hands dirty by running our tutorial data validation notebook in google colab. First checkout the pre-requisite and hospworks setup to follow the guide below. Create a project, install the hopsworks client and connect via the generated API key. You are ready to load your data in a DataFrame. The second step is a short introduction to the relevant Great Expectations API to build data validation suited to your data. Third and final step shows how to attach your Expectation Suite to the Feature Group to benefit from automatic validation on insertion capabilities.","title":"Code"},{"location":"user_guides/fs/feature_group/data_validation/#step-1-pre-requisite","text":"In order to define and validate an expectation when writing to a Feature Group, you will need: A Hopsworks project. If you don't have a project yet you can go to managed.hopsworks.ai , signup with your email and create your first project. An API key, you can get one by following the instructions here The hopsworks python library installed in your client","title":"Step 1: Pre-requisite"},{"location":"user_guides/fs/feature_group/data_validation/#connect-your-notebook-to-hopsworks","text":"Connect the client running your notebooks to Hopsworks. import hopsworks project = hopsworks . login () fs = project . get_feature_store () You will be prompt to paste your API key to connect the notebook to your project. The fs Feature Store entity is now ready to be used to insert or read data from Hopsworks.","title":"Connect your notebook to Hopsworks"},{"location":"user_guides/fs/feature_group/data_validation/#import-your-data","text":"Load your data in a DataFrame using the usual pandas API. import pandas as pd df = pd . read_csv ( \"https://repo.hops.works/master/hopsworks-tutorials/data/card_fraud_data/transactions.csv\" , parse_dates = [ \"datetime\" ]) df . head ( 3 )","title":"Import your data"},{"location":"user_guides/fs/feature_group/data_validation/#step-2-great-expectation-introduction","text":"To validate the data, we will use the Great Expectations library. Below is a short introduction on how to build an Expectation Suite to validate your data. Everything is done using the Great Expectations API so you can re-use any prior knowledge you may have of the library.","title":"Step 2: Great Expectation Introduction"},{"location":"user_guides/fs/feature_group/data_validation/#create-an-expectation-suite","text":"Create (or import an existing) expectation suite using the Great Expectations library. This suite will hold all the validation tests we want to perform on our data before inserting them into Hopsworks. import great_expectations as ge expectation_suite = ge . core . ExpectationSuite ( expectation_suite_name = \"validate_on_insert_suite\" )","title":"Create an Expectation Suite"},{"location":"user_guides/fs/feature_group/data_validation/#add-expectations-in-the-source-code","text":"Add some expectation to your suite. Each expectation configuration corresponds to a validation test to be run against your data. expectation_suite . add_expectation ( ge . core . ExpectationConfiguration ( expectation_type = \"expect_column_min_to_be_between\" , kwargs = { \"column\" : \"foo_id\" , \"min_value\" : 0 , \"max_value\" : 1 } ) ) expectation_suite . add_expectation ( ge . core . ExpectationConfiguration ( expectation_type = \"expect_column_value_lengths_to_be_between\" , kwargs = { \"column\" : \"bar_name\" , \"min_value\" : 3 , \"max_value\" : 10 } ) )","title":"Add Expectations in the Source Code"},{"location":"user_guides/fs/feature_group/data_validation/#using-great-expectations-profiler","text":"Building Expectation Suite by hand can be a major time commitment when you have dozens of Features. Great Expectations offers Profiler classes to inspect a sample of your data and infers a suitable Expectation Suite that you will be able to register with Hopsworks. ge_profiler = ge . profile . BasicSuiteBuilderProfiler () expectation_suite_profiler , _ = ge_profiler . profile ( ge . from_pandas ( df )) Once you have built an Expectation Suite you are satisfied with, it is time to create your first validation enabled Feature Group.","title":"Using Great Expectations Profiler"},{"location":"user_guides/fs/feature_group/data_validation/#step-3-attach-an-expectation-suite-to-your-feature-group-to-enable-automatic-validation-on-insertion","text":"Writing data in Hopsworks is done using Feature Groups. Once a Feature Group is registered in the Feature Store, you can use it to insert your pandas DataFrames. For more information see create Feature Group . To benefit from automatic validation on insertion, attach your newly created Expectation Suite when creating the Feature Group: fg = fs . create_feature_group ( \"fg_with_data_validation\" , version = 1 , description = \"Validated data\" , primary_key = [ 'foo_id' ], online_enabled = False expectation_suite = expectation_suite ) or, if the Feature Group already exist, you can simply run: fg . save_expectation_suite ( expectation_suite ) That is all there is to it. Hopsworks will now automatically use your suite to validate the DataFrames you want to write to the Feature Group. Try it out! job , validation_report = fg . insert ( df . head ( 5 )) As you can see, Hopsworks runs the validation in the client before attempting to insert the data. By default, Hopsworks will try to insert the data even if validation fails to prevent data loss. However it can be configured for production setup to be more restrictive, checkout the data validation advanced guide . Info Note that once the Expectation Suite is attached to the Feature Group, any subsequent attempt to insert to this Feature Group will apply the Data Validation step even from a different client or in a scheduled job.","title":"Step 3: Attach an Expectation Suite to your Feature Group to enable Automatic Validation on Insertion."},{"location":"user_guides/fs/feature_group/data_validation/#step-4-data-quality-monitoring","text":"Upon running validation, Great Expectations generates a report to help you assess the quality of your data. Nothing to do here, Hopsworks client automatically uploads the validation report to the backend when ingesting new data. It enables you to monitor the quality of the inserted data in the Feature Group over time. You can checkout a summary of the reports in the UI on your Feature Group page. As you can see, your Feature Group conveniently gather all in one place: your data, the Expectation Suite and the reports generated each time you inserted data! Hopsworks client API allows you to retrieve validation reports for further analysis. # load multiple reports validation_reports = fg . get_validation_reports () # convenience method for rapid development ge_latest_report = fg . get_latest_validation_report () Similarly you can retrieve the historic of validation results for a particular expectation, e.g to plot a time-series of a given expectation observed value over time. validation_history = fg . get_validation_history ( expectationId = 1 ) You can find the expectationIds in the UI or using fg.get_expectation_suite and looking it up in the expectation's meta field. Info If Validation Reports or Results are too long, they can be truncated to fit in the database. A full version of the reports can be downloaded from the UI.","title":"Step 4: Data Quality Monitoring"},{"location":"user_guides/fs/feature_group/data_validation/#conclusion","text":"The integration between Hopsworks and Great Expectations makes it simple to add a data validation step to your feature engineering pipeline. Build your Expectation Suite and attach it to your Feature Group with a single line of code. No need to add any code to your pipeline or job scripts, calling fg.insert will now automatically validate the data before inserting them in the Feature Group. The validation reports are stored along your data in Hopsworks allowing us to provide basic monitoring capabilities to quickly spot a data quality issue in the UI.","title":"Conclusion"},{"location":"user_guides/fs/feature_group/data_validation/#going-further","text":"If you wish to find out more about how to use the data validation API or best practices for development or production pipelines in Hopsworks, checkout the advanced guide .","title":"Going further"},{"location":"user_guides/fs/feature_group/deprecation/","text":"How to deprecate a Feature Group # Introduction # To discourage the usage of specific feature groups it is possible to deprecate them. When a feature group is deprecated, user will be warned when they try to use it or use a feature view that depends on it. In this guide you will learn how to deprecate a feature group within Hopsworks, showing examples in HSFS APIs as well as the user interface. Prerequisites # Before you begin this guide it is expected that there is an existing feature group in your project. You can familiarize yourself with the creation of a feature group in the user guide. Deprecate using the HSFS APIs # Retrieve the feature group # To deprecate a feature group using the HSFS APIs you need to provide a Feature Group . Python fg = fs . get_feature_group ( name = \"feature_group_name\" , version = feature_group_version ) Deprecate Feature Group # Feature group deprecation occurs by calling the update_deprecated method on the feature group. Python fg . update_deprecated () Users can also un-deprecate the feature group if need be, by setting the deprecate parameter to False. Python fg . update_deprecated ( deprecate = False ) Deprecate using the UI # You can deprecate/de-deprecate feature groups through the UI. For this, navigate to the Feature Groups section and select a feature group. Subsequently, make sure that the necessary feature group version is picked. Finally, click on the button with three vertical dots in the right corner and select Deprecate . The Feature group can be de-deprecated by selecting the Undeprecate option on a deprecated feature group.","title":"Deprecating"},{"location":"user_guides/fs/feature_group/deprecation/#how-to-deprecate-a-feature-group","text":"","title":"How to deprecate a Feature Group"},{"location":"user_guides/fs/feature_group/deprecation/#introduction","text":"To discourage the usage of specific feature groups it is possible to deprecate them. When a feature group is deprecated, user will be warned when they try to use it or use a feature view that depends on it. In this guide you will learn how to deprecate a feature group within Hopsworks, showing examples in HSFS APIs as well as the user interface.","title":"Introduction"},{"location":"user_guides/fs/feature_group/deprecation/#prerequisites","text":"Before you begin this guide it is expected that there is an existing feature group in your project. You can familiarize yourself with the creation of a feature group in the user guide.","title":"Prerequisites"},{"location":"user_guides/fs/feature_group/deprecation/#deprecate-using-the-hsfs-apis","text":"","title":"Deprecate using the HSFS APIs"},{"location":"user_guides/fs/feature_group/deprecation/#retrieve-the-feature-group","text":"To deprecate a feature group using the HSFS APIs you need to provide a Feature Group . Python fg = fs . get_feature_group ( name = \"feature_group_name\" , version = feature_group_version )","title":"Retrieve the feature group"},{"location":"user_guides/fs/feature_group/deprecation/#deprecate-feature-group","text":"Feature group deprecation occurs by calling the update_deprecated method on the feature group. Python fg . update_deprecated () Users can also un-deprecate the feature group if need be, by setting the deprecate parameter to False. Python fg . update_deprecated ( deprecate = False )","title":"Deprecate Feature Group"},{"location":"user_guides/fs/feature_group/deprecation/#deprecate-using-the-ui","text":"You can deprecate/de-deprecate feature groups through the UI. For this, navigate to the Feature Groups section and select a feature group. Subsequently, make sure that the necessary feature group version is picked. Finally, click on the button with three vertical dots in the right corner and select Deprecate . The Feature group can be de-deprecated by selecting the Undeprecate option on a deprecated feature group.","title":"Deprecate using the UI"},{"location":"user_guides/fs/feature_group/statistics/","text":"How to compute statistics on feature data # Introduction # In this guide you will learn how to configure, compute and visualize statistics for the features registered with Hopsworks. Hopsworks groups features in four categories: Descriptive : These are the basic statistics Hopsworks computes. They include an approximate count of the distinctive values and the completeness (i.e. the percentage of non null values). For numerical features Hopsworks also computes the minimum, maximum, mean, standard deviation and the sum of each feature. Enabled by default. Histograms : Hopsworks computes the distribution of the values of a feature. Exact histograms are computed as long as the number of distinct values is less than 20. If a feature has a numerical data type (e.g. integer, float, double, ...) and has more than 20 unique values, then the values are bucketed in 20 buckets and the histogram represents the distribution of values in those buckets. By default histograms are disabled. Correlation : If enabled, Hopsworks computes the Pearson correlation between features of numerical data type within a feature group. By default correlation is disabled. Exact Statistics : Exact statistics are an enhancement of the descriptive statistics that provide an exact count of distinctive values, entropy, uniqueness and distinctiveness of the value of a feature. These statistics are more expensive to compute as they take into consideration all the values and they don't use approximations. By default they are disabled. When statistics are enabled, they are computed every time new data is written into the offline storage of a feature group. Statistics are then displayed on the Hopsworks UI and users can track how data has changed over time. Prerequisites # Before you begin this guide we suggest you read the Feature Group concept page to understand what a feature group is and how it fits in the ML pipeline. We also suggest you familiarize with the APIs to create a feature group . Enable statistics when creating a feature group # As mentioned above, by default only descriptive statistics are enabled when creating a feature group. To enable histograms, correlations or exact statistics the statistics_config configuration parameter can be provided in the create statement. The statistics_config parameter takes a dictionary with the keys: enabled , correlations , histograms and exact_uniqueness and, as values, a boolean to describe whether or not to compute the specific class of statistics. Additionally it is possible to restrict the statistics computation to only a subset of columns. This is configurable by adding a columns key to the statistics_config parameter. The key should contain the list of columns for which to compute statistics. By default the value is empty list [] and the statistics are computed for all columns in the feature group. Python fg = feature_store . create_feature_group ( name = \"weather\" , version = 1 , description = \"Weather Features\" , online_enabled = True , primary_key = [ 'location_id' ], partition_key = [ 'day' ], event_time = 'event_time' , statistics_config = { \"enabled\" : True , \"histograms\" : True , \"correlations\" : True , \"exact_uniqueness\" : False , \"columns\" : [] } ) Enable statistics after creating a feature group # It is possible users to change the statistics configuration after a feature group was created. Either to add or remove a class of statistics, or to change the set of features for which to compute statistics. Python fg . statistics_config = { \"enabled\" : True , \"histograms\" : False , \"correlations\" : False , \"exact_uniqueness\" : False \"columns\" : [ 'location_id' , 'min_temp' , 'max_temp' ] } fg . update_statistics_config () Explicitly compute statistics # As mentioned above, the statistics are computed every time new data is written into the offline storage of a feature group. By invoking the compute_statistics method, users can trigger explicitly the statistics computation for the data available in a feature group. This is useful when a feature group is receiving frequent updates. Users can schedule periodic statistics computation that take into consideration several data commits. By default, the compute_statistics method computes statistics on the most recent version of the data available in a feature group. Users can provide a specific time using the wallclock_time parameter, to compute the statistics for a previous version of the data. Hopsworks can compute statistics of external feature groups. As external feature groups are read only from an Hopsworks perspective, statistics computation can be triggered using the compute_statistics method. Python fg . compute_statistics ( wallclock_time = '20220611 20:00' ) Inspect statistics # You can also create a new feature group through the UI.","title":"Statistics"},{"location":"user_guides/fs/feature_group/statistics/#how-to-compute-statistics-on-feature-data","text":"","title":"How to compute statistics on feature data"},{"location":"user_guides/fs/feature_group/statistics/#introduction","text":"In this guide you will learn how to configure, compute and visualize statistics for the features registered with Hopsworks. Hopsworks groups features in four categories: Descriptive : These are the basic statistics Hopsworks computes. They include an approximate count of the distinctive values and the completeness (i.e. the percentage of non null values). For numerical features Hopsworks also computes the minimum, maximum, mean, standard deviation and the sum of each feature. Enabled by default. Histograms : Hopsworks computes the distribution of the values of a feature. Exact histograms are computed as long as the number of distinct values is less than 20. If a feature has a numerical data type (e.g. integer, float, double, ...) and has more than 20 unique values, then the values are bucketed in 20 buckets and the histogram represents the distribution of values in those buckets. By default histograms are disabled. Correlation : If enabled, Hopsworks computes the Pearson correlation between features of numerical data type within a feature group. By default correlation is disabled. Exact Statistics : Exact statistics are an enhancement of the descriptive statistics that provide an exact count of distinctive values, entropy, uniqueness and distinctiveness of the value of a feature. These statistics are more expensive to compute as they take into consideration all the values and they don't use approximations. By default they are disabled. When statistics are enabled, they are computed every time new data is written into the offline storage of a feature group. Statistics are then displayed on the Hopsworks UI and users can track how data has changed over time.","title":"Introduction"},{"location":"user_guides/fs/feature_group/statistics/#prerequisites","text":"Before you begin this guide we suggest you read the Feature Group concept page to understand what a feature group is and how it fits in the ML pipeline. We also suggest you familiarize with the APIs to create a feature group .","title":"Prerequisites"},{"location":"user_guides/fs/feature_group/statistics/#enable-statistics-when-creating-a-feature-group","text":"As mentioned above, by default only descriptive statistics are enabled when creating a feature group. To enable histograms, correlations or exact statistics the statistics_config configuration parameter can be provided in the create statement. The statistics_config parameter takes a dictionary with the keys: enabled , correlations , histograms and exact_uniqueness and, as values, a boolean to describe whether or not to compute the specific class of statistics. Additionally it is possible to restrict the statistics computation to only a subset of columns. This is configurable by adding a columns key to the statistics_config parameter. The key should contain the list of columns for which to compute statistics. By default the value is empty list [] and the statistics are computed for all columns in the feature group. Python fg = feature_store . create_feature_group ( name = \"weather\" , version = 1 , description = \"Weather Features\" , online_enabled = True , primary_key = [ 'location_id' ], partition_key = [ 'day' ], event_time = 'event_time' , statistics_config = { \"enabled\" : True , \"histograms\" : True , \"correlations\" : True , \"exact_uniqueness\" : False , \"columns\" : [] } )","title":"Enable statistics when creating a feature group"},{"location":"user_guides/fs/feature_group/statistics/#enable-statistics-after-creating-a-feature-group","text":"It is possible users to change the statistics configuration after a feature group was created. Either to add or remove a class of statistics, or to change the set of features for which to compute statistics. Python fg . statistics_config = { \"enabled\" : True , \"histograms\" : False , \"correlations\" : False , \"exact_uniqueness\" : False \"columns\" : [ 'location_id' , 'min_temp' , 'max_temp' ] } fg . update_statistics_config ()","title":"Enable statistics after creating a feature group"},{"location":"user_guides/fs/feature_group/statistics/#explicitly-compute-statistics","text":"As mentioned above, the statistics are computed every time new data is written into the offline storage of a feature group. By invoking the compute_statistics method, users can trigger explicitly the statistics computation for the data available in a feature group. This is useful when a feature group is receiving frequent updates. Users can schedule periodic statistics computation that take into consideration several data commits. By default, the compute_statistics method computes statistics on the most recent version of the data available in a feature group. Users can provide a specific time using the wallclock_time parameter, to compute the statistics for a previous version of the data. Hopsworks can compute statistics of external feature groups. As external feature groups are read only from an Hopsworks perspective, statistics computation can be triggered using the compute_statistics method. Python fg . compute_statistics ( wallclock_time = '20220611 20:00' )","title":"Explicitly compute statistics"},{"location":"user_guides/fs/feature_group/statistics/#inspect-statistics","text":"You can also create a new feature group through the UI.","title":"Inspect statistics"},{"location":"user_guides/fs/feature_view/","text":"Feature View User Guides # This section serves to provide guides and examples for the common usage of abstractions and functionality of the Feature View through the Hopsworks UI and APIs. Create a Feature View Create a Training Data with different splits Batch Data Feature Vectors Feature Server Query Transformation Functions","title":"Feature View User Guides"},{"location":"user_guides/fs/feature_view/#feature-view-user-guides","text":"This section serves to provide guides and examples for the common usage of abstractions and functionality of the Feature View through the Hopsworks UI and APIs. Create a Feature View Create a Training Data with different splits Batch Data Feature Vectors Feature Server Query Transformation Functions","title":"Feature View User Guides"},{"location":"user_guides/fs/feature_view/batch-data/","text":"Batch data (analytical ML systems) # Creation # It is very common that ML models are deployed in a \"batch\" setting where ML pipelines score incoming new data at a regular interval, for example, daily or weekly. Feature views support batch prediction by returning batch data as a DataFrame over a time range, by start_time and end_time . The resultant DataFrame (or batch-scoring DataFrame) can then be fed to models to make predictions. Python Java # get batch data df = feature_view . get_batch_data ( start_time = \"20220620\" , end_time = \"20220627\" ) # return a dataframe Dataset < Row > ds = featureView . getBatchData ( \"20220620\" , \"20220627\" ) For Python-clients, handling small or moderately-sized data, we recommend enabling the ArrowFlight Server with DuckDB , which will provide significant speedups over Spark/Hive for reading batch data. If the service is enabled, and you want to read this particular batch data with Hive instead, you can set the read_options to {\"use_hive\": True} . # get batch data with Hive df = feature_view . get_batch_data ( start_time = \"20220620\" , end_time = \"20220627\" , read_options = { \"use_hive: True}) ) Creation with transformation # If you have specified transformation functions when creating a feature view, you will get back transformed batch data as well. If your transformation functions require statistics of training dataset, you must also provide the training data version. init_batch_scoring will then fetch the statistics and initialize the functions with required statistics. Then you can follow the above examples and create the batch data. Please note that transformed batch data can only be returned in the python client but not in the java client. feature_view . init_batch_scoring ( training_dataset_version = 1 ) It is important to note that in addition to the filters defined in feature view, extra filters will be applied if they are defined in the given training dataset version.","title":"Batch data"},{"location":"user_guides/fs/feature_view/batch-data/#batch-data-analytical-ml-systems","text":"","title":"Batch data (analytical ML systems)"},{"location":"user_guides/fs/feature_view/batch-data/#creation","text":"It is very common that ML models are deployed in a \"batch\" setting where ML pipelines score incoming new data at a regular interval, for example, daily or weekly. Feature views support batch prediction by returning batch data as a DataFrame over a time range, by start_time and end_time . The resultant DataFrame (or batch-scoring DataFrame) can then be fed to models to make predictions. Python Java # get batch data df = feature_view . get_batch_data ( start_time = \"20220620\" , end_time = \"20220627\" ) # return a dataframe Dataset < Row > ds = featureView . getBatchData ( \"20220620\" , \"20220627\" ) For Python-clients, handling small or moderately-sized data, we recommend enabling the ArrowFlight Server with DuckDB , which will provide significant speedups over Spark/Hive for reading batch data. If the service is enabled, and you want to read this particular batch data with Hive instead, you can set the read_options to {\"use_hive\": True} . # get batch data with Hive df = feature_view . get_batch_data ( start_time = \"20220620\" , end_time = \"20220627\" , read_options = { \"use_hive: True}) )","title":"Creation"},{"location":"user_guides/fs/feature_view/batch-data/#creation-with-transformation","text":"If you have specified transformation functions when creating a feature view, you will get back transformed batch data as well. If your transformation functions require statistics of training dataset, you must also provide the training data version. init_batch_scoring will then fetch the statistics and initialize the functions with required statistics. Then you can follow the above examples and create the batch data. Please note that transformed batch data can only be returned in the python client but not in the java client. feature_view . init_batch_scoring ( training_dataset_version = 1 ) It is important to note that in addition to the filters defined in feature view, extra filters will be applied if they are defined in the given training dataset version.","title":"Creation with transformation"},{"location":"user_guides/fs/feature_view/feature-server/","text":"Feature Store REST API Server # This API server allows users to retrieve single/batch feature vectors from a feature view. How to use # Hopsworks 3.3 includes a preview of the feature store REST API, version 0.1.0. By default, the server listens on the 0.0.0.0:4406 . Please refer to /srv/hops/mysql-cluster/rdrs_config.json config file located on machines running the REST Server for additional configuration parameters. Single feature vector # Request # POST /{api-version}/feature_store Body { \"featureStoreName\": \"fsdb002\", \"featureViewName\": \"sample_2\", \"featureViewVersion\": 1, \"passedFeatures\": {}, \"entries\": { \"id1\": 36 }, \"metadataOptions\": { \"featureName\": true, \"featureType\": true } } Parameters parameter type note featureStoreName string featureViewName string featureViewVersion number(int) entries objects Map of serving key of feature view as key and value of serving key as value. Serving key are a set of the primary key of feature groups which are included in the feature view query. If feature groups are joint with prefix, the primary key needs to be attached with prefix. passedFeatures objects Optional. Map of feature name as key and feature value as value. This overwrites feature values in the response. metadataOptions objects Optional. Map of metadataoption as key and boolean as value. Default metadata option is false. Metadata is returned on request. Metadata options available: 1. featureName 2. featureType Response # { \"features\": [ 36, \"2022-01-24\", \"int24\", \"str14\" ], \"metadata\": [ { \"featureName\": \"id1\", \"featureType\": \"bigint\" }, { \"featureName\": \"ts\", \"featureType\": \"date\" }, { \"featureName\": \"data1\", \"featureType\": \"string\" }, { \"featureName\": \"data2\", \"featureType\": \"string\" } ], \"status\": \"COMPLETE\" } Error handling # Code reason response 200 400 Requested metadata does not exist 400 Error in pk or passed feature value 401 Access denied Access unshared feature store failed 500 Failed to read feature store metadata Response with pk/pass feature error { \"code\": 12, \"message\": \"Wrong primay-key column. Column: ts\", \"reason\": \"Incorrect primary key.\" } Response with metadata error { \"code\": 2, \"message\": \"\", \"reason\": \"Feature store does not exist.\" } Pk value no match { \"features\": [ 9876543, null, null, null ], \"metadata\": null, \"status\": \"MISSING\" } Batch feature vectors # Request # POST /{api-version}/batch_feature_store Body { \"featureStoreName\": \"fsdb002\", \"featureViewName\": \"sample_2\", \"featureViewVersion\": 1, \"passedFeatures\": [], \"entries\": [ { \"id1\": 16 }, { \"id1\": 36 }, { \"id1\": 71 }, { \"id1\": 48 }, { \"id1\": 29 } ], \"requestId\": null, \"metadataOptions\": { \"featureName\": true, \"featureType\": true } } Parameters parameter type note featureStoreName string featureViewName string featureViewVersion number(int) entries array<objects> Each items is a map of serving key as key and value of serving key as value. Serving key of feature view. passedFeatures array<objects> Optional. Each items is a map of feature name as key and feature value as value. This overwrites feature values in the response. If provided, its size and order has to be equal to the size of entries. Item can be null. metadataOptions objects Optional. Map of metadataoption as key and boolean as value. Default metadata option is false. Metadata is returned on request. Metadata options available: 1. featureName 2. featureType Response # { \"features\": [ [ 16, \"2022-01-27\", \"int31\", \"str24\" ], [ 36, \"2022-01-24\", \"int24\", \"str14\" ], [ 71, \"2022-01-22\", \"int3\", \"str97\" ], [ 48, \"2022-01-26\", \"int92\", \"str31\" ], [ 29, \"2022-01-03\", \"int53\", \"str91\" ] ], \"metadata\": [ { \"featureName\": \"id1\", \"featureType\": \"bigint\" }, { \"featureName\": \"ts\", \"featureType\": \"date\" }, { \"featureName\": \"data1\", \"featureType\": \"string\" }, { \"featureName\": \"data2\", \"featureType\": \"string\" } ], \"status\": [ \"COMPLETE\", \"COMPLETE\", \"COMPLETE\", \"COMPLETE\", \"COMPLETE\" ] } note: Order of the returned features are the same as the order of entries in the request. Error handling # Code reason response 200 400 Requested metadata does not exist 401 Access denied Access unshared feature store failed 500 Failed to read feature store metadata Response with partial failure { \"features\": [ [ 81, \"id81\", \"2022-01-29 00:00:00\", 6 ], null, [ 51, \"id51\", \"2022-01-10 00:00:00\", 49 ] ], \"metadata\": null, \"status\": [ \"COMPLETE\", \"ERROR\", \"COMPLETE\" ] } Access control to feature store # Currently, the REST API server only supports Hopsworks API Keys for authentication and authorization. Add the API key to the HTTP requests using the X-API-KEY header.","title":"Feature server"},{"location":"user_guides/fs/feature_view/feature-server/#feature-store-rest-api-server","text":"This API server allows users to retrieve single/batch feature vectors from a feature view.","title":"Feature Store REST API Server"},{"location":"user_guides/fs/feature_view/feature-server/#how-to-use","text":"Hopsworks 3.3 includes a preview of the feature store REST API, version 0.1.0. By default, the server listens on the 0.0.0.0:4406 . Please refer to /srv/hops/mysql-cluster/rdrs_config.json config file located on machines running the REST Server for additional configuration parameters.","title":"How to use"},{"location":"user_guides/fs/feature_view/feature-server/#single-feature-vector","text":"","title":"Single feature vector"},{"location":"user_guides/fs/feature_view/feature-server/#request","text":"POST /{api-version}/feature_store Body { \"featureStoreName\": \"fsdb002\", \"featureViewName\": \"sample_2\", \"featureViewVersion\": 1, \"passedFeatures\": {}, \"entries\": { \"id1\": 36 }, \"metadataOptions\": { \"featureName\": true, \"featureType\": true } } Parameters parameter type note featureStoreName string featureViewName string featureViewVersion number(int) entries objects Map of serving key of feature view as key and value of serving key as value. Serving key are a set of the primary key of feature groups which are included in the feature view query. If feature groups are joint with prefix, the primary key needs to be attached with prefix. passedFeatures objects Optional. Map of feature name as key and feature value as value. This overwrites feature values in the response. metadataOptions objects Optional. Map of metadataoption as key and boolean as value. Default metadata option is false. Metadata is returned on request. Metadata options available: 1. featureName 2. featureType","title":"Request"},{"location":"user_guides/fs/feature_view/feature-server/#response","text":"{ \"features\": [ 36, \"2022-01-24\", \"int24\", \"str14\" ], \"metadata\": [ { \"featureName\": \"id1\", \"featureType\": \"bigint\" }, { \"featureName\": \"ts\", \"featureType\": \"date\" }, { \"featureName\": \"data1\", \"featureType\": \"string\" }, { \"featureName\": \"data2\", \"featureType\": \"string\" } ], \"status\": \"COMPLETE\" }","title":"Response"},{"location":"user_guides/fs/feature_view/feature-server/#error-handling","text":"Code reason response 200 400 Requested metadata does not exist 400 Error in pk or passed feature value 401 Access denied Access unshared feature store failed 500 Failed to read feature store metadata Response with pk/pass feature error { \"code\": 12, \"message\": \"Wrong primay-key column. Column: ts\", \"reason\": \"Incorrect primary key.\" } Response with metadata error { \"code\": 2, \"message\": \"\", \"reason\": \"Feature store does not exist.\" } Pk value no match { \"features\": [ 9876543, null, null, null ], \"metadata\": null, \"status\": \"MISSING\" }","title":"Error handling"},{"location":"user_guides/fs/feature_view/feature-server/#batch-feature-vectors","text":"","title":"Batch feature vectors"},{"location":"user_guides/fs/feature_view/feature-server/#request_1","text":"POST /{api-version}/batch_feature_store Body { \"featureStoreName\": \"fsdb002\", \"featureViewName\": \"sample_2\", \"featureViewVersion\": 1, \"passedFeatures\": [], \"entries\": [ { \"id1\": 16 }, { \"id1\": 36 }, { \"id1\": 71 }, { \"id1\": 48 }, { \"id1\": 29 } ], \"requestId\": null, \"metadataOptions\": { \"featureName\": true, \"featureType\": true } } Parameters parameter type note featureStoreName string featureViewName string featureViewVersion number(int) entries array<objects> Each items is a map of serving key as key and value of serving key as value. Serving key of feature view. passedFeatures array<objects> Optional. Each items is a map of feature name as key and feature value as value. This overwrites feature values in the response. If provided, its size and order has to be equal to the size of entries. Item can be null. metadataOptions objects Optional. Map of metadataoption as key and boolean as value. Default metadata option is false. Metadata is returned on request. Metadata options available: 1. featureName 2. featureType","title":"Request"},{"location":"user_guides/fs/feature_view/feature-server/#response_1","text":"{ \"features\": [ [ 16, \"2022-01-27\", \"int31\", \"str24\" ], [ 36, \"2022-01-24\", \"int24\", \"str14\" ], [ 71, \"2022-01-22\", \"int3\", \"str97\" ], [ 48, \"2022-01-26\", \"int92\", \"str31\" ], [ 29, \"2022-01-03\", \"int53\", \"str91\" ] ], \"metadata\": [ { \"featureName\": \"id1\", \"featureType\": \"bigint\" }, { \"featureName\": \"ts\", \"featureType\": \"date\" }, { \"featureName\": \"data1\", \"featureType\": \"string\" }, { \"featureName\": \"data2\", \"featureType\": \"string\" } ], \"status\": [ \"COMPLETE\", \"COMPLETE\", \"COMPLETE\", \"COMPLETE\", \"COMPLETE\" ] } note: Order of the returned features are the same as the order of entries in the request.","title":"Response"},{"location":"user_guides/fs/feature_view/feature-server/#error-handling_1","text":"Code reason response 200 400 Requested metadata does not exist 401 Access denied Access unshared feature store failed 500 Failed to read feature store metadata Response with partial failure { \"features\": [ [ 81, \"id81\", \"2022-01-29 00:00:00\", 6 ], null, [ 51, \"id51\", \"2022-01-10 00:00:00\", 49 ] ], \"metadata\": null, \"status\": [ \"COMPLETE\", \"ERROR\", \"COMPLETE\" ] }","title":"Error handling"},{"location":"user_guides/fs/feature_view/feature-server/#access-control-to-feature-store","text":"Currently, the REST API server only supports Hopsworks API Keys for authentication and authorization. Add the API key to the HTTP requests using the X-API-KEY header.","title":"Access control to feature store"},{"location":"user_guides/fs/feature_view/feature-vectors/","text":"Feature Vectors # Once you have trained a model, it is time to deploy it. You can get back all the features required to feed into an ML model with a single method call. A feature view provides great flexibility for you to retrieve a vector (or row) of features from any environment, whether you are either inside the Hopsworks platform, a model serving platform, or in an external environment, such as your application server. Harnessing the powerful RonDB , feature vectors are served at in-memory latency. If you want to understand more about the concept of feature vectors, you can refer to here . Retrieval # You can get back feature vectors from either python or java client by providing the primary key value(s) for the feature view. Note that filters defined in feature view and training data will not be applied when feature vectors are returned. Python Java # get a single vector feature_view . get_feature_vector ( entry = { \"pk1\" : 1 , \"pk2\" : 2 } ) # get multiple vectors feature_view . get_feature_vectors ( entry = [ { \"pk1\" : 1 , \"pk2\" : 2 }, { \"pk1\" : 3 , \"pk2\" : 4 }, { \"pk1\" : 5 , \"pk2\" : 6 } ] ) // get a single vector Map < String , Object > entry1 = Maps . newHashMap (); entry1 . put ( \"pk1\" , 1 ); entry1 . put ( \"pk2\" , 2 ); featureView . getFeatureVector ( entry1 ); // get multiple vectors Map < String , Object > entry2 = Maps . newHashMap (); entry2 . put ( \"pk1\" , 3 ); entry2 . put ( \"pk2\" , 4 ); featureView . getFeatureVectors ( Lists . newArrayList ( entry1 , entry2 ); Missing Primary Key Entries # It can happen that some of the primary key entries are not available in some or all of the feature groups used by a feature view. Take the above example assuming the feature view consists of two joined feature groups, first one with primary key column pk1 , the second feature group with primary key column pk2 . # get a single vector feature_view . get_feature_vector ( entry = { \"pk1\" : 1 , \"pk2\" : 2 } ) This call will raise an exception if pk1 = 1 OR pk2 = 2 can't be found but also if pk1 = 1 AND pk2 = 2 can't be found, meaning, it will not return a partial or empty feature vector. When retrieving a batch of vectors, the behaviour is slightly different. # get multiple vectors feature_view . get_feature_vectors ( entry = [ { \"pk1\" : 1 , \"pk2\" : 2 }, { \"pk1\" : 3 , \"pk2\" : 4 }, { \"pk1\" : 5 , \"pk2\" : 6 } ] ) This call will raise an exception if for example for the third entry pk1 = 5 OR pk2 = 6 can't be found, however, it will simply not return a vector for this entry if pk1 = 5 AND pk2 = 6 can't be found. That means, get_feature_vectors will never return partial feature vector, but will omit empty feature vectors. If you are aware of missing featurs, you can use the passed features functionality, described down below. Retrieval with transformation # If you have specified transformation functions when creating a feature view, you receive transformed feature vectors. If your transformation functions require statistics of training dataset, you must also provide the training data version. init_serving will then fetch the statistics and initialize the functions with the required statistics. Then you can follow the above examples and retrieve the feature vectors. Please note that transformed feature vectors can only be returned in the python client but not in the java client. feature_view . init_serving ( training_dataset_version = 1 ) Passed features # If some of the features values are only known at prediction time and cannot be computed and cached in the online feature store, you can provide those values as passed_features option. The get_feature_vector method is going to use the passed values to construct the final feature vector to submit to the model. You can use the passed_features parameter to overwrite individual features being retrieved from the online feature store. The feature view will apply the necessary transformations to the passed features as it does for the feature data retrieved from the online feature store. Python # get a single vector feature_view . get_feature_vector ( entry = { \"pk1\" : 1 , \"pk2\" : 2 }, passed_features = { \"feature_a\" : \"value_a\" } ) # get multiple vectors feature_view . get_feature_vectors ( entry = [ { \"pk1\" : 1 , \"pk2\" : 2 }, { \"pk1\" : 3 , \"pk2\" : 4 }, { \"pk1\" : 5 , \"pk2\" : 6 } ], passed_features = [ { \"feature_a\" : \"value_a1\" }, { \"feature_a\" : \"value_a2\" }, { \"feature_a\" : \"value_a3\" }, ] ) You can also use the parameter to provide values for all the features which are part of a specific feature group and used in the feature view. In this second case, you do not have to provide the primary key value for that feature group as no data needs to be retrieved from the online feature store. Python # get a single vector, replace values from an entire feature group # note how in this example you don't have to provide the value of # pk2, but you need to provide the features coming from that feature group # in this case feature_b and feature_c feature_view . get_feature_vector ( entry = { \"pk1\" : 1 }, passed_features = { \"feature_a\" : \"value_a\" , \"feature_b\" : \"value_b\" , \"feature_c\" : \"value_c\" } ) Feature Server # In addition to Python/Java clients, from Hopsworks 3.3, a new feature server implemented in Go is introduced. With this new API, single or batch feature vectors can be retrieved in any programming language.","title":"Feature vectors"},{"location":"user_guides/fs/feature_view/feature-vectors/#feature-vectors","text":"Once you have trained a model, it is time to deploy it. You can get back all the features required to feed into an ML model with a single method call. A feature view provides great flexibility for you to retrieve a vector (or row) of features from any environment, whether you are either inside the Hopsworks platform, a model serving platform, or in an external environment, such as your application server. Harnessing the powerful RonDB , feature vectors are served at in-memory latency. If you want to understand more about the concept of feature vectors, you can refer to here .","title":"Feature Vectors"},{"location":"user_guides/fs/feature_view/feature-vectors/#retrieval","text":"You can get back feature vectors from either python or java client by providing the primary key value(s) for the feature view. Note that filters defined in feature view and training data will not be applied when feature vectors are returned. Python Java # get a single vector feature_view . get_feature_vector ( entry = { \"pk1\" : 1 , \"pk2\" : 2 } ) # get multiple vectors feature_view . get_feature_vectors ( entry = [ { \"pk1\" : 1 , \"pk2\" : 2 }, { \"pk1\" : 3 , \"pk2\" : 4 }, { \"pk1\" : 5 , \"pk2\" : 6 } ] ) // get a single vector Map < String , Object > entry1 = Maps . newHashMap (); entry1 . put ( \"pk1\" , 1 ); entry1 . put ( \"pk2\" , 2 ); featureView . getFeatureVector ( entry1 ); // get multiple vectors Map < String , Object > entry2 = Maps . newHashMap (); entry2 . put ( \"pk1\" , 3 ); entry2 . put ( \"pk2\" , 4 ); featureView . getFeatureVectors ( Lists . newArrayList ( entry1 , entry2 );","title":"Retrieval"},{"location":"user_guides/fs/feature_view/feature-vectors/#missing-primary-key-entries","text":"It can happen that some of the primary key entries are not available in some or all of the feature groups used by a feature view. Take the above example assuming the feature view consists of two joined feature groups, first one with primary key column pk1 , the second feature group with primary key column pk2 . # get a single vector feature_view . get_feature_vector ( entry = { \"pk1\" : 1 , \"pk2\" : 2 } ) This call will raise an exception if pk1 = 1 OR pk2 = 2 can't be found but also if pk1 = 1 AND pk2 = 2 can't be found, meaning, it will not return a partial or empty feature vector. When retrieving a batch of vectors, the behaviour is slightly different. # get multiple vectors feature_view . get_feature_vectors ( entry = [ { \"pk1\" : 1 , \"pk2\" : 2 }, { \"pk1\" : 3 , \"pk2\" : 4 }, { \"pk1\" : 5 , \"pk2\" : 6 } ] ) This call will raise an exception if for example for the third entry pk1 = 5 OR pk2 = 6 can't be found, however, it will simply not return a vector for this entry if pk1 = 5 AND pk2 = 6 can't be found. That means, get_feature_vectors will never return partial feature vector, but will omit empty feature vectors. If you are aware of missing featurs, you can use the passed features functionality, described down below.","title":"Missing Primary Key Entries"},{"location":"user_guides/fs/feature_view/feature-vectors/#retrieval-with-transformation","text":"If you have specified transformation functions when creating a feature view, you receive transformed feature vectors. If your transformation functions require statistics of training dataset, you must also provide the training data version. init_serving will then fetch the statistics and initialize the functions with the required statistics. Then you can follow the above examples and retrieve the feature vectors. Please note that transformed feature vectors can only be returned in the python client but not in the java client. feature_view . init_serving ( training_dataset_version = 1 )","title":"Retrieval with transformation"},{"location":"user_guides/fs/feature_view/feature-vectors/#passed-features","text":"If some of the features values are only known at prediction time and cannot be computed and cached in the online feature store, you can provide those values as passed_features option. The get_feature_vector method is going to use the passed values to construct the final feature vector to submit to the model. You can use the passed_features parameter to overwrite individual features being retrieved from the online feature store. The feature view will apply the necessary transformations to the passed features as it does for the feature data retrieved from the online feature store. Python # get a single vector feature_view . get_feature_vector ( entry = { \"pk1\" : 1 , \"pk2\" : 2 }, passed_features = { \"feature_a\" : \"value_a\" } ) # get multiple vectors feature_view . get_feature_vectors ( entry = [ { \"pk1\" : 1 , \"pk2\" : 2 }, { \"pk1\" : 3 , \"pk2\" : 4 }, { \"pk1\" : 5 , \"pk2\" : 6 } ], passed_features = [ { \"feature_a\" : \"value_a1\" }, { \"feature_a\" : \"value_a2\" }, { \"feature_a\" : \"value_a3\" }, ] ) You can also use the parameter to provide values for all the features which are part of a specific feature group and used in the feature view. In this second case, you do not have to provide the primary key value for that feature group as no data needs to be retrieved from the online feature store. Python # get a single vector, replace values from an entire feature group # note how in this example you don't have to provide the value of # pk2, but you need to provide the features coming from that feature group # in this case feature_b and feature_c feature_view . get_feature_vector ( entry = { \"pk1\" : 1 }, passed_features = { \"feature_a\" : \"value_a\" , \"feature_b\" : \"value_b\" , \"feature_c\" : \"value_c\" } )","title":"Passed features"},{"location":"user_guides/fs/feature_view/feature-vectors/#feature-server","text":"In addition to Python/Java clients, from Hopsworks 3.3, a new feature server implemented in Go is introduced. With this new API, single or batch feature vectors can be retrieved in any programming language.","title":"Feature Server"},{"location":"user_guides/fs/feature_view/overview/","text":"Feature View # A feature view is a set of features that come from one or more feature groups. It is a logical view over the feature groups, as the feature data is only stored in feature groups. Feature views are used to read feature data for both training and serving (online and batch). You can create training datasets , create batch data and get feature vectors . If you want to understand more about the concept of feature view, you can refer to here . Creation # Query and transformation function are the building blocks of a feature view. You can define your set of features by building a query . You can also define which columns in your feature view are the labels , which is useful for supervised machine learning tasks. Furthermore, in python client, each feature can be attached to its own transformation function. This way, when a feature is read (for training or scoring), the transformation is executed on-demand - just before the feature data is returned. For example, when a client reads a numerical feature, the feature value could be normalized by a StandardScalar transformation function before it is returned to the client. Python Java # create a simple feature view feature_view = fs . create_feature_view ( name = 'transactions_view' , query = query ) # create a feature view with transformation and label feature_view = fs . create_feature_view ( name = 'transactions_view' , query = query , labels = [ \"fraud_label\" ], transformation_functions = { \"amount\" : fs . get_transformation_function ( name = \"standard_scaler\" , version = 1 ) } ) // create a simple feature view FeatureView featureView = featureStore . createFeatureView () . name ( \"transactions_view) .query(query) .build(); // create a feature view with label FeatureView featureView = featureStore.createFeatureView() .name(\" transactions_view ) . query ( query ) . labels ( Lists . newArrayList ( \"fraud_label\" ) . build (); You can refer to query and transformation function for creating query and transformation_function . To see a full example of how to create a feature view, you can read this notebook . Retrieval # Once you have created a feature view, you can retrieve it by its name and version. Python Java feature_view = fs . get_feature_view ( name = \"transactions_view\" , version = 1 ) FeatureView featureView = featureStore . getFeatureView ( \"transactions_view\" , 1 ) Deletion # If there are some feature view instances which you do not use anymore, you can delete a feature view. It is important to mention that all training datasets (include all materialised hopsfs training data) will be deleted along with the feature view. Python Java feature_view . delete () featureView . delete () Tags # Feature views also support tags. You can attach, get, and remove tags. You can refer to here if you want to learn more about how tags work. Python Java # attach feature_view . add_tag ( name = \"tag_schema\" , value = { \"key\" , \"value\" } # get feature_view . get_tag ( name = \"tag_schema\" ) #remove feature_view . delete_tag ( name = \"tag_schema\" ) // attach Map < String , String > tag = Maps . newHashMap (); tag . put ( \"key\" , \"value\" ); featureView . addTag ( \"tag_schema\" , tag ) // get featureView . getTag ( \"tag_schema\" ) // remove featureView . deleteTag ( \"tag_schema\" ) Next # Once you have created a feature view, you can now create training data","title":"Overview"},{"location":"user_guides/fs/feature_view/overview/#feature-view","text":"A feature view is a set of features that come from one or more feature groups. It is a logical view over the feature groups, as the feature data is only stored in feature groups. Feature views are used to read feature data for both training and serving (online and batch). You can create training datasets , create batch data and get feature vectors . If you want to understand more about the concept of feature view, you can refer to here .","title":"Feature View"},{"location":"user_guides/fs/feature_view/overview/#creation","text":"Query and transformation function are the building blocks of a feature view. You can define your set of features by building a query . You can also define which columns in your feature view are the labels , which is useful for supervised machine learning tasks. Furthermore, in python client, each feature can be attached to its own transformation function. This way, when a feature is read (for training or scoring), the transformation is executed on-demand - just before the feature data is returned. For example, when a client reads a numerical feature, the feature value could be normalized by a StandardScalar transformation function before it is returned to the client. Python Java # create a simple feature view feature_view = fs . create_feature_view ( name = 'transactions_view' , query = query ) # create a feature view with transformation and label feature_view = fs . create_feature_view ( name = 'transactions_view' , query = query , labels = [ \"fraud_label\" ], transformation_functions = { \"amount\" : fs . get_transformation_function ( name = \"standard_scaler\" , version = 1 ) } ) // create a simple feature view FeatureView featureView = featureStore . createFeatureView () . name ( \"transactions_view) .query(query) .build(); // create a feature view with label FeatureView featureView = featureStore.createFeatureView() .name(\" transactions_view ) . query ( query ) . labels ( Lists . newArrayList ( \"fraud_label\" ) . build (); You can refer to query and transformation function for creating query and transformation_function . To see a full example of how to create a feature view, you can read this notebook .","title":"Creation"},{"location":"user_guides/fs/feature_view/overview/#retrieval","text":"Once you have created a feature view, you can retrieve it by its name and version. Python Java feature_view = fs . get_feature_view ( name = \"transactions_view\" , version = 1 ) FeatureView featureView = featureStore . getFeatureView ( \"transactions_view\" , 1 )","title":"Retrieval"},{"location":"user_guides/fs/feature_view/overview/#deletion","text":"If there are some feature view instances which you do not use anymore, you can delete a feature view. It is important to mention that all training datasets (include all materialised hopsfs training data) will be deleted along with the feature view. Python Java feature_view . delete () featureView . delete ()","title":"Deletion"},{"location":"user_guides/fs/feature_view/overview/#tags","text":"Feature views also support tags. You can attach, get, and remove tags. You can refer to here if you want to learn more about how tags work. Python Java # attach feature_view . add_tag ( name = \"tag_schema\" , value = { \"key\" , \"value\" } # get feature_view . get_tag ( name = \"tag_schema\" ) #remove feature_view . delete_tag ( name = \"tag_schema\" ) // attach Map < String , String > tag = Maps . newHashMap (); tag . put ( \"key\" , \"value\" ); featureView . addTag ( \"tag_schema\" , tag ) // get featureView . getTag ( \"tag_schema\" ) // remove featureView . deleteTag ( \"tag_schema\" )","title":"Tags"},{"location":"user_guides/fs/feature_view/overview/#next","text":"Once you have created a feature view, you can now create training data","title":"Next"},{"location":"user_guides/fs/feature_view/query/","text":"Query vs DataFrame # HSFS provides a DataFrame API to ingest data into the Hopsworks Feature Store. You can also retrieve feature data in a DataFrame, that can either be used directly to train models or materialized to file(s) for later use to train models. The idea of the Feature Store is to have pre-computed features available for both training and serving models. The key functionality required to generate training datasets from reusable features are: feature selection, joins, filters, and point in time queries. The Query object enables you to select features from different feature groups to join together to be used in a feature view. The joining functionality is heavily inspired by the APIs used by Pandas to merge DataFrames. The APIs allow you to specify which features to select from which feature group, how to join them and which features to use in join conditions. Python Scala # create a query feature_join = rain_fg . select_all () \\ . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) \\ . join ( location_fg . select_all ()) # save the query to feature view feature_view = fs . create_feature_view ( name = 'rain_dataset' , query = feature_join ) # retrieve the query back from the feature view feature_view = fs . get_feature_view ( \u201c rain_dataset \u201d , version = 1 ) query = feature_view . query // create a query val featureJoin = ( rainFg . selectAll () . join ( temperatureFg . selectAll (), on = Seq ( \"date\" , \"location_id\" )) . join ( locationFg . selectAll ())) val featureView = featureStore . createFeatureView () . name ( \"rain_dataset\" ) . query ( featureJoin ) . build (); // retrieve the query back from the feature view val featureView = fs . getFeatureView ( \u201c rain_dataset \u201d , 1 ) val query = featureView . getQuery () If a data scientist wants to modify a new feature that is not available in the feature store, she can write code to compute the new feature (using existing features or external data) and ingest the new feature values into the feature store. If the new feature is based solely on existing feature values in the Feature Store, we call it a derived feature. The same HSFS APIs can be used to compute derived features as well as features using external data sources. The Query Abstraction # Most operations performed on FeatureGroup metadata objects will return a Query with the applied operation. Examples # Selecting features from a feature group is a lazy operation, returning a query with the selected features only: Python Scala rain_fg = fs . get_feature_group ( \"rain_fg\" ) # Returns Query feature_join = rain_fg . select ([ \"location_id\" , \"weekly_rainfall\" ]) val rainFg = fs . getFeatureGroup ( \"rain_fg\" ) # Returns Query val featureJoin = rainFg . select ( Seq ( \"location_id\" , \"weekly_rainfall\" )) Join # Similarly, joins return query objects. The simplest join in one where we join all of the features together from two different feature groups without specifying a join key - HSFS will infer the join key as a common primary key between the two feature groups. By default, Hopsworks will use the maximal matching subset of the primary keys of the two feature groups as joining key(s), if not specified otherwise. Python Scala # Returns Query feature_join = rain_fg . join ( temperature_fg ) // Returns Query val featureJoin = rainFg . join ( temperatureFg ) More complex joins are possible by selecting subsets of features from the joined feature groups and by specifying a join key and type. Possible join types are \"inner\", \"left\" or \"right\". Furthermore, it is possible to specify different features for the join key of the left and right feature group. The join key lists should contain the names of the features to join on. Python Scala feature_join = rain_fg . select_all () \\ . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) \\ . join ( location_fg . select_all (), left_on = [ \"location_id\" ], right_on = [ \"id\" ], join_type = \"left\" ) val featureJoin = ( rainFg . selectAll () . join ( temperatureFg . selectAll (), Seq ( \"date\" , \"location_id\" )) . join ( locationFg . selectAll (), Seq ( \"location_id\" ), Seq ( \"id\" ), \"left\" )) Nested Joins The API currently does not support nested joins. That is joins of joins. You can fall back to Spark DataFrames to cover these cases. However, if you have to use joins of joins, most likely there is potential to optimise your feature group structure. Filter # In the same way as joins, applying filters to feature groups creates a query with the applied filter. Filters are constructed with Python Operators == , >= , <= , != , > , < and additionally with the methods isin and like . Bitwise Operators & and | are used to construct conjunctions. For the Scala part of the API, equivalent methods are available in the Feature and Filter classes. Python Scala filtered_rain = rain_fg . filter ( rain_fg . location_id == 10 ) val filteredRain = rainFg . filter ( rainFg . getFeature ( \"location_id\" ). eq ( 10 )) Filters are fully compatible with joins: Python Scala feature_join = rain_fg . select_all () \\ . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) \\ . join ( location_fg . select_all (), left_on = [ \"location_id\" ], right_on = [ \"id\" ], join_type = \"left\" ) \\ . filter (( rain_fg . location_id == 10 ) | ( rain_fg . location_id == 20 )) val featureJoin = ( rainFg . selectAll () . join ( temperatureFg . selectAll (), Seq ( \"date\" , \"location_id\" )) . join ( locationFg . selectAll (), Seq ( \"location_id\" ), Seq ( \"id\" ), \"left\" ) . filter ( rainFg . getFeature ( \"location_id\" ). eq ( 10 ). or ( rainFg . getFeature ( \"location_id\" ). eq ( 20 )))) The filters can be applied at any point of the query: Python Scala feature_join = rain_fg . select_all () \\ . join ( temperature_fg . select_all () . filter ( temperature_fg . avg_temp >= 22 ), on = [ \"date\" , \"location_id\" ]) \\ . join ( location_fg . select_all (), left_on = [ \"location_id\" ], right_on = [ \"id\" ], join_type = \"left\" ) \\ . filter ( rain_fg . location_id == 10 ) val featureJoin = ( rainFg . selectAll () . join ( temperatureFg . selectAll (). filter ( temperatureFg . getFeature ( \"avg_temp\" ). ge ( 22 )), Seq ( \"date\" , \"location_id\" )) . join ( locationFg . selectAll (), Seq ( \"location_id\" ), Seq ( \"id\" ), \"left\" ) . filter ( rainFg . getFeature ( \"location_id\" ). eq ( 10 ))) Joins and/or Filters on feature view query # The query retrieved from a feature view can be extended with new joins and/or new filters. However, this operation will not update the metadata and persist the updated query of the feature view itself. This query can then be used to create a new feature view. Python Scala fs = ... wind_speed_fg = fs . get_feature_group ( name = \"wind_speed_fg\" , version = 1 ) rain_fg = fs . get_feature_group ( name = \"rain_fg\" , version = 1 ) feature_view = fs . get_feature_view ( \u201c rain_dataset \u201d , version = 1 ) feature_view . query \\ . join ( wind_speed_fg . select_all ()) \\ . filter (( rain_fg . location_id == 54 ) val fs = ... val windSpeedFg = fs . getFeatureGroup ( \"wind_speed_fg\" , 1 ) val rainFg = fs . getFeatureGroup ( \"rain_fg\" , 1 ) val featureView = fs . getFeatureView ( \u201c rain_dataset \u201d , 1 ) featureView . getQuery () . join ( windSpeedFg . selectAll ()) . filter ( rainFg . getFeature ( \"location_id\" ). eq ( 54 )) Warning Every join/filter operation applied to an existing feature view query instance will update its state and accumulate. To successfully apply new join/filter logic it is recommended to refresh the query instance by re-fetching the feature view: Python Scala fs = ... wind_speed_fg = fs . get_feature_group ( name = \"wind_speed_fg\" , version = 1 ) solar_irradiance_fg = fs . get_feature_group ( name = \"solar_irradiance_fg\" , version = 1 ) rain_fg = fs . get_feature_group ( name = \"rain_fg\" , version = 1 ) # fetch new feature view and its query instance feature_view = fs . get_feature_view ( \u201c rain_dataset \u201d , version = 1 ) # apply join/filter logic based on location and wind speed feature_view . query . join ( wind_speed_fg . select_all ()) \\ . filter (( rain_fg . location_id == 54 ) # to apply new logic independent of location and wind speed from above # re-fetch new feature view and its query instance feature_view = fs . get_feature_view ( \u201c rain_dataset \u201d , version = 1 ) # apply new join/filter logic based on solar irradiance feature_view . query . join ( solar_irradiance_fg . select_all ()) \\ . filter ( solar_irradiance_fg . location_id == 28 ) fs = ... windSpeedFg = fs . getFeatureGroup ( \"wind_speed_fg\" , 1 ) solarIrradianceFg = fs . getFeatureGroup ( \"solar_irradiance_fg\" , 1 ) rainFg = fs . getFeatureGroup ( \"rain_fg\" , 1 ) // fetch new feature view and its query instance val featureView = fs . getFeatureView ( \u201c rain_dataset \u201d , version = 1 ) // apply join/filter logic based on location and wind speed featureView . getQuery . join ( windSpeedFg . selectAll ()) . filter ( rainFg . getFeature ( \"location_id\" ). eq ( 54 )) // to apply new logic independent of location and wind speed from above // re-fetch new feature view and its query instance val featureView = fs . getFeatureView ( \u201c rain_dataset \u201d , 1 ) // apply new join/filter logic based on solar irradiance featureView . getQuery . join ( solarIrradianceFg . selectAll ()) . filter ( solarIrradianceFg . getFeature ( \"location_id\" ). eq ( 28 ))","title":"Query"},{"location":"user_guides/fs/feature_view/query/#query-vs-dataframe","text":"HSFS provides a DataFrame API to ingest data into the Hopsworks Feature Store. You can also retrieve feature data in a DataFrame, that can either be used directly to train models or materialized to file(s) for later use to train models. The idea of the Feature Store is to have pre-computed features available for both training and serving models. The key functionality required to generate training datasets from reusable features are: feature selection, joins, filters, and point in time queries. The Query object enables you to select features from different feature groups to join together to be used in a feature view. The joining functionality is heavily inspired by the APIs used by Pandas to merge DataFrames. The APIs allow you to specify which features to select from which feature group, how to join them and which features to use in join conditions. Python Scala # create a query feature_join = rain_fg . select_all () \\ . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) \\ . join ( location_fg . select_all ()) # save the query to feature view feature_view = fs . create_feature_view ( name = 'rain_dataset' , query = feature_join ) # retrieve the query back from the feature view feature_view = fs . get_feature_view ( \u201c rain_dataset \u201d , version = 1 ) query = feature_view . query // create a query val featureJoin = ( rainFg . selectAll () . join ( temperatureFg . selectAll (), on = Seq ( \"date\" , \"location_id\" )) . join ( locationFg . selectAll ())) val featureView = featureStore . createFeatureView () . name ( \"rain_dataset\" ) . query ( featureJoin ) . build (); // retrieve the query back from the feature view val featureView = fs . getFeatureView ( \u201c rain_dataset \u201d , 1 ) val query = featureView . getQuery () If a data scientist wants to modify a new feature that is not available in the feature store, she can write code to compute the new feature (using existing features or external data) and ingest the new feature values into the feature store. If the new feature is based solely on existing feature values in the Feature Store, we call it a derived feature. The same HSFS APIs can be used to compute derived features as well as features using external data sources.","title":"Query vs DataFrame"},{"location":"user_guides/fs/feature_view/query/#the-query-abstraction","text":"Most operations performed on FeatureGroup metadata objects will return a Query with the applied operation.","title":"The Query Abstraction"},{"location":"user_guides/fs/feature_view/query/#examples","text":"Selecting features from a feature group is a lazy operation, returning a query with the selected features only: Python Scala rain_fg = fs . get_feature_group ( \"rain_fg\" ) # Returns Query feature_join = rain_fg . select ([ \"location_id\" , \"weekly_rainfall\" ]) val rainFg = fs . getFeatureGroup ( \"rain_fg\" ) # Returns Query val featureJoin = rainFg . select ( Seq ( \"location_id\" , \"weekly_rainfall\" ))","title":"Examples"},{"location":"user_guides/fs/feature_view/query/#join","text":"Similarly, joins return query objects. The simplest join in one where we join all of the features together from two different feature groups without specifying a join key - HSFS will infer the join key as a common primary key between the two feature groups. By default, Hopsworks will use the maximal matching subset of the primary keys of the two feature groups as joining key(s), if not specified otherwise. Python Scala # Returns Query feature_join = rain_fg . join ( temperature_fg ) // Returns Query val featureJoin = rainFg . join ( temperatureFg ) More complex joins are possible by selecting subsets of features from the joined feature groups and by specifying a join key and type. Possible join types are \"inner\", \"left\" or \"right\". Furthermore, it is possible to specify different features for the join key of the left and right feature group. The join key lists should contain the names of the features to join on. Python Scala feature_join = rain_fg . select_all () \\ . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) \\ . join ( location_fg . select_all (), left_on = [ \"location_id\" ], right_on = [ \"id\" ], join_type = \"left\" ) val featureJoin = ( rainFg . selectAll () . join ( temperatureFg . selectAll (), Seq ( \"date\" , \"location_id\" )) . join ( locationFg . selectAll (), Seq ( \"location_id\" ), Seq ( \"id\" ), \"left\" )) Nested Joins The API currently does not support nested joins. That is joins of joins. You can fall back to Spark DataFrames to cover these cases. However, if you have to use joins of joins, most likely there is potential to optimise your feature group structure.","title":"Join"},{"location":"user_guides/fs/feature_view/query/#filter","text":"In the same way as joins, applying filters to feature groups creates a query with the applied filter. Filters are constructed with Python Operators == , >= , <= , != , > , < and additionally with the methods isin and like . Bitwise Operators & and | are used to construct conjunctions. For the Scala part of the API, equivalent methods are available in the Feature and Filter classes. Python Scala filtered_rain = rain_fg . filter ( rain_fg . location_id == 10 ) val filteredRain = rainFg . filter ( rainFg . getFeature ( \"location_id\" ). eq ( 10 )) Filters are fully compatible with joins: Python Scala feature_join = rain_fg . select_all () \\ . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) \\ . join ( location_fg . select_all (), left_on = [ \"location_id\" ], right_on = [ \"id\" ], join_type = \"left\" ) \\ . filter (( rain_fg . location_id == 10 ) | ( rain_fg . location_id == 20 )) val featureJoin = ( rainFg . selectAll () . join ( temperatureFg . selectAll (), Seq ( \"date\" , \"location_id\" )) . join ( locationFg . selectAll (), Seq ( \"location_id\" ), Seq ( \"id\" ), \"left\" ) . filter ( rainFg . getFeature ( \"location_id\" ). eq ( 10 ). or ( rainFg . getFeature ( \"location_id\" ). eq ( 20 )))) The filters can be applied at any point of the query: Python Scala feature_join = rain_fg . select_all () \\ . join ( temperature_fg . select_all () . filter ( temperature_fg . avg_temp >= 22 ), on = [ \"date\" , \"location_id\" ]) \\ . join ( location_fg . select_all (), left_on = [ \"location_id\" ], right_on = [ \"id\" ], join_type = \"left\" ) \\ . filter ( rain_fg . location_id == 10 ) val featureJoin = ( rainFg . selectAll () . join ( temperatureFg . selectAll (). filter ( temperatureFg . getFeature ( \"avg_temp\" ). ge ( 22 )), Seq ( \"date\" , \"location_id\" )) . join ( locationFg . selectAll (), Seq ( \"location_id\" ), Seq ( \"id\" ), \"left\" ) . filter ( rainFg . getFeature ( \"location_id\" ). eq ( 10 )))","title":"Filter"},{"location":"user_guides/fs/feature_view/query/#joins-andor-filters-on-feature-view-query","text":"The query retrieved from a feature view can be extended with new joins and/or new filters. However, this operation will not update the metadata and persist the updated query of the feature view itself. This query can then be used to create a new feature view. Python Scala fs = ... wind_speed_fg = fs . get_feature_group ( name = \"wind_speed_fg\" , version = 1 ) rain_fg = fs . get_feature_group ( name = \"rain_fg\" , version = 1 ) feature_view = fs . get_feature_view ( \u201c rain_dataset \u201d , version = 1 ) feature_view . query \\ . join ( wind_speed_fg . select_all ()) \\ . filter (( rain_fg . location_id == 54 ) val fs = ... val windSpeedFg = fs . getFeatureGroup ( \"wind_speed_fg\" , 1 ) val rainFg = fs . getFeatureGroup ( \"rain_fg\" , 1 ) val featureView = fs . getFeatureView ( \u201c rain_dataset \u201d , 1 ) featureView . getQuery () . join ( windSpeedFg . selectAll ()) . filter ( rainFg . getFeature ( \"location_id\" ). eq ( 54 )) Warning Every join/filter operation applied to an existing feature view query instance will update its state and accumulate. To successfully apply new join/filter logic it is recommended to refresh the query instance by re-fetching the feature view: Python Scala fs = ... wind_speed_fg = fs . get_feature_group ( name = \"wind_speed_fg\" , version = 1 ) solar_irradiance_fg = fs . get_feature_group ( name = \"solar_irradiance_fg\" , version = 1 ) rain_fg = fs . get_feature_group ( name = \"rain_fg\" , version = 1 ) # fetch new feature view and its query instance feature_view = fs . get_feature_view ( \u201c rain_dataset \u201d , version = 1 ) # apply join/filter logic based on location and wind speed feature_view . query . join ( wind_speed_fg . select_all ()) \\ . filter (( rain_fg . location_id == 54 ) # to apply new logic independent of location and wind speed from above # re-fetch new feature view and its query instance feature_view = fs . get_feature_view ( \u201c rain_dataset \u201d , version = 1 ) # apply new join/filter logic based on solar irradiance feature_view . query . join ( solar_irradiance_fg . select_all ()) \\ . filter ( solar_irradiance_fg . location_id == 28 ) fs = ... windSpeedFg = fs . getFeatureGroup ( \"wind_speed_fg\" , 1 ) solarIrradianceFg = fs . getFeatureGroup ( \"solar_irradiance_fg\" , 1 ) rainFg = fs . getFeatureGroup ( \"rain_fg\" , 1 ) // fetch new feature view and its query instance val featureView = fs . getFeatureView ( \u201c rain_dataset \u201d , version = 1 ) // apply join/filter logic based on location and wind speed featureView . getQuery . join ( windSpeedFg . selectAll ()) . filter ( rainFg . getFeature ( \"location_id\" ). eq ( 54 )) // to apply new logic independent of location and wind speed from above // re-fetch new feature view and its query instance val featureView = fs . getFeatureView ( \u201c rain_dataset \u201d , 1 ) // apply new join/filter logic based on solar irradiance featureView . getQuery . join ( solarIrradianceFg . selectAll ()) . filter ( solarIrradianceFg . getFeature ( \"location_id\" ). eq ( 28 ))","title":"Joins and/or Filters on feature view query"},{"location":"user_guides/fs/feature_view/spine-query/","text":"Using Spines # In this section we will illustrate how to use a Spine Group instead of a regular Feature Group for performing point-in-time joins when reading batch data for inference or when creating training datasets. Prerequisites # Make sure you have read the concept section about spines in feature and inference pipelines. Make sure you have gone through the Spine Group creation guide . Make sure you understand the concept of feature views and how to create them using the query abstraction Feature View with a Spine Group # Step 1: Query Definition # The first step before creating a Feature View, is to construct the query by selecting the label and features which are needed: # Select features for training data. ds_query = trans_fg . select ([ \"fraud_label\" ]) \\ . join ( window_aggs_fg . select_except ([ \"cc_num\" ]), on = \"cc_num\" ) ds_query . show ( 5 ) Similarly you can construct the query using a previously created spine equivalent. However, there are two thing to note: If you want to use the query for a feature view to be used for online serving, you can only select the \"label\" or target feature from the spine. Spine groups can only be used on the left side of the join. Think of the left side of the join as the base set of entities that should be included in you batch of data or training dataset, which we enrich with the relevant and point-in-time correct feature values. trans_spine = fs . get_or_create_spine_group ( name = \"spine_transactions\" , version = 1 , description = \"Transaction data\" , primary_key = [ 'cc_num' ], event_time = 'datetime' , dataframe = trans_df ) # Select features for training data. ds_query_spine = trans_spine . select ([ \"fraud_label\" ]) \\ . join ( window_aggs_fg . select_except ([ \"cc_num\" ]), on = \"cc_num\" ) Calling the show() or read() method of this query object will use the spine dataframe included in the Spine Group object to perform the join. ds_query_spine . show ( 10 ) Step 2: Feature View Creation # With the above defined query, we can continue to create the Feature View in the same way we would do it also without a spine: feature_view_spine = fs . get_or_create_feature_view ( name = 'transactions_view_spine' , query = ds_query_spine , version = 1 , labels = [ \"fraud_label\" ], ) Step 3: Training Dataset Creation # With the regular feature view, the labels are fetched from the feature store, but with the feature view created with a spine, you need to provide the dataframe. Here you have the chance to pass a different set of entities to generate the training dataset. X_train , X_test , y_train , y_test = feature_view_spine . train_test_split ( 0.2 , spine = new_entities_df ) X_train . show () Step 4: Retrieving New Batches Inference Data # You can now use the offline and online API of the feature stores to read features for inference. Similarly to training dataset creation, every time you read up a new batch of data, you can pass a different spine dataframe. feature_view_spine . get_batch_data ( spine = scroing_spine_df ) . show () Step 5: Online Feature Lookup # For the online lookup, the label is not required, therefore it was important to only select label from the left spine group, so that we don't need to provide a spine for online serving: # Note: no spine needs to be passed feature_view . get_feature_vector ({ \"cc_num\" : 4473593503484549 }) Replacing a Regular Feature Group with a Spine at Serving Time # In the case where you create a feature view with a regular feature group, but you would like to retrieve batch inference data using IDs (primary key values), you can use a spine to replace the left feature group. To do this, you can pass the Spine Group instead of a dataframe. # Note: here feature_view was created with regular feature groups only # and trans_spine is of type SpineGroup instead of a dataframe feature_view . get_batch_data ( spine = trans_spine ) . show ()","title":"Spines"},{"location":"user_guides/fs/feature_view/spine-query/#using-spines","text":"In this section we will illustrate how to use a Spine Group instead of a regular Feature Group for performing point-in-time joins when reading batch data for inference or when creating training datasets.","title":"Using Spines"},{"location":"user_guides/fs/feature_view/spine-query/#prerequisites","text":"Make sure you have read the concept section about spines in feature and inference pipelines. Make sure you have gone through the Spine Group creation guide . Make sure you understand the concept of feature views and how to create them using the query abstraction","title":"Prerequisites"},{"location":"user_guides/fs/feature_view/spine-query/#feature-view-with-a-spine-group","text":"","title":"Feature View with a Spine Group"},{"location":"user_guides/fs/feature_view/spine-query/#step-1-query-definition","text":"The first step before creating a Feature View, is to construct the query by selecting the label and features which are needed: # Select features for training data. ds_query = trans_fg . select ([ \"fraud_label\" ]) \\ . join ( window_aggs_fg . select_except ([ \"cc_num\" ]), on = \"cc_num\" ) ds_query . show ( 5 ) Similarly you can construct the query using a previously created spine equivalent. However, there are two thing to note: If you want to use the query for a feature view to be used for online serving, you can only select the \"label\" or target feature from the spine. Spine groups can only be used on the left side of the join. Think of the left side of the join as the base set of entities that should be included in you batch of data or training dataset, which we enrich with the relevant and point-in-time correct feature values. trans_spine = fs . get_or_create_spine_group ( name = \"spine_transactions\" , version = 1 , description = \"Transaction data\" , primary_key = [ 'cc_num' ], event_time = 'datetime' , dataframe = trans_df ) # Select features for training data. ds_query_spine = trans_spine . select ([ \"fraud_label\" ]) \\ . join ( window_aggs_fg . select_except ([ \"cc_num\" ]), on = \"cc_num\" ) Calling the show() or read() method of this query object will use the spine dataframe included in the Spine Group object to perform the join. ds_query_spine . show ( 10 )","title":"Step 1: Query Definition"},{"location":"user_guides/fs/feature_view/spine-query/#step-2-feature-view-creation","text":"With the above defined query, we can continue to create the Feature View in the same way we would do it also without a spine: feature_view_spine = fs . get_or_create_feature_view ( name = 'transactions_view_spine' , query = ds_query_spine , version = 1 , labels = [ \"fraud_label\" ], )","title":"Step 2: Feature View Creation"},{"location":"user_guides/fs/feature_view/spine-query/#step-3-training-dataset-creation","text":"With the regular feature view, the labels are fetched from the feature store, but with the feature view created with a spine, you need to provide the dataframe. Here you have the chance to pass a different set of entities to generate the training dataset. X_train , X_test , y_train , y_test = feature_view_spine . train_test_split ( 0.2 , spine = new_entities_df ) X_train . show ()","title":"Step 3: Training Dataset Creation"},{"location":"user_guides/fs/feature_view/spine-query/#step-4-retrieving-new-batches-inference-data","text":"You can now use the offline and online API of the feature stores to read features for inference. Similarly to training dataset creation, every time you read up a new batch of data, you can pass a different spine dataframe. feature_view_spine . get_batch_data ( spine = scroing_spine_df ) . show ()","title":"Step 4: Retrieving New Batches Inference Data"},{"location":"user_guides/fs/feature_view/spine-query/#step-5-online-feature-lookup","text":"For the online lookup, the label is not required, therefore it was important to only select label from the left spine group, so that we don't need to provide a spine for online serving: # Note: no spine needs to be passed feature_view . get_feature_vector ({ \"cc_num\" : 4473593503484549 })","title":"Step 5: Online Feature Lookup"},{"location":"user_guides/fs/feature_view/spine-query/#replacing-a-regular-feature-group-with-a-spine-at-serving-time","text":"In the case where you create a feature view with a regular feature group, but you would like to retrieve batch inference data using IDs (primary key values), you can use a spine to replace the left feature group. To do this, you can pass the Spine Group instead of a dataframe. # Note: here feature_view was created with regular feature groups only # and trans_spine is of type SpineGroup instead of a dataframe feature_view . get_batch_data ( spine = trans_spine ) . show ()","title":"Replacing a Regular Feature Group with a Spine at Serving Time"},{"location":"user_guides/fs/feature_view/training-data/","text":"Training data # Training data can be created from the feature view and used by different ML libraries for training different models. You can read training data concepts for more details. To see a full example of how to create training data, you can read this notebook . For Python-clients, handling small or moderately-sized data, we recommend enabling the ArrowFlight Server with DuckDB service, which will provide significant speedups over Spark/Hive for reading and creating in-memory training datasets. Creation # It can be created as in-memory DataFrames or materialised as tfrecords , parquet , csv , or tsv files to HopsFS or in all other locations, for example, S3, GCS. If you materialise a training dataset, a PySparkJob will be launched. By default, create_training_data waits for the job to finish. However, you can run the job asynchronously by passing write_options={\"wait_for_job\": False} . You can monitor the job status in the jobs overview UI . # create a training dataset as dataframe feature_df , label_df = feature_view . training_data ( description = 'transactions fraud batch training dataset' , ) # materialise a training dataset version , job = feature_view . create_training_data ( description = 'transactions fraud batch training dataset' , data_format = 'csv' , write_options = { \"wait_for_job\" : False } ) # By default, it is materialised to HopsFS print ( job . id ) # get the job's id and view the job status in the UI Extra filters # Sometimes data scientists need to train different models using subsets of a dataset. For example, there can be different models for different countries, seasons, and different groups. One way is to create different feature views for training different models. Another way is to add extra filters on top of the feature view when creating training data. In the transaction fraud example , there are different transaction categories, for example: \"Health/Beauty\", \"Restaurant/Cafeteria\", \"Holliday/Travel\" etc. Examples below show how to create training data for different transaction categories. # Create a training dataset for Health/Beauty df_health = feature_view . training_data ( description = 'transactions fraud batch training dataset for Health/Beauty' , extra_filter = trans_fg . category == \"Health/Beauty\" ) # Create a training dataset for Restaurant/Cafeteria and Holliday/Travel df_restaurant_travel = feature_view . training_data ( description = 'transactions fraud batch training dataset for Restaurant/Cafeteria and Holliday/Travel' , extra_filter = trans_fg . category == \"Restaurant/Cafeteria\" and trans_fg . category == \"Holliday/Travel\" ) Train/Validation/Test Splits # In most cases, ML practitioners want to slice a dataset into multiple splits, most commonly train-test splits or train-validation-test splits, so that they can train and test their models. Feature view provides a sklearn-like API for this purpose, so it is very easy to create a training dataset with different splits. Create a training dataset (as in-memory DataFrames) or materialise a training dataset with train and test splits. # create a training dataset X_train , X_test , y_train , y_test = feature_view . train_test_split ( test_size = 0.2 ) # materialise a training dataset version , job = feature_view . create_train_test_split ( test_size = 0.2 , description = 'transactions fraud batch training dataset' , data_format = 'csv' ) Create a training dataset (as in-memory DataFrames) or materialise a training dataset with train, validation, and test splits. # create a training dataset as DataFrame X_train , X_val , X_test , y_train , y_val , y_test = feature_view . train_validation_test_split ( validation_size = 0.3 , test_size = 0.2 ) # materialise a training dataset version , job = feature_view . create_train_validation_test_split ( validation_size = 0.3 , test_size = 0.2 description = 'transactions fraud batch training dataset' , data_format = 'csv' ) If the ArrowFlight Server with DuckDB service is enabled, and you want to create a particular in-memory training dataset with Hive instead, you can set read_options={\"use_hive\": True} . # create a training dataset as DataFrame with Hive X_train , X_test , y_train , y_test = feature_view . train_test_split ( test_size = 0.2 , read_options = { \"use_hive: True}) Read Training Data # Once you have created a training dataset, all its metadata are saved in Hopsworks. This enables you to reproduce exactly the same dataset at a later point in time. This holds for training data as both DataFrames or files. That is, you can delete the training data files (for example, to reduce storage costs), but still reproduce the training data files later on if you need to. # get a training dataset feature_df , label_df = feature_view . get_training_data ( training_dataset_version = 1 ) # get a training dataset with train and test splits X_train , X_test , y_train , y_test = feature_view . get_train_test_split ( training_dataset_version = 1 ) # get a training dataset with train, validation and test splits X_train , X_val , X_test , y_train , y_val , y_test = feature_view . get_train_validation_test_split ( training_dataset_version = 1 ) Deletion # To clean up unused training data, you can delete all training data or for a particular version. Note that all metadata of training data and materialised files stored in HopsFS will be deleted and cannot be recreated anymore. # delete a training data version feature_view . delete_training_dataset ( training_dataset_version = 1 ) # delete all training datasets feature_view . delete_all_training_datasets () It is also possible to keep the metadata and delete only the materialised files. Then you can recreate the deleted files by just specifying a version, and you get back the exact same dataset again. This is useful when you are running out of storage. # delete files of a training data version feature_view . purge_training_data ( training_dataset_version = 1 ) # delete files of all training datasets feature_view . purge_all_training_data () To recreate a training dataset: feature_view . recreate_training_dataset ( training_dataset_version = 1 ) Tags # Similar to feature view, You can attach, get, and remove tags. You can refer to here if you want to learn more about how tags work. # attach feature_view . add_training_dataset_tag ( training_dataset_version = 1 , name = \"tag_schema\" , value = { \"key\" , \"value\" } ) # get feature_view . get_training_dataset_tag ( training_dataset_version = 1 , name = \"tag_schema\" ) #remove feature_view . delete_training_dataset_tag ( training_dataset_version = 1 , name = \"tag_schema\" ) Next # Once you have created a training dataset and trained your model, you can deploy your model in a \"batch\" or \"online\" setting. Next, you can learn how to create batch data and get feature vectors .","title":"Training data"},{"location":"user_guides/fs/feature_view/training-data/#training-data","text":"Training data can be created from the feature view and used by different ML libraries for training different models. You can read training data concepts for more details. To see a full example of how to create training data, you can read this notebook . For Python-clients, handling small or moderately-sized data, we recommend enabling the ArrowFlight Server with DuckDB service, which will provide significant speedups over Spark/Hive for reading and creating in-memory training datasets.","title":"Training data"},{"location":"user_guides/fs/feature_view/training-data/#creation","text":"It can be created as in-memory DataFrames or materialised as tfrecords , parquet , csv , or tsv files to HopsFS or in all other locations, for example, S3, GCS. If you materialise a training dataset, a PySparkJob will be launched. By default, create_training_data waits for the job to finish. However, you can run the job asynchronously by passing write_options={\"wait_for_job\": False} . You can monitor the job status in the jobs overview UI . # create a training dataset as dataframe feature_df , label_df = feature_view . training_data ( description = 'transactions fraud batch training dataset' , ) # materialise a training dataset version , job = feature_view . create_training_data ( description = 'transactions fraud batch training dataset' , data_format = 'csv' , write_options = { \"wait_for_job\" : False } ) # By default, it is materialised to HopsFS print ( job . id ) # get the job's id and view the job status in the UI","title":"Creation"},{"location":"user_guides/fs/feature_view/training-data/#extra-filters","text":"Sometimes data scientists need to train different models using subsets of a dataset. For example, there can be different models for different countries, seasons, and different groups. One way is to create different feature views for training different models. Another way is to add extra filters on top of the feature view when creating training data. In the transaction fraud example , there are different transaction categories, for example: \"Health/Beauty\", \"Restaurant/Cafeteria\", \"Holliday/Travel\" etc. Examples below show how to create training data for different transaction categories. # Create a training dataset for Health/Beauty df_health = feature_view . training_data ( description = 'transactions fraud batch training dataset for Health/Beauty' , extra_filter = trans_fg . category == \"Health/Beauty\" ) # Create a training dataset for Restaurant/Cafeteria and Holliday/Travel df_restaurant_travel = feature_view . training_data ( description = 'transactions fraud batch training dataset for Restaurant/Cafeteria and Holliday/Travel' , extra_filter = trans_fg . category == \"Restaurant/Cafeteria\" and trans_fg . category == \"Holliday/Travel\" )","title":"Extra filters"},{"location":"user_guides/fs/feature_view/training-data/#trainvalidationtest-splits","text":"In most cases, ML practitioners want to slice a dataset into multiple splits, most commonly train-test splits or train-validation-test splits, so that they can train and test their models. Feature view provides a sklearn-like API for this purpose, so it is very easy to create a training dataset with different splits. Create a training dataset (as in-memory DataFrames) or materialise a training dataset with train and test splits. # create a training dataset X_train , X_test , y_train , y_test = feature_view . train_test_split ( test_size = 0.2 ) # materialise a training dataset version , job = feature_view . create_train_test_split ( test_size = 0.2 , description = 'transactions fraud batch training dataset' , data_format = 'csv' ) Create a training dataset (as in-memory DataFrames) or materialise a training dataset with train, validation, and test splits. # create a training dataset as DataFrame X_train , X_val , X_test , y_train , y_val , y_test = feature_view . train_validation_test_split ( validation_size = 0.3 , test_size = 0.2 ) # materialise a training dataset version , job = feature_view . create_train_validation_test_split ( validation_size = 0.3 , test_size = 0.2 description = 'transactions fraud batch training dataset' , data_format = 'csv' ) If the ArrowFlight Server with DuckDB service is enabled, and you want to create a particular in-memory training dataset with Hive instead, you can set read_options={\"use_hive\": True} . # create a training dataset as DataFrame with Hive X_train , X_test , y_train , y_test = feature_view . train_test_split ( test_size = 0.2 , read_options = { \"use_hive: True})","title":"Train/Validation/Test Splits"},{"location":"user_guides/fs/feature_view/training-data/#read-training-data","text":"Once you have created a training dataset, all its metadata are saved in Hopsworks. This enables you to reproduce exactly the same dataset at a later point in time. This holds for training data as both DataFrames or files. That is, you can delete the training data files (for example, to reduce storage costs), but still reproduce the training data files later on if you need to. # get a training dataset feature_df , label_df = feature_view . get_training_data ( training_dataset_version = 1 ) # get a training dataset with train and test splits X_train , X_test , y_train , y_test = feature_view . get_train_test_split ( training_dataset_version = 1 ) # get a training dataset with train, validation and test splits X_train , X_val , X_test , y_train , y_val , y_test = feature_view . get_train_validation_test_split ( training_dataset_version = 1 )","title":"Read Training Data"},{"location":"user_guides/fs/feature_view/training-data/#deletion","text":"To clean up unused training data, you can delete all training data or for a particular version. Note that all metadata of training data and materialised files stored in HopsFS will be deleted and cannot be recreated anymore. # delete a training data version feature_view . delete_training_dataset ( training_dataset_version = 1 ) # delete all training datasets feature_view . delete_all_training_datasets () It is also possible to keep the metadata and delete only the materialised files. Then you can recreate the deleted files by just specifying a version, and you get back the exact same dataset again. This is useful when you are running out of storage. # delete files of a training data version feature_view . purge_training_data ( training_dataset_version = 1 ) # delete files of all training datasets feature_view . purge_all_training_data () To recreate a training dataset: feature_view . recreate_training_dataset ( training_dataset_version = 1 )","title":"Deletion"},{"location":"user_guides/fs/feature_view/training-data/#tags","text":"Similar to feature view, You can attach, get, and remove tags. You can refer to here if you want to learn more about how tags work. # attach feature_view . add_training_dataset_tag ( training_dataset_version = 1 , name = \"tag_schema\" , value = { \"key\" , \"value\" } ) # get feature_view . get_training_dataset_tag ( training_dataset_version = 1 , name = \"tag_schema\" ) #remove feature_view . delete_training_dataset_tag ( training_dataset_version = 1 , name = \"tag_schema\" )","title":"Tags"},{"location":"user_guides/fs/feature_view/training-data/#next","text":"Once you have created a training dataset and trained your model, you can deploy your model in a \"batch\" or \"online\" setting. Next, you can learn how to create batch data and get feature vectors .","title":"Next"},{"location":"user_guides/fs/feature_view/transformation-function/","text":"Transformation Functions # HSFS provides functionality to attach transformation functions to feature views . User defined, custom transformation functions need to be registered in the feature store to make them accessible for feature view creation. To register them in the feature store, they either have to be part of the library installed in Hopsworks or attached when starting a Jupyter notebook or Hopsworks job . Pyspark decorators Don't decorate transformation functions with Pyspark @udf or @pandas_udf , and also make sure not to use any Pyspark dependencies. That is because, the transformation functions may be executed by Python clients. HSFS will decorate transformation function for you only if it is used inside Pyspark application. Creation # Hopsworks ships built-in transformation functions such as min_max_scaler , standard_scaler , robust_scaler and label_encoder . You can also create new functions. Let's assume that you have already installed Python library transformation_fn_template containing the transformation function plus_one . Python Register transformation function plus_one in the Hopsworks feature store. from custom_functions import transformations plus_one_meta = fs . create_transformation_function ( transformation_function = transformations . plus_one , output_type = int , version = 1 ) plus_one_meta . save () Retrieval # To retrieve all transformation functions from the feature store, use get_transformation_functions which will return the list of available TransformationFunction objects. A specific transformation function can be retrieved with the get_transformation_function method where you can provide its name and version of the transformation function. If only the function name is provided then it will default to version 1. Python Retrieving transformation functions from the feature store # get all transformation functions fs . get_transformation_functions () # get transformation function by name. This will default to version 1 plus_one_fn = fs . get_transformation_function ( name = \"plus_one\" ) # get built-in transformation function min max scaler min_max_scaler_fn = fs . get_transformation_function ( name = \"min_max_scaler\" ) # get transformation function by name and version. plus_one_fn = fs . get_transformation_function ( name = \"plus_one\" , version = 2 ) Apply transformation functions to features # You can define in the feature view transformation functions as dict, where key is feature name and value is online transformation function name. Then the transformation functions are applied when you read training data , read batch data , or get feature vectors . Python Attaching transformation functions to the feature view plus_one_fn = fs . get_transformation_function ( name = \"plus_one\" , version = 1 ) feature_view = fs . create_feature_view ( name = 'transactions_view' , query = query , labels = [ \"fraud_label\" ], transformation_functions = { \"amount_spent\" : plus_one_fn } ) Built-in transformation functions are attached in the same way. The only difference is that it will compute the necessary statistics for the specific function in the background. For example min and max values for min_max_scaler ; mean and standard deviation for standard_scaler etc. Python Attaching built-in transformation functions to the feature view min_max_scaler = fs . get_transformation_function ( name = \"min_max_scaler\" ) standard_scaler = fs . get_transformation_function ( name = \"standard_scaler\" ) robust_scaler = fs . get_transformation_function ( name = \"robust_scaler\" ) label_encoder = fs . get_transformation_function ( name = \"label_encoder\" ) feature_view = fs . create_feature_view ( name = 'transactions_view' , query = query , labels = [ \"fraud_label\" ], transformation_functions = { \"category\" : label_encoder , \"amount\" : robust_scaler , \"loc_delta\" : min_max_scaler , \"age_at_transaction\" : standard_scaler } ) Java/Scala support Creating and attaching Transformation functions to feature views are not supported for HSFS Java or Scala client. If feature view with transformation function was created using python client, you cannot get training data or get feature vectors from HSFS Java or Scala client.","title":"Transformation Functions"},{"location":"user_guides/fs/feature_view/transformation-function/#transformation-functions","text":"HSFS provides functionality to attach transformation functions to feature views . User defined, custom transformation functions need to be registered in the feature store to make them accessible for feature view creation. To register them in the feature store, they either have to be part of the library installed in Hopsworks or attached when starting a Jupyter notebook or Hopsworks job . Pyspark decorators Don't decorate transformation functions with Pyspark @udf or @pandas_udf , and also make sure not to use any Pyspark dependencies. That is because, the transformation functions may be executed by Python clients. HSFS will decorate transformation function for you only if it is used inside Pyspark application.","title":"Transformation Functions"},{"location":"user_guides/fs/feature_view/transformation-function/#creation","text":"Hopsworks ships built-in transformation functions such as min_max_scaler , standard_scaler , robust_scaler and label_encoder . You can also create new functions. Let's assume that you have already installed Python library transformation_fn_template containing the transformation function plus_one . Python Register transformation function plus_one in the Hopsworks feature store. from custom_functions import transformations plus_one_meta = fs . create_transformation_function ( transformation_function = transformations . plus_one , output_type = int , version = 1 ) plus_one_meta . save ()","title":"Creation"},{"location":"user_guides/fs/feature_view/transformation-function/#retrieval","text":"To retrieve all transformation functions from the feature store, use get_transformation_functions which will return the list of available TransformationFunction objects. A specific transformation function can be retrieved with the get_transformation_function method where you can provide its name and version of the transformation function. If only the function name is provided then it will default to version 1. Python Retrieving transformation functions from the feature store # get all transformation functions fs . get_transformation_functions () # get transformation function by name. This will default to version 1 plus_one_fn = fs . get_transformation_function ( name = \"plus_one\" ) # get built-in transformation function min max scaler min_max_scaler_fn = fs . get_transformation_function ( name = \"min_max_scaler\" ) # get transformation function by name and version. plus_one_fn = fs . get_transformation_function ( name = \"plus_one\" , version = 2 )","title":"Retrieval"},{"location":"user_guides/fs/feature_view/transformation-function/#apply-transformation-functions-to-features","text":"You can define in the feature view transformation functions as dict, where key is feature name and value is online transformation function name. Then the transformation functions are applied when you read training data , read batch data , or get feature vectors . Python Attaching transformation functions to the feature view plus_one_fn = fs . get_transformation_function ( name = \"plus_one\" , version = 1 ) feature_view = fs . create_feature_view ( name = 'transactions_view' , query = query , labels = [ \"fraud_label\" ], transformation_functions = { \"amount_spent\" : plus_one_fn } ) Built-in transformation functions are attached in the same way. The only difference is that it will compute the necessary statistics for the specific function in the background. For example min and max values for min_max_scaler ; mean and standard deviation for standard_scaler etc. Python Attaching built-in transformation functions to the feature view min_max_scaler = fs . get_transformation_function ( name = \"min_max_scaler\" ) standard_scaler = fs . get_transformation_function ( name = \"standard_scaler\" ) robust_scaler = fs . get_transformation_function ( name = \"robust_scaler\" ) label_encoder = fs . get_transformation_function ( name = \"label_encoder\" ) feature_view = fs . create_feature_view ( name = 'transactions_view' , query = query , labels = [ \"fraud_label\" ], transformation_functions = { \"category\" : label_encoder , \"amount\" : robust_scaler , \"loc_delta\" : min_max_scaler , \"age_at_transaction\" : standard_scaler } ) Java/Scala support Creating and attaching Transformation functions to feature views are not supported for HSFS Java or Scala client. If feature view with transformation function was created using python client, you cannot get training data or get feature vectors from HSFS Java or Scala client.","title":"Apply transformation functions to features"},{"location":"user_guides/fs/provenance/provenance/","text":"Provenance # Introduction # Hopsworks feature store allows users to track provenance (lineage) between feature groups, feature views and training dataset. Tracking lineage allows users to determine where/if a feature group is being used. You can track if feature groups are being used to create additional (derived) feature groups or feature views. You can interact with the provenance graph using the UI and the APIs. Step 1: Feature group lineage # Assign parents to a feature group # When creating a feature group, it is possible to specify a list of feature groups used to create the derived features. For example, you could have an external feature group defined over a Snowflake or Redshift table, which you use to compute the features and save them in a feature group. You can mark the external feature group as parent of the feature group you are creating by using the parents parameter in the get_or_create_feature_group or create_feature_group methods: Python # Retrieve the feature group profiles_fg = fs . get_external_feature_group ( \"user_profiles\" , version = 1 ) # Do feature engineering age_df = transaction_df . merge ( profiles_fg . read (), on = \"cc_num\" , how = \"left\" ) transaction_df [ \"age_at_transaction\" ] = ( age_df [ \"datetime\" ] - age_df [ \"birthdate\" ]) / np . timedelta64 ( 1 , \"Y\" ) # Create the transaction feature group transaction_fg = fs . get_or_create_feature_group ( name = \"transaction_fraud_batch\" , version = 1 , description = \"Transaction features\" , primary_key = [ \"cc_num\" ], event_time = \"datetime\" , parents = [ profiles_fg ] ) transaction_fg . insert ( transaction_df ) Another example use case for derived feature group is if you have a feature group containing features with daily resolution and you are using the content of that feature group to populate a second feature group with monthly resolution: Python # Retrieve the feature group daily_transaction_fg = fs . get_feature_group ( \"daily_transaction\" , version = 1 ) daily_transaction_df = daily_transaction_fg . read () # Do feature engineering cc_group = daily_transaction_df [[ \"cc_num\" , \"amount\" , \"datetime\" ]] \\ . groupby ( \"cc_num\" ) \\ . rolling ( \"1M\" , on = \"datetime\" ) monthly_transaction_df = pd . DataFrame ( cc_group . mean ()) # Create the transaction feature group monthly_transaction_fg = fs . get_or_create_feature_group ( name = \"monthly_transaction_fraud_batch\" , version = 1 , description = \"Transaction features - monthly aggregates\" , primary_key = [ \"cc_num\" ], event_time = \"datetime\" , parents = [ daily_transaction_fg ] ) monthly_transaction_fg . insert ( monthly_transaction_df ) List feature group parents # You can query the provenance graph of a feature group using the UI and the APIs. From the APIs you can list the parent feature groups by calling the method get_parent_feature_groups Python lineage = transaction_fg . get_parent_feature_groups () # List all accessible parent feature groups lineage . accessible # List all deleted parent feature groups lineage . deleted # List all the inaccessible parent feature groups lineage . inaccessible A parent is marked as deleted (and added to the deleted list) if the parent feature group was deleted. inaccessible if you no longer have access to the parent feature group (e.g. the parent feature group belongs to a project you no longer have access to). To traverse the provenance graph in the opposite direction (i.e. from the parent feature group to the child), you can use the get_generate_feature_groups method. When navigating the provenance graph downstream, the deleted feature groups are not tracked by provenance, as such, the deleted property will always return an empty list. Python lineage = transaction_fg . get_generated_feature_groups () # List all accessible child feature groups lineage . accessible # List all the inaccessible child feature groups lineage . inaccessible You can also visualize the relationship between the parent and child feature groups in the UI. In each feature group overview page you can find a provenance section with the graph of parent feature groups and child feature groups/feature views. Provenance graph of derived feature groups Step 2: Feature view lineage # The relationship between feature groups and feature views is captured automatically when you create a feature view. You can inspect the relationship between feature groups and feature views using the APIs or the UI. Using the APIs # Starting from a feature view metadata object, you can traverse upstream the provenance graph to retrieve the metadata objects of the feature groups that are part of the feature view. To do so, you can use the get_parent_feature_groups method. Python lineage = fraud_fv . get_parent_feature_groups () # List all accessible parent feature groups lineage . accessible # List all deleted parent feature groups lineage . deleted # List all the inaccessible parent feature groups lineage . inaccessible You can also traverse the provenance graph in the opposite direction. Starting from a feature group you can navigate downstream and list all the feature views the feature group is used in. As for the derived feature group example above, when navigating the provenance graph downstream deleted feature views are not tracked. As such, the deleted property will always be empty. Python lineage = transaction_fg . get_generated_feature_views () # List all accessible downstream feature views lineage . accessible # List all the inaccessible downstream feature views lineage . inaccessible Using the UI # In the feature view overview UI you can explore the provenance graph of the feature view: Feature view provenance graph","title":"Provenance"},{"location":"user_guides/fs/provenance/provenance/#provenance","text":"","title":"Provenance"},{"location":"user_guides/fs/provenance/provenance/#introduction","text":"Hopsworks feature store allows users to track provenance (lineage) between feature groups, feature views and training dataset. Tracking lineage allows users to determine where/if a feature group is being used. You can track if feature groups are being used to create additional (derived) feature groups or feature views. You can interact with the provenance graph using the UI and the APIs.","title":"Introduction"},{"location":"user_guides/fs/provenance/provenance/#step-1-feature-group-lineage","text":"","title":"Step 1: Feature group lineage"},{"location":"user_guides/fs/provenance/provenance/#assign-parents-to-a-feature-group","text":"When creating a feature group, it is possible to specify a list of feature groups used to create the derived features. For example, you could have an external feature group defined over a Snowflake or Redshift table, which you use to compute the features and save them in a feature group. You can mark the external feature group as parent of the feature group you are creating by using the parents parameter in the get_or_create_feature_group or create_feature_group methods: Python # Retrieve the feature group profiles_fg = fs . get_external_feature_group ( \"user_profiles\" , version = 1 ) # Do feature engineering age_df = transaction_df . merge ( profiles_fg . read (), on = \"cc_num\" , how = \"left\" ) transaction_df [ \"age_at_transaction\" ] = ( age_df [ \"datetime\" ] - age_df [ \"birthdate\" ]) / np . timedelta64 ( 1 , \"Y\" ) # Create the transaction feature group transaction_fg = fs . get_or_create_feature_group ( name = \"transaction_fraud_batch\" , version = 1 , description = \"Transaction features\" , primary_key = [ \"cc_num\" ], event_time = \"datetime\" , parents = [ profiles_fg ] ) transaction_fg . insert ( transaction_df ) Another example use case for derived feature group is if you have a feature group containing features with daily resolution and you are using the content of that feature group to populate a second feature group with monthly resolution: Python # Retrieve the feature group daily_transaction_fg = fs . get_feature_group ( \"daily_transaction\" , version = 1 ) daily_transaction_df = daily_transaction_fg . read () # Do feature engineering cc_group = daily_transaction_df [[ \"cc_num\" , \"amount\" , \"datetime\" ]] \\ . groupby ( \"cc_num\" ) \\ . rolling ( \"1M\" , on = \"datetime\" ) monthly_transaction_df = pd . DataFrame ( cc_group . mean ()) # Create the transaction feature group monthly_transaction_fg = fs . get_or_create_feature_group ( name = \"monthly_transaction_fraud_batch\" , version = 1 , description = \"Transaction features - monthly aggregates\" , primary_key = [ \"cc_num\" ], event_time = \"datetime\" , parents = [ daily_transaction_fg ] ) monthly_transaction_fg . insert ( monthly_transaction_df )","title":"Assign parents to a feature group"},{"location":"user_guides/fs/provenance/provenance/#list-feature-group-parents","text":"You can query the provenance graph of a feature group using the UI and the APIs. From the APIs you can list the parent feature groups by calling the method get_parent_feature_groups Python lineage = transaction_fg . get_parent_feature_groups () # List all accessible parent feature groups lineage . accessible # List all deleted parent feature groups lineage . deleted # List all the inaccessible parent feature groups lineage . inaccessible A parent is marked as deleted (and added to the deleted list) if the parent feature group was deleted. inaccessible if you no longer have access to the parent feature group (e.g. the parent feature group belongs to a project you no longer have access to). To traverse the provenance graph in the opposite direction (i.e. from the parent feature group to the child), you can use the get_generate_feature_groups method. When navigating the provenance graph downstream, the deleted feature groups are not tracked by provenance, as such, the deleted property will always return an empty list. Python lineage = transaction_fg . get_generated_feature_groups () # List all accessible child feature groups lineage . accessible # List all the inaccessible child feature groups lineage . inaccessible You can also visualize the relationship between the parent and child feature groups in the UI. In each feature group overview page you can find a provenance section with the graph of parent feature groups and child feature groups/feature views. Provenance graph of derived feature groups","title":"List feature group parents"},{"location":"user_guides/fs/provenance/provenance/#step-2-feature-view-lineage","text":"The relationship between feature groups and feature views is captured automatically when you create a feature view. You can inspect the relationship between feature groups and feature views using the APIs or the UI.","title":"Step 2: Feature view lineage"},{"location":"user_guides/fs/provenance/provenance/#using-the-apis","text":"Starting from a feature view metadata object, you can traverse upstream the provenance graph to retrieve the metadata objects of the feature groups that are part of the feature view. To do so, you can use the get_parent_feature_groups method. Python lineage = fraud_fv . get_parent_feature_groups () # List all accessible parent feature groups lineage . accessible # List all deleted parent feature groups lineage . deleted # List all the inaccessible parent feature groups lineage . inaccessible You can also traverse the provenance graph in the opposite direction. Starting from a feature group you can navigate downstream and list all the feature views the feature group is used in. As for the derived feature group example above, when navigating the provenance graph downstream deleted feature views are not tracked. As such, the deleted property will always be empty. Python lineage = transaction_fg . get_generated_feature_views () # List all accessible downstream feature views lineage . accessible # List all the inaccessible downstream feature views lineage . inaccessible","title":"Using the APIs"},{"location":"user_guides/fs/provenance/provenance/#using-the-ui","text":"In the feature view overview UI you can explore the provenance graph of the feature view: Feature view provenance graph","title":"Using the UI"},{"location":"user_guides/fs/sharing/sharing/","text":"Sharing # Introduction # Hopsworks allows artifacts (e.g. feature groups, feature views) to be shared between projects. There are two main use cases for sharing features between projects: If you have multiple teams working on the same Hopsworks deployment. Each team works within its own set of projects. If team A wants to leverage features built by team B, they can do so by sharing the feature groups from a team A project to a team B project. By creating different projects for the different stages of the development lifecycle (e.g. a dev project, a testing project, and a production project), you can make sure that changes on the development project don't impact the features in the production project. At the same time, you might want to leverage production features to develop new models or additional features. In this case, you can share the production feature store with the development feature store in read-only mode. Step 1: Open the project of the feature store that you would like to share on Hopsworks. # In the Project Settings navigate to the Shared with other projects section. Step 2: Share # Click Share feature store to bring up the dialog for sharing. In the Project section choose project you wish to share the feature store with. Feature stores can be shared exclusively using read-only permission. This means that a member is not capable of enacting any changes on the shared project. Step 3: Accept the Invitation # In the project where the feature store was shared (step 2) go to Project Settings and navigate to the Shared from other projects section. Click accept . After accepting the share, the shared feature store is listed under the Shared from other projects section. Use features from a shared feature store # Step 1: Get feature store handles # To access features from a shared feature store you need to first retrieve the handle for the shared feature store. To retrieve the handle use the get_feature_store() method and provide the name of the shared feature store import hsfs connection = hsfs . connection () project_feature_store = connection . get_feature_store () shared_feature_store = connection . get_feature_store ( name = \"name_of_shared_feature_store\" ) Step 2: Fetch feature groups # # fetch feature group object from shared feature store shared_fg = shared_feature_store . get_feature_group ( name = \"shared_fg_name\" , version = \"1\" ) # fetch feature group object from project feature store fg_a = project_feature_store . get_or_create_feature_group ( name = \"feature_group_name\" , version = 1 ) Step 3: Join feature groups # # join above feature groups query = shared_fg . select_all () . join ( fg_a . select_all ()) Conclusion # In this guide, you learned how to share a feature store and how to join features from different feature stores.","title":"Sharing"},{"location":"user_guides/fs/sharing/sharing/#sharing","text":"","title":"Sharing"},{"location":"user_guides/fs/sharing/sharing/#introduction","text":"Hopsworks allows artifacts (e.g. feature groups, feature views) to be shared between projects. There are two main use cases for sharing features between projects: If you have multiple teams working on the same Hopsworks deployment. Each team works within its own set of projects. If team A wants to leverage features built by team B, they can do so by sharing the feature groups from a team A project to a team B project. By creating different projects for the different stages of the development lifecycle (e.g. a dev project, a testing project, and a production project), you can make sure that changes on the development project don't impact the features in the production project. At the same time, you might want to leverage production features to develop new models or additional features. In this case, you can share the production feature store with the development feature store in read-only mode.","title":"Introduction"},{"location":"user_guides/fs/sharing/sharing/#step-1-open-the-project-of-the-feature-store-that-you-would-like-to-share-on-hopsworks","text":"In the Project Settings navigate to the Shared with other projects section.","title":"Step 1: Open the project of the feature store that you would like to share on Hopsworks."},{"location":"user_guides/fs/sharing/sharing/#step-2-share","text":"Click Share feature store to bring up the dialog for sharing. In the Project section choose project you wish to share the feature store with. Feature stores can be shared exclusively using read-only permission. This means that a member is not capable of enacting any changes on the shared project.","title":"Step 2: Share"},{"location":"user_guides/fs/sharing/sharing/#step-3-accept-the-invitation","text":"In the project where the feature store was shared (step 2) go to Project Settings and navigate to the Shared from other projects section. Click accept . After accepting the share, the shared feature store is listed under the Shared from other projects section.","title":"Step 3: Accept the Invitation"},{"location":"user_guides/fs/sharing/sharing/#use-features-from-a-shared-feature-store","text":"","title":"Use features from a shared feature store"},{"location":"user_guides/fs/sharing/sharing/#step-1-get-feature-store-handles","text":"To access features from a shared feature store you need to first retrieve the handle for the shared feature store. To retrieve the handle use the get_feature_store() method and provide the name of the shared feature store import hsfs connection = hsfs . connection () project_feature_store = connection . get_feature_store () shared_feature_store = connection . get_feature_store ( name = \"name_of_shared_feature_store\" )","title":"Step 1: Get feature store handles"},{"location":"user_guides/fs/sharing/sharing/#step-2-fetch-feature-groups","text":"# fetch feature group object from shared feature store shared_fg = shared_feature_store . get_feature_group ( name = \"shared_fg_name\" , version = \"1\" ) # fetch feature group object from project feature store fg_a = project_feature_store . get_or_create_feature_group ( name = \"feature_group_name\" , version = 1 )","title":"Step 2: Fetch feature groups"},{"location":"user_guides/fs/sharing/sharing/#step-3-join-feature-groups","text":"# join above feature groups query = shared_fg . select_all () . join ( fg_a . select_all ())","title":"Step 3: Join feature groups"},{"location":"user_guides/fs/sharing/sharing/#conclusion","text":"In this guide, you learned how to share a feature store and how to join features from different feature stores.","title":"Conclusion"},{"location":"user_guides/fs/storage_connector/","text":"Storage Connector Guides # You can define storage connectors in Hopsworks for batch and streaming data sources. Storage connectors securely store the authentication information about how to connect to an external data store. They can be used from programs within Hopsworks or externally. There are three main use cases for Storage Connectors: Simply use it to read data from the storage into a dataframe. External (on-demand) Feature Groups can be defined with storage connectors as data source. This way, Hopsworks stores only the metadata about the features, but does not keep a copy of the data itself. This is also called the Connector API. Write training data to an external storage system to make it accessible by third parties. Storage connectors provide two main mechanisms for authentication: using credentials or an authentication role (IAM Role on AWS or Managed Identity on Azure). Hopsworks supports both a single IAM role (AWS) or Managed Identity (Azure) for the whole Hopsworks cluster or multiple IAM roles (AWS) or Managed Identities (Azure) that can only be assumed by users with a specific role in a specific project. By default, each project is created with three default Storage Connectors: A JDBC connector to the online feature store, a HopsFS connector to the Training Datasets directory of the project and a JDBC connector to the offline feature store. The Storage Connector View in the User Interface Cloud Agnostic # Cloud agnostic storage systems: JDBC : Connect to JDBC compatible databases and query them using SQL. Snowflake : Query Snowflake databases and tables using SQL. Kafka : Read data from a Kafka cluster into a Spark Structured Streaming Dataframe. HopsFS : Easily connect and read from directories of Hopsworks' internal File System. AWS # For AWS the following storage systems are supported: S3 : Read data from a variety of file based storage in S3 such as parquet or CSV. Redshift : Query Redshift databases and tables using SQL. Azure # For AWS the following storage systems are supported: ADLS : Read data from a variety of file based storage in ADLS such as parquet or CSV. GCP # For GCP the following storage systems are supported: BigQuery : Query BigQuery databases and tables using SQL. GCS : Read data from a variety of file based storage in Google Cloud Storage such as parquet or CSV. Next Steps # Move on to the Configuration and Creation Guides to learn how to set up a storage connector.","title":"Storage Connector Guides"},{"location":"user_guides/fs/storage_connector/#storage-connector-guides","text":"You can define storage connectors in Hopsworks for batch and streaming data sources. Storage connectors securely store the authentication information about how to connect to an external data store. They can be used from programs within Hopsworks or externally. There are three main use cases for Storage Connectors: Simply use it to read data from the storage into a dataframe. External (on-demand) Feature Groups can be defined with storage connectors as data source. This way, Hopsworks stores only the metadata about the features, but does not keep a copy of the data itself. This is also called the Connector API. Write training data to an external storage system to make it accessible by third parties. Storage connectors provide two main mechanisms for authentication: using credentials or an authentication role (IAM Role on AWS or Managed Identity on Azure). Hopsworks supports both a single IAM role (AWS) or Managed Identity (Azure) for the whole Hopsworks cluster or multiple IAM roles (AWS) or Managed Identities (Azure) that can only be assumed by users with a specific role in a specific project. By default, each project is created with three default Storage Connectors: A JDBC connector to the online feature store, a HopsFS connector to the Training Datasets directory of the project and a JDBC connector to the offline feature store. The Storage Connector View in the User Interface","title":"Storage Connector Guides"},{"location":"user_guides/fs/storage_connector/#cloud-agnostic","text":"Cloud agnostic storage systems: JDBC : Connect to JDBC compatible databases and query them using SQL. Snowflake : Query Snowflake databases and tables using SQL. Kafka : Read data from a Kafka cluster into a Spark Structured Streaming Dataframe. HopsFS : Easily connect and read from directories of Hopsworks' internal File System.","title":"Cloud Agnostic"},{"location":"user_guides/fs/storage_connector/#aws","text":"For AWS the following storage systems are supported: S3 : Read data from a variety of file based storage in S3 such as parquet or CSV. Redshift : Query Redshift databases and tables using SQL.","title":"AWS"},{"location":"user_guides/fs/storage_connector/#azure","text":"For AWS the following storage systems are supported: ADLS : Read data from a variety of file based storage in ADLS such as parquet or CSV.","title":"Azure"},{"location":"user_guides/fs/storage_connector/#gcp","text":"For GCP the following storage systems are supported: BigQuery : Query BigQuery databases and tables using SQL. GCS : Read data from a variety of file based storage in Google Cloud Storage such as parquet or CSV.","title":"GCP"},{"location":"user_guides/fs/storage_connector/#next-steps","text":"Move on to the Configuration and Creation Guides to learn how to set up a storage connector.","title":"Next Steps"},{"location":"user_guides/fs/storage_connector/usage/","text":"Storage Connector Usage # Here, we look at how to use a Storage Connector after it has been created. Storage Connectors provide an important first step for integrating with external data sources. The 3 fundamental functionalities where storage connectors are used are: Reading data into Spark Dataframes Creating external feature groups Writing training data We will walk through each functionality in the sections below. Retrieving a Storage Connector # We retrieve a storage connector simply by its unique name. PySpark Scala import hsfs # Connect to the Hopsworks feature store hsfs_connection = hsfs . connection () # Retrieve the metadata handle feature_store = hsfs_connection . get_feature_store () # Retrieve storage connector connector = feature_store . get_storage_connector ( 'connector_name' ) import com . logicalclocks . hsfs . _ val connection = HopsworksConnection . builder (). build (); val featureStore = connection . getFeatureStore (); // get directly via connector sub-type class e.g. for GCS type val connector = featureStore . getGcsConnector ( \"connector_name\" ) Reading a Spark Dataframe from a Storage Connector # One of the most common usages of a Storage Connector is to read data directly into a Spark Dataframe. It's achieved via the read API of the connector object, which hides all the complexity of authentication and integration with a data storage source. The read API primarily has two parameters for specifying the data source, path and query , depending on the storage connector type. The exact behaviour could change depending on the storage connector type, but broadly they could be classified as below Data lake/object based connectors # For data sources based on object/file storage such as AWS S3, ADLS, GCS, we set the full object path in the path argument and users should pass a Spark data format (parquet, csv, orc, hudi, delta) to the data_format argument. PySpark Scala # read data into dataframe using path df = connector . read ( data_format = 'data_format' , path = 'fileScheme://bucket/path/' ) // read data into dataframe using path val df = connector . read ( \"\" , \"data_format\" , new HashMap (), \"fileScheme://bucket/path/\" ) Prepare Spark API # Additionally, for reading file based data sources, another way to read the data is using the prepare_spark method. This method can be used if you are reading the data directly through Spark. Firstly, it handles the setup of all Spark configurations or properties necessary for a particular type of connector and prepares the absolute path to read from, along with bucket name and the appropriate file scheme of the data source. A Spark session can handle only one configuration setup at a time, so HSFS cannot set the Spark configurations when retrieving the connector since it would lead to only always initialising the last connector being retrieved. Instead, user can do this setup explicitly with the prepare_spark method and therefore potentially use multiple connectors in one Spark session. prepare_spark handles only one bucket associated with that particular connector, however, it is possible to set up multiple connectors with different types as long as their Spark properties do not interfere with each other. So, for example a S3 connector and a Snowflake connector can be used in the same session, without calling prepare_spark multiple times, as the properties don\u2019t interfere with each other. If the storage connector is used in another API call, prepare_spark gets implicitly invoked, for example, when a user materialises a training dataset using a storage connector or uses the storage connector to set up an External Feature Group. So users do not need to call prepare_spark every time they do an operation with a connector, it is only necessary when reading directly using Spark . Using prepare_spark is also not necessary when using the read API. For example, to read directly from a S3 connector, we use the prepare_spark as follows PySpark connector . prepare_spark () spark . read . format ( \"json\" ) . load ( \"s3a://[bucket]/path\" ) # or spark . read . format ( \"json\" ) . load ( connector . prepare_spark ( \"s3a://[bucket]/path\" )) Data warehouse/SQL based connectors # For data sources accessed via SQL such as data warehouses and JDBC compliant databases, e.g. Redshift, Snowflake, BigQuery, JDBC, users pass the SQL query to read the data to the query argument. In most cases, this will be some form of a SELECT query. Depending on the connector type, users can also just set the table path and read the whole table without explicitly passing any SQL query to the query argument. This is mostly relevant for Google BigQuery. PySpark Scala # read results from a SQL df = connector . read ( query = \"SELECT * FROM TABLE\" ) # or directly read a table if set on connector df = connector . read () // read results from a SQL val df = connector . read ( \"SELECT * FROM TABLE\" , \"\" , new HashMap (), \"\" ) Streaming based connector # For reading data streams, the Kafka Storage Connector supports reading a Kafka topic into Spark Structured Streaming Dataframes instead of a static Dataframe as in other connector types. PySpark df = connector . read_stream ( topic = 'kafka_topic_name' ) Creating an External Feature Group # Another important aspect of a storage connector is its ability to facilitate creation of external feature groups with the Connector API . External feature groups are basically offline feature groups and essentially stored as tables on external data sources. The Connector API relies on storage connectors behind the scenes to integrate with external datasource. This enables seamless integration with any data source as long as there is a storage connector defined. To create an external feature group, we use the create_external_feature_group API, also known as Connector API , and simply pass the storage connector created before to the storage_connector argument. Depending on the external data source, we should set either the query argument for data warehouse based data sources, or the path and data_format arguments for data lake based sources, similar to reading into dataframes as explained in above section. Example for any data warehouse/SQL based external sources, we set the desired SQL to query argument, and set the storage_connector argument to the storage connector object of desired data source. PySpark fg = feature_store . create_external_feature_group ( name = \"sales\" , version = 1 description = \"Physical shop sales features\" , query = \"SELECT * FROM TABLE\" , storage_connector = connector , primary_key = [ 'ss_store_sk' ], event_time = 'sale_date' ) Connector API (external feature groups) only stores the metadata about the features within Hopsworks, while the actual data is still stored externally. This enables users to create feature groups within Hopsworks without the hassle of data migration. For more information on Connector API , read detailed guide about external feature groups . Writing Training Data # Storage connectors are also used while writing training data to external sources. While calling the Feature View API create_training_data , we can pass the storage_connector argument which is necessary to materialise the data to external sources, as shown below. PySpark # materialise a training dataset version , job = feature_view . create_training_data ( description = 'describe training data' , data_format = 'spark_data_format' , # e.g. data_format = \"parquet\" or data_format = \"csv\" write_options = { \"wait_for_job\" : False }, storage_connector = connector ) Read more about training data creation here . Next Steps # We have gone through the basic use cases of a storage connector. For more details about the API functionality for any specific connector type, checkout the API section .","title":"Usage"},{"location":"user_guides/fs/storage_connector/usage/#storage-connector-usage","text":"Here, we look at how to use a Storage Connector after it has been created. Storage Connectors provide an important first step for integrating with external data sources. The 3 fundamental functionalities where storage connectors are used are: Reading data into Spark Dataframes Creating external feature groups Writing training data We will walk through each functionality in the sections below.","title":"Storage Connector Usage"},{"location":"user_guides/fs/storage_connector/usage/#retrieving-a-storage-connector","text":"We retrieve a storage connector simply by its unique name. PySpark Scala import hsfs # Connect to the Hopsworks feature store hsfs_connection = hsfs . connection () # Retrieve the metadata handle feature_store = hsfs_connection . get_feature_store () # Retrieve storage connector connector = feature_store . get_storage_connector ( 'connector_name' ) import com . logicalclocks . hsfs . _ val connection = HopsworksConnection . builder (). build (); val featureStore = connection . getFeatureStore (); // get directly via connector sub-type class e.g. for GCS type val connector = featureStore . getGcsConnector ( \"connector_name\" )","title":"Retrieving a Storage Connector"},{"location":"user_guides/fs/storage_connector/usage/#reading-a-spark-dataframe-from-a-storage-connector","text":"One of the most common usages of a Storage Connector is to read data directly into a Spark Dataframe. It's achieved via the read API of the connector object, which hides all the complexity of authentication and integration with a data storage source. The read API primarily has two parameters for specifying the data source, path and query , depending on the storage connector type. The exact behaviour could change depending on the storage connector type, but broadly they could be classified as below","title":"Reading a Spark Dataframe from a Storage Connector"},{"location":"user_guides/fs/storage_connector/usage/#data-lakeobject-based-connectors","text":"For data sources based on object/file storage such as AWS S3, ADLS, GCS, we set the full object path in the path argument and users should pass a Spark data format (parquet, csv, orc, hudi, delta) to the data_format argument. PySpark Scala # read data into dataframe using path df = connector . read ( data_format = 'data_format' , path = 'fileScheme://bucket/path/' ) // read data into dataframe using path val df = connector . read ( \"\" , \"data_format\" , new HashMap (), \"fileScheme://bucket/path/\" )","title":"Data lake/object based connectors"},{"location":"user_guides/fs/storage_connector/usage/#prepare-spark-api","text":"Additionally, for reading file based data sources, another way to read the data is using the prepare_spark method. This method can be used if you are reading the data directly through Spark. Firstly, it handles the setup of all Spark configurations or properties necessary for a particular type of connector and prepares the absolute path to read from, along with bucket name and the appropriate file scheme of the data source. A Spark session can handle only one configuration setup at a time, so HSFS cannot set the Spark configurations when retrieving the connector since it would lead to only always initialising the last connector being retrieved. Instead, user can do this setup explicitly with the prepare_spark method and therefore potentially use multiple connectors in one Spark session. prepare_spark handles only one bucket associated with that particular connector, however, it is possible to set up multiple connectors with different types as long as their Spark properties do not interfere with each other. So, for example a S3 connector and a Snowflake connector can be used in the same session, without calling prepare_spark multiple times, as the properties don\u2019t interfere with each other. If the storage connector is used in another API call, prepare_spark gets implicitly invoked, for example, when a user materialises a training dataset using a storage connector or uses the storage connector to set up an External Feature Group. So users do not need to call prepare_spark every time they do an operation with a connector, it is only necessary when reading directly using Spark . Using prepare_spark is also not necessary when using the read API. For example, to read directly from a S3 connector, we use the prepare_spark as follows PySpark connector . prepare_spark () spark . read . format ( \"json\" ) . load ( \"s3a://[bucket]/path\" ) # or spark . read . format ( \"json\" ) . load ( connector . prepare_spark ( \"s3a://[bucket]/path\" ))","title":"Prepare Spark API"},{"location":"user_guides/fs/storage_connector/usage/#data-warehousesql-based-connectors","text":"For data sources accessed via SQL such as data warehouses and JDBC compliant databases, e.g. Redshift, Snowflake, BigQuery, JDBC, users pass the SQL query to read the data to the query argument. In most cases, this will be some form of a SELECT query. Depending on the connector type, users can also just set the table path and read the whole table without explicitly passing any SQL query to the query argument. This is mostly relevant for Google BigQuery. PySpark Scala # read results from a SQL df = connector . read ( query = \"SELECT * FROM TABLE\" ) # or directly read a table if set on connector df = connector . read () // read results from a SQL val df = connector . read ( \"SELECT * FROM TABLE\" , \"\" , new HashMap (), \"\" )","title":"Data warehouse/SQL based connectors"},{"location":"user_guides/fs/storage_connector/usage/#streaming-based-connector","text":"For reading data streams, the Kafka Storage Connector supports reading a Kafka topic into Spark Structured Streaming Dataframes instead of a static Dataframe as in other connector types. PySpark df = connector . read_stream ( topic = 'kafka_topic_name' )","title":"Streaming based connector"},{"location":"user_guides/fs/storage_connector/usage/#creating-an-external-feature-group","text":"Another important aspect of a storage connector is its ability to facilitate creation of external feature groups with the Connector API . External feature groups are basically offline feature groups and essentially stored as tables on external data sources. The Connector API relies on storage connectors behind the scenes to integrate with external datasource. This enables seamless integration with any data source as long as there is a storage connector defined. To create an external feature group, we use the create_external_feature_group API, also known as Connector API , and simply pass the storage connector created before to the storage_connector argument. Depending on the external data source, we should set either the query argument for data warehouse based data sources, or the path and data_format arguments for data lake based sources, similar to reading into dataframes as explained in above section. Example for any data warehouse/SQL based external sources, we set the desired SQL to query argument, and set the storage_connector argument to the storage connector object of desired data source. PySpark fg = feature_store . create_external_feature_group ( name = \"sales\" , version = 1 description = \"Physical shop sales features\" , query = \"SELECT * FROM TABLE\" , storage_connector = connector , primary_key = [ 'ss_store_sk' ], event_time = 'sale_date' ) Connector API (external feature groups) only stores the metadata about the features within Hopsworks, while the actual data is still stored externally. This enables users to create feature groups within Hopsworks without the hassle of data migration. For more information on Connector API , read detailed guide about external feature groups .","title":"Creating an External Feature Group"},{"location":"user_guides/fs/storage_connector/usage/#writing-training-data","text":"Storage connectors are also used while writing training data to external sources. While calling the Feature View API create_training_data , we can pass the storage_connector argument which is necessary to materialise the data to external sources, as shown below. PySpark # materialise a training dataset version , job = feature_view . create_training_data ( description = 'describe training data' , data_format = 'spark_data_format' , # e.g. data_format = \"parquet\" or data_format = \"csv\" write_options = { \"wait_for_job\" : False }, storage_connector = connector ) Read more about training data creation here .","title":"Writing Training Data"},{"location":"user_guides/fs/storage_connector/usage/#next-steps","text":"We have gone through the basic use cases of a storage connector. For more details about the API functionality for any specific connector type, checkout the API section .","title":"Next Steps"},{"location":"user_guides/fs/storage_connector/creation/adls/","text":"How-To set up a ADLS Storage Connector # Introduction # Azure Data Lake Storage (ADLS) Gen2 is a HDFS-compatible filesystem on Azure for data analytics. The ADLS Gen2 filesystem stores its data in Azure Blob storage, ensuring low-cost storage, high availability, and disaster recovery. In Hopsworks, you can access ADLS Gen2 by defining a Storage Connector and creating and granting permissions to a service principal. In this guide, you will configure a Storage Connector in Hopsworks to save all the authentication information needed in order to set up a connection to your Azure ADLS filesystem. When you're finished, you'll be able to read files using Spark through HSFS APIs. You can also use the connector to write out training data from the Feature Store, in order to make it accessible by third parties. Note Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically. Prerequisites # Before you begin this guide you'll need to retrieve the following information from your Azure ADLS account: Data Lake Storage Gen2 Account: Create an Azure Data Lake Storage Gen2 account and initialize a filesystem, enabling the hierarchical namespace . Note that your storage account must belong to an Azure resource group. Azure AD application and service principal: Create an Azure AD application and service principal that can access your ADLS storage account and its resource group. Service Principal Registration: Register the service principal, granting it a role assignment such as Storage Blob Data Contributor, on the Azure Data Lake Storage Gen2 account. Info When you specify the 'container name' in the ADLS storage connector, you need to have previously created that container - the Hopsworks Feature Store will not create that storage container for you. Creation in the UI # Step 1: Set up new storage connector # Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2). The Storage Connector View in the User Interface Step 2: Enter ADLS Information # Enter the details for your ADLS connector. Start by giving it a name and an optional description . ADLS Connector Creation Form Step 3: Azure Create an ADLS Resource # When programmatically signing in, you need to pass the tenant ID with your authentication request and the application ID. You also need a certificate or an authentication key (described in the following section). To get those values, use the following steps: Select Azure Active Directory. From App registrations in Azure AD, select your application. Copy the Directory (tenant) ID and store it in your application code. You need to copy the Directory (tenant) id and paste it to the Hopsworks ADLS storage connector \"Directory id\" text field. Copy the Application ID and store it in your application code. >You need to copy the Application id and paste it to the Hopsworks ADLS storage connector \"Application id\" text field. Create an Application Secret and copy it into the Service Credential field. You need to copy the Application Secret and paste it to the Hopsworks ADLS storage connector \"Service Credential\" text field. Common Problems # If you get a permission denied error when writing or reading to/from a ADLS container, it is often because the storage principal (app) does not have the correct permissions. Have you added the \"Storage Blob Data Owner\" or \"Storage Blob Data Contributor\" role to the resource group for your storage account (or the subscription for your storage group, if you apply roles at the subscription level)? Go to your resource group, then in \"Access Control (IAM)\", click the \"Add\" button to add a \"role assignment\". If you get an error \"StatusCode=404 StatusDescription=The specified filesystem does not exist.\", then maybe you have not created the storage account or the storage container. References # How to create a service principal on Azure Next Steps # Move on to the usage guide for storage connectors to see how you can use your newly created ADLS connector.","title":"ADLS"},{"location":"user_guides/fs/storage_connector/creation/adls/#how-to-set-up-a-adls-storage-connector","text":"","title":"How-To set up a ADLS Storage Connector"},{"location":"user_guides/fs/storage_connector/creation/adls/#introduction","text":"Azure Data Lake Storage (ADLS) Gen2 is a HDFS-compatible filesystem on Azure for data analytics. The ADLS Gen2 filesystem stores its data in Azure Blob storage, ensuring low-cost storage, high availability, and disaster recovery. In Hopsworks, you can access ADLS Gen2 by defining a Storage Connector and creating and granting permissions to a service principal. In this guide, you will configure a Storage Connector in Hopsworks to save all the authentication information needed in order to set up a connection to your Azure ADLS filesystem. When you're finished, you'll be able to read files using Spark through HSFS APIs. You can also use the connector to write out training data from the Feature Store, in order to make it accessible by third parties. Note Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically.","title":"Introduction"},{"location":"user_guides/fs/storage_connector/creation/adls/#prerequisites","text":"Before you begin this guide you'll need to retrieve the following information from your Azure ADLS account: Data Lake Storage Gen2 Account: Create an Azure Data Lake Storage Gen2 account and initialize a filesystem, enabling the hierarchical namespace . Note that your storage account must belong to an Azure resource group. Azure AD application and service principal: Create an Azure AD application and service principal that can access your ADLS storage account and its resource group. Service Principal Registration: Register the service principal, granting it a role assignment such as Storage Blob Data Contributor, on the Azure Data Lake Storage Gen2 account. Info When you specify the 'container name' in the ADLS storage connector, you need to have previously created that container - the Hopsworks Feature Store will not create that storage container for you.","title":"Prerequisites"},{"location":"user_guides/fs/storage_connector/creation/adls/#creation-in-the-ui","text":"","title":"Creation in the UI"},{"location":"user_guides/fs/storage_connector/creation/adls/#step-1-set-up-new-storage-connector","text":"Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2). The Storage Connector View in the User Interface","title":"Step 1: Set up new storage connector"},{"location":"user_guides/fs/storage_connector/creation/adls/#step-2-enter-adls-information","text":"Enter the details for your ADLS connector. Start by giving it a name and an optional description . ADLS Connector Creation Form","title":"Step 2: Enter ADLS Information"},{"location":"user_guides/fs/storage_connector/creation/adls/#step-3-azure-create-an-adls-resource","text":"When programmatically signing in, you need to pass the tenant ID with your authentication request and the application ID. You also need a certificate or an authentication key (described in the following section). To get those values, use the following steps: Select Azure Active Directory. From App registrations in Azure AD, select your application. Copy the Directory (tenant) ID and store it in your application code. You need to copy the Directory (tenant) id and paste it to the Hopsworks ADLS storage connector \"Directory id\" text field. Copy the Application ID and store it in your application code. >You need to copy the Application id and paste it to the Hopsworks ADLS storage connector \"Application id\" text field. Create an Application Secret and copy it into the Service Credential field. You need to copy the Application Secret and paste it to the Hopsworks ADLS storage connector \"Service Credential\" text field.","title":"Step 3: Azure Create an ADLS Resource"},{"location":"user_guides/fs/storage_connector/creation/adls/#common-problems","text":"If you get a permission denied error when writing or reading to/from a ADLS container, it is often because the storage principal (app) does not have the correct permissions. Have you added the \"Storage Blob Data Owner\" or \"Storage Blob Data Contributor\" role to the resource group for your storage account (or the subscription for your storage group, if you apply roles at the subscription level)? Go to your resource group, then in \"Access Control (IAM)\", click the \"Add\" button to add a \"role assignment\". If you get an error \"StatusCode=404 StatusDescription=The specified filesystem does not exist.\", then maybe you have not created the storage account or the storage container.","title":"Common Problems"},{"location":"user_guides/fs/storage_connector/creation/adls/#references","text":"How to create a service principal on Azure","title":"References"},{"location":"user_guides/fs/storage_connector/creation/adls/#next-steps","text":"Move on to the usage guide for storage connectors to see how you can use your newly created ADLS connector.","title":"Next Steps"},{"location":"user_guides/fs/storage_connector/creation/bigquery/","text":"How-To set up a BigQuery Storage Connector # Introduction # A BigQuery storage connector provides integration to Google Cloud BigQuery. BigQuery is Google Cloud's managed data warehouse supporting that lets you run analytics and execute SQL queries over large scale data. Such data warehouses are often the source of raw data for feature engineering pipelines. In this guide, you will configure a Storage Connector in Hopsworks to connect to your BigQuery project by saving the necessary information. When you're finished, you'll be able to execute queries and read results of BigQuery using Spark through HSFS APIs. The storage connector uses the Google spark-bigquery-connector behind the scenes. To read more about the spark connector, like the spark options or usage, check Apache Spark SQL connector for Google BigQuery. Note Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically. Prerequisites # Before you begin this guide you'll need to retrieve the following information about your GCP account: BigQuery Project: You need a BigQuery project, dataset and table created and have read access to it. Or, if you wish to query a public dataset you need its corresponding details. Authentication Method: Authentication to GCP account is handled by uploading the JSON keyfile for service account to the Hopsworks Project. You will need to create this JSON keyfile from GCP. For more information on service accounts and creating keyfile in GCP, read Google Cloud documentation. Note To read data, the BigQuery service account user needs permission to create read sesssion which is available in BigQuery Admin role . Creation in the UI # Step 1: Set up new storage connector # Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2). The Storage Connector View in the User Interface Step 2: Enter connector details # Enter the details for your BigQuery connector. Start by giving it a unique name and an optional description . BigQuery Connector Creation Form Choose Google BigQuery from the connector options. Next, set the name of the parent BigQuery project. This is used for billing by GCP. Authentication: Here you should upload your JSON keyfile for service account used for authentication. You can choose to either upload from your local using Upload new file or choose an existing file within project using From Project . Read Options: There are two ways to read via BigQuery, using the BigQuery Table or BigQuery Query option: BigQuery Table - This option reads directly from BigQuery table reference. Note that it can only be used in read API for reading data from BigQuery. Creating external Feature Groups using this option is not yet supported. In the UI set the below fields, BigQuery Project : The BigQuery project BigQuery Dataset : The dataset of the table BigQuery Table : The table to read BigQuery Query - This option executes a SQL query at runtime. It can be used for both reading data and creating external Feature Groups . Materialization Dataset : Temporary dataset used by BigQuery for writing. It must be set to a dataset where the GCP user has table creation permission. The queried table must be in the same location as the materializationDataset (e.g 'EU' or 'US'). Also, if a table in the SQL statement is from project other than the parentProject then use the fully qualified table name i.e. [project].[dataset].[table] (Read more details from Google documentation on usage of query for BigQuery spark connector here ). Spark Options: Optionally, you can set additional spark options using the Key - Value pairs. Next Steps # Move on to the usage guide for storage connectors to see how you can use your newly created BigQuery connector.","title":"BigQuery"},{"location":"user_guides/fs/storage_connector/creation/bigquery/#how-to-set-up-a-bigquery-storage-connector","text":"","title":"How-To set up a BigQuery Storage Connector"},{"location":"user_guides/fs/storage_connector/creation/bigquery/#introduction","text":"A BigQuery storage connector provides integration to Google Cloud BigQuery. BigQuery is Google Cloud's managed data warehouse supporting that lets you run analytics and execute SQL queries over large scale data. Such data warehouses are often the source of raw data for feature engineering pipelines. In this guide, you will configure a Storage Connector in Hopsworks to connect to your BigQuery project by saving the necessary information. When you're finished, you'll be able to execute queries and read results of BigQuery using Spark through HSFS APIs. The storage connector uses the Google spark-bigquery-connector behind the scenes. To read more about the spark connector, like the spark options or usage, check Apache Spark SQL connector for Google BigQuery. Note Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically.","title":"Introduction"},{"location":"user_guides/fs/storage_connector/creation/bigquery/#prerequisites","text":"Before you begin this guide you'll need to retrieve the following information about your GCP account: BigQuery Project: You need a BigQuery project, dataset and table created and have read access to it. Or, if you wish to query a public dataset you need its corresponding details. Authentication Method: Authentication to GCP account is handled by uploading the JSON keyfile for service account to the Hopsworks Project. You will need to create this JSON keyfile from GCP. For more information on service accounts and creating keyfile in GCP, read Google Cloud documentation. Note To read data, the BigQuery service account user needs permission to create read sesssion which is available in BigQuery Admin role .","title":"Prerequisites"},{"location":"user_guides/fs/storage_connector/creation/bigquery/#creation-in-the-ui","text":"","title":"Creation in the UI"},{"location":"user_guides/fs/storage_connector/creation/bigquery/#step-1-set-up-new-storage-connector","text":"Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2). The Storage Connector View in the User Interface","title":"Step 1: Set up new storage connector"},{"location":"user_guides/fs/storage_connector/creation/bigquery/#step-2-enter-connector-details","text":"Enter the details for your BigQuery connector. Start by giving it a unique name and an optional description . BigQuery Connector Creation Form Choose Google BigQuery from the connector options. Next, set the name of the parent BigQuery project. This is used for billing by GCP. Authentication: Here you should upload your JSON keyfile for service account used for authentication. You can choose to either upload from your local using Upload new file or choose an existing file within project using From Project . Read Options: There are two ways to read via BigQuery, using the BigQuery Table or BigQuery Query option: BigQuery Table - This option reads directly from BigQuery table reference. Note that it can only be used in read API for reading data from BigQuery. Creating external Feature Groups using this option is not yet supported. In the UI set the below fields, BigQuery Project : The BigQuery project BigQuery Dataset : The dataset of the table BigQuery Table : The table to read BigQuery Query - This option executes a SQL query at runtime. It can be used for both reading data and creating external Feature Groups . Materialization Dataset : Temporary dataset used by BigQuery for writing. It must be set to a dataset where the GCP user has table creation permission. The queried table must be in the same location as the materializationDataset (e.g 'EU' or 'US'). Also, if a table in the SQL statement is from project other than the parentProject then use the fully qualified table name i.e. [project].[dataset].[table] (Read more details from Google documentation on usage of query for BigQuery spark connector here ). Spark Options: Optionally, you can set additional spark options using the Key - Value pairs.","title":"Step 2: Enter connector details"},{"location":"user_guides/fs/storage_connector/creation/bigquery/#next-steps","text":"Move on to the usage guide for storage connectors to see how you can use your newly created BigQuery connector.","title":"Next Steps"},{"location":"user_guides/fs/storage_connector/creation/gcs/","text":"How-To set up a GCS Storage Connector # Introduction # This particular type of storage connector provides integration to Google Cloud Storage (GCS). GCS is an object storage service offered by Google Cloud. An object could be simply any piece of immutable data consisting of a file of any format, for example a CSV or PARQUET . These objects are stored in containers called as buckets . These types of storages are often the source for raw data from which features can be engineered. In this guide, you will configure a Storage Connector in Hopsworks to connect to your GCS bucket by saving the necessary information. When you're finished, you'll be able to read files from the GCS bucket using Spark through HSFS APIs. The storage connector uses the Google gcs-connector-hadoop behind the scenes. For more information, check out Google Cloud Storage Connector for Spark and Hadoop Note Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically. Prerequisites # Before you begin this guide you'll need to retrieve the following information about your GCP account and bucket: Bucket: You need a GCS bucket created and have read access to it. The bucket is identified by its name. Authentication Method: Authentication to GCP account is handled by uploading the JSON keyfile for service account to the Hopsworks Project. You will need to create this JSON keyfile from GCP. For more information on service accounts and creating keyfile in GCP, read Google Cloud documentation. Server-side Encryption GCS encrypts the data on server side by default. The connector additionally supports the optional encryption method Customer Supplied Encryption Key by GCP. You can choose the encryption option AES-256 and provide AES-256 key and hash, encoded in standard Base64. The encryption details are stored as Secrets in the Hopsworks for keeping it secure. Read more about encryption on Google Documentation. Creation in the UI # Step 1: Set up new storage connector # Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2). The Storage Connector View in the User Interface Step 2: Enter connector details # Enter the details for your GCS connector. Start by giving it a unique name and an optional description . GCS Connector Creation Form Choose Google Cloud Storage from the connector options. Next, set the name of the GCS Bucket you wish to connect with. Authentication: Here you should upload your JSON keyfile for service account used for authentication. You can choose to either upload from your local using Upload new file or choose an existing file within project using From Project . GCS Server Side Encryption: You can leave this to Default Encryption if you do not wish to provide explicit encrypting keys. Otherwise, optionally you can set the encryption setting for AES-256 and provide the encryption key and hash when selected. Next Steps # Move on to the usage guide for storage connectors to see how you can use your newly created GCS connector.","title":"GCS"},{"location":"user_guides/fs/storage_connector/creation/gcs/#how-to-set-up-a-gcs-storage-connector","text":"","title":"How-To set up a GCS Storage Connector"},{"location":"user_guides/fs/storage_connector/creation/gcs/#introduction","text":"This particular type of storage connector provides integration to Google Cloud Storage (GCS). GCS is an object storage service offered by Google Cloud. An object could be simply any piece of immutable data consisting of a file of any format, for example a CSV or PARQUET . These objects are stored in containers called as buckets . These types of storages are often the source for raw data from which features can be engineered. In this guide, you will configure a Storage Connector in Hopsworks to connect to your GCS bucket by saving the necessary information. When you're finished, you'll be able to read files from the GCS bucket using Spark through HSFS APIs. The storage connector uses the Google gcs-connector-hadoop behind the scenes. For more information, check out Google Cloud Storage Connector for Spark and Hadoop Note Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically.","title":"Introduction"},{"location":"user_guides/fs/storage_connector/creation/gcs/#prerequisites","text":"Before you begin this guide you'll need to retrieve the following information about your GCP account and bucket: Bucket: You need a GCS bucket created and have read access to it. The bucket is identified by its name. Authentication Method: Authentication to GCP account is handled by uploading the JSON keyfile for service account to the Hopsworks Project. You will need to create this JSON keyfile from GCP. For more information on service accounts and creating keyfile in GCP, read Google Cloud documentation. Server-side Encryption GCS encrypts the data on server side by default. The connector additionally supports the optional encryption method Customer Supplied Encryption Key by GCP. You can choose the encryption option AES-256 and provide AES-256 key and hash, encoded in standard Base64. The encryption details are stored as Secrets in the Hopsworks for keeping it secure. Read more about encryption on Google Documentation.","title":"Prerequisites"},{"location":"user_guides/fs/storage_connector/creation/gcs/#creation-in-the-ui","text":"","title":"Creation in the UI"},{"location":"user_guides/fs/storage_connector/creation/gcs/#step-1-set-up-new-storage-connector","text":"Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2). The Storage Connector View in the User Interface","title":"Step 1: Set up new storage connector"},{"location":"user_guides/fs/storage_connector/creation/gcs/#step-2-enter-connector-details","text":"Enter the details for your GCS connector. Start by giving it a unique name and an optional description . GCS Connector Creation Form Choose Google Cloud Storage from the connector options. Next, set the name of the GCS Bucket you wish to connect with. Authentication: Here you should upload your JSON keyfile for service account used for authentication. You can choose to either upload from your local using Upload new file or choose an existing file within project using From Project . GCS Server Side Encryption: You can leave this to Default Encryption if you do not wish to provide explicit encrypting keys. Otherwise, optionally you can set the encryption setting for AES-256 and provide the encryption key and hash when selected.","title":"Step 2: Enter connector details"},{"location":"user_guides/fs/storage_connector/creation/gcs/#next-steps","text":"Move on to the usage guide for storage connectors to see how you can use your newly created GCS connector.","title":"Next Steps"},{"location":"user_guides/fs/storage_connector/creation/hopsfs/","text":"How-To set up a HopsFS Storage Connector # Introduction # HopsFS is a HDFS-compatible filesystem on AWS/Azure/on-premises for data analytics. HopsFS stores its data on object storage in the cloud (S3 in AWs and Blob storage on Azure) and on commodity servers on-premises, ensuring low-cost storage, high availability, and disaster recovery. In Hopsworks, you can access HopsFS natively in programs (Spark, TensorFlow, etc) without the need to define a Storage Connector. By default, every Project has a Storage Connector for Training Datasets. When you create training datasets from features in the Feature Store the HopsFS connector is the default Storage Connector. However, if you want to output data to a different dataset, you can define a new Storage Connector for that dataset. In this guide, you will configure a HopsFS Storage Connector in Hopsworks which points at a different directory on the file system than the Training Datasets directory. When you're finished, you'll be able to write training data to different locations in your cluster through HSFS APIs. Note Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically. Prerequisites # Before you begin this guide you'll need to identify a directory on the filesystem of Hopsworks, to which you want to point the Storage Connector that you are going to create. Creation in the UI # Step 1: Set up new storage connector # Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2). The Storage Connector View in the User Interface Step 2: Enter HopsFS Settings # Enter the details for your HopsFS connector. Start by giving it a name and an optional description . Select \"HopsFS\" as connector protocol. Select the top-level directory to point the connector to. Click \"Setup storage connector\". HopsFS Connector Creation Form Next Steps # Move on to the usage guide for storage connectors to see how you can use your newly created HopsFS connector.","title":"HopsFS"},{"location":"user_guides/fs/storage_connector/creation/hopsfs/#how-to-set-up-a-hopsfs-storage-connector","text":"","title":"How-To set up a HopsFS Storage Connector"},{"location":"user_guides/fs/storage_connector/creation/hopsfs/#introduction","text":"HopsFS is a HDFS-compatible filesystem on AWS/Azure/on-premises for data analytics. HopsFS stores its data on object storage in the cloud (S3 in AWs and Blob storage on Azure) and on commodity servers on-premises, ensuring low-cost storage, high availability, and disaster recovery. In Hopsworks, you can access HopsFS natively in programs (Spark, TensorFlow, etc) without the need to define a Storage Connector. By default, every Project has a Storage Connector for Training Datasets. When you create training datasets from features in the Feature Store the HopsFS connector is the default Storage Connector. However, if you want to output data to a different dataset, you can define a new Storage Connector for that dataset. In this guide, you will configure a HopsFS Storage Connector in Hopsworks which points at a different directory on the file system than the Training Datasets directory. When you're finished, you'll be able to write training data to different locations in your cluster through HSFS APIs. Note Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically.","title":"Introduction"},{"location":"user_guides/fs/storage_connector/creation/hopsfs/#prerequisites","text":"Before you begin this guide you'll need to identify a directory on the filesystem of Hopsworks, to which you want to point the Storage Connector that you are going to create.","title":"Prerequisites"},{"location":"user_guides/fs/storage_connector/creation/hopsfs/#creation-in-the-ui","text":"","title":"Creation in the UI"},{"location":"user_guides/fs/storage_connector/creation/hopsfs/#step-1-set-up-new-storage-connector","text":"Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2). The Storage Connector View in the User Interface","title":"Step 1: Set up new storage connector"},{"location":"user_guides/fs/storage_connector/creation/hopsfs/#step-2-enter-hopsfs-settings","text":"Enter the details for your HopsFS connector. Start by giving it a name and an optional description . Select \"HopsFS\" as connector protocol. Select the top-level directory to point the connector to. Click \"Setup storage connector\". HopsFS Connector Creation Form","title":"Step 2: Enter HopsFS Settings"},{"location":"user_guides/fs/storage_connector/creation/hopsfs/#next-steps","text":"Move on to the usage guide for storage connectors to see how you can use your newly created HopsFS connector.","title":"Next Steps"},{"location":"user_guides/fs/storage_connector/creation/jdbc/","text":"How-To set up a JDBC Storage Connector # Introduction # JDBC is an API provided by many database systems. Using JDBC connections one can query and update data in a database, usually oriented towards relational databases. Examples of databases you can connect to using JDBC are MySQL, Postgres, Oracle, DB2, MongoDB or Microsoft SQLServer. In this guide, you will configure a Storage Connector in Hopsworks to save all the authentication information needed in order to set up a JDBC connection to your database of choice. When you're finished, you'll be able to query the database using Spark through HSFS APIs. Note Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically. Prerequisites # Before you begin this guide you'll need to retrieve the following information from your JDBC compatible database: JDBC Connection URL: Consult the documentation of your target database to determine the correct JDBC URL and parameters. As an example, for MySQL the URL could be: jdbc:mysql://10.0.2.15:3306/[databaseName]?useSSL=false&allowPublicKeyRetrieval=true Username and Password: Typically, you will need to add username and password in your JDBC URL or as key/value parameters. So make sure you have retrieved a username and password with the suitable permissions for the database and table you want to query. Creation in the UI # Step 1: Set up new storage connector # Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2). The Storage Connector View in the User Interface Step 2: Enter JDBC Settings # Enter the details for your JDBC enabled database. Select \"JDBC\" as connector protocol. Enter the JDBC connection url. This can for example also contain the username and password. Add additional key/value arguments to be passed to the connection. These might differ by database. It can be the username and password. Click \"Setup storage connector\". JDBC Connector Creation Form Next Steps # Move on to the usage guide for storage connectors to see how you can use your newly created JDBC connector.","title":"JDBC"},{"location":"user_guides/fs/storage_connector/creation/jdbc/#how-to-set-up-a-jdbc-storage-connector","text":"","title":"How-To set up a JDBC Storage Connector"},{"location":"user_guides/fs/storage_connector/creation/jdbc/#introduction","text":"JDBC is an API provided by many database systems. Using JDBC connections one can query and update data in a database, usually oriented towards relational databases. Examples of databases you can connect to using JDBC are MySQL, Postgres, Oracle, DB2, MongoDB or Microsoft SQLServer. In this guide, you will configure a Storage Connector in Hopsworks to save all the authentication information needed in order to set up a JDBC connection to your database of choice. When you're finished, you'll be able to query the database using Spark through HSFS APIs. Note Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically.","title":"Introduction"},{"location":"user_guides/fs/storage_connector/creation/jdbc/#prerequisites","text":"Before you begin this guide you'll need to retrieve the following information from your JDBC compatible database: JDBC Connection URL: Consult the documentation of your target database to determine the correct JDBC URL and parameters. As an example, for MySQL the URL could be: jdbc:mysql://10.0.2.15:3306/[databaseName]?useSSL=false&allowPublicKeyRetrieval=true Username and Password: Typically, you will need to add username and password in your JDBC URL or as key/value parameters. So make sure you have retrieved a username and password with the suitable permissions for the database and table you want to query.","title":"Prerequisites"},{"location":"user_guides/fs/storage_connector/creation/jdbc/#creation-in-the-ui","text":"","title":"Creation in the UI"},{"location":"user_guides/fs/storage_connector/creation/jdbc/#step-1-set-up-new-storage-connector","text":"Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2). The Storage Connector View in the User Interface","title":"Step 1: Set up new storage connector"},{"location":"user_guides/fs/storage_connector/creation/jdbc/#step-2-enter-jdbc-settings","text":"Enter the details for your JDBC enabled database. Select \"JDBC\" as connector protocol. Enter the JDBC connection url. This can for example also contain the username and password. Add additional key/value arguments to be passed to the connection. These might differ by database. It can be the username and password. Click \"Setup storage connector\". JDBC Connector Creation Form","title":"Step 2: Enter JDBC Settings"},{"location":"user_guides/fs/storage_connector/creation/jdbc/#next-steps","text":"Move on to the usage guide for storage connectors to see how you can use your newly created JDBC connector.","title":"Next Steps"},{"location":"user_guides/fs/storage_connector/creation/kafka/","text":"How-To set up a Kafka Storage Connector # Introduction # Apache Kafka is a distributed event store and stream-processing platform. It's a very popular framework for handling realtime data streams and is often used as a message broker for events coming from production systems until they are being processed and either loaded into a data warehouse or aggregated into features for Machine Learning. In this guide, you will configure a Storage Connector in Hopsworks to save all the authentication information needed in order to set up a connection to your Kafka cluster. When you're finished, you'll be able to read from Kafka topics in your cluster using Spark through HSFS APIs. Note Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically. Prerequisites # Before you begin this guide you'll need to retrieve the following information from Kafka cluster, the following options are mandatory : Kafka Bootstrap servers: It is the url of one of the Kafka brokers which you give to fetch the initial metadata about your Kafka cluster. The metadata consists of the topics, their partitions, the leader brokers for those partitions etc. Depending upon this metadata your producer or consumer produces or consumes the data. Security Protocol: The security protocol you want to use to authenticate with your Kafka cluster. Make sure the chosen protocol is supported by your cluster. For an overview of the available protocols, please see the Confluent Kafka Documentation . Certificates: Depending on the chosen security protocol, you might need TrustStore and KeyStore files along with the corresponding key password. Contact your Kafka administrator, if you don't know how to retrieve these. If you want to setup a storage connector to Hopsworks' internal Kafka cluster, you can download the needed certificates from the integration tab in your project settings. Creation in the UI # Step 1: Set up new storage connector # Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2). The Storage Connector View in the User Interface Step 2: Enter Kafka Settings # Enter the details for your Kafka connector. Start by giving it a name and an optional description . Select \"Kafka\" as connector protocol. Add all the bootstrap server addresses and ports that you want the consumers/producers to connect to. The client will make use of all servers irrespective of which servers are specified here for bootstrapping\u2014this list only impacts the initial hosts used to discover the full set of servers. Choose the Security protocol. TSL/SSL By default, Apache Kafka communicates in PLAINTEXT , which means that all data is sent in the clear. To encrypt communication, you should configure all the Confluent Platform components in your deployment to use TLS/SSL encryption. TLS uses private-key/certificate pairs, which are used during the TLS handshake process. Each broker needs its own private-key/certificate pair, and the client uses the certificate to authenticate the broker. Each logical client needs a private-key/certificate pair if client authentication is enabled, and the broker uses the certificate to authenticate the client. These are provided in the form of TrustStore and KeyStore JKS files together with a key password. For more information, refer to the official Apacha Kafka Guide for TSL/SSL authentication . SASL SSL or SASL plaintext Apache Kafka brokers support client authentication using SASL. SASL authentication can be enabled concurrently with TLS/SSL encryption (TLS/SSL client authentication will be disabled). This authentication method often requires extra arguments depending on your setup. Make use of the optional additional key/value arguments (5) to provide these. SASL authentication can be enabled concurrently with TLS/SSL encryption (TLS/SSL client authentication will be disabled). For more information, please refer to the offical Apache Kafka Guide for SASL authentication . The endpoint identification algorithm used by clients to validate server host name. The default value is https . Clients including client connections created by the broker for inter-broker communication verify that the broker host name matches the host name in the broker\u2019s certificate. Optional additional key/value arguments. Click \"Setup storage connector\". Kafka Connector Creation Form Next Steps # Move on to the usage guide for storage connectors to see how you can use your newly created Kafka connector.","title":"Kafka"},{"location":"user_guides/fs/storage_connector/creation/kafka/#how-to-set-up-a-kafka-storage-connector","text":"","title":"How-To set up a Kafka Storage Connector"},{"location":"user_guides/fs/storage_connector/creation/kafka/#introduction","text":"Apache Kafka is a distributed event store and stream-processing platform. It's a very popular framework for handling realtime data streams and is often used as a message broker for events coming from production systems until they are being processed and either loaded into a data warehouse or aggregated into features for Machine Learning. In this guide, you will configure a Storage Connector in Hopsworks to save all the authentication information needed in order to set up a connection to your Kafka cluster. When you're finished, you'll be able to read from Kafka topics in your cluster using Spark through HSFS APIs. Note Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically.","title":"Introduction"},{"location":"user_guides/fs/storage_connector/creation/kafka/#prerequisites","text":"Before you begin this guide you'll need to retrieve the following information from Kafka cluster, the following options are mandatory : Kafka Bootstrap servers: It is the url of one of the Kafka brokers which you give to fetch the initial metadata about your Kafka cluster. The metadata consists of the topics, their partitions, the leader brokers for those partitions etc. Depending upon this metadata your producer or consumer produces or consumes the data. Security Protocol: The security protocol you want to use to authenticate with your Kafka cluster. Make sure the chosen protocol is supported by your cluster. For an overview of the available protocols, please see the Confluent Kafka Documentation . Certificates: Depending on the chosen security protocol, you might need TrustStore and KeyStore files along with the corresponding key password. Contact your Kafka administrator, if you don't know how to retrieve these. If you want to setup a storage connector to Hopsworks' internal Kafka cluster, you can download the needed certificates from the integration tab in your project settings.","title":"Prerequisites"},{"location":"user_guides/fs/storage_connector/creation/kafka/#creation-in-the-ui","text":"","title":"Creation in the UI"},{"location":"user_guides/fs/storage_connector/creation/kafka/#step-1-set-up-new-storage-connector","text":"Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2). The Storage Connector View in the User Interface","title":"Step 1: Set up new storage connector"},{"location":"user_guides/fs/storage_connector/creation/kafka/#step-2-enter-kafka-settings","text":"Enter the details for your Kafka connector. Start by giving it a name and an optional description . Select \"Kafka\" as connector protocol. Add all the bootstrap server addresses and ports that you want the consumers/producers to connect to. The client will make use of all servers irrespective of which servers are specified here for bootstrapping\u2014this list only impacts the initial hosts used to discover the full set of servers. Choose the Security protocol. TSL/SSL By default, Apache Kafka communicates in PLAINTEXT , which means that all data is sent in the clear. To encrypt communication, you should configure all the Confluent Platform components in your deployment to use TLS/SSL encryption. TLS uses private-key/certificate pairs, which are used during the TLS handshake process. Each broker needs its own private-key/certificate pair, and the client uses the certificate to authenticate the broker. Each logical client needs a private-key/certificate pair if client authentication is enabled, and the broker uses the certificate to authenticate the client. These are provided in the form of TrustStore and KeyStore JKS files together with a key password. For more information, refer to the official Apacha Kafka Guide for TSL/SSL authentication . SASL SSL or SASL plaintext Apache Kafka brokers support client authentication using SASL. SASL authentication can be enabled concurrently with TLS/SSL encryption (TLS/SSL client authentication will be disabled). This authentication method often requires extra arguments depending on your setup. Make use of the optional additional key/value arguments (5) to provide these. SASL authentication can be enabled concurrently with TLS/SSL encryption (TLS/SSL client authentication will be disabled). For more information, please refer to the offical Apache Kafka Guide for SASL authentication . The endpoint identification algorithm used by clients to validate server host name. The default value is https . Clients including client connections created by the broker for inter-broker communication verify that the broker host name matches the host name in the broker\u2019s certificate. Optional additional key/value arguments. Click \"Setup storage connector\". Kafka Connector Creation Form","title":"Step 2: Enter Kafka Settings"},{"location":"user_guides/fs/storage_connector/creation/kafka/#next-steps","text":"Move on to the usage guide for storage connectors to see how you can use your newly created Kafka connector.","title":"Next Steps"},{"location":"user_guides/fs/storage_connector/creation/redshift/","text":"How-To set up a Redshift Storage Connector # Introduction # Amazon Redshift is a popular managed data warehouse on AWS, used as a data warehouse in many enterprises. Data warehouses are often the source of raw data for feature engineering pipelines and Redshift supports scalable feature computation with SQL. However, Redshift is not viable as an online feature store that serves features to models in production, with its columnar database layout its latency is too high compared to OLTP databases or key-value stores. In this guide, you will configure a Storage Connector in Hopsworks to save all the authentication information needed in order to set up a connection to your AWS Redshift cluster. When you're finished, you'll be able to query the database using Spark through HSFS APIs. Note Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically. Prerequisites # Before you begin this guide you'll need to retrieve the following information from your AWS account and Redshift database, the following options are mandatory : Cluster identifier: The name of the cluster. Database endpoint: The endpoint for the database. Should be in the format of [UUID].eu-west-1.redshift.amazonaws.com . Database name: The name of the database to query. Database port: The port of the cluster. Defaults to 5349. Authentication method: There are three options available for authenticating with the Redshift cluster. The first option is to configure a username and a password. The second option is to configure an IAM role. With IAM roles, Jobs or notebooks launched on Hopsworks do not need to explicitly authenticate with Redshift, as the HSFS library will transparently use the IAM role to acquire a temporary credential to authenticate the specified user. Read more about IAM roles in our AWS credentials passthrough guide . Lastly, option Instance Role will use the default ARN Role configured for the cluster instance. Creation in the UI # Step 1: Set up new storage connector # Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2). The Storage Connector View in the User Interface Step 2: Enter The Connector Information # Enter the details for your Redshift connector. Start by giving it a name and an optional description . Select \"Redshift\" as connector protocol. The name of the cluster. The database endpoint. Should be in the format [UUID].eu-west-1.redshift.amazonaws.com . For example, if the endpoint info displayed in Redshift is cluster-id.uuid.eu-north-1.redshift.amazonaws.com:5439/dev the value to enter here is just uuid.eu-north-1.redshift.amazonaws.com The database name. The database port. The database username, here you have the possibility to let Hopsworks auto-create the username for you. Database Driver (optional): You can use the default JDBC Redshift Driver com.amazon.redshift.jdbc42.Driver included in Hopsworks or set a different driver (More on this later). Optionally provide the database group and table for the connector. A database group is the group created for the user if applicable. More information, at redshift documentation Set the appropriate authentication method. Redshift Connector Creation Form Session Duration By default, the session duration that the role will be assumed for is 1 hour or 3600 seconds. This means if you want to use the storage connector for example to read or create an external Feature Group from Redshift , the operation cannot take longer than one hour. Your administrator can change the default session duration for AWS storage connectors, by first increasing the max session duration of the IAM Role that you are assuming. And then changing the fs_storage_connector_session_duration configuration property to the appropriate value in seconds. Step 3: Upload the Redshift database driver (optional) # The redshift-jdbc42 JDBC driver is included by default in the Hopsworks distribution. If you wish to use a different driver, you need to upload it on Hopsworks and add it as a dependency of Jobs and Jupyter Notebooks that need it. First, you need to download the library . Select the driver version without the AWS SDK. Add the driver to Jupyter Notebooks and Spark jobs # You can now add the driver file to the default job and Jupyter configuration. This way, all jobs and Jupyter instances in the project will have the driver attached so that Spark can access it. Go into the Project's settings. Select \"Compute configuration\". Select \"Spark\". Under \"Additional Jars\" choose \"Upload new file\" to upload the driver jar file. Attaching the Redshift Driver to all Jobs and Jupyter Instances of the Project Alternatively, you can choose the \"From Project\" option. You will first have to upload the jar file to the Project using the File Browser. After you have uploaded the jar file, you can select it using the \"From Project\" option. To upload the jar file to the Project through the File Browser, see the example below: Open File Browser Navigate to \"Resources\" directory Upload the jar file Redshift Driver Upload in the File Browser Tip If you face network connectivity issues to your Redshift cluster, a common cause could be the cluster database port not being accessible from outside the Redshift cluster VPC network. A quick and dirty way to enable connectivity is to Enable Publicly Accessible . However, in a production setting, you should use VPC peering or some equivalent mechanism for connecting the clusters. Next Steps # Move on to the usage guide for storage connectors to see how you can use your newly created Redshift connector.","title":"Redshift"},{"location":"user_guides/fs/storage_connector/creation/redshift/#how-to-set-up-a-redshift-storage-connector","text":"","title":"How-To set up a Redshift Storage Connector"},{"location":"user_guides/fs/storage_connector/creation/redshift/#introduction","text":"Amazon Redshift is a popular managed data warehouse on AWS, used as a data warehouse in many enterprises. Data warehouses are often the source of raw data for feature engineering pipelines and Redshift supports scalable feature computation with SQL. However, Redshift is not viable as an online feature store that serves features to models in production, with its columnar database layout its latency is too high compared to OLTP databases or key-value stores. In this guide, you will configure a Storage Connector in Hopsworks to save all the authentication information needed in order to set up a connection to your AWS Redshift cluster. When you're finished, you'll be able to query the database using Spark through HSFS APIs. Note Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically.","title":"Introduction"},{"location":"user_guides/fs/storage_connector/creation/redshift/#prerequisites","text":"Before you begin this guide you'll need to retrieve the following information from your AWS account and Redshift database, the following options are mandatory : Cluster identifier: The name of the cluster. Database endpoint: The endpoint for the database. Should be in the format of [UUID].eu-west-1.redshift.amazonaws.com . Database name: The name of the database to query. Database port: The port of the cluster. Defaults to 5349. Authentication method: There are three options available for authenticating with the Redshift cluster. The first option is to configure a username and a password. The second option is to configure an IAM role. With IAM roles, Jobs or notebooks launched on Hopsworks do not need to explicitly authenticate with Redshift, as the HSFS library will transparently use the IAM role to acquire a temporary credential to authenticate the specified user. Read more about IAM roles in our AWS credentials passthrough guide . Lastly, option Instance Role will use the default ARN Role configured for the cluster instance.","title":"Prerequisites"},{"location":"user_guides/fs/storage_connector/creation/redshift/#creation-in-the-ui","text":"","title":"Creation in the UI"},{"location":"user_guides/fs/storage_connector/creation/redshift/#step-1-set-up-new-storage-connector","text":"Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2). The Storage Connector View in the User Interface","title":"Step 1: Set up new storage connector"},{"location":"user_guides/fs/storage_connector/creation/redshift/#step-2-enter-the-connector-information","text":"Enter the details for your Redshift connector. Start by giving it a name and an optional description . Select \"Redshift\" as connector protocol. The name of the cluster. The database endpoint. Should be in the format [UUID].eu-west-1.redshift.amazonaws.com . For example, if the endpoint info displayed in Redshift is cluster-id.uuid.eu-north-1.redshift.amazonaws.com:5439/dev the value to enter here is just uuid.eu-north-1.redshift.amazonaws.com The database name. The database port. The database username, here you have the possibility to let Hopsworks auto-create the username for you. Database Driver (optional): You can use the default JDBC Redshift Driver com.amazon.redshift.jdbc42.Driver included in Hopsworks or set a different driver (More on this later). Optionally provide the database group and table for the connector. A database group is the group created for the user if applicable. More information, at redshift documentation Set the appropriate authentication method. Redshift Connector Creation Form Session Duration By default, the session duration that the role will be assumed for is 1 hour or 3600 seconds. This means if you want to use the storage connector for example to read or create an external Feature Group from Redshift , the operation cannot take longer than one hour. Your administrator can change the default session duration for AWS storage connectors, by first increasing the max session duration of the IAM Role that you are assuming. And then changing the fs_storage_connector_session_duration configuration property to the appropriate value in seconds.","title":"Step 2: Enter The Connector Information"},{"location":"user_guides/fs/storage_connector/creation/redshift/#step-3-upload-the-redshift-database-driver-optional","text":"The redshift-jdbc42 JDBC driver is included by default in the Hopsworks distribution. If you wish to use a different driver, you need to upload it on Hopsworks and add it as a dependency of Jobs and Jupyter Notebooks that need it. First, you need to download the library . Select the driver version without the AWS SDK.","title":"Step 3: Upload the Redshift database driver (optional)"},{"location":"user_guides/fs/storage_connector/creation/redshift/#add-the-driver-to-jupyter-notebooks-and-spark-jobs","text":"You can now add the driver file to the default job and Jupyter configuration. This way, all jobs and Jupyter instances in the project will have the driver attached so that Spark can access it. Go into the Project's settings. Select \"Compute configuration\". Select \"Spark\". Under \"Additional Jars\" choose \"Upload new file\" to upload the driver jar file. Attaching the Redshift Driver to all Jobs and Jupyter Instances of the Project Alternatively, you can choose the \"From Project\" option. You will first have to upload the jar file to the Project using the File Browser. After you have uploaded the jar file, you can select it using the \"From Project\" option. To upload the jar file to the Project through the File Browser, see the example below: Open File Browser Navigate to \"Resources\" directory Upload the jar file Redshift Driver Upload in the File Browser Tip If you face network connectivity issues to your Redshift cluster, a common cause could be the cluster database port not being accessible from outside the Redshift cluster VPC network. A quick and dirty way to enable connectivity is to Enable Publicly Accessible . However, in a production setting, you should use VPC peering or some equivalent mechanism for connecting the clusters.","title":"Add the driver to Jupyter Notebooks and Spark jobs"},{"location":"user_guides/fs/storage_connector/creation/redshift/#next-steps","text":"Move on to the usage guide for storage connectors to see how you can use your newly created Redshift connector.","title":"Next Steps"},{"location":"user_guides/fs/storage_connector/creation/s3/","text":"How-To set up a S3 Storage Connector # Introduction # Amazon S3 or Amazon Simple Storage Service is a service offered by AWS that provides object storage. That means you can store arbitrary objects associated with a key. These kind of storage systems are often used as Data Lakes with large volumes of unstructured data or file based storage. Popular file formats are CSV or PARQUET . There are so called Data Lake House technologies such as Delta Lake or Apache Hudi, building an additional layer on top of object based storage with files, to provide database semantics like ACID transactions among others. This has the advantage that cheap storage can be turned into a cloud native data warehouse. These kind of storages are often the source for raw data from which features can be engineered. In this guide, you will configure a Storage Connector in Hopsworks to save all the authentication information needed in order to set up a connection to your AWS S3 bucket. When you're finished, you'll be able to read files using Spark through HSFS APIs. You can also use the connector to write out training data from the Feature Store, in order to make it accessible by third parties. Note Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically. Prerequisites # Before you begin this guide you'll need to retrieve the following information from your AWS S3 account and bucket: Bucket: You will need a S3 bucket that you have access to. The bucket is identified by its name. Authentication Method: You can authenticate using Access Key/Secret, or use IAM roles. If you want to use an IAM role it either needs to be attached to the entire Hopsworks cluster or Hopsworks needs to be able to assume the role. See IAM role documentation for more information. Server Side Encryption details: If your bucket has server side encryption (SSE) enabled, make sure you know which algorithm it is using (AES256 or SSE-KMS). If you are using SSE-KMS, you need the resource ARN of the managed key. Creation in the UI # Step 1: Set up new storage connector # Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2). The Storage Connector View in the User Interface Step 2: Enter Bucket Information # Enter the details for your S3 connector. Start by giving it a name and an optional description . And set the name of the S3 Bucket you want to point the connector to. S3 Connector Creation Form Step 3: Configure Authentication # Instance Role # Choose instance role if you have an EC2 instance profile attached to your Hopsworks cluster nodes with a role which grants you access to the specified bucket. Temporary Credentials # Choose temporary credentials if you are using AWS Role chaining to control the access permission on a project and user role base. Once you have selected Temporary Credentials select the role that give access to the specified bucket. For this role to appear in the list it needs to have been configured by an administrator, see the AWS Role chaining documentation for more details. Session Duration By default, the session duration that the role will be assumed for is 1 hour or 3600 seconds. This means if you want to use the storage connector for example to write training data to S3 , the training dataset creation cannot take longer than one hour. Your administrator can change the default session duration for AWS storage connectors, by first increasing the max session duration of the IAM Role that you are assuming. And then changing the fs_storage_connector_session_duration configuration variable to the appropriate value in seconds. Access Key/Secret # The most simple authentication method are Access Key/Secret, choose this option to get started quickly, if you are able to retrieve the keys using the IAM user administration. Step 4: Configure Server Side Encryption # Additionally, you can specify if your Bucket has SSE enabled. AES256 # For AES256, there is nothing to do but enabling the encryption by toggling the AES256 option. This is using S3-Managed Keys, also called SSE-S3 . SSE-KMS # With this option the encryption key is managed by AWS KMS , with some additional benefits and charges for using this service. The difference is that you need to provide the resource ARN of the key. If you have SSE-KMS enabled for your bucket, you can find the key ARN in the \"Properties\" section of the bucket details on AWS. Step 5: Add Spark Options (optional) # Here you can specify any additional spark options that you wish to add to the spark context at runtime. Multiple options can be added as key - value pairs. Tip To connect to a S3 compatiable storage other than AWS S3, you can add the option with key as fs.s3a.endpoint and the endpoint you want to use as value. The storage connector will then be able to read from your specified S3 compatible storage. Next Steps # Move on to the usage guide for storage connectors to see how you can use your newly created S3 connector.","title":"S3"},{"location":"user_guides/fs/storage_connector/creation/s3/#how-to-set-up-a-s3-storage-connector","text":"","title":"How-To set up a S3 Storage Connector"},{"location":"user_guides/fs/storage_connector/creation/s3/#introduction","text":"Amazon S3 or Amazon Simple Storage Service is a service offered by AWS that provides object storage. That means you can store arbitrary objects associated with a key. These kind of storage systems are often used as Data Lakes with large volumes of unstructured data or file based storage. Popular file formats are CSV or PARQUET . There are so called Data Lake House technologies such as Delta Lake or Apache Hudi, building an additional layer on top of object based storage with files, to provide database semantics like ACID transactions among others. This has the advantage that cheap storage can be turned into a cloud native data warehouse. These kind of storages are often the source for raw data from which features can be engineered. In this guide, you will configure a Storage Connector in Hopsworks to save all the authentication information needed in order to set up a connection to your AWS S3 bucket. When you're finished, you'll be able to read files using Spark through HSFS APIs. You can also use the connector to write out training data from the Feature Store, in order to make it accessible by third parties. Note Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically.","title":"Introduction"},{"location":"user_guides/fs/storage_connector/creation/s3/#prerequisites","text":"Before you begin this guide you'll need to retrieve the following information from your AWS S3 account and bucket: Bucket: You will need a S3 bucket that you have access to. The bucket is identified by its name. Authentication Method: You can authenticate using Access Key/Secret, or use IAM roles. If you want to use an IAM role it either needs to be attached to the entire Hopsworks cluster or Hopsworks needs to be able to assume the role. See IAM role documentation for more information. Server Side Encryption details: If your bucket has server side encryption (SSE) enabled, make sure you know which algorithm it is using (AES256 or SSE-KMS). If you are using SSE-KMS, you need the resource ARN of the managed key.","title":"Prerequisites"},{"location":"user_guides/fs/storage_connector/creation/s3/#creation-in-the-ui","text":"","title":"Creation in the UI"},{"location":"user_guides/fs/storage_connector/creation/s3/#step-1-set-up-new-storage-connector","text":"Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2). The Storage Connector View in the User Interface","title":"Step 1: Set up new storage connector"},{"location":"user_guides/fs/storage_connector/creation/s3/#step-2-enter-bucket-information","text":"Enter the details for your S3 connector. Start by giving it a name and an optional description . And set the name of the S3 Bucket you want to point the connector to. S3 Connector Creation Form","title":"Step 2: Enter Bucket Information"},{"location":"user_guides/fs/storage_connector/creation/s3/#step-3-configure-authentication","text":"","title":"Step 3: Configure Authentication"},{"location":"user_guides/fs/storage_connector/creation/s3/#instance-role","text":"Choose instance role if you have an EC2 instance profile attached to your Hopsworks cluster nodes with a role which grants you access to the specified bucket.","title":"Instance Role"},{"location":"user_guides/fs/storage_connector/creation/s3/#temporary-credentials","text":"Choose temporary credentials if you are using AWS Role chaining to control the access permission on a project and user role base. Once you have selected Temporary Credentials select the role that give access to the specified bucket. For this role to appear in the list it needs to have been configured by an administrator, see the AWS Role chaining documentation for more details. Session Duration By default, the session duration that the role will be assumed for is 1 hour or 3600 seconds. This means if you want to use the storage connector for example to write training data to S3 , the training dataset creation cannot take longer than one hour. Your administrator can change the default session duration for AWS storage connectors, by first increasing the max session duration of the IAM Role that you are assuming. And then changing the fs_storage_connector_session_duration configuration variable to the appropriate value in seconds.","title":"Temporary Credentials"},{"location":"user_guides/fs/storage_connector/creation/s3/#access-keysecret","text":"The most simple authentication method are Access Key/Secret, choose this option to get started quickly, if you are able to retrieve the keys using the IAM user administration.","title":"Access Key/Secret"},{"location":"user_guides/fs/storage_connector/creation/s3/#step-4-configure-server-side-encryption","text":"Additionally, you can specify if your Bucket has SSE enabled.","title":"Step 4: Configure Server Side Encryption"},{"location":"user_guides/fs/storage_connector/creation/s3/#aes256","text":"For AES256, there is nothing to do but enabling the encryption by toggling the AES256 option. This is using S3-Managed Keys, also called SSE-S3 .","title":"AES256"},{"location":"user_guides/fs/storage_connector/creation/s3/#sse-kms","text":"With this option the encryption key is managed by AWS KMS , with some additional benefits and charges for using this service. The difference is that you need to provide the resource ARN of the key. If you have SSE-KMS enabled for your bucket, you can find the key ARN in the \"Properties\" section of the bucket details on AWS.","title":"SSE-KMS"},{"location":"user_guides/fs/storage_connector/creation/s3/#step-5-add-spark-options-optional","text":"Here you can specify any additional spark options that you wish to add to the spark context at runtime. Multiple options can be added as key - value pairs. Tip To connect to a S3 compatiable storage other than AWS S3, you can add the option with key as fs.s3a.endpoint and the endpoint you want to use as value. The storage connector will then be able to read from your specified S3 compatible storage.","title":"Step 5: Add Spark Options (optional)"},{"location":"user_guides/fs/storage_connector/creation/s3/#next-steps","text":"Move on to the usage guide for storage connectors to see how you can use your newly created S3 connector.","title":"Next Steps"},{"location":"user_guides/fs/storage_connector/creation/snowflake/","text":"How-To set up a Snowflake Storage Connector # Introduction # Snowflake provides a cloud-based data storage and analytics service, used as a data warehouse in many enterprises. Data warehouses are often the source of raw data for feature engineering pipelines and Snowflake supports scalable feature computation with SQL. However, Snowflake is not viable as an online feature store that serves features to models in production, with its columnar database layout its latency is too high compared to OLTP databases or key-value stores. In this guide, you will configure a Storage Connector in Hopsworks to save all the authentication information needed in order to set up a connection to your Snowflake database. When you're finished, you'll be able to query the database using Spark through HSFS APIs. Note Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically. Prerequisites # Before you begin this guide you'll need to retrieve the following information from your Snowflake account and database, the following options are mandatory : Snowflake Connection URL: Consult the documentation of your target snowflake account to determine the correct connection URL. This is usually some form of your Snowflake account identifier . For example: <account_identifier>.snowflakecomputing.com OR: https://<orgname>-<account_name>.snowflakecomputing.com The account and organization details can be viewed in the Snowsight UI under Admin > Account or by querying it in SQL, as explained in Snowflake documentation . Below is an example of how to view the account and organization to get the account identifier from the Snowsight UI. Viewing Snowflake account identifier Token-based authentication or password based The Snowflake storage connector supports both username and password authentication as well as token-based authentication. Currently token-based authentication is in beta phase. Users are advised to use username/password and/or create a service account for accessing Snowflake from Hopsworks. Username and Password: Login name for the Snowflake user and password. This is often also referred to as sfUser and sfPassword . Warehouse: The warehouse to use for the session after connecting Database: The database to use for the session after connecting. Schema: The schema to use for the session after connecting. These are a few additional optional arguments: Role: The role field can be used to specify which Snowflake security role to assume for the session after the connection is established. Application: The application field can also be specified to have better observability in Snowflake with regards to which application is running which query. The application field can be a simple string like \u201cHopsworks\u201d or, for instance, the project name, to track usage and queries from each Hopsworks project. Creation in the UI # Step 1: Set up new storage connector # Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2). The Storage Connector View in the User Interface Step 2: Enter Snowflake Settings # Enter the details for your Snowflake connector. Start by giving it a name and an optional description . Select \"Snowflake\" as connector protocol. Specify the hostname for your account in the following format <account_identifier>.snowflakecomputing.com or https://<orgname>-<account_name>.snowflakecomputing.com . Login name for the Snowflake user. Password for the Snowflake user or Token. The database to connect to. The schema to use for the connection to the database. Additional optional arguments. For example, you can point the connector to a specific table in the database only. Optional additional key/value arguments. Click \"Setup storage connector\". Snowflake Connector Creation Form Next Steps # Move on to the usage guide for storage connectors to see how you can use your newly created Snowflake connector.","title":"Snowflake"},{"location":"user_guides/fs/storage_connector/creation/snowflake/#how-to-set-up-a-snowflake-storage-connector","text":"","title":"How-To set up a Snowflake Storage Connector"},{"location":"user_guides/fs/storage_connector/creation/snowflake/#introduction","text":"Snowflake provides a cloud-based data storage and analytics service, used as a data warehouse in many enterprises. Data warehouses are often the source of raw data for feature engineering pipelines and Snowflake supports scalable feature computation with SQL. However, Snowflake is not viable as an online feature store that serves features to models in production, with its columnar database layout its latency is too high compared to OLTP databases or key-value stores. In this guide, you will configure a Storage Connector in Hopsworks to save all the authentication information needed in order to set up a connection to your Snowflake database. When you're finished, you'll be able to query the database using Spark through HSFS APIs. Note Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically.","title":"Introduction"},{"location":"user_guides/fs/storage_connector/creation/snowflake/#prerequisites","text":"Before you begin this guide you'll need to retrieve the following information from your Snowflake account and database, the following options are mandatory : Snowflake Connection URL: Consult the documentation of your target snowflake account to determine the correct connection URL. This is usually some form of your Snowflake account identifier . For example: <account_identifier>.snowflakecomputing.com OR: https://<orgname>-<account_name>.snowflakecomputing.com The account and organization details can be viewed in the Snowsight UI under Admin > Account or by querying it in SQL, as explained in Snowflake documentation . Below is an example of how to view the account and organization to get the account identifier from the Snowsight UI. Viewing Snowflake account identifier Token-based authentication or password based The Snowflake storage connector supports both username and password authentication as well as token-based authentication. Currently token-based authentication is in beta phase. Users are advised to use username/password and/or create a service account for accessing Snowflake from Hopsworks. Username and Password: Login name for the Snowflake user and password. This is often also referred to as sfUser and sfPassword . Warehouse: The warehouse to use for the session after connecting Database: The database to use for the session after connecting. Schema: The schema to use for the session after connecting. These are a few additional optional arguments: Role: The role field can be used to specify which Snowflake security role to assume for the session after the connection is established. Application: The application field can also be specified to have better observability in Snowflake with regards to which application is running which query. The application field can be a simple string like \u201cHopsworks\u201d or, for instance, the project name, to track usage and queries from each Hopsworks project.","title":"Prerequisites"},{"location":"user_guides/fs/storage_connector/creation/snowflake/#creation-in-the-ui","text":"","title":"Creation in the UI"},{"location":"user_guides/fs/storage_connector/creation/snowflake/#step-1-set-up-new-storage-connector","text":"Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2). The Storage Connector View in the User Interface","title":"Step 1: Set up new storage connector"},{"location":"user_guides/fs/storage_connector/creation/snowflake/#step-2-enter-snowflake-settings","text":"Enter the details for your Snowflake connector. Start by giving it a name and an optional description . Select \"Snowflake\" as connector protocol. Specify the hostname for your account in the following format <account_identifier>.snowflakecomputing.com or https://<orgname>-<account_name>.snowflakecomputing.com . Login name for the Snowflake user. Password for the Snowflake user or Token. The database to connect to. The schema to use for the connection to the database. Additional optional arguments. For example, you can point the connector to a specific table in the database only. Optional additional key/value arguments. Click \"Setup storage connector\". Snowflake Connector Creation Form","title":"Step 2: Enter Snowflake Settings"},{"location":"user_guides/fs/storage_connector/creation/snowflake/#next-steps","text":"Move on to the usage guide for storage connectors to see how you can use your newly created Snowflake connector.","title":"Next Steps"},{"location":"user_guides/fs/tags/tags/","text":"Tags # Introduction # Hopsworks feature store enables users to attach tags to artifacts, such as feature groups, feature views or training datasets. A tag is a {key: value} pair which provides additional information about the data managed by Hopsworks. Tags allow you to design custom metadata for your artifacts. For example, you could design a tag schema that encodes governance rules for your feature store, such as classifying data as personally identifiable, defining a data retention period for the data, and defining who signed off on the creation of some feature. Prerequisites # Tags have a schema. Before you can attach a tag to an artifact and fill in the tag values, you first need to select an existing tag schema or create a new tag schema. Tag schemas can be defined by Hopsworks administrator in the Cluster settings section of the platform. Schemas are defined globally across all projects. When users attach tags to an artifact, the tag will be validated against a specific schema. This allows tags to be consistent no matter the project or the team generating them. Immutable Tag schemas are immutable. Once defined, a tag schema cannot be edited nor deleted. Step 1: Define a tag schema # Tag schemas can be defined using the UI wizard in the Cluster settings > Tag schemas section. Tag schemas have a name, the name is used to uniquely identify the schema. You can also provide an optional description. You can define a schema by using the UI tool or by providing the schema in JSON format. If you use the UI tool, you should provide the name of the property in the schema, the type of the property, whether or not the property is required and an optional description. UI tag schema definition The UI tool allows you to define simple not-nested schemas. For more advanced use cases, more complex schemas (e.g. nested schemas) might be required to fully express the content of a given artifact. In such cases it is possible to provide the schema directly as JSON string. The JSON should follow the standard https://json-schema.org . An example of complex schema is the following: { \"type\" : \"object\", \"properties\" : { \"first_name\" : { \"type\" : \"string\" }, \"last_name\" : { \"type\" : \"string\" }, \"age\" : { \"type\" : \"integer\" }, \"hobbies\" : { \"type\" : \"array\", \"items\" : { \"type\" : \"string\" } } }, \"required\" : [\"first_name\", \"last_name\", \"age\"], \"additionalProperties\": false } Additionally it is also possible to define a single property as tag. You can achieve this by defining a JSON schema like the following: { \"type\" : \"string\" } Where the type is a valid primitive type: string , boolean , integer , number . Step 2: Attach a tag to an artifact # Once the tag schema has been created, you can attach a tag with that schema to a feature group, feature view or training datasets either using the feature store APIs, or by using the UI. Using the API # You can attach tags to feature groups and feature views by using the add_tag() method of the feature store APIs: Python # Retrieve the feature group fg = fs . get_feature_group ( \"transactions_4h_aggs_fraud_batch_fg\" , version = 1 ) # Define the tag tag = { 'business_unit' : 'Fraud' , 'data_owner' : 'email@hopsworks.ai' , 'pii' : True } # Attach the tag fg . add_tag ( \"data_privacy\" , tag ) You can see the list of tags attached to a given artifact by using the get_tags() method: Python # Retrieve the feature group fg = fs . get_feature_group ( \"transactions_4h_aggs_fraud_batch_fg\" , version = 1 ) # Retrieve the tags for this feature group fg . get_tags () Finally you can remove a tag from a given artifact by calling the delete_tag() method: Python # Retrieve the feature group fg = fs . get_feature_group ( \"transactions_4h_aggs_fraud_batch_fg\" , version = 1 ) # Retrieve the tags for this feature group fg . delete_tag ( \"data_privacy\" ) The same APIs work for feature views and training dataset alike. Using the UI # You can attach tags to feature groups and feature views directly from the UI. You can navigate on the artifact page and click on the Add tags button. From there you can select the tag schema of the tag you want to attach and populate the values as shown in the gif below. Attach tag to a feature group Step 3: Search # Hopsworks indexes the tags attached to feature groups, feature views and training datasets. The tags will then be searchable using the free text search box located at the top of the UI. Search for tags in the feature store","title":"Tags"},{"location":"user_guides/fs/tags/tags/#tags","text":"","title":"Tags"},{"location":"user_guides/fs/tags/tags/#introduction","text":"Hopsworks feature store enables users to attach tags to artifacts, such as feature groups, feature views or training datasets. A tag is a {key: value} pair which provides additional information about the data managed by Hopsworks. Tags allow you to design custom metadata for your artifacts. For example, you could design a tag schema that encodes governance rules for your feature store, such as classifying data as personally identifiable, defining a data retention period for the data, and defining who signed off on the creation of some feature.","title":"Introduction"},{"location":"user_guides/fs/tags/tags/#prerequisites","text":"Tags have a schema. Before you can attach a tag to an artifact and fill in the tag values, you first need to select an existing tag schema or create a new tag schema. Tag schemas can be defined by Hopsworks administrator in the Cluster settings section of the platform. Schemas are defined globally across all projects. When users attach tags to an artifact, the tag will be validated against a specific schema. This allows tags to be consistent no matter the project or the team generating them. Immutable Tag schemas are immutable. Once defined, a tag schema cannot be edited nor deleted.","title":"Prerequisites"},{"location":"user_guides/fs/tags/tags/#step-1-define-a-tag-schema","text":"Tag schemas can be defined using the UI wizard in the Cluster settings > Tag schemas section. Tag schemas have a name, the name is used to uniquely identify the schema. You can also provide an optional description. You can define a schema by using the UI tool or by providing the schema in JSON format. If you use the UI tool, you should provide the name of the property in the schema, the type of the property, whether or not the property is required and an optional description. UI tag schema definition The UI tool allows you to define simple not-nested schemas. For more advanced use cases, more complex schemas (e.g. nested schemas) might be required to fully express the content of a given artifact. In such cases it is possible to provide the schema directly as JSON string. The JSON should follow the standard https://json-schema.org . An example of complex schema is the following: { \"type\" : \"object\", \"properties\" : { \"first_name\" : { \"type\" : \"string\" }, \"last_name\" : { \"type\" : \"string\" }, \"age\" : { \"type\" : \"integer\" }, \"hobbies\" : { \"type\" : \"array\", \"items\" : { \"type\" : \"string\" } } }, \"required\" : [\"first_name\", \"last_name\", \"age\"], \"additionalProperties\": false } Additionally it is also possible to define a single property as tag. You can achieve this by defining a JSON schema like the following: { \"type\" : \"string\" } Where the type is a valid primitive type: string , boolean , integer , number .","title":"Step 1: Define a tag schema"},{"location":"user_guides/fs/tags/tags/#step-2-attach-a-tag-to-an-artifact","text":"Once the tag schema has been created, you can attach a tag with that schema to a feature group, feature view or training datasets either using the feature store APIs, or by using the UI.","title":"Step 2: Attach a tag to an artifact"},{"location":"user_guides/fs/tags/tags/#using-the-api","text":"You can attach tags to feature groups and feature views by using the add_tag() method of the feature store APIs: Python # Retrieve the feature group fg = fs . get_feature_group ( \"transactions_4h_aggs_fraud_batch_fg\" , version = 1 ) # Define the tag tag = { 'business_unit' : 'Fraud' , 'data_owner' : 'email@hopsworks.ai' , 'pii' : True } # Attach the tag fg . add_tag ( \"data_privacy\" , tag ) You can see the list of tags attached to a given artifact by using the get_tags() method: Python # Retrieve the feature group fg = fs . get_feature_group ( \"transactions_4h_aggs_fraud_batch_fg\" , version = 1 ) # Retrieve the tags for this feature group fg . get_tags () Finally you can remove a tag from a given artifact by calling the delete_tag() method: Python # Retrieve the feature group fg = fs . get_feature_group ( \"transactions_4h_aggs_fraud_batch_fg\" , version = 1 ) # Retrieve the tags for this feature group fg . delete_tag ( \"data_privacy\" ) The same APIs work for feature views and training dataset alike.","title":"Using the API"},{"location":"user_guides/fs/tags/tags/#using-the-ui","text":"You can attach tags to feature groups and feature views directly from the UI. You can navigate on the artifact page and click on the Add tags button. From there you can select the tag schema of the tag you want to attach and populate the values as shown in the gif below. Attach tag to a feature group","title":"Using the UI"},{"location":"user_guides/fs/tags/tags/#step-3-search","text":"Hopsworks indexes the tags attached to feature groups, feature views and training datasets. The tags will then be searchable using the free text search box located at the top of the UI. Search for tags in the feature store","title":"Step 3: Search"},{"location":"user_guides/integrations/","text":"Client Integrations # Hopsworks is an open platform aiming to be accessible from a variety of tools. Learn in this section how to connect to Hopsworks from Python Databricks AWS SageMaker AWS EMR Azure HDInsight Azure Machine Learning Apache Spark","title":"Client Integrations"},{"location":"user_guides/integrations/#client-integrations","text":"Hopsworks is an open platform aiming to be accessible from a variety of tools. Learn in this section how to connect to Hopsworks from Python Databricks AWS SageMaker AWS EMR Azure HDInsight Azure Machine Learning Apache Spark","title":"Client Integrations"},{"location":"user_guides/integrations/assume_role/","text":"Assuming a role # When deploying Hopsworks on EC2 instances you might need to assume different roles to access resources on AWS. These roles are configured in AWS and mapped to projects in Hopsworks, for a guide on how to configure this see role mapping . After an administrator configured role mappings in Hopsworks you can see the roles you can assume by going to your project settings. Cloud roles mapped to project. You can then use the Hops Python and Java APIs to assume the roles listed in your project's settings page. When calling the assume role method you can pass the role ARN string or use the get role method that takes the role id as an argument. If you assign a default role for your project you can call the assume role method with no argument. You can assign (if you are a Data owner in that project) a default role to you project by clicking on the Default button above the role you want to make default. You can set one default per project role. If a default is set for a project role (Data scientist or Data owner) and all members (ALL) the default set for the project role will take precedence over the default set for all members. In the image above if a Data scientist called the assume role method with no arguments she will assume the role with id 1029 but if a Data owner called the same method she will assume the role with id 1. Use temporary credentials. Python from hops.credentials_provider import get_role , assume_role credentials = assume_role ( role_arn = get_role ( 1 )) spark . read . csv ( \"s3a://resource/test.csv\" ) . show () Scala import io . hops . util . CredentialsProvider val creds = CredentialsProvider . assumeRole ( CredentialsProvider . getRole ( 1 )) spark . read . csv ( \"s3a://resource/test.csv\" ). show () The assume role method sets spark hadoop configurations that will allow spark to read s3 buckets. The code examples above show how to read s3 buckets using Python and Scala. Assume role also sets environment variables AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and AWS_SESSION_TOKEN so that programs running in the container can use the credentials for the newly assumed role. To read s3 buckets with TensorFlow you also need to set AWS_REGION environment variable (s3 bucket region). The code below shows how to read training and validation datasets from s3 bucket using TensorFlow. Use temporary credentials with TensorFlow. from hops.credentials_provider import get_role , assume_role import tensorflow as tf import os assume_role ( role_arn = get_role ( 1 )) # s3 bucket region need to be set for TensorFlow os . environ [ \"AWS_REGION\" ] = \"eu-north-1\" train_filenames = [ \"s3://resource/train/train.tfrecords\" ] validation_filenames = [ \"s3://resourcet/validation/validation.tfrecords\" ] train_dataset = tf . data . TFRecordDataset ( train_filenames ) validation_dataset = tf . data . TFRecordDataset ( validation_filenames ) for raw_record in train_dataset . take ( 1 ): example = tf . train . Example () example . ParseFromString ( raw_record . numpy ()) print ( example )","title":"Assuming a role"},{"location":"user_guides/integrations/assume_role/#assuming-a-role","text":"When deploying Hopsworks on EC2 instances you might need to assume different roles to access resources on AWS. These roles are configured in AWS and mapped to projects in Hopsworks, for a guide on how to configure this see role mapping . After an administrator configured role mappings in Hopsworks you can see the roles you can assume by going to your project settings. Cloud roles mapped to project. You can then use the Hops Python and Java APIs to assume the roles listed in your project's settings page. When calling the assume role method you can pass the role ARN string or use the get role method that takes the role id as an argument. If you assign a default role for your project you can call the assume role method with no argument. You can assign (if you are a Data owner in that project) a default role to you project by clicking on the Default button above the role you want to make default. You can set one default per project role. If a default is set for a project role (Data scientist or Data owner) and all members (ALL) the default set for the project role will take precedence over the default set for all members. In the image above if a Data scientist called the assume role method with no arguments she will assume the role with id 1029 but if a Data owner called the same method she will assume the role with id 1. Use temporary credentials. Python from hops.credentials_provider import get_role , assume_role credentials = assume_role ( role_arn = get_role ( 1 )) spark . read . csv ( \"s3a://resource/test.csv\" ) . show () Scala import io . hops . util . CredentialsProvider val creds = CredentialsProvider . assumeRole ( CredentialsProvider . getRole ( 1 )) spark . read . csv ( \"s3a://resource/test.csv\" ). show () The assume role method sets spark hadoop configurations that will allow spark to read s3 buckets. The code examples above show how to read s3 buckets using Python and Scala. Assume role also sets environment variables AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and AWS_SESSION_TOKEN so that programs running in the container can use the credentials for the newly assumed role. To read s3 buckets with TensorFlow you also need to set AWS_REGION environment variable (s3 bucket region). The code below shows how to read training and validation datasets from s3 bucket using TensorFlow. Use temporary credentials with TensorFlow. from hops.credentials_provider import get_role , assume_role import tensorflow as tf import os assume_role ( role_arn = get_role ( 1 )) # s3 bucket region need to be set for TensorFlow os . environ [ \"AWS_REGION\" ] = \"eu-north-1\" train_filenames = [ \"s3://resource/train/train.tfrecords\" ] validation_filenames = [ \"s3://resourcet/validation/validation.tfrecords\" ] train_dataset = tf . data . TFRecordDataset ( train_filenames ) validation_dataset = tf . data . TFRecordDataset ( validation_filenames ) for raw_record in train_dataset . take ( 1 ): example = tf . train . Example () example . ParseFromString ( raw_record . numpy ()) print ( example )","title":"Assuming a role"},{"location":"user_guides/integrations/beam/","text":"Apache Beam Dataflow Runner # Connecting to the Feature Store from an Apache Beam Dataflow Runner, requires configuring the Hopsworks certificates. For this in your Beam Java application pom.xml file include following snippet: <resources> <resource> <directory>java.io.tmpdir</directory> <includes> <include>**/*.jks</include> </includes> </resource> </resources> Generating an API Key # For instructions on how to generate an API key follow this user guide . For the Beam integration to work correctly make sure you add the following scopes to your API key: featurestore project job kafka Connecting to the Feature Store # You are now ready to connect to the Hopsworks Feature Store from Beam: //Establish connection with Hopsworks. HopsworksConnection hopsworksConnection = HopsworksConnection . builder () . host ( \"my_instance\" ) // DNS of your Feature Store instance . port ( 443 ) // Port to reach your Hopsworks instance, defaults to 443 . project ( \"my_project\" ) // Name of your Hopsworks Feature Store project . apiKeyValue ( \"api_key\" ) // The API key to authenticate with the feature store . hostnameVerification ( false ) // Disable for self-signed certificates . build (); //get feature store handle FeatureStore fs = hopsworksConnection . getFeatureStore (); Next Steps # For more information and how to integrate Beam feature pipeline to the Hopsworks Feature store follow the tutorial .","title":"Apache Beam"},{"location":"user_guides/integrations/beam/#apache-beam-dataflow-runner","text":"Connecting to the Feature Store from an Apache Beam Dataflow Runner, requires configuring the Hopsworks certificates. For this in your Beam Java application pom.xml file include following snippet: <resources> <resource> <directory>java.io.tmpdir</directory> <includes> <include>**/*.jks</include> </includes> </resource> </resources>","title":"Apache Beam Dataflow Runner"},{"location":"user_guides/integrations/beam/#generating-an-api-key","text":"For instructions on how to generate an API key follow this user guide . For the Beam integration to work correctly make sure you add the following scopes to your API key: featurestore project job kafka","title":"Generating an API Key"},{"location":"user_guides/integrations/beam/#connecting-to-the-feature-store","text":"You are now ready to connect to the Hopsworks Feature Store from Beam: //Establish connection with Hopsworks. HopsworksConnection hopsworksConnection = HopsworksConnection . builder () . host ( \"my_instance\" ) // DNS of your Feature Store instance . port ( 443 ) // Port to reach your Hopsworks instance, defaults to 443 . project ( \"my_project\" ) // Name of your Hopsworks Feature Store project . apiKeyValue ( \"api_key\" ) // The API key to authenticate with the feature store . hostnameVerification ( false ) // Disable for self-signed certificates . build (); //get feature store handle FeatureStore fs = hopsworksConnection . getFeatureStore ();","title":"Connecting to the Feature Store"},{"location":"user_guides/integrations/beam/#next-steps","text":"For more information and how to integrate Beam feature pipeline to the Hopsworks Feature store follow the tutorial .","title":"Next Steps"},{"location":"user_guides/integrations/flink/","text":"Flink Integration # Connecting to the Feature Store from an external Flink cluster, such as AWS EMR and GCP DataProc requires configuring it with the Hopsworks certificates. This guide explains step by step how to connect to the Feature Store from an external Flink cluster. Download the Hopsworks Certificates # In the Project Settings , select the integration tab and scroll to the Configure Spark Integration section. Click on Download certificates . The Spark Integration gives access to Jars and configuration for an external Spark cluster Hopsworks uses X.509 certificates for authentication and authorization. If you are interested in the Hopsworks security model, you can read more about it in this blog post . The certificates are composed of three different components: the keyStore.jks containing the private key and the certificate for your project user, the trustStore.jks containing the certificates for the Hopsworks certificates authority, and a password to unlock the private key in the keyStore.jks . The password is displayed in a pop-up when downloading the certificate and should be saved in a file named material_passwd . Warning When you copy-paste the password to the material_passwd file, pay attention to not introduce additional empty spaces or new lines. The three files ( keyStore.jks , trustStore.jks and material_passwd ) should be accessible by your Flink application. Configure your Flink cluster # Add the following configuration to the flink-conf.yaml : flink.hadoop.hops.ssl.keystore.name: replace_this_with_actual_path/keyStore.jks flink.hadoop.hops.ssl.truststore.name: replace_this_with_actual_path/trustStore.jks flink.hadoop.hops.ssl.keystores.passwd.name: replace_this_with_actual_path/material_passwd Generating an API Key # For instructions on how to generate an API key follow this user guide . For the Flink integration to work correctly make sure you add the following scopes to your API key: featurestore project job kafka Connecting to the Feature Store # You are now ready to connect to the Hopsworks Feature Store from Flink: //Establish connection with Hopsworks. HopsworksConnection hopsworksConnection = HopsworksConnection . builder () . host ( \"my_instance\" ) // DNS of your Feature Store instance . port ( 443 ) // Port to reach your Hopsworks instance, defaults to 443 . project ( \"my_project\" ) // Name of your Hopsworks Feature Store project . apiKeyValue ( \"api_key\" ) // The API key to authenticate with the feature store . hostnameVerification ( false ) // Disable for self-signed certificates . build (); //get feature store handle FeatureStore fs = hopsworksConnection . getFeatureStore (); Next Steps # For more information and how to integrate Flink streaming feature pipeline to the Hopsworks Feature store follow the tutorial .","title":"Apache Flink"},{"location":"user_guides/integrations/flink/#flink-integration","text":"Connecting to the Feature Store from an external Flink cluster, such as AWS EMR and GCP DataProc requires configuring it with the Hopsworks certificates. This guide explains step by step how to connect to the Feature Store from an external Flink cluster.","title":"Flink Integration"},{"location":"user_guides/integrations/flink/#download-the-hopsworks-certificates","text":"In the Project Settings , select the integration tab and scroll to the Configure Spark Integration section. Click on Download certificates . The Spark Integration gives access to Jars and configuration for an external Spark cluster Hopsworks uses X.509 certificates for authentication and authorization. If you are interested in the Hopsworks security model, you can read more about it in this blog post . The certificates are composed of three different components: the keyStore.jks containing the private key and the certificate for your project user, the trustStore.jks containing the certificates for the Hopsworks certificates authority, and a password to unlock the private key in the keyStore.jks . The password is displayed in a pop-up when downloading the certificate and should be saved in a file named material_passwd . Warning When you copy-paste the password to the material_passwd file, pay attention to not introduce additional empty spaces or new lines. The three files ( keyStore.jks , trustStore.jks and material_passwd ) should be accessible by your Flink application.","title":"Download the Hopsworks Certificates"},{"location":"user_guides/integrations/flink/#configure-your-flink-cluster","text":"Add the following configuration to the flink-conf.yaml : flink.hadoop.hops.ssl.keystore.name: replace_this_with_actual_path/keyStore.jks flink.hadoop.hops.ssl.truststore.name: replace_this_with_actual_path/trustStore.jks flink.hadoop.hops.ssl.keystores.passwd.name: replace_this_with_actual_path/material_passwd","title":"Configure your Flink cluster"},{"location":"user_guides/integrations/flink/#generating-an-api-key","text":"For instructions on how to generate an API key follow this user guide . For the Flink integration to work correctly make sure you add the following scopes to your API key: featurestore project job kafka","title":"Generating an API Key"},{"location":"user_guides/integrations/flink/#connecting-to-the-feature-store","text":"You are now ready to connect to the Hopsworks Feature Store from Flink: //Establish connection with Hopsworks. HopsworksConnection hopsworksConnection = HopsworksConnection . builder () . host ( \"my_instance\" ) // DNS of your Feature Store instance . port ( 443 ) // Port to reach your Hopsworks instance, defaults to 443 . project ( \"my_project\" ) // Name of your Hopsworks Feature Store project . apiKeyValue ( \"api_key\" ) // The API key to authenticate with the feature store . hostnameVerification ( false ) // Disable for self-signed certificates . build (); //get feature store handle FeatureStore fs = hopsworksConnection . getFeatureStore ();","title":"Connecting to the Feature Store"},{"location":"user_guides/integrations/flink/#next-steps","text":"For more information and how to integrate Flink streaming feature pipeline to the Hopsworks Feature store follow the tutorial .","title":"Next Steps"},{"location":"user_guides/integrations/hdinsight/","text":"Configure HDInsight for the Hopsworks Feature Store # To enable HDInsight to access the Hopsworks Feature Store, you need to set up a Hopsworks API key, add a script action and configurations to your HDInsight cluster. Prerequisites A HDInsight cluster with cluster type Spark is required to connect to the Feature Store. You can either use an existing cluster or create a new one. Network Connectivity To be able to connect to the Feature Store, please ensure that your HDInsight cluster and the Hopsworks Feature Store are either in the same Virtual Network or Virtual Network Peering is set up between the different networks. In addition, ensure that the Network Security Group of your Hopsworks instance is configured to allow incoming traffic from your HDInsight cluster on ports 443, 3306, 8020, 30010, 9083 and 9085 (443,3306,8020,30010,9083,9085). See Network security groups for more information. Step 1: Set up a Hopsworks API key # For instructions on how to generate an API key follow this user guide . For the HDInsight integration to work correctly make sure you add the following scopes to your API key: featurestore project job kafka Step 2: Use a script action to install the Feature Store connector # HDInsight requires Hopsworks connectors to be able to communicate with the Hopsworks Feature Store. These connectors can be installed with the script action shown below. Copy the content into a file, name the file hopsworks.sh and replace MY_INSTANCE, MY_PROJECT, MY_VERSION, MY_API_KEY and MY_CONDA_ENV with your values. Copy the hopsworks.sh file into any storage that is readable by your HDInsight clusters and take note of the URI of that file e.g., https://account.blob.core.windows.net/scripts/hopsworks.sh . The script action needs to be applied head and worker nodes and can be applied during cluster creation or to an existing cluster. Ensure to persist the script action so that it is run on newly created nodes. For more information about how to use script actions, see Customize Azure HDInsight clusters by using script actions . Matching Hopsworks version The major version of HSFS needs to match the major version of Hopsworks . Check PyPI for available releases. You find the Hopsworks version inside any of your Project's settings tab on Hopsworks Feature Store script action: set -e HOST = \"MY_INSTANCE.cloud.hopsworks.ai\" # DNS of your Feature Store instance PROJECT = \"MY_PROJECT\" # Port to reach your Hopsworks instance, defaults to 443 HSFS_VERSION = \"MY_VERSION\" # The major version of HSFS needs to match the major version of Hopsworks API_KEY = \"MY_API_KEY\" # The API key to authenticate with Hopsworks CONDA_ENV = \"MY_CONDA_ENV\" # py35 is the default for HDI 3.6 apt-get --assume-yes install python3-dev apt-get --assume-yes install jq /usr/bin/anaconda/envs/ $CONDA_ENV /bin/pip install hsfs == $HSFS_VERSION PROJECT_ID = $( curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/getProjectInfo/ $PROJECT | jq -r .projectId ) mkdir -p /usr/lib/hopsworks chown root:hadoop /usr/lib/hopsworks cd /usr/lib/hopsworks curl -o client.tar.gz -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /client tar -xvf client.tar.gz tar -xzf client/apache-hive-*-bin.tar.gz mv apache-hive-*-bin apache-hive-bin rm client.tar.gz rm client/apache-hive-*-bin.tar.gz curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .kStore | base64 -d > keyStore.jks curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .tStore | base64 -d > trustStore.jks echo -n $( curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .password ) > material_passwd chown -R root:hadoop /usr/lib/hopsworks Step 3: Configure HDInsight for Feature Store access # The Hadoop and Spark installations of the HDInsight cluster need to be configured in order to access the Feature Store. This can be achieved either by using a bootstrap script when creating clusters or using Ambari on existing clusters. Apply the following configurations to your HDInsight cluster. Using Hive and the Feature Store HDInsight clusters cannot use their local Hive when being configured for the Feature Store as the Feature Store relies on custom Hive binaries and its own Metastore which will overwrite the local one. If you rely on Hive for feature engineering then it is advised to write your data to an external data storage such as ADLS from your main HDInsight cluster and in the Feature Store, create an on-demand Feature Group on the storage container in ADLS. Hadoop hadoop-env.sh: export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/usr/lib/hopsworks/client/* Hadoop core-site.xml: hops.ipc.server.ssl.enabled=true fs.hopsfs.impl=io.hops.hopsfs.client.HopsFileSystem client.rpc.ssl.enabled.protocol=TLSv1.2 hops.ssl.keystore.name=/usr/lib/hopsworks/keyStore.jks hops.rpc.socket.factory.class.default=io.hops.hadoop.shaded.org.apache.hadoop.net.HopsSSLSocketFactory hops.ssl.keystores.passwd.name=/usr/lib/hopsworks/material_passwd hops.ssl.hostname.verifier=ALLOW_ALL hops.ssl.trustore.name=/usr/lib/hopsworks/trustStore.jks Spark spark-defaults.conf: spark.executor.extraClassPath=/usr/lib/hopsworks/client/* spark.driver.extraClassPath=/usr/lib/hopsworks/client/* spark.sql.hive.metastore.jars=path spark.sql.hive.metastore.jars.path=/usr/lib/hopsworks/apache-hive-bin/lib/* Spark hive-site.xml: hive.metastore.uris=thrift://MY_HOPSWORKS_INSTANCE_PRIVATE_IP:9083 Info Replace MY_HOPSWORKS_INSTANCE_PRIVATE_IP with the private IP address of you Hopsworks Feature Store. Step 5: Connect to the Feature Store # You are now ready to connect to the Hopsworks Feature Store, for instance using a Jupyter notebook in HDInsight with a PySpark3 kernel: import hsfs # Put the API key into Key Vault for any production setup: # See, https://azure.microsoft.com/en-us/services/key-vault/ secret_value = 'MY_API_KEY' # Create a connection conn = hsfs . connection ( host = 'MY_INSTANCE.cloud.hopsworks.ai' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'MY_PROJECT' , # Name of your Hopsworks Feature Store project api_key_value = secret_value , # The API key to authenticate with Hopsworks hostname_verification = True # Disable for self-signed certificates ) # Get the feature store handle for the project's feature store fs = conn . get_feature_store () Next Steps # For more information about how to use the Feature Store, see the Quickstart Guide .","title":"Azure HDInsight"},{"location":"user_guides/integrations/hdinsight/#configure-hdinsight-for-the-hopsworks-feature-store","text":"To enable HDInsight to access the Hopsworks Feature Store, you need to set up a Hopsworks API key, add a script action and configurations to your HDInsight cluster. Prerequisites A HDInsight cluster with cluster type Spark is required to connect to the Feature Store. You can either use an existing cluster or create a new one. Network Connectivity To be able to connect to the Feature Store, please ensure that your HDInsight cluster and the Hopsworks Feature Store are either in the same Virtual Network or Virtual Network Peering is set up between the different networks. In addition, ensure that the Network Security Group of your Hopsworks instance is configured to allow incoming traffic from your HDInsight cluster on ports 443, 3306, 8020, 30010, 9083 and 9085 (443,3306,8020,30010,9083,9085). See Network security groups for more information.","title":"Configure HDInsight for the Hopsworks Feature Store"},{"location":"user_guides/integrations/hdinsight/#step-1-set-up-a-hopsworks-api-key","text":"For instructions on how to generate an API key follow this user guide . For the HDInsight integration to work correctly make sure you add the following scopes to your API key: featurestore project job kafka","title":"Step 1: Set up a Hopsworks API key"},{"location":"user_guides/integrations/hdinsight/#step-2-use-a-script-action-to-install-the-feature-store-connector","text":"HDInsight requires Hopsworks connectors to be able to communicate with the Hopsworks Feature Store. These connectors can be installed with the script action shown below. Copy the content into a file, name the file hopsworks.sh and replace MY_INSTANCE, MY_PROJECT, MY_VERSION, MY_API_KEY and MY_CONDA_ENV with your values. Copy the hopsworks.sh file into any storage that is readable by your HDInsight clusters and take note of the URI of that file e.g., https://account.blob.core.windows.net/scripts/hopsworks.sh . The script action needs to be applied head and worker nodes and can be applied during cluster creation or to an existing cluster. Ensure to persist the script action so that it is run on newly created nodes. For more information about how to use script actions, see Customize Azure HDInsight clusters by using script actions . Matching Hopsworks version The major version of HSFS needs to match the major version of Hopsworks . Check PyPI for available releases. You find the Hopsworks version inside any of your Project's settings tab on Hopsworks Feature Store script action: set -e HOST = \"MY_INSTANCE.cloud.hopsworks.ai\" # DNS of your Feature Store instance PROJECT = \"MY_PROJECT\" # Port to reach your Hopsworks instance, defaults to 443 HSFS_VERSION = \"MY_VERSION\" # The major version of HSFS needs to match the major version of Hopsworks API_KEY = \"MY_API_KEY\" # The API key to authenticate with Hopsworks CONDA_ENV = \"MY_CONDA_ENV\" # py35 is the default for HDI 3.6 apt-get --assume-yes install python3-dev apt-get --assume-yes install jq /usr/bin/anaconda/envs/ $CONDA_ENV /bin/pip install hsfs == $HSFS_VERSION PROJECT_ID = $( curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/getProjectInfo/ $PROJECT | jq -r .projectId ) mkdir -p /usr/lib/hopsworks chown root:hadoop /usr/lib/hopsworks cd /usr/lib/hopsworks curl -o client.tar.gz -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /client tar -xvf client.tar.gz tar -xzf client/apache-hive-*-bin.tar.gz mv apache-hive-*-bin apache-hive-bin rm client.tar.gz rm client/apache-hive-*-bin.tar.gz curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .kStore | base64 -d > keyStore.jks curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .tStore | base64 -d > trustStore.jks echo -n $( curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .password ) > material_passwd chown -R root:hadoop /usr/lib/hopsworks","title":"Step 2:  Use a script action to install the Feature Store connector"},{"location":"user_guides/integrations/hdinsight/#step-3-configure-hdinsight-for-feature-store-access","text":"The Hadoop and Spark installations of the HDInsight cluster need to be configured in order to access the Feature Store. This can be achieved either by using a bootstrap script when creating clusters or using Ambari on existing clusters. Apply the following configurations to your HDInsight cluster. Using Hive and the Feature Store HDInsight clusters cannot use their local Hive when being configured for the Feature Store as the Feature Store relies on custom Hive binaries and its own Metastore which will overwrite the local one. If you rely on Hive for feature engineering then it is advised to write your data to an external data storage such as ADLS from your main HDInsight cluster and in the Feature Store, create an on-demand Feature Group on the storage container in ADLS. Hadoop hadoop-env.sh: export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/usr/lib/hopsworks/client/* Hadoop core-site.xml: hops.ipc.server.ssl.enabled=true fs.hopsfs.impl=io.hops.hopsfs.client.HopsFileSystem client.rpc.ssl.enabled.protocol=TLSv1.2 hops.ssl.keystore.name=/usr/lib/hopsworks/keyStore.jks hops.rpc.socket.factory.class.default=io.hops.hadoop.shaded.org.apache.hadoop.net.HopsSSLSocketFactory hops.ssl.keystores.passwd.name=/usr/lib/hopsworks/material_passwd hops.ssl.hostname.verifier=ALLOW_ALL hops.ssl.trustore.name=/usr/lib/hopsworks/trustStore.jks Spark spark-defaults.conf: spark.executor.extraClassPath=/usr/lib/hopsworks/client/* spark.driver.extraClassPath=/usr/lib/hopsworks/client/* spark.sql.hive.metastore.jars=path spark.sql.hive.metastore.jars.path=/usr/lib/hopsworks/apache-hive-bin/lib/* Spark hive-site.xml: hive.metastore.uris=thrift://MY_HOPSWORKS_INSTANCE_PRIVATE_IP:9083 Info Replace MY_HOPSWORKS_INSTANCE_PRIVATE_IP with the private IP address of you Hopsworks Feature Store.","title":"Step 3: Configure HDInsight for Feature Store access"},{"location":"user_guides/integrations/hdinsight/#step-5-connect-to-the-feature-store","text":"You are now ready to connect to the Hopsworks Feature Store, for instance using a Jupyter notebook in HDInsight with a PySpark3 kernel: import hsfs # Put the API key into Key Vault for any production setup: # See, https://azure.microsoft.com/en-us/services/key-vault/ secret_value = 'MY_API_KEY' # Create a connection conn = hsfs . connection ( host = 'MY_INSTANCE.cloud.hopsworks.ai' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'MY_PROJECT' , # Name of your Hopsworks Feature Store project api_key_value = secret_value , # The API key to authenticate with Hopsworks hostname_verification = True # Disable for self-signed certificates ) # Get the feature store handle for the project's feature store fs = conn . get_feature_store ()","title":"Step 5: Connect to the Feature Store"},{"location":"user_guides/integrations/hdinsight/#next-steps","text":"For more information about how to use the Feature Store, see the Quickstart Guide .","title":"Next Steps"},{"location":"user_guides/integrations/mlstudio_designer/","text":"Azure Machine Learning Designer Integration # Connecting to the Feature Store from the Azure Machine Learning Designer requires setting up a Feature Store API key for the Designer and installing the HSFS on the Designer. This guide explains step by step how to connect to the Feature Store from Azure Machine Learning Designer. Network Connectivity To be able to connect to the Feature Store, please ensure that the Network Security Group of your Hopsworks instance on Azure is configured to allow incoming traffic from your compute target on ports 443, 9083 and 9085 (443,9083,9085). See Network security groups for more information. If your compute target is not in the same VNet as your Hopsworks instance and the Hopsworks instance is not accessible from the internet then you will need to configure Virtual Network Peering . Generate an API key # For instructions on how to generate an API key follow this user guide . For the Azure ML Designer integration to work correctly make sure you add the following scopes to your API key: featurestore project job kafka Connect to the Feature Store # To connect to the Feature Store from the Azure Machine Learning Designer, create a new pipeline or open an existing one: Add an Execute Python Script step In the pipeline, add a new Execute Python Script step and replace the Python script from the next step: Add the code to access the Feature Store Updating the script Replace MY_VERSION, MY_API_KEY, MY_INSTANCE, MY_PROJECT and MY_FEATURE_GROUP with the respective values. The major version set for MY_VERSION needs to match the major version of Hopsworks. Check PyPI for available releases. You find the Hopsworks version inside any of your Project's settings tab on Hopsworks import os import importlib.util package_name = 'hsfs' version = 'MY_VERSION' spec = importlib . util . find_spec ( package_name ) if spec is None : import os os . system ( f \"pip install %s[python]==%s\" % ( package_name , version )) # Put the API key into Key Vault for any production setup: # See, https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-secrets-in-runs #from azureml.core import Experiment, Run #run = Run.get_context() #secret_value = run.get_secret(name=\"fs-api-key\") secret_value = 'MY_API_KEY' def azureml_main ( dataframe1 = None , dataframe2 = None ): import hsfs conn = hsfs . connection ( host = 'MY_INSTANCE.cloud.hopsworks.ai' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'MY_PROJECT' , # Name of your Hopsworks Feature Store project api_key_value = secret_value , # The API key to authenticate with Hopsworks hostname_verification = True , # Disable for self-signed certificates engine = 'python' # Choose python as engine ) fs = conn . get_feature_store () # Get the project's default feature store return fs . get_feature_group ( 'MY_FEATURE_GROUP' , version = 1 ) . read (), Select a compute target and save the step. The step is now ready to use: Select a compute target As a next step, you have to connect the previously created Execute Python Script step with the next step in the pipeline. For instance, to export the features to a CSV file, create a Export Data step: Add an Export Data step Configure the Export Data step to write to you data store of choice: Configure the Export Data step Connect the to steps by drawing a line between them: Connect the steps Finally, submit the pipeline and wait for it to finish: Performance on the first execution The Execute Python Script step can be slow when being executed for the first time as the HSFS library needs to be installed on the compute target. Subsequent executions on the same compute target should use the already installed library. Execute the pipeline Next Steps # For more information about how to use the Feature Store, see the Quickstart Guide .","title":"Designer"},{"location":"user_guides/integrations/mlstudio_designer/#azure-machine-learning-designer-integration","text":"Connecting to the Feature Store from the Azure Machine Learning Designer requires setting up a Feature Store API key for the Designer and installing the HSFS on the Designer. This guide explains step by step how to connect to the Feature Store from Azure Machine Learning Designer. Network Connectivity To be able to connect to the Feature Store, please ensure that the Network Security Group of your Hopsworks instance on Azure is configured to allow incoming traffic from your compute target on ports 443, 9083 and 9085 (443,9083,9085). See Network security groups for more information. If your compute target is not in the same VNet as your Hopsworks instance and the Hopsworks instance is not accessible from the internet then you will need to configure Virtual Network Peering .","title":"Azure Machine Learning Designer Integration"},{"location":"user_guides/integrations/mlstudio_designer/#generate-an-api-key","text":"For instructions on how to generate an API key follow this user guide . For the Azure ML Designer integration to work correctly make sure you add the following scopes to your API key: featurestore project job kafka","title":"Generate an API key"},{"location":"user_guides/integrations/mlstudio_designer/#connect-to-the-feature-store","text":"To connect to the Feature Store from the Azure Machine Learning Designer, create a new pipeline or open an existing one: Add an Execute Python Script step In the pipeline, add a new Execute Python Script step and replace the Python script from the next step: Add the code to access the Feature Store Updating the script Replace MY_VERSION, MY_API_KEY, MY_INSTANCE, MY_PROJECT and MY_FEATURE_GROUP with the respective values. The major version set for MY_VERSION needs to match the major version of Hopsworks. Check PyPI for available releases. You find the Hopsworks version inside any of your Project's settings tab on Hopsworks import os import importlib.util package_name = 'hsfs' version = 'MY_VERSION' spec = importlib . util . find_spec ( package_name ) if spec is None : import os os . system ( f \"pip install %s[python]==%s\" % ( package_name , version )) # Put the API key into Key Vault for any production setup: # See, https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-secrets-in-runs #from azureml.core import Experiment, Run #run = Run.get_context() #secret_value = run.get_secret(name=\"fs-api-key\") secret_value = 'MY_API_KEY' def azureml_main ( dataframe1 = None , dataframe2 = None ): import hsfs conn = hsfs . connection ( host = 'MY_INSTANCE.cloud.hopsworks.ai' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'MY_PROJECT' , # Name of your Hopsworks Feature Store project api_key_value = secret_value , # The API key to authenticate with Hopsworks hostname_verification = True , # Disable for self-signed certificates engine = 'python' # Choose python as engine ) fs = conn . get_feature_store () # Get the project's default feature store return fs . get_feature_group ( 'MY_FEATURE_GROUP' , version = 1 ) . read (), Select a compute target and save the step. The step is now ready to use: Select a compute target As a next step, you have to connect the previously created Execute Python Script step with the next step in the pipeline. For instance, to export the features to a CSV file, create a Export Data step: Add an Export Data step Configure the Export Data step to write to you data store of choice: Configure the Export Data step Connect the to steps by drawing a line between them: Connect the steps Finally, submit the pipeline and wait for it to finish: Performance on the first execution The Execute Python Script step can be slow when being executed for the first time as the HSFS library needs to be installed on the compute target. Subsequent executions on the same compute target should use the already installed library. Execute the pipeline","title":"Connect to the Feature Store"},{"location":"user_guides/integrations/mlstudio_designer/#next-steps","text":"For more information about how to use the Feature Store, see the Quickstart Guide .","title":"Next Steps"},{"location":"user_guides/integrations/mlstudio_notebooks/","text":"Azure Machine Learning Notebooks Integration # Connecting to the Feature Store from Azure Machine Learning Notebooks requires setting up a Feature Store API key for Azure Machine Learning Notebooks and installing the HSFS on the notebook. This guide explains step by step how to connect to the Feature Store from Azure Machine Learning Notebooks. Network Connectivity To be able to connect to the Feature Store, please ensure that the Network Security Group of your Hopsworks instance on Azure is configured to allow incoming traffic from your compute target on ports 443, 9083 and 9085 (443,9083,9085). See Network security groups for more information. If your compute target is not in the same VNet as your Hopsworks instance and the Hopsworks instance is not accessible from the internet then you will need to configure Virtual Network Peering . Generate an API key # For instructions on how to generate an API key follow this user guide . For the Azure ML Notebooks integration to work correctly make sure you add the following scopes to your API key: featurestore project job kafka Connect from an Azure Machine Learning Notebook # To access the Feature Store from Azure Machine Learning, open a Python notebook and proceed with the following steps to install HSFS and connect to the Feature Store: Connecting from an Azure Machine Learning Notebook Install HSFS # To be able to access the Hopsworks Feature Store, the HSFS Python library needs to be installed. One way of achieving this is by opening a Python notebook in Azure Machine Learning and installing the HSFS with a magic command and pip: !pip install hsfs[python]~=[HOPSWORKS_VERSION] Hive Dependencies By default, HSFS assumes Spark is used as execution engine and therefore Hive dependencies are not installed. Hence, if you are using a regular Python Kernel without Spark , make sure to install the \"python\" extra dependencies ( hsfs[python] ). Matching Hopsworks version The major version of HSFS needs to match the major version of Hopsworks . Check PyPI for available releases. You find the Hopsworks version inside any of your Project's settings tab on Hopsworks Connect to the Feature Store # You are now ready to connect to the Hopsworks Feature Store from the notebook: import hsfs # Put the API key into Key Vault for any production setup: # See, https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-secrets-in-runs #from azureml.core import Experiment, Run #run = Run.get_context() #secret_value = run.get_secret(name=\"fs-api-key\") secret_value = 'MY_API_KEY' # Create a connection conn = hsfs . connection ( host = 'MY_INSTANCE.cloud.hopsworks.ai' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'MY_PROJECT' , # Name of your Hopsworks Feature Store project api_key_value = secret_value , # The API key to authenticate with Hopsworks hostname_verification = True , # Disable for self-signed certificates engine = 'python' # Choose Python as engine ) # Get the feature store handle for the project's feature store fs = conn . get_feature_store () Next Steps # For more information about how to use the Feature Store, see the Quickstart Guide .","title":"Notebooks"},{"location":"user_guides/integrations/mlstudio_notebooks/#azure-machine-learning-notebooks-integration","text":"Connecting to the Feature Store from Azure Machine Learning Notebooks requires setting up a Feature Store API key for Azure Machine Learning Notebooks and installing the HSFS on the notebook. This guide explains step by step how to connect to the Feature Store from Azure Machine Learning Notebooks. Network Connectivity To be able to connect to the Feature Store, please ensure that the Network Security Group of your Hopsworks instance on Azure is configured to allow incoming traffic from your compute target on ports 443, 9083 and 9085 (443,9083,9085). See Network security groups for more information. If your compute target is not in the same VNet as your Hopsworks instance and the Hopsworks instance is not accessible from the internet then you will need to configure Virtual Network Peering .","title":"Azure Machine Learning Notebooks Integration"},{"location":"user_guides/integrations/mlstudio_notebooks/#generate-an-api-key","text":"For instructions on how to generate an API key follow this user guide . For the Azure ML Notebooks integration to work correctly make sure you add the following scopes to your API key: featurestore project job kafka","title":"Generate an API key"},{"location":"user_guides/integrations/mlstudio_notebooks/#connect-from-an-azure-machine-learning-notebook","text":"To access the Feature Store from Azure Machine Learning, open a Python notebook and proceed with the following steps to install HSFS and connect to the Feature Store: Connecting from an Azure Machine Learning Notebook","title":"Connect from an Azure Machine Learning Notebook"},{"location":"user_guides/integrations/mlstudio_notebooks/#install-hsfs","text":"To be able to access the Hopsworks Feature Store, the HSFS Python library needs to be installed. One way of achieving this is by opening a Python notebook in Azure Machine Learning and installing the HSFS with a magic command and pip: !pip install hsfs[python]~=[HOPSWORKS_VERSION] Hive Dependencies By default, HSFS assumes Spark is used as execution engine and therefore Hive dependencies are not installed. Hence, if you are using a regular Python Kernel without Spark , make sure to install the \"python\" extra dependencies ( hsfs[python] ). Matching Hopsworks version The major version of HSFS needs to match the major version of Hopsworks . Check PyPI for available releases. You find the Hopsworks version inside any of your Project's settings tab on Hopsworks","title":"Install HSFS"},{"location":"user_guides/integrations/mlstudio_notebooks/#connect-to-the-feature-store","text":"You are now ready to connect to the Hopsworks Feature Store from the notebook: import hsfs # Put the API key into Key Vault for any production setup: # See, https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-secrets-in-runs #from azureml.core import Experiment, Run #run = Run.get_context() #secret_value = run.get_secret(name=\"fs-api-key\") secret_value = 'MY_API_KEY' # Create a connection conn = hsfs . connection ( host = 'MY_INSTANCE.cloud.hopsworks.ai' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'MY_PROJECT' , # Name of your Hopsworks Feature Store project api_key_value = secret_value , # The API key to authenticate with Hopsworks hostname_verification = True , # Disable for self-signed certificates engine = 'python' # Choose Python as engine ) # Get the feature store handle for the project's feature store fs = conn . get_feature_store ()","title":"Connect to the Feature Store"},{"location":"user_guides/integrations/mlstudio_notebooks/#next-steps","text":"For more information about how to use the Feature Store, see the Quickstart Guide .","title":"Next Steps"},{"location":"user_guides/integrations/python/","text":"Python Environments (Local or Kubeflow) # Connecting to the Feature Store from any Python environment requires setting up a Feature Store API key and installing the library. This guide explains step by step how to connect to the Feature Store from any Python environment such as your local environment or KubeFlow. Install HSFS # To be able to access the Hopsworks Feature Store, the HSFS Python library needs to be installed in the environment from which you want to connect to the Feature Store. You can install the library through pip. We recommend using a Python environment manager such as virtualenv or conda . pip install hsfs[python]~=[HOPSWORKS_VERSION] Hive Dependencies By default, HSFS assumes Spark/EMR is used as execution engine and therefore Hive dependencies are not installed. Hence, on a local Python evnironment, if you are planning to use a regular Python Kernel without Spark/EMR , make sure to install the \"python\" extra dependencies ( hsfs[python] ). Matching Hopsworks version The major version of HSFS needs to match the major version of Hopsworks . You find the Hopsworks version inside any of your Project's settings tab on Hopsworks Generate an API key # For instructions on how to generate an API key follow this user guide . For the Python client to work correctly make sure you add the following scopes to your API key: featurestore project job kafka Connect to the Feature Store # You are now ready to connect to the Hopsworks Feature Store from your Python environment: import hsfs conn = hsfs . connection ( host = 'my_instance' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'my_project' , # Name of your Hopsworks Feature Store project api_key_value = 'apikey' , # The API key to authenticate with Hopsworks hostname_verification = True , # Disable for self-signed certificates engine = \"python\" # Set to \"spark\" if you are using Spark/EMR ) fs = conn . get_feature_store () # Get the project's default feature store Engine HSFS uses either Apache Spark or Pandas on Python as an execution engine to perform queries against the feature store. The engine option of the connection let's you overwrite the default behaviour by setting it to \"python\" or \"spark\" . By default, HSFS will try to use Spark as engine if PySpark is available. So if you have PySpark installed in your local Python environment, but you have not configured Spark, you will have to set engine='python' . Please refer to the Spark integration guide to configure your local Spark cluster to be able to connect to the Hopsworks Feature Store. Ports If you have trouble to connect, please ensure that your Feature Store can receive incoming traffic from your Python environment on ports 443, 9083 and 9085 (443,9083,9085). If you deployed your Hopsworks Feature Store instance with managed.hopsworks.ai , it suffices to enable outside access of the Feature Store and Online Feature Store services . Next Steps # For more information about how to connect, see the Connection API reference. Or continue with the Data Source guide to import your own data to the Feature Store.","title":"Python"},{"location":"user_guides/integrations/python/#python-environments-local-or-kubeflow","text":"Connecting to the Feature Store from any Python environment requires setting up a Feature Store API key and installing the library. This guide explains step by step how to connect to the Feature Store from any Python environment such as your local environment or KubeFlow.","title":"Python Environments (Local or Kubeflow)"},{"location":"user_guides/integrations/python/#install-hsfs","text":"To be able to access the Hopsworks Feature Store, the HSFS Python library needs to be installed in the environment from which you want to connect to the Feature Store. You can install the library through pip. We recommend using a Python environment manager such as virtualenv or conda . pip install hsfs[python]~=[HOPSWORKS_VERSION] Hive Dependencies By default, HSFS assumes Spark/EMR is used as execution engine and therefore Hive dependencies are not installed. Hence, on a local Python evnironment, if you are planning to use a regular Python Kernel without Spark/EMR , make sure to install the \"python\" extra dependencies ( hsfs[python] ). Matching Hopsworks version The major version of HSFS needs to match the major version of Hopsworks . You find the Hopsworks version inside any of your Project's settings tab on Hopsworks","title":"Install HSFS"},{"location":"user_guides/integrations/python/#generate-an-api-key","text":"For instructions on how to generate an API key follow this user guide . For the Python client to work correctly make sure you add the following scopes to your API key: featurestore project job kafka","title":"Generate an API key"},{"location":"user_guides/integrations/python/#connect-to-the-feature-store","text":"You are now ready to connect to the Hopsworks Feature Store from your Python environment: import hsfs conn = hsfs . connection ( host = 'my_instance' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'my_project' , # Name of your Hopsworks Feature Store project api_key_value = 'apikey' , # The API key to authenticate with Hopsworks hostname_verification = True , # Disable for self-signed certificates engine = \"python\" # Set to \"spark\" if you are using Spark/EMR ) fs = conn . get_feature_store () # Get the project's default feature store Engine HSFS uses either Apache Spark or Pandas on Python as an execution engine to perform queries against the feature store. The engine option of the connection let's you overwrite the default behaviour by setting it to \"python\" or \"spark\" . By default, HSFS will try to use Spark as engine if PySpark is available. So if you have PySpark installed in your local Python environment, but you have not configured Spark, you will have to set engine='python' . Please refer to the Spark integration guide to configure your local Spark cluster to be able to connect to the Hopsworks Feature Store. Ports If you have trouble to connect, please ensure that your Feature Store can receive incoming traffic from your Python environment on ports 443, 9083 and 9085 (443,9083,9085). If you deployed your Hopsworks Feature Store instance with managed.hopsworks.ai , it suffices to enable outside access of the Feature Store and Online Feature Store services .","title":"Connect to the Feature Store"},{"location":"user_guides/integrations/python/#next-steps","text":"For more information about how to connect, see the Connection API reference. Or continue with the Data Source guide to import your own data to the Feature Store.","title":"Next Steps"},{"location":"user_guides/integrations/role_mapping/","text":"IAM role mapping # Using an EC2 instance profile enables your Hopsworks cluster to access AWS resources. This forces all Hopsworks users to share the instance profile role and the resource access policies attached to that role. To allow for per project access policies you could have your users use AWS credentials directly in their programs which is not recommended so you should instead use Role chaining . To use Role chaining, you need to first setup IAM roles in AWS: Create an instance profile role that contains the different resource roles that we want to allow selected users to be able to assume in the Hopsworks cluster. In the example below, we define 4 different resource roles: test-role, s3-role, dev-s3-role, and redshift - and later we will define which users will be allowed to assume which of these resources roles. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"AssumeDataRoles\" , \"Effect\" : \"Allow\" , \"Action\" : \"sts:AssumeRole\" , \"Resource\" : [ \"arn:aws:iam::123456789011:role/test-role\" , \"arn:aws:iam::xxxxxxxxxxxx:role/s3-role\" , \"arn:aws:iam::xxxxxxxxxxxx:role/dev-s3-role\" , \"arn:aws:iam::xxxxxxxxxxxx:role/redshift\" ] } ] } Example policy for assuming four roles. Create the resource roles and edit the trust relationship and add a policy document that will allow the instance profile to assume the resource roles. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"arn:aws:iam::xxxxxxxxxxxx:role/instance-profile\" }, \"Action\" : \"sts:AssumeRole\" } ] } Example policy document. Finally attach the instance profile to the master node of your Hopsworks AWS instance. Role chaining allows the instance profile to assume any of the 4 resource roles in the policy that was attached in step 1. Typically, we will not want any user in Hopsworks to assume any of the resource roles. You can grant selected users the ability to assume any of the 4 resource roles from the admin page in hopsworks. In particular, we specify in which project(s) a given resource role can be used. Within a given project, we can further restrict who can assume the resource role by mapping the role to the group of users (data owners or data scientists). Resource role mapping. By clicking the 'Resource role mapping' icon in the admin page shown in the image above you can add mappings by entering the project name and which roles in that project can access the resource role. Optionally, you can set a role mapping as default by marking the default checkbox. The default roles can only be changed by a Data owner who can do so in the project settings page. Add resource role to project mapping. Any member of a project can then go to the project settings page to see which roles they can assume. Resource role mapped to project. For instructions on how to use the assume role API see assuming a role .","title":"IAM role mapping"},{"location":"user_guides/integrations/role_mapping/#iam-role-mapping","text":"Using an EC2 instance profile enables your Hopsworks cluster to access AWS resources. This forces all Hopsworks users to share the instance profile role and the resource access policies attached to that role. To allow for per project access policies you could have your users use AWS credentials directly in their programs which is not recommended so you should instead use Role chaining . To use Role chaining, you need to first setup IAM roles in AWS: Create an instance profile role that contains the different resource roles that we want to allow selected users to be able to assume in the Hopsworks cluster. In the example below, we define 4 different resource roles: test-role, s3-role, dev-s3-role, and redshift - and later we will define which users will be allowed to assume which of these resources roles. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"AssumeDataRoles\" , \"Effect\" : \"Allow\" , \"Action\" : \"sts:AssumeRole\" , \"Resource\" : [ \"arn:aws:iam::123456789011:role/test-role\" , \"arn:aws:iam::xxxxxxxxxxxx:role/s3-role\" , \"arn:aws:iam::xxxxxxxxxxxx:role/dev-s3-role\" , \"arn:aws:iam::xxxxxxxxxxxx:role/redshift\" ] } ] } Example policy for assuming four roles. Create the resource roles and edit the trust relationship and add a policy document that will allow the instance profile to assume the resource roles. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"arn:aws:iam::xxxxxxxxxxxx:role/instance-profile\" }, \"Action\" : \"sts:AssumeRole\" } ] } Example policy document. Finally attach the instance profile to the master node of your Hopsworks AWS instance. Role chaining allows the instance profile to assume any of the 4 resource roles in the policy that was attached in step 1. Typically, we will not want any user in Hopsworks to assume any of the resource roles. You can grant selected users the ability to assume any of the 4 resource roles from the admin page in hopsworks. In particular, we specify in which project(s) a given resource role can be used. Within a given project, we can further restrict who can assume the resource role by mapping the role to the group of users (data owners or data scientists). Resource role mapping. By clicking the 'Resource role mapping' icon in the admin page shown in the image above you can add mappings by entering the project name and which roles in that project can access the resource role. Optionally, you can set a role mapping as default by marking the default checkbox. The default roles can only be changed by a Data owner who can do so in the project settings page. Add resource role to project mapping. Any member of a project can then go to the project settings page to see which roles they can assume. Resource role mapped to project. For instructions on how to use the assume role API see assuming a role .","title":"IAM role mapping"},{"location":"user_guides/integrations/sagemaker/","text":"AWS SageMaker Integration # Connecting to the Feature Store from SageMaker requires setting up a Feature Store API key for SageMaker and installing the HSFS on SageMaker. This guide explains step by step how to connect to the Feature Store from SageMaker. Generate an API key # For instructions on how to generate an API key follow this user guide . For the SageMaker integration to work make sure you add the following scopes to your API key: featurestore project job kafka Quickstart API key Argument # API key as Argument To get started quickly, without saving the Hopsworks API in a secret storage, you can simply supply it as an argument when instantiating a connection: import hsfs conn = hsfs . connection ( host = 'my_instance' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'my_project' , # Name of your Hopsworks Feature Store project api_key_value = 'apikey' , # The API key to authenticate with Hopsworks hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store Store the API key on AWS # The API key now needs to be stored on AWS, so it can be retrieved from within SageMaker notebooks. Identify your SageMaker role # You need to know the IAM role used by your SageMaker instance to set up the API key for it. You can find it in the overview of your SageMaker notebook instance of the AWS Management Console. In this example, the name of the role is AmazonSageMaker-ExecutionRole-20190511T072435 . The role is attached to your SageMaker notebook instance Store the API key # You have two options to make your API key accessible from SageMaker: Option 1: Using the AWS Systems Manager Parameter Store # Store the API key in the AWS Systems Manager Parameter Store # In the AWS Management Console, ensure that your active region is the region you use for SageMaker. Go to the AWS Systems Manager choose Parameter Store in the left navigation bar and select Create Parameter . As name, enter /hopsworks/role/[MY_SAGEMAKER_ROLE]/type/api-key replacing [MY_SAGEMAKER_ROLE] with the AWS role used by the SageMaker instance that should access the Feature Store. Select Secure String as type and create the parameter . Store the API key in the AWS Systems Manager Parameter Store Grant access to the Parameter Store from the SageMaker notebook role # In the AWS Management Console, go to IAM , select Roles and then the role that is used when creating SageMaker notebook instances. Select Add inline policy . Choose Systems Manager as service, expand the Read access level and check GetParameter . Expand Resources and select Add ARN . Enter the region of the Systems Manager as well as the name of the parameter WITHOUT the leading slash e.g. hopsworks/role/[MY_SAGEMAKER_ROLE]/type/api-key and click Add . Click on Review , give the policy a name und click on Create policy . Grant access to the Parameter Store from the SageMaker notebook role Option 2: Using the AWS Secrets Manager # Store the API key in the AWS Secrets Manager # In the AWS Management Console, ensure that your active region is the region you use for SageMaker. Go to the AWS Secrets Manager and select Store new secret . Select Other type of secrets and add api-key as the key and paste the API key created in the previous step as the value. Click next. Store the API key in the AWS Secrets Manager As secret name, enter hopsworks/role/[MY_SAGEMAKER_ROLE] replacing [MY_SAGEMAKER_ROLE] with the AWS role used by the SageMaker instance that should access the Feature Store. Select next twice and finally store the secret. Then click on the secret in the secrets list and take note of the Secret ARN . Store the API key in the AWS Secrets Manager Grant access to the SecretsManager to the SageMaker notebook role # In the AWS Management Console, go to IAM , select Roles and then the role that is used when creating SageMaker notebook instances. Select Add inline policy . Choose Secrets Manager as service, expand the Read access level and check GetSecretValue . Expand Resources and select Add ARN . Paste the ARN of the secret created in the previous step. Click on Review , give the policy a name und click on Create policy . Grant access to the SecretsManager to the SageMaker notebook role Install HSFS # To be able to access the Hopsworks Feature Store, the HSFS Python library needs to be installed. One way of achieving this is by opening a Python notebook in SageMaker and installing the HSFS with a magic command and pip: !pip install hsfs[python]~=[HOPSWORKS_VERSION] Hive Dependencies By default, HSFS assumes Spark/EMR is used as execution engine and therefore Hive dependencies are not installed. Hence, on AWS SageMaker, if you are planning to use a regular Python Kernel without Spark/EMR , make sure to install the \"python\" extra dependencies ( hsfs[python] ). Matching Hopsworks version The major version of HSFS needs to match the major version of Hopsworks . You find the Hopsworks version inside any of your Project's settings tab on Hopsworks Note that the library will not be persistent. For information around how to permanently install a library to SageMaker, see Install External Libraries and Kernels in Notebook Instances. Connect to the Feature Store # You are now ready to connect to the Hopsworks Feature Store from SageMaker: import hsfs conn = hsfs . connection ( 'my_instance' , # DNS of your Feature Store instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks Feature Store project secrets_store = 'secretsmanager' , # Either parameterstore or secretsmanager hostname_verification = True , # Disable for self-signed certificates engine = 'python' # Choose Python as engine if you haven't set up AWS EMR ) fs = conn . get_feature_store () # Get the project's default feature store Engine HSFS uses either Apache Spark or Pandas on Python as an execution engine to perform queries against the feature store. Most AWS SageMaker Kernels have PySpark installed but are not connected to AWS EMR by default, hence, the engine option of the connection let's you overwrite the default behaviour. By default, HSFS will try to use Spark as engine if PySpark is available, however, if Spark/EMR is not configured, you will have to set the engine manually to \"python\" . Please refer to the EMR integration guide to setup EMR with the Hopsworks Feature Store. Ports If you have trouble connecting, please ensure that the Security Group of your Hopsworks instance on AWS is configured to allow incoming traffic from your SageMaker instance on ports 443, 9083 and 9085 (443,9083,9085). See VPC Security Groups for more information. If your SageMaker instances are not in the same VPC as your Hopsworks instance and the Hopsworks instance is not accessible from the internet then you will need to configure VPC Peering on AWS . Next Steps # For more information about how to use the Feature Store, see the Quickstart Guide .","title":"AWS Sagemaker"},{"location":"user_guides/integrations/sagemaker/#aws-sagemaker-integration","text":"Connecting to the Feature Store from SageMaker requires setting up a Feature Store API key for SageMaker and installing the HSFS on SageMaker. This guide explains step by step how to connect to the Feature Store from SageMaker.","title":"AWS SageMaker Integration"},{"location":"user_guides/integrations/sagemaker/#generate-an-api-key","text":"For instructions on how to generate an API key follow this user guide . For the SageMaker integration to work make sure you add the following scopes to your API key: featurestore project job kafka","title":"Generate an API key"},{"location":"user_guides/integrations/sagemaker/#quickstart-api-key-argument","text":"API key as Argument To get started quickly, without saving the Hopsworks API in a secret storage, you can simply supply it as an argument when instantiating a connection: import hsfs conn = hsfs . connection ( host = 'my_instance' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'my_project' , # Name of your Hopsworks Feature Store project api_key_value = 'apikey' , # The API key to authenticate with Hopsworks hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store","title":"Quickstart API key Argument"},{"location":"user_guides/integrations/sagemaker/#store-the-api-key-on-aws","text":"The API key now needs to be stored on AWS, so it can be retrieved from within SageMaker notebooks.","title":"Store the API key on AWS"},{"location":"user_guides/integrations/sagemaker/#identify-your-sagemaker-role","text":"You need to know the IAM role used by your SageMaker instance to set up the API key for it. You can find it in the overview of your SageMaker notebook instance of the AWS Management Console. In this example, the name of the role is AmazonSageMaker-ExecutionRole-20190511T072435 . The role is attached to your SageMaker notebook instance","title":"Identify your SageMaker role"},{"location":"user_guides/integrations/sagemaker/#store-the-api-key","text":"You have two options to make your API key accessible from SageMaker:","title":"Store the API key"},{"location":"user_guides/integrations/sagemaker/#option-1-using-the-aws-systems-manager-parameter-store","text":"","title":"Option 1: Using the AWS Systems Manager Parameter Store"},{"location":"user_guides/integrations/sagemaker/#store-the-api-key-in-the-aws-systems-manager-parameter-store","text":"In the AWS Management Console, ensure that your active region is the region you use for SageMaker. Go to the AWS Systems Manager choose Parameter Store in the left navigation bar and select Create Parameter . As name, enter /hopsworks/role/[MY_SAGEMAKER_ROLE]/type/api-key replacing [MY_SAGEMAKER_ROLE] with the AWS role used by the SageMaker instance that should access the Feature Store. Select Secure String as type and create the parameter . Store the API key in the AWS Systems Manager Parameter Store","title":"Store the API key in the AWS Systems Manager Parameter Store"},{"location":"user_guides/integrations/sagemaker/#grant-access-to-the-parameter-store-from-the-sagemaker-notebook-role","text":"In the AWS Management Console, go to IAM , select Roles and then the role that is used when creating SageMaker notebook instances. Select Add inline policy . Choose Systems Manager as service, expand the Read access level and check GetParameter . Expand Resources and select Add ARN . Enter the region of the Systems Manager as well as the name of the parameter WITHOUT the leading slash e.g. hopsworks/role/[MY_SAGEMAKER_ROLE]/type/api-key and click Add . Click on Review , give the policy a name und click on Create policy . Grant access to the Parameter Store from the SageMaker notebook role","title":"Grant access to the Parameter Store from the SageMaker notebook role"},{"location":"user_guides/integrations/sagemaker/#option-2-using-the-aws-secrets-manager","text":"","title":"Option 2: Using the AWS Secrets Manager"},{"location":"user_guides/integrations/sagemaker/#store-the-api-key-in-the-aws-secrets-manager","text":"In the AWS Management Console, ensure that your active region is the region you use for SageMaker. Go to the AWS Secrets Manager and select Store new secret . Select Other type of secrets and add api-key as the key and paste the API key created in the previous step as the value. Click next. Store the API key in the AWS Secrets Manager As secret name, enter hopsworks/role/[MY_SAGEMAKER_ROLE] replacing [MY_SAGEMAKER_ROLE] with the AWS role used by the SageMaker instance that should access the Feature Store. Select next twice and finally store the secret. Then click on the secret in the secrets list and take note of the Secret ARN . Store the API key in the AWS Secrets Manager","title":"Store the API key in the AWS Secrets Manager"},{"location":"user_guides/integrations/sagemaker/#grant-access-to-the-secretsmanager-to-the-sagemaker-notebook-role","text":"In the AWS Management Console, go to IAM , select Roles and then the role that is used when creating SageMaker notebook instances. Select Add inline policy . Choose Secrets Manager as service, expand the Read access level and check GetSecretValue . Expand Resources and select Add ARN . Paste the ARN of the secret created in the previous step. Click on Review , give the policy a name und click on Create policy . Grant access to the SecretsManager to the SageMaker notebook role","title":"Grant access to the SecretsManager to the SageMaker notebook role"},{"location":"user_guides/integrations/sagemaker/#install-hsfs","text":"To be able to access the Hopsworks Feature Store, the HSFS Python library needs to be installed. One way of achieving this is by opening a Python notebook in SageMaker and installing the HSFS with a magic command and pip: !pip install hsfs[python]~=[HOPSWORKS_VERSION] Hive Dependencies By default, HSFS assumes Spark/EMR is used as execution engine and therefore Hive dependencies are not installed. Hence, on AWS SageMaker, if you are planning to use a regular Python Kernel without Spark/EMR , make sure to install the \"python\" extra dependencies ( hsfs[python] ). Matching Hopsworks version The major version of HSFS needs to match the major version of Hopsworks . You find the Hopsworks version inside any of your Project's settings tab on Hopsworks Note that the library will not be persistent. For information around how to permanently install a library to SageMaker, see Install External Libraries and Kernels in Notebook Instances.","title":"Install HSFS"},{"location":"user_guides/integrations/sagemaker/#connect-to-the-feature-store","text":"You are now ready to connect to the Hopsworks Feature Store from SageMaker: import hsfs conn = hsfs . connection ( 'my_instance' , # DNS of your Feature Store instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks Feature Store project secrets_store = 'secretsmanager' , # Either parameterstore or secretsmanager hostname_verification = True , # Disable for self-signed certificates engine = 'python' # Choose Python as engine if you haven't set up AWS EMR ) fs = conn . get_feature_store () # Get the project's default feature store Engine HSFS uses either Apache Spark or Pandas on Python as an execution engine to perform queries against the feature store. Most AWS SageMaker Kernels have PySpark installed but are not connected to AWS EMR by default, hence, the engine option of the connection let's you overwrite the default behaviour. By default, HSFS will try to use Spark as engine if PySpark is available, however, if Spark/EMR is not configured, you will have to set the engine manually to \"python\" . Please refer to the EMR integration guide to setup EMR with the Hopsworks Feature Store. Ports If you have trouble connecting, please ensure that the Security Group of your Hopsworks instance on AWS is configured to allow incoming traffic from your SageMaker instance on ports 443, 9083 and 9085 (443,9083,9085). See VPC Security Groups for more information. If your SageMaker instances are not in the same VPC as your Hopsworks instance and the Hopsworks instance is not accessible from the internet then you will need to configure VPC Peering on AWS .","title":"Connect to the Feature Store"},{"location":"user_guides/integrations/sagemaker/#next-steps","text":"For more information about how to use the Feature Store, see the Quickstart Guide .","title":"Next Steps"},{"location":"user_guides/integrations/spark/","text":"Spark Integration # Connecting to the Feature Store from an external Spark cluster, such as Cloudera, requires configuring it with the Hopsworks client jars and configuration. This guide explains step by step how to connect to the Feature Store from an external Spark cluster. Download the Hopsworks Client Jars # In the Project Settings , select the integration tab and scroll to the Configure Spark Integration section. Click on Download client Jars . This will start the download of the client.tar.gz archive. The archive contains two jar files for HopsFS, the Apache Hudi jar and the Java version of the HSFS library. You should upload these libraries to your Spark cluster and attach them as local resources to your Job. If you are using spark-submit , you should specify the --jar option. For more details see: Spark Dependency Management . The Spark Integration gives access to Jars and configuration for an external Spark cluster Download the certificates # Download the certificates from the same section as above. Hopsworks uses X.509 certificates for authentication and authorization. If you are interested in the Hopsworks security model, you can read more about it in this blog post . The certificates are composed of three different components: the keyStore.jks containing the private key and the certificate for your project user, the trustStore.jks containing the certificates for the Hopsworks certificates authority, and a password to unlock the private key in the keyStore.jks . The password is displayed in a pop-up when downloading the certificate and should be saved in a file named material_passwd . Warning When you copy-paste the password to the material_passwd file, pay attention to not introduce additional empty spaces or new lines. The three files ( keyStore.jks , trustStore.jks and material_passwd ) should be attached as resources to your Spark application as well. Configure your Spark cluster # Spark version limitation Currently Spark version 3.3.x is suggested to be able to use the full suite of Hopsworks Feature Store capabilities. Add the following configuration to the Spark application: spark.hadoop.fs.hopsfs.impl io.hops.hopsfs.client.HopsFileSystem spark.hadoop.hops.ipc.server.ssl.enabled true spark.hadoop.hops.ssl.hostname.verifier ALLOW_ALL spark.hadoop.hops.rpc.socket.factory.class.default io.hops.hadoop.shaded.org.apache.hadoop.net.HopsSSLSocketFactory spark.hadoop.client.rpc.ssl.enabled.protocol TLSv1.2 spark.hadoop.hops.ssl.keystores.passwd.name material_passwd spark.hadoop.hops.ssl.keystore.name keyStore.jks spark.hadoop.hops.ssl.trustore.name trustStore.jks spark.sql.hive.metastore.jars path spark.sql.hive.metastore.jars.path [Path to the Hopsworks Hive Jars] spark.hadoop.hive.metastore.uris thrift://[metastore_ip]:[metastore_port] spark.sql.hive.metastore.jars.path should point to the path with the jars from the uncompressed Hive archive you can find in clients.tar.gz . PySpark # To use PySpark, install the HSFS Python library which can be found on PyPi . Matching Hopsworks version The major version of HSFS needs to match the major version of Hopsworks . Generating an API Key # For instructions on how to generate an API key follow this user guide . For the Spark integration to work correctly make sure you add the following scopes to your API key: featurestore project job kafka Connecting to the Feature Store # You are now ready to connect to the Hopsworks Feature Store from Spark: import hsfs conn = hsfs . connection ( host = 'my_instance' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'my_project' , # Name of your Hopsworks Feature Store project api_key_value = 'api_key' , # The API key to authenticate with the feature store hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store Engine HSFS uses either Apache Spark or Pandas on Python as an execution engine to perform queries against the feature store. The engine option of the connection let's you overwrite the default behaviour by setting it to \"python\" or \"spark\" . By default, HSFS will try to use Spark as engine if PySpark is available, hence, no further action should be required if you setup Spark correctly as described above. Next Steps # For more information about how to connect, see the Connection API reference. Or continue with the Data Source guide to import your own data to the Feature Store.","title":"Apache Spark"},{"location":"user_guides/integrations/spark/#spark-integration","text":"Connecting to the Feature Store from an external Spark cluster, such as Cloudera, requires configuring it with the Hopsworks client jars and configuration. This guide explains step by step how to connect to the Feature Store from an external Spark cluster.","title":"Spark Integration"},{"location":"user_guides/integrations/spark/#download-the-hopsworks-client-jars","text":"In the Project Settings , select the integration tab and scroll to the Configure Spark Integration section. Click on Download client Jars . This will start the download of the client.tar.gz archive. The archive contains two jar files for HopsFS, the Apache Hudi jar and the Java version of the HSFS library. You should upload these libraries to your Spark cluster and attach them as local resources to your Job. If you are using spark-submit , you should specify the --jar option. For more details see: Spark Dependency Management . The Spark Integration gives access to Jars and configuration for an external Spark cluster","title":"Download the Hopsworks Client Jars"},{"location":"user_guides/integrations/spark/#download-the-certificates","text":"Download the certificates from the same section as above. Hopsworks uses X.509 certificates for authentication and authorization. If you are interested in the Hopsworks security model, you can read more about it in this blog post . The certificates are composed of three different components: the keyStore.jks containing the private key and the certificate for your project user, the trustStore.jks containing the certificates for the Hopsworks certificates authority, and a password to unlock the private key in the keyStore.jks . The password is displayed in a pop-up when downloading the certificate and should be saved in a file named material_passwd . Warning When you copy-paste the password to the material_passwd file, pay attention to not introduce additional empty spaces or new lines. The three files ( keyStore.jks , trustStore.jks and material_passwd ) should be attached as resources to your Spark application as well.","title":"Download the certificates"},{"location":"user_guides/integrations/spark/#configure-your-spark-cluster","text":"Spark version limitation Currently Spark version 3.3.x is suggested to be able to use the full suite of Hopsworks Feature Store capabilities. Add the following configuration to the Spark application: spark.hadoop.fs.hopsfs.impl io.hops.hopsfs.client.HopsFileSystem spark.hadoop.hops.ipc.server.ssl.enabled true spark.hadoop.hops.ssl.hostname.verifier ALLOW_ALL spark.hadoop.hops.rpc.socket.factory.class.default io.hops.hadoop.shaded.org.apache.hadoop.net.HopsSSLSocketFactory spark.hadoop.client.rpc.ssl.enabled.protocol TLSv1.2 spark.hadoop.hops.ssl.keystores.passwd.name material_passwd spark.hadoop.hops.ssl.keystore.name keyStore.jks spark.hadoop.hops.ssl.trustore.name trustStore.jks spark.sql.hive.metastore.jars path spark.sql.hive.metastore.jars.path [Path to the Hopsworks Hive Jars] spark.hadoop.hive.metastore.uris thrift://[metastore_ip]:[metastore_port] spark.sql.hive.metastore.jars.path should point to the path with the jars from the uncompressed Hive archive you can find in clients.tar.gz .","title":"Configure your Spark cluster"},{"location":"user_guides/integrations/spark/#pyspark","text":"To use PySpark, install the HSFS Python library which can be found on PyPi . Matching Hopsworks version The major version of HSFS needs to match the major version of Hopsworks .","title":"PySpark"},{"location":"user_guides/integrations/spark/#generating-an-api-key","text":"For instructions on how to generate an API key follow this user guide . For the Spark integration to work correctly make sure you add the following scopes to your API key: featurestore project job kafka","title":"Generating an API Key"},{"location":"user_guides/integrations/spark/#connecting-to-the-feature-store","text":"You are now ready to connect to the Hopsworks Feature Store from Spark: import hsfs conn = hsfs . connection ( host = 'my_instance' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'my_project' , # Name of your Hopsworks Feature Store project api_key_value = 'api_key' , # The API key to authenticate with the feature store hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store Engine HSFS uses either Apache Spark or Pandas on Python as an execution engine to perform queries against the feature store. The engine option of the connection let's you overwrite the default behaviour by setting it to \"python\" or \"spark\" . By default, HSFS will try to use Spark as engine if PySpark is available, hence, no further action should be required if you setup Spark correctly as described above.","title":"Connecting to the Feature Store"},{"location":"user_guides/integrations/spark/#next-steps","text":"For more information about how to connect, see the Connection API reference. Or continue with the Data Source guide to import your own data to the Feature Store.","title":"Next Steps"},{"location":"user_guides/integrations/databricks/api_key/","text":"Hopsworks API key # In order for the Databricks cluster to be able to communicate with the Hopsworks Feature Store, the clients running on Databricks need to be able to access a Hopsworks API key. Generate an API key # For instructions on how to generate an API key follow this user guide . For the Databricks integration to work make sure you add the following scopes to your API key: featurestore project job kafka Quickstart API key Argument # API key as Argument To get started quickly, without saving the Hopsworks API in a secret storage, you can simply supply it as an argument when instantiating a connection: import hsfs conn = hsfs . connection ( host = 'my_instance' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'my_project' , # Name of your Hopsworks Feature Store project api_key_value = 'apikey' , # The API key to authenticate with Hopsworks hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store Store the API key # AWS # Step 1: Create an instance profile to attach to your Databricks clusters # Go to the AWS IAM choose Roles and click on Create Role . Select AWS Service as the type of trusted entity and EC2 as the use case as shown below: Create an instance profile Click on Next: Permissions , Next:Tags , and then Next: Review . Name the instance profile role and then click Create role . Step 2: Storing the API Key # Option 1: Using the AWS Systems Manager Parameter Store In the AWS Management Console, ensure that your active region is the region you use for Databricks. Go to the AWS Systems Manager choose Parameter Store and select Create Parameter . As name enter /hopsworks/role/[MY_DATABRICKS_ROLE]/type/api-key replacing [MY_DATABRICKS_ROLE] with the name of the AWS role you have created in Step 1 . Select Secure String as type and create the parameter. Storing the Feature Store API key in the Parameter Store Once the API Key is stored, you need to grant access to it from the AWS role that you have created in Step 1 . In the AWS Management Console, go to IAM , select Roles and then search for the role that you have created in Step 1 . Select Add inline policy . Choose Systems Manager as service, expand the Read access level and check GetParameter . Expand Resources and select Add ARN . Enter the region of the Systems Manager as well as the name of the parameter WITHOUT the leading slash e.g. hopsworks/role/[MY_DATABRICKS_ROLE]/type/api-key and click Add . Click on Review , give the policy a name und click on Create policy . Configuring the access policy for the Parameter Store Option 2: Using the AWS Secrets Manager In the AWS management console ensure that your active region is the region you use for Databricks. Go to the AWS Secrets Manager and select Store new secret . Select Other type of secrets and add api-key as the key and paste the API key created in the previous step as the value. Click next. Storing a Feature Store API key in the Secrets Manager Step 1 As secret name, enter hopsworks/role/[MY_DATABRICKS_ROLE] replacing [MY_DATABRICKS_ROLE] with the AWS role you have created in Step 1 . Select next twice and finally store the secret. Then click on the secret in the secrets list and take note of the Secret ARN . Storing a Feature Store API key in the Secrets Manager Step 2 Once the API Key is stored, you need to grant access to it from the AWS role that you have created in Step 1 . In the AWS Management Console, go to IAM , select Roles and then the role that that you have created in Step 1 . Select Add inline policy . Choose Secrets Manager as service, expand the Read access level and check GetSecretValue . Expand Resources and select Add ARN . Paste the ARN of the secret created in the previous step. Click on Review , give the policy a name und click on Create policy . Configuring the access policy for the Secrets Manager Step 3: Allow Databricks to use the AWS role created in Step 1 # First you need to get the AWS role used by Databricks for deployments as described in this step . Once you get the role name, go to AWS IAM , search for the role, and click on it. Then, select the Permissions tab, click on Add inline policy , select the JSON tab, and paste the following snippet. Replace [ACCOUNT_ID] with your AWS account id, and [MY_DATABRICKS_ROLE] with the AWS role name created in Step 1 . { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"PassRole\" , \"Effect\" : \"Allow\" , \"Action\" : \"iam:PassRole\" , \"Resource\" : \"arn:aws:iam::[ACCOUNT_ID]:role/[MY_DATABRICKS_ROLE]\" } ] } Click Review Policy , name the policy, and click Create Policy . Then, go to your Databricks workspace and follow this step to add the instance profile to your workspace. Finally, when launching Databricks clusters, select Advanced settings and choose the instance profile you have just added. Azure # On Azure we currently do not support storing the API key in a secret storage. Instead just store the API key in a file in your Databricks workspace so you can access it when connecting to the Feature Store. Next Steps # Continue with the configuration guide to finalize the configuration of the Databricks Cluster to communicate with the Hopsworks Feature Store.","title":"Hopsworks API Key"},{"location":"user_guides/integrations/databricks/api_key/#hopsworks-api-key","text":"In order for the Databricks cluster to be able to communicate with the Hopsworks Feature Store, the clients running on Databricks need to be able to access a Hopsworks API key.","title":"Hopsworks API key"},{"location":"user_guides/integrations/databricks/api_key/#generate-an-api-key","text":"For instructions on how to generate an API key follow this user guide . For the Databricks integration to work make sure you add the following scopes to your API key: featurestore project job kafka","title":"Generate an API key"},{"location":"user_guides/integrations/databricks/api_key/#quickstart-api-key-argument","text":"API key as Argument To get started quickly, without saving the Hopsworks API in a secret storage, you can simply supply it as an argument when instantiating a connection: import hsfs conn = hsfs . connection ( host = 'my_instance' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'my_project' , # Name of your Hopsworks Feature Store project api_key_value = 'apikey' , # The API key to authenticate with Hopsworks hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store","title":"Quickstart API key Argument"},{"location":"user_guides/integrations/databricks/api_key/#store-the-api-key","text":"","title":"Store the API key"},{"location":"user_guides/integrations/databricks/api_key/#aws","text":"","title":"AWS"},{"location":"user_guides/integrations/databricks/api_key/#step-1-create-an-instance-profile-to-attach-to-your-databricks-clusters","text":"Go to the AWS IAM choose Roles and click on Create Role . Select AWS Service as the type of trusted entity and EC2 as the use case as shown below: Create an instance profile Click on Next: Permissions , Next:Tags , and then Next: Review . Name the instance profile role and then click Create role .","title":"Step 1: Create an instance profile to attach to your Databricks clusters"},{"location":"user_guides/integrations/databricks/api_key/#step-2-storing-the-api-key","text":"Option 1: Using the AWS Systems Manager Parameter Store In the AWS Management Console, ensure that your active region is the region you use for Databricks. Go to the AWS Systems Manager choose Parameter Store and select Create Parameter . As name enter /hopsworks/role/[MY_DATABRICKS_ROLE]/type/api-key replacing [MY_DATABRICKS_ROLE] with the name of the AWS role you have created in Step 1 . Select Secure String as type and create the parameter. Storing the Feature Store API key in the Parameter Store Once the API Key is stored, you need to grant access to it from the AWS role that you have created in Step 1 . In the AWS Management Console, go to IAM , select Roles and then search for the role that you have created in Step 1 . Select Add inline policy . Choose Systems Manager as service, expand the Read access level and check GetParameter . Expand Resources and select Add ARN . Enter the region of the Systems Manager as well as the name of the parameter WITHOUT the leading slash e.g. hopsworks/role/[MY_DATABRICKS_ROLE]/type/api-key and click Add . Click on Review , give the policy a name und click on Create policy . Configuring the access policy for the Parameter Store Option 2: Using the AWS Secrets Manager In the AWS management console ensure that your active region is the region you use for Databricks. Go to the AWS Secrets Manager and select Store new secret . Select Other type of secrets and add api-key as the key and paste the API key created in the previous step as the value. Click next. Storing a Feature Store API key in the Secrets Manager Step 1 As secret name, enter hopsworks/role/[MY_DATABRICKS_ROLE] replacing [MY_DATABRICKS_ROLE] with the AWS role you have created in Step 1 . Select next twice and finally store the secret. Then click on the secret in the secrets list and take note of the Secret ARN . Storing a Feature Store API key in the Secrets Manager Step 2 Once the API Key is stored, you need to grant access to it from the AWS role that you have created in Step 1 . In the AWS Management Console, go to IAM , select Roles and then the role that that you have created in Step 1 . Select Add inline policy . Choose Secrets Manager as service, expand the Read access level and check GetSecretValue . Expand Resources and select Add ARN . Paste the ARN of the secret created in the previous step. Click on Review , give the policy a name und click on Create policy . Configuring the access policy for the Secrets Manager","title":"Step 2: Storing the API Key"},{"location":"user_guides/integrations/databricks/api_key/#step-3-allow-databricks-to-use-the-aws-role-created-in-step-1","text":"First you need to get the AWS role used by Databricks for deployments as described in this step . Once you get the role name, go to AWS IAM , search for the role, and click on it. Then, select the Permissions tab, click on Add inline policy , select the JSON tab, and paste the following snippet. Replace [ACCOUNT_ID] with your AWS account id, and [MY_DATABRICKS_ROLE] with the AWS role name created in Step 1 . { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"PassRole\" , \"Effect\" : \"Allow\" , \"Action\" : \"iam:PassRole\" , \"Resource\" : \"arn:aws:iam::[ACCOUNT_ID]:role/[MY_DATABRICKS_ROLE]\" } ] } Click Review Policy , name the policy, and click Create Policy . Then, go to your Databricks workspace and follow this step to add the instance profile to your workspace. Finally, when launching Databricks clusters, select Advanced settings and choose the instance profile you have just added.","title":"Step 3: Allow Databricks to use the AWS role created in Step 1"},{"location":"user_guides/integrations/databricks/api_key/#azure","text":"On Azure we currently do not support storing the API key in a secret storage. Instead just store the API key in a file in your Databricks workspace so you can access it when connecting to the Feature Store.","title":"Azure"},{"location":"user_guides/integrations/databricks/api_key/#next-steps","text":"Continue with the configuration guide to finalize the configuration of the Databricks Cluster to communicate with the Hopsworks Feature Store.","title":"Next Steps"},{"location":"user_guides/integrations/databricks/configuration/","text":"Databricks Integration # Users can configure their Databricks clusters to write the results of feature engineering pipelines in the Hopsworks Feature Store using HSFS. Configuring a Databricks cluster can be done from the Hopsworks Feature Store UI. This guide explains each step. Prerequisites # In order to be able to configure a Databricks cluster to use the Feature Store of your Hopsworks instance, it is necessary to ensure networking is setup correctly between the instances and that the Databricks cluster has access to the Hopsworks API key to perform requests with HSFS from Databricks to Hopsworks. Networking # If you haven't done so already, follow the networking guides for either AWS or Azure for instructions on how to configure networking properly between Databricks' VPC (or Virtual Network on Azure) and the managed.hopsworks.ai VPC/VNet. Hopsworks API key # In order for the Feature Store API to be able to communicate with the user's Hopsworks instance, the client library (HSFS) needs to have access to a previously generated API key from Hopsworks. For ways to setup and store the Hopsworks API key, please refer to the API key guide for Databricks . Databricks API key # Hopsworks uses the Databricks REST APIs to communicate with the Databricks instance and configure clusters on behalf of users. To achieve that, the first step is to register an instance and a valid API key in Hopsworks. Users can get a valid Databricks API key by following the Databricks Documentation Cluster access control If users have enabled Databricks Cluster access control , it is important that the users running the cluster configuration (i.e. the user generating the API key) has Can Manage privileges on the cluster they are trying to configure. Register a new Databricks Instance # To register a new Databricks instance, first navigate to Project settings , which can be found on the left-hand side of a Project's landing page, then select the Integrations tab. Hopsworks's Integrations page Registering a Databricks instance requires adding the instance address and the Databricks API key. The instance name corresponds to the address of the Databricks instance and should be in the format [UUID].cloud.databricks.com (or adb-[UUID].19.azuredatabricks.net for Databricks on Azure), essentially the same web address used to reach the Databricks instance from the browser. Register a Databricks Instance along with a Databricks API key The API key will be stored in the Hopsworks secret store for the user and will be available only for that user. If multiple users need to configure Databricks clusters, each has to generate an API key and register an instance. The Databricks instance registration does not have a project scope, meaning that once registered, the user can configure clusters for all projects they are part of. Databricks Cluster # A cluster needs to exist before users can configure it using the Hopsworks UI. The cluster can be in any state prior to the configuration. Runtime limitation Currently Runtime 12.2 LTS is suggested to be able to use the full suite of Hopsworks Feature Store capabilities. Configure a cluster # Clusters are configured for a project user, which, in Hopsworks terms, means a user operating within the scope of a project. To configure a cluster, click on the Configure button. By default the cluster will be configured for the user making the request. If the user doesn't have Can Manage privilege on the cluster, they can ask a project Data Owner to configure it for them. Hopsworks Data Owners are allowed to configure clusters for other project users, as long as they have the required Databricks privileges. Configure a Databricks Cluster from Hopsworks During the cluster configuration the following steps will be taken: Upload an archive to DBFS containing the necessary Jars for HSFS and HopsFS to be able to read and write from the Hopsworks Feature Store Add an initScript to configure the Jars when the cluster is started Install hsfs python library Configure the necessary Spark properties to authenticate and communicate with the Feature Store When a cluster is configured for a specific project user, all the operations with the Hopsworks Feature Store will be executed as that project user. If another user needs to re-use the same cluster, the cluster can be reconfigured by following the same steps above. Connecting to the Feature Store # At the end of the configuration, Hopsworks will start the cluster. Once the cluster is running users can establish a connection to the Hopsworks Feature Store from Databricks: API key on Azure Please note, for Azure it is necessary to store the Hopsworks API key locally on the cluster as a file. As we currently do not support storing the API key on an Azure Secret Management Service as we do for AWS. Consult the API key guide for Azure , for more information. AWS Azure import hsfs conn = hsfs . connection ( 'my_instance' , # DNS of your Feature Store instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks Feature Store project secrets_store = 'secretsmanager' , # Either parameterstore or secretsmanager hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store import hsfs conn = hsfs . connection ( 'my_instance' , # DNS of your Feature Store instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks Feature Store project secrets_store = 'local' , api_key_file = \"featurestore.key\" , # For Azure, store the API key locally secrets_store = \"local\" , hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store Next Steps # For more information about how to connect, see the Connection API reference. Or continue with the Data Source guide to import your own data to the Feature Store.","title":"Configuration"},{"location":"user_guides/integrations/databricks/configuration/#databricks-integration","text":"Users can configure their Databricks clusters to write the results of feature engineering pipelines in the Hopsworks Feature Store using HSFS. Configuring a Databricks cluster can be done from the Hopsworks Feature Store UI. This guide explains each step.","title":"Databricks Integration"},{"location":"user_guides/integrations/databricks/configuration/#prerequisites","text":"In order to be able to configure a Databricks cluster to use the Feature Store of your Hopsworks instance, it is necessary to ensure networking is setup correctly between the instances and that the Databricks cluster has access to the Hopsworks API key to perform requests with HSFS from Databricks to Hopsworks.","title":"Prerequisites"},{"location":"user_guides/integrations/databricks/configuration/#networking","text":"If you haven't done so already, follow the networking guides for either AWS or Azure for instructions on how to configure networking properly between Databricks' VPC (or Virtual Network on Azure) and the managed.hopsworks.ai VPC/VNet.","title":"Networking"},{"location":"user_guides/integrations/databricks/configuration/#hopsworks-api-key","text":"In order for the Feature Store API to be able to communicate with the user's Hopsworks instance, the client library (HSFS) needs to have access to a previously generated API key from Hopsworks. For ways to setup and store the Hopsworks API key, please refer to the API key guide for Databricks .","title":"Hopsworks API key"},{"location":"user_guides/integrations/databricks/configuration/#databricks-api-key","text":"Hopsworks uses the Databricks REST APIs to communicate with the Databricks instance and configure clusters on behalf of users. To achieve that, the first step is to register an instance and a valid API key in Hopsworks. Users can get a valid Databricks API key by following the Databricks Documentation Cluster access control If users have enabled Databricks Cluster access control , it is important that the users running the cluster configuration (i.e. the user generating the API key) has Can Manage privileges on the cluster they are trying to configure.","title":"Databricks API key"},{"location":"user_guides/integrations/databricks/configuration/#register-a-new-databricks-instance","text":"To register a new Databricks instance, first navigate to Project settings , which can be found on the left-hand side of a Project's landing page, then select the Integrations tab. Hopsworks's Integrations page Registering a Databricks instance requires adding the instance address and the Databricks API key. The instance name corresponds to the address of the Databricks instance and should be in the format [UUID].cloud.databricks.com (or adb-[UUID].19.azuredatabricks.net for Databricks on Azure), essentially the same web address used to reach the Databricks instance from the browser. Register a Databricks Instance along with a Databricks API key The API key will be stored in the Hopsworks secret store for the user and will be available only for that user. If multiple users need to configure Databricks clusters, each has to generate an API key and register an instance. The Databricks instance registration does not have a project scope, meaning that once registered, the user can configure clusters for all projects they are part of.","title":"Register a new Databricks Instance"},{"location":"user_guides/integrations/databricks/configuration/#databricks-cluster","text":"A cluster needs to exist before users can configure it using the Hopsworks UI. The cluster can be in any state prior to the configuration. Runtime limitation Currently Runtime 12.2 LTS is suggested to be able to use the full suite of Hopsworks Feature Store capabilities.","title":"Databricks Cluster"},{"location":"user_guides/integrations/databricks/configuration/#configure-a-cluster","text":"Clusters are configured for a project user, which, in Hopsworks terms, means a user operating within the scope of a project. To configure a cluster, click on the Configure button. By default the cluster will be configured for the user making the request. If the user doesn't have Can Manage privilege on the cluster, they can ask a project Data Owner to configure it for them. Hopsworks Data Owners are allowed to configure clusters for other project users, as long as they have the required Databricks privileges. Configure a Databricks Cluster from Hopsworks During the cluster configuration the following steps will be taken: Upload an archive to DBFS containing the necessary Jars for HSFS and HopsFS to be able to read and write from the Hopsworks Feature Store Add an initScript to configure the Jars when the cluster is started Install hsfs python library Configure the necessary Spark properties to authenticate and communicate with the Feature Store When a cluster is configured for a specific project user, all the operations with the Hopsworks Feature Store will be executed as that project user. If another user needs to re-use the same cluster, the cluster can be reconfigured by following the same steps above.","title":"Configure a cluster"},{"location":"user_guides/integrations/databricks/configuration/#connecting-to-the-feature-store","text":"At the end of the configuration, Hopsworks will start the cluster. Once the cluster is running users can establish a connection to the Hopsworks Feature Store from Databricks: API key on Azure Please note, for Azure it is necessary to store the Hopsworks API key locally on the cluster as a file. As we currently do not support storing the API key on an Azure Secret Management Service as we do for AWS. Consult the API key guide for Azure , for more information. AWS Azure import hsfs conn = hsfs . connection ( 'my_instance' , # DNS of your Feature Store instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks Feature Store project secrets_store = 'secretsmanager' , # Either parameterstore or secretsmanager hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store import hsfs conn = hsfs . connection ( 'my_instance' , # DNS of your Feature Store instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks Feature Store project secrets_store = 'local' , api_key_file = \"featurestore.key\" , # For Azure, store the API key locally secrets_store = \"local\" , hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store","title":"Connecting to the Feature Store"},{"location":"user_guides/integrations/databricks/configuration/#next-steps","text":"For more information about how to connect, see the Connection API reference. Or continue with the Data Source guide to import your own data to the Feature Store.","title":"Next Steps"},{"location":"user_guides/integrations/databricks/networking/","text":"Networking # In order for Spark to communicate with the Feature Store from Databricks, networking needs to be set up correctly. This includes deploying the Hopsworks Instance to either the same VPC or enable VPC/VNet peering between the VPC/VNet of the Databricks Cluster and the Hopsworks Cluster. AWS # Step 1: Ensure network connectivity # The DataFrame API needs to be able to connect directly to the IP on which the Feature Store is listening. This means that if you deploy the Feature Store on AWS you will either need to deploy the Feature Store in the same VPC as your Databricks cluster or to set up VPC Peering between your Databricks VPC and the Feature Store VPC. Option 1: Deploy the Feature Store in the Databricks VPC When you deploy the Feature Store Hopsworks instance, select the Databricks VPC and Availability Zone as the VPC and Availability Zone of your Feature Store cluster. Identify your Databricks VPC by searching for VPCs containing Databricks in their name in your Databricks AWS region in the AWS Management Console: Identify the Databricks VPC Hopsworks installer If you are performing an installation using the Hopsworks installer script , ensure that the virtual machines you install Hopsworks on are deployed in the EMR VPC. managed.hopsworks.ai If you are working on managed.hopsworks.ai , you can directly deploy the Hopsworks instance to the Databricks VPC, by simply selecting it at the VPC selection step during cluster creation . Option 2: Set up VPC peering Follow the guide VPC Peering to set up VPC peering between the Feature Store cluster and Databricks. Get your Feature Store VPC ID and CIDR by searching for thr Feature Store VPC in the AWS Management Console: managed.hopsworks.ai On managed.hopsworks.ai , the VPC is shown in the cluster details. Identify the Feature Store VPC Step 2: Configure the Security Group # The Feature Store Security Group needs to be configured to allow traffic from your Databricks clusters to be able to connect to the Feature Store. managed.hopsworks.ai If you deployed your Hopsworks Feature Store with managed.hopsworks.ai , you only need to enable outside access of the Feature Store, Online Feature Store, and Kafka services . Open your feature store instance under EC2 in the AWS Management Console and ensure that ports 443 , 3306 , 9083 , 9085 , 8020 , 50010 , and 9092 are reachable from the Databricks Security Group: Hopsworks Feature Store Security Group Connectivity from the Databricks Security Group can be allowed by opening the Security Group, adding a port to the Inbound rules and searching for dbe-worker in the source field. Selecting any of the dbe-worker Security Groups will be sufficient: Hopsworks Feature Store Security Group details Azure # Step 1: Set up VNet peering between Hopsworks and Databricks # VNet peering between the Hopsworks and the Databricks virtual network is required to be able to connect to the Feature Store from Databricks. In the Azure portal, go to Azure Databricks and go to Virtual Network Peering : Azure Databricks Select Add Peering : Add peering Name the peering and select the virtual network used by your Hopsworks cluster. The virtual network is shown in the cluster details on managed.hopsworks.ai (see the next picture). Ensure to press the copy button on the bottom of the page and save the value somewhere. Press Add and create the peering: Configure peering The virtual network used by your cluster is shown under Details : Check the Hopsworks virtual network The peering connection should now be listed as initiated: Peering connection initiated On the Azure portal, go to Virtual networks and search for the virtual network used by your Hopsworks cluster: Virtual networks Open the network and select Peerings : Select peerings Choose to add a peering connection: Add a peering connection Name the peering connection and select I know my resource ID . Paste the string copied when creating the peering from Databricks Azure. If you haven't copied that string, then manually select the virtual network used by Databricks and press OK to create the peering: Configure peering The peering should now be Updating : Cloud account settings Wait for the peering to show up as Connected . There should now be bi-directional network connectivity between the Feature Store and Databricks: Cloud account settings Step 2: Configure the Network Security Group # The virtual network peering will allow full access between the Hopsworks virtual network and the Databricks virtual network by default. However, if you have a different setup, ensure that the Network Security Group of the Feature Store is configured to allow traffic from your Databricks clusters. Ensure that ports 443 , 9083 , 9085 , 8020 , 50010 , and 9092 are reachable from the Databricks cluster Network Security Group . managed.hopsworks.ai If you deployed your Hopsworks Feature Store instance with managed.hopsworks.ai , it suffices to enable outside access of the Feature Store, Online Feature Store, and Kafka services . Next Steps # Continue with the Hopsworks API key guide to setup access to a Hopsworks API key from the Databricks Cluster, in order to be able to use the Hopsworks Feature Store.","title":"Networking"},{"location":"user_guides/integrations/databricks/networking/#networking","text":"In order for Spark to communicate with the Feature Store from Databricks, networking needs to be set up correctly. This includes deploying the Hopsworks Instance to either the same VPC or enable VPC/VNet peering between the VPC/VNet of the Databricks Cluster and the Hopsworks Cluster.","title":"Networking"},{"location":"user_guides/integrations/databricks/networking/#aws","text":"","title":"AWS"},{"location":"user_guides/integrations/databricks/networking/#step-1-ensure-network-connectivity","text":"The DataFrame API needs to be able to connect directly to the IP on which the Feature Store is listening. This means that if you deploy the Feature Store on AWS you will either need to deploy the Feature Store in the same VPC as your Databricks cluster or to set up VPC Peering between your Databricks VPC and the Feature Store VPC. Option 1: Deploy the Feature Store in the Databricks VPC When you deploy the Feature Store Hopsworks instance, select the Databricks VPC and Availability Zone as the VPC and Availability Zone of your Feature Store cluster. Identify your Databricks VPC by searching for VPCs containing Databricks in their name in your Databricks AWS region in the AWS Management Console: Identify the Databricks VPC Hopsworks installer If you are performing an installation using the Hopsworks installer script , ensure that the virtual machines you install Hopsworks on are deployed in the EMR VPC. managed.hopsworks.ai If you are working on managed.hopsworks.ai , you can directly deploy the Hopsworks instance to the Databricks VPC, by simply selecting it at the VPC selection step during cluster creation . Option 2: Set up VPC peering Follow the guide VPC Peering to set up VPC peering between the Feature Store cluster and Databricks. Get your Feature Store VPC ID and CIDR by searching for thr Feature Store VPC in the AWS Management Console: managed.hopsworks.ai On managed.hopsworks.ai , the VPC is shown in the cluster details. Identify the Feature Store VPC","title":"Step 1: Ensure network connectivity"},{"location":"user_guides/integrations/databricks/networking/#step-2-configure-the-security-group","text":"The Feature Store Security Group needs to be configured to allow traffic from your Databricks clusters to be able to connect to the Feature Store. managed.hopsworks.ai If you deployed your Hopsworks Feature Store with managed.hopsworks.ai , you only need to enable outside access of the Feature Store, Online Feature Store, and Kafka services . Open your feature store instance under EC2 in the AWS Management Console and ensure that ports 443 , 3306 , 9083 , 9085 , 8020 , 50010 , and 9092 are reachable from the Databricks Security Group: Hopsworks Feature Store Security Group Connectivity from the Databricks Security Group can be allowed by opening the Security Group, adding a port to the Inbound rules and searching for dbe-worker in the source field. Selecting any of the dbe-worker Security Groups will be sufficient: Hopsworks Feature Store Security Group details","title":"Step 2: Configure the Security Group"},{"location":"user_guides/integrations/databricks/networking/#azure","text":"","title":"Azure"},{"location":"user_guides/integrations/databricks/networking/#step-1-set-up-vnet-peering-between-hopsworks-and-databricks","text":"VNet peering between the Hopsworks and the Databricks virtual network is required to be able to connect to the Feature Store from Databricks. In the Azure portal, go to Azure Databricks and go to Virtual Network Peering : Azure Databricks Select Add Peering : Add peering Name the peering and select the virtual network used by your Hopsworks cluster. The virtual network is shown in the cluster details on managed.hopsworks.ai (see the next picture). Ensure to press the copy button on the bottom of the page and save the value somewhere. Press Add and create the peering: Configure peering The virtual network used by your cluster is shown under Details : Check the Hopsworks virtual network The peering connection should now be listed as initiated: Peering connection initiated On the Azure portal, go to Virtual networks and search for the virtual network used by your Hopsworks cluster: Virtual networks Open the network and select Peerings : Select peerings Choose to add a peering connection: Add a peering connection Name the peering connection and select I know my resource ID . Paste the string copied when creating the peering from Databricks Azure. If you haven't copied that string, then manually select the virtual network used by Databricks and press OK to create the peering: Configure peering The peering should now be Updating : Cloud account settings Wait for the peering to show up as Connected . There should now be bi-directional network connectivity between the Feature Store and Databricks: Cloud account settings","title":"Step 1: Set up VNet peering between Hopsworks and Databricks"},{"location":"user_guides/integrations/databricks/networking/#step-2-configure-the-network-security-group","text":"The virtual network peering will allow full access between the Hopsworks virtual network and the Databricks virtual network by default. However, if you have a different setup, ensure that the Network Security Group of the Feature Store is configured to allow traffic from your Databricks clusters. Ensure that ports 443 , 9083 , 9085 , 8020 , 50010 , and 9092 are reachable from the Databricks cluster Network Security Group . managed.hopsworks.ai If you deployed your Hopsworks Feature Store instance with managed.hopsworks.ai , it suffices to enable outside access of the Feature Store, Online Feature Store, and Kafka services .","title":"Step 2: Configure the Network Security Group"},{"location":"user_guides/integrations/databricks/networking/#next-steps","text":"Continue with the Hopsworks API key guide to setup access to a Hopsworks API key from the Databricks Cluster, in order to be able to use the Hopsworks Feature Store.","title":"Next Steps"},{"location":"user_guides/integrations/emr/emr_configuration/","text":"Configure EMR for the Hopsworks Feature Store # To enable EMR to access the Hopsworks Feature Store, you need to set up a Hopsworks API key, add a bootstrap action and configurations to your EMR cluster. Info Ensure Networking is set up correctly before proceeding with this guide. Step 1: Set up a Hopsworks API key # For instructions on how to generate an API key follow this user guide . For the EMR integration to work correctly make sure you add the following scopes to your API key: featurestore project job kafka Store the API key in the AWS Secrets Manager # In the AWS management console ensure that your active region is the region you use for EMR. Go to the AWS Secrets Manager and select Store new secret . Select Other type of secrets and add api-key as the key and paste the API key created in the previous step as the value. Click next. Store a Hopsworks API key in the Secrets Manager As a secret name, enter hopsworks/featurestore . Select next twice and finally store the secret. Then click on the secret in the secrets list and take note of the Secret ARN . Name the secret Grant access to the secret to the EMR EC2 instance profile # Identify your EMR EC2 instance profile in the EMR cluster summary: Identify your EMR EC2 instance profile In the AWS Management Console, go to IAM , select Roles and then the EC2 instance profile used by your EMR cluster. Select Add inline policy . Choose Secrets Manager as a service, expand the Read access level and check GetSecretValue . Expand Resources and select Add ARN . Paste the ARN of the secret created in the previous step. Click on Review , give the policy a name und click on Create policy . Configure the access policy for the Secrets Manager Step 2: Configure your EMR cluster # Add the Hopsworks Feature Store configuration to your EMR cluster # In order for EMR to be able to talk to the Feature Store, you need to update the Hadoop and Spark configurations. Copy the configuration below and replace ip-XXX-XX-XX-XXX.XX-XXXX-X.compute.internal with the private DNS name of your Hopsworks master node. [ { \"Classification\" : \"hadoop-env\" , \"Properties\" : { }, \"Configurations\" : [ { \"Classification\" : \"export\" , \"Properties\" : { \"HADOOP_CLASSPATH\" : \"$HADOOP_CLASSPATH:/usr/lib/hopsworks/client/*\" }, \"Configurations\" : [ ] } ] }, { \"Classification\" : \"spark-defaults\" , \"Properties\" : { \"spark.hadoop.hops.ipc.server.ssl.enabled\" : true , \"spark.hadoop.fs.hopsfs.impl\" : \"io.hops.hopsfs.client.HopsFileSystem\" , \"spark.hadoop.client.rpc.ssl.enabled.protocol\" : \"TLSv1.2\" , \"spark.hadoop.hops.ssl.hostname.verifier\" : \"ALLOW_ALL\" , \"spark.hadoop.hops.rpc.socket.factory.class.default\" : \"io.hops.hadoop.shaded.org.apache.hadoop.net.HopsSSLSocketFactory\" , \"spark.hadoop.hops.ssl.keystores.passwd.name\" : \"/usr/lib/hopsworks/material_passwd\" , \"spark.hadoop.hops.ssl.keystore.name\" : \"/usr/lib/hopsworks/keyStore.jks\" , \"spark.hadoop.hops.ssl.trustore.name\" : \"/usr/lib/hopsworks/trustStore.jks\" , \"spark.serializer\" : \"org.apache.spark.serializer.KryoSerializer\" , \"spark.executor.extraClassPath\" : \"/usr/lib/hopsworks/client/*\" , \"spark.driver.extraClassPath\" : \"/usr/lib/hopsworks/client/*\" , \"spark.sql.hive.metastore.jars\" : \"path\" , \"spark.sql.hive.metastore.jars.path\" : \"/usr/lib/hopsworks/apache-hive-bin/lib/*\" , \"spark.hadoop.hive.metastore.uris\" : \"thrift://ip-XXX-XX-XX-XXX.XX-XXXX-X.compute.internal:9083\" } }, ] When you create your EMR cluster, add the configuration: Note Don't forget to replace ip-XXX-XX-XX-XXX.XX-XXXX-X.compute.internal with the private DNS name of your Hopsworks master node. Configure EMR to access the Feature Store Add the Bootstrap Action to your EMR cluster # EMR requires Hopsworks connectors to be able to communicate with the Hopsworks Feature Store. These connectors can be installed with the bootstrap action shown below. Copy the content into a file and name the file hopsworks.sh . Copy that file into any S3 bucket that is readable by your EMR clusters and take note of the S3 URI of that file e.g., s3://my-emr-init/hopsworks.sh . #!/bin/bash set -e if [ \" $# \" -ne 3 ] ; then echo \"Usage hopsworks.sh HOPSWORKS_API_KEY_SECRET, HOPSWORKS_HOST, PROJECT_NAME\" exit 1 fi SECRET_NAME = $1 HOST = $2 PROJECT = $3 API_KEY = $( aws secretsmanager get-secret-value --secret-id $SECRET_NAME | jq -r .SecretString | jq -r '.[\"api-key\"]' ) PROJECT_ID = $( curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/getProjectInfo/ $PROJECT | jq -r .projectId ) sudo yum -y install python3-devel.x86_64 || true sudo mkdir /usr/lib/hopsworks sudo chown hadoop:hadoop /usr/lib/hopsworks cd /usr/lib/hopsworks curl -o client.tar.gz -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /client tar -xvf client.tar.gz tar -xzf client/apache-hive-*-bin.tar.gz || true mv apache-hive-*-bin apache-hive-bin rm client.tar.gz rm client/apache-hive-*-bin.tar.gz curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .kStore | base64 -d > keyStore.jks curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .tStore | base64 -d > trustStore.jks echo -n $( curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .password ) > material_passwd chmod -R o-rwx /usr/lib/hopsworks sudo pip3 install --upgrade hsfs~ = X.X.0 Note Don't forget to replace X.X.0 with the major and minor version of your Hopsworks deployment. To find your Hopsworks version, enter any of your projects and go to the settings tab inside your project. Add the bootstrap actions when configuring your EMR cluster. Provide 3 arguments to the bootstrap action: The name of the API key secret e.g., hopsworks/featurestore , the public DNS name of your Hopsworks cluster, such as ad005770-33b5-11eb-b5a7-bfabd757769f.cloud.hopsworks.ai , and the name of your Hopsworks project, e.g. demo_fs_meb10179 . Set the bootstrap action for EMR Your EMR cluster will now be able to access your Hopsworks Feature Store. Next Steps # Use the Connection API to connect to the Hopsworks Feature Store. For more information about how to use the Feature Store, see the Quickstart Guide .","title":"Configure EMR for the Hopsworks Feature Store"},{"location":"user_guides/integrations/emr/emr_configuration/#configure-emr-for-the-hopsworks-feature-store","text":"To enable EMR to access the Hopsworks Feature Store, you need to set up a Hopsworks API key, add a bootstrap action and configurations to your EMR cluster. Info Ensure Networking is set up correctly before proceeding with this guide.","title":"Configure EMR for the Hopsworks Feature Store"},{"location":"user_guides/integrations/emr/emr_configuration/#step-1-set-up-a-hopsworks-api-key","text":"For instructions on how to generate an API key follow this user guide . For the EMR integration to work correctly make sure you add the following scopes to your API key: featurestore project job kafka","title":"Step 1: Set up a Hopsworks API key"},{"location":"user_guides/integrations/emr/emr_configuration/#store-the-api-key-in-the-aws-secrets-manager","text":"In the AWS management console ensure that your active region is the region you use for EMR. Go to the AWS Secrets Manager and select Store new secret . Select Other type of secrets and add api-key as the key and paste the API key created in the previous step as the value. Click next. Store a Hopsworks API key in the Secrets Manager As a secret name, enter hopsworks/featurestore . Select next twice and finally store the secret. Then click on the secret in the secrets list and take note of the Secret ARN . Name the secret","title":"Store the API key in the AWS Secrets Manager"},{"location":"user_guides/integrations/emr/emr_configuration/#grant-access-to-the-secret-to-the-emr-ec2-instance-profile","text":"Identify your EMR EC2 instance profile in the EMR cluster summary: Identify your EMR EC2 instance profile In the AWS Management Console, go to IAM , select Roles and then the EC2 instance profile used by your EMR cluster. Select Add inline policy . Choose Secrets Manager as a service, expand the Read access level and check GetSecretValue . Expand Resources and select Add ARN . Paste the ARN of the secret created in the previous step. Click on Review , give the policy a name und click on Create policy . Configure the access policy for the Secrets Manager","title":"Grant access to the secret to the EMR EC2 instance profile"},{"location":"user_guides/integrations/emr/emr_configuration/#step-2-configure-your-emr-cluster","text":"","title":"Step 2: Configure your EMR cluster"},{"location":"user_guides/integrations/emr/emr_configuration/#add-the-hopsworks-feature-store-configuration-to-your-emr-cluster","text":"In order for EMR to be able to talk to the Feature Store, you need to update the Hadoop and Spark configurations. Copy the configuration below and replace ip-XXX-XX-XX-XXX.XX-XXXX-X.compute.internal with the private DNS name of your Hopsworks master node. [ { \"Classification\" : \"hadoop-env\" , \"Properties\" : { }, \"Configurations\" : [ { \"Classification\" : \"export\" , \"Properties\" : { \"HADOOP_CLASSPATH\" : \"$HADOOP_CLASSPATH:/usr/lib/hopsworks/client/*\" }, \"Configurations\" : [ ] } ] }, { \"Classification\" : \"spark-defaults\" , \"Properties\" : { \"spark.hadoop.hops.ipc.server.ssl.enabled\" : true , \"spark.hadoop.fs.hopsfs.impl\" : \"io.hops.hopsfs.client.HopsFileSystem\" , \"spark.hadoop.client.rpc.ssl.enabled.protocol\" : \"TLSv1.2\" , \"spark.hadoop.hops.ssl.hostname.verifier\" : \"ALLOW_ALL\" , \"spark.hadoop.hops.rpc.socket.factory.class.default\" : \"io.hops.hadoop.shaded.org.apache.hadoop.net.HopsSSLSocketFactory\" , \"spark.hadoop.hops.ssl.keystores.passwd.name\" : \"/usr/lib/hopsworks/material_passwd\" , \"spark.hadoop.hops.ssl.keystore.name\" : \"/usr/lib/hopsworks/keyStore.jks\" , \"spark.hadoop.hops.ssl.trustore.name\" : \"/usr/lib/hopsworks/trustStore.jks\" , \"spark.serializer\" : \"org.apache.spark.serializer.KryoSerializer\" , \"spark.executor.extraClassPath\" : \"/usr/lib/hopsworks/client/*\" , \"spark.driver.extraClassPath\" : \"/usr/lib/hopsworks/client/*\" , \"spark.sql.hive.metastore.jars\" : \"path\" , \"spark.sql.hive.metastore.jars.path\" : \"/usr/lib/hopsworks/apache-hive-bin/lib/*\" , \"spark.hadoop.hive.metastore.uris\" : \"thrift://ip-XXX-XX-XX-XXX.XX-XXXX-X.compute.internal:9083\" } }, ] When you create your EMR cluster, add the configuration: Note Don't forget to replace ip-XXX-XX-XX-XXX.XX-XXXX-X.compute.internal with the private DNS name of your Hopsworks master node. Configure EMR to access the Feature Store","title":"Add the Hopsworks Feature Store configuration to your EMR cluster"},{"location":"user_guides/integrations/emr/emr_configuration/#add-the-bootstrap-action-to-your-emr-cluster","text":"EMR requires Hopsworks connectors to be able to communicate with the Hopsworks Feature Store. These connectors can be installed with the bootstrap action shown below. Copy the content into a file and name the file hopsworks.sh . Copy that file into any S3 bucket that is readable by your EMR clusters and take note of the S3 URI of that file e.g., s3://my-emr-init/hopsworks.sh . #!/bin/bash set -e if [ \" $# \" -ne 3 ] ; then echo \"Usage hopsworks.sh HOPSWORKS_API_KEY_SECRET, HOPSWORKS_HOST, PROJECT_NAME\" exit 1 fi SECRET_NAME = $1 HOST = $2 PROJECT = $3 API_KEY = $( aws secretsmanager get-secret-value --secret-id $SECRET_NAME | jq -r .SecretString | jq -r '.[\"api-key\"]' ) PROJECT_ID = $( curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/getProjectInfo/ $PROJECT | jq -r .projectId ) sudo yum -y install python3-devel.x86_64 || true sudo mkdir /usr/lib/hopsworks sudo chown hadoop:hadoop /usr/lib/hopsworks cd /usr/lib/hopsworks curl -o client.tar.gz -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /client tar -xvf client.tar.gz tar -xzf client/apache-hive-*-bin.tar.gz || true mv apache-hive-*-bin apache-hive-bin rm client.tar.gz rm client/apache-hive-*-bin.tar.gz curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .kStore | base64 -d > keyStore.jks curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .tStore | base64 -d > trustStore.jks echo -n $( curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .password ) > material_passwd chmod -R o-rwx /usr/lib/hopsworks sudo pip3 install --upgrade hsfs~ = X.X.0 Note Don't forget to replace X.X.0 with the major and minor version of your Hopsworks deployment. To find your Hopsworks version, enter any of your projects and go to the settings tab inside your project. Add the bootstrap actions when configuring your EMR cluster. Provide 3 arguments to the bootstrap action: The name of the API key secret e.g., hopsworks/featurestore , the public DNS name of your Hopsworks cluster, such as ad005770-33b5-11eb-b5a7-bfabd757769f.cloud.hopsworks.ai , and the name of your Hopsworks project, e.g. demo_fs_meb10179 . Set the bootstrap action for EMR Your EMR cluster will now be able to access your Hopsworks Feature Store.","title":"Add the Bootstrap Action to your EMR cluster"},{"location":"user_guides/integrations/emr/emr_configuration/#next-steps","text":"Use the Connection API to connect to the Hopsworks Feature Store. For more information about how to use the Feature Store, see the Quickstart Guide .","title":"Next Steps"},{"location":"user_guides/integrations/emr/networking/","text":"Networking # In order for Spark to communicate with the Hopsworks Feature Store from EMR, networking needs to be set up correctly. This includes deploying the Hopsworks Feature Store to either the same VPC or enable VPC peering between the VPC of the EMR cluster and the Hopsworks Feature Store. Step 1: Ensure network connectivity # The DataFrame API needs to be able to connect directly to the IP on which the Feature Store is listening. This means that if you deploy the Feature Store on AWS you will either need to deploy the Feature Store in the same VPC as your EMR cluster or to set up VPC Peering between your EMR VPC and the Feature Store VPC. Option 1: Deploy the Feature Store in the EMR VPC When deploying the Hopsworks Feature Store, select the EMR VPC and Availability Zone as the VPC and Availability Zone of your Feature Store. Identify your EMR VPC in the Summary of your EMR cluster: Identify the EMR VPC Identify the EMR VPC Hopsworks installer If you are performing an installation using the Hopsworks installer script , ensure that the virtual machines you install Hopsworks on are deployed in the EMR VPC. managed.hopsworks.ai If you are on managed.hopsworks.ai , you can directly deploy Hopsworks to the EMR VPC, by simply selecting it at the VPC selection step during cluster creation . Option 2: Set up VPC peering Follow the guide VPC Peering to set up VPC peering between the Feature Store and EMR. Get your Feature Store VPC ID and CIDR by searching for the Feature Store VPC in the AWS Management Console: managed.hopsworks.ai On managed.hopsworks.ai , the VPC is shown in the cluster details. Identify the Feature Store VPC Step 2: Configure the Security Group # The Feature Store Security Group needs to be configured to allow traffic from your EMR clusters to be able to connect to the Feature Store. managed.hopsworks.ai If you deployed your Hopsworks Feature Store with managed.hopsworks.ai , you only need to enable outside access of the Feature Store services . Open your feature store instance under EC2 in the AWS Management Console and ensure that ports 443 , 3306 , 9083 , 9085 , 8020 and 30010 (443,3306,8020,30010,9083,9085) are reachable from the EMR Security Group: Hopsworks Feature Store Security Group Connectivity from the EMR Security Group can be allowed by opening the Security Group, adding a port to the Inbound rules and setting the EMR master and core security group as source: Hopsworks Feature Store Security Group details You can find your EMR security groups in the EMR cluster summary: EMR Security Groups Next Steps # Continue with the Configure EMR for the Hopsworks Feature Store , in order to be able to use the Hopsworks Feature Store.","title":"Networking"},{"location":"user_guides/integrations/emr/networking/#networking","text":"In order for Spark to communicate with the Hopsworks Feature Store from EMR, networking needs to be set up correctly. This includes deploying the Hopsworks Feature Store to either the same VPC or enable VPC peering between the VPC of the EMR cluster and the Hopsworks Feature Store.","title":"Networking"},{"location":"user_guides/integrations/emr/networking/#step-1-ensure-network-connectivity","text":"The DataFrame API needs to be able to connect directly to the IP on which the Feature Store is listening. This means that if you deploy the Feature Store on AWS you will either need to deploy the Feature Store in the same VPC as your EMR cluster or to set up VPC Peering between your EMR VPC and the Feature Store VPC. Option 1: Deploy the Feature Store in the EMR VPC When deploying the Hopsworks Feature Store, select the EMR VPC and Availability Zone as the VPC and Availability Zone of your Feature Store. Identify your EMR VPC in the Summary of your EMR cluster: Identify the EMR VPC Identify the EMR VPC Hopsworks installer If you are performing an installation using the Hopsworks installer script , ensure that the virtual machines you install Hopsworks on are deployed in the EMR VPC. managed.hopsworks.ai If you are on managed.hopsworks.ai , you can directly deploy Hopsworks to the EMR VPC, by simply selecting it at the VPC selection step during cluster creation . Option 2: Set up VPC peering Follow the guide VPC Peering to set up VPC peering between the Feature Store and EMR. Get your Feature Store VPC ID and CIDR by searching for the Feature Store VPC in the AWS Management Console: managed.hopsworks.ai On managed.hopsworks.ai , the VPC is shown in the cluster details. Identify the Feature Store VPC","title":"Step 1: Ensure network connectivity"},{"location":"user_guides/integrations/emr/networking/#step-2-configure-the-security-group","text":"The Feature Store Security Group needs to be configured to allow traffic from your EMR clusters to be able to connect to the Feature Store. managed.hopsworks.ai If you deployed your Hopsworks Feature Store with managed.hopsworks.ai , you only need to enable outside access of the Feature Store services . Open your feature store instance under EC2 in the AWS Management Console and ensure that ports 443 , 3306 , 9083 , 9085 , 8020 and 30010 (443,3306,8020,30010,9083,9085) are reachable from the EMR Security Group: Hopsworks Feature Store Security Group Connectivity from the EMR Security Group can be allowed by opening the Security Group, adding a port to the Inbound rules and setting the EMR master and core security group as source: Hopsworks Feature Store Security Group details You can find your EMR security groups in the EMR cluster summary: EMR Security Groups","title":"Step 2: Configure the Security Group"},{"location":"user_guides/integrations/emr/networking/#next-steps","text":"Continue with the Configure EMR for the Hopsworks Feature Store , in order to be able to use the Hopsworks Feature Store.","title":"Next Steps"},{"location":"user_guides/migration/30_migration/","text":"3.0 Migration Guide # Breaking Changes # Feature View # Feature View is a new core abstraction introduced in version 3.0. The Feature View extends and replaces the old Training Dataset. Feature views are now the gateway for users to access feature data from the feature store. Pre-3.0 3.0 td = fs . create_training_dataset () First create the feature view using a Query object: fv = fs . create_feature_view () You can then create training data from the feature view by writing it as new files with .create_ methods: td_df = fv . create_training_data () fv . create_train_test_split () fv . create_train_validation_test_split () Or you can directly create the training data in memory by below methods: fv . training_data () fv . train_test_split () fv . train_validation_test_split () This means that the process for generating training data changes slightly and training data is grouped by feature views. This has the following advantages: It will be easier to create new versions of training data for re-training of models in the future. You do not have to keep references to training data in your model serving code, instead you use the Feature View object. The feature view offers the same interface for retrieving batch data for batch scoring, so you don\u2019t have to execute SQL against the feature store explicitly anymore. A feature view uses the Query abstraction to define the schema of the view, and therefore, the Query is a mandatory argument, this effectively removes the possibility to create a training dataset directly from a Spark Dataframe. Required changes # After the upgrade all existing training datasets will be located under a Feature View with the same name. The versions of the training datasets stay untouched. You can use feature_view.get_training_data to get back existing training datasets. Except that training datasets created directly from Spark Dataframe are not migrated to feature view. In this case, you can use the old APIs for retrieving these training datasets, so that your existing training pipelines are still functional. However, if you have any pipelines, that automatically create training datasets, you will have to adjust these to the above workflow. fs.create_training_dataset() has been removed. To learn more about the new Feature View, have a look at the dedicated concept and guide section in the documentation. Deequ-based Data Validation in favour of Great Expectations # Unfortunately, the Deequ data validation library is no longer actively maintained which makes it impossible for us to maintain the functionality within Hopsworks. Therefore, we are dropping the entire support for Deequ as validation engine in Hopsworks 3.0 in favour of Great Expectations (GE). This has the following advantages: Great Expectations has become the defacto standard for data validation within the community. Hopsworks is fully compatible with GE native objects, that means you can bring your existing expectation suites without the need for rewriting them. GE is both available for Spark and for Pandas Dataframes, whereas Deequ was only supporting Spark. Required changes # All APIs regarding data validation have been redesigned to accomodate the functionality of GE. This means that you will have to redesign your previous expectations in the form of GE expectation suites that you can attach to Feature Groups. Please refer to the data validation guide for a full specification of the functionality. Limitations # GE is a Python library and therefore we can support synchronous data validation only in Python and PySpark kernels and not on Java/Scala Spark kernels. However, you have the possibility to launch a job asynchronously after writing with Java/Scala in order to perform data validation. Deprecated Features # These changes or new features introduce changes in APIs which might break your pipelines in the future. We try to keep old APIs around until the next major release in order to give you some time to adapt your pipelines, however, this is not always possible, and these methods might be removed in any upcoming release, so we recommend addressing these changes as soon as possible. For this reason, we list some of the changes as breaking change, even though they are still backwards compatible. On-Demand Feature Groups are now called External Feature Groups # Most data engineers but also many data scientists have a background where they at least partially where exposed to database terminology. Therefore, we decided to rename On-Demand Feature Groups to simply External Feature Groups. We think this makes the abstraction clearer, as practitioners are usually familiar with the concept of Extern Tables in a database. This lead to a change in HSFS APIs: Pre-3.0 3.0 fs . create_on_demand_feature_group () fs . get_on_demand_feature_group () fs . get_on_demand_feature_groups () fs . create_external_feature_group () fs . get_external_feature_group () fs . get_external_feature_groups () Note, pre-3.0 methods are marked as deprecated and still available in the library for backwards compatibility. Streaming API for writing becomes the Python Default # Hopsworks provides three write APIs to the Feature Store to accommodate the different use cases: Batch Write: This was the default mode prior to version 3.0. It involves writing a DataFrame in batch either to the offline feature store, or the online one, or both. This mode is still the default when you are writing Spark DataFrames on Hopsworks. External connectors: This mode allows users to mount external tables existing in DataWarehouses like Snowflake, Redshift and BigQuery as feature groups in Hopsworks. In this case the data is not moved and remains on the external data storage. Stream Write: This mode was introduced in version 2.0 and expanded in version 3.0. This mode has a \"Kappa-style\" architecture, where the DataFrame gets streamed into a Kafka topic and, as explained later in the post, the data is picked up from Kafka by Hopsworks and written into the desired stores. In Hopsworks 3.0 this is the default mode for Python clients. With 3.0 the stream API becomes the default for Feature Groups created from pure Python environments with Pandas Dataframes. This has the following advantages: Reduced write amplification: Instead of uploading data to Hopsworks and subsequently starting a Spark job to upsert the data on offline storage and writing it to Kafka for the online storage upsert, the data is directly written to Kafka and from there it\u2019s being upserted directly to offline and/or online. Fresher features: Since new data gets written to Kafka directly without prior upload, the data ends up in the online feature store with subsecond latency, which is a massive improvement given it is written from Python without any Streaming framework. Batching of offline upserts: You can control now yourself how often the Spark application that performs the upsert on the offline feature store is running. Either you run it synchronously with every new Dataframe ingestion, or you batch multiple Dataframes by launching the job less regularly. Required changes # Your existing feature groups will not be affected by this change, that means all existing feature groups will continue to use the old upload path for ingestion. However, we strongly recommend creating new versions of your existing feature groups that use ingest to using Python, in order to leverage the above advantages. Built-in transformation functions don\u2019t have to be registered explicitly for every project # In Hopsworks 2.5 users had to register the built-in transformation functions (min-max scaler, standard scaler, label encoder and robust scaler) explicitly for every project by calling fs.register_builtin_transformation_functions() . This is no longer necessary, as all new projects will have the functions registered by default. Hive installation extra deprecated in favour of Python extra # In the past when using HSFS in pure Python environments without PySpark, users had to install the hive extra when installing the PyPi package. This extra got deprecated and users now have to install an extra called python to reflect the environment: Pre-3.0 3.0 pip install hsfs [ hive ] pip install hsfs [ python ] More restrictive feature types # With Hopsworks 3.0 we made feature types more strict and therefore made ingestion pipelines more robust. Both Spark and Pandas are quite forgiving when it comes to types, which often led to schema incompatibilities when ingesting to a feature group. In this release we narrowed down the allowed Python types, and defined a clear mapping to Spark and Online Feature Store types. Please refer to the feature type guide in the documentation for the exact mapping. Required Changes # The most common Python/Pandas types are still supported, we recommend you double check your feature groups with the type mapping above. Deprecation of .save methods in favour of .insert together with .get_or_create_ # The .save() methods to create feature store entities has been deprecated in favour of .insert() . That means if there is no metadata for an entity in the feature store, a call to .insert() will create it. Together with the new .get_or_create_ APIs this will avoid that users have to change their code between creating entities and deploying the same code into production. Pre-3.0 3.0 try: fg = fs.get_feature_group ( ... ) fg.insert ( df ) except RESTError as e: fg = fs.create_feature_group ( ... ) fg.save ( df ) fg = fs.get_or_create_feature_group ( ... ) fg.insert ( df ) hops python library superseded by Hopsworks library # The hops python library is now deprecated and is superseded by the hopsworks python library. hopsworks is essentially a reimplementation of hops , but with an object-oriented API, similar in style with hsfs . For guides on how to use the API follow the Projects guides . Furthermore, the functionality provided by the model and serving module in hops , is now ported to the hsml python library. To create models and serving follow the MLOps guides . New Feature Highlights # This list is meant to serve as a starting point to explore the new features of the Hopsworks 3.0 release, which can significantly improve your workflows. Added new Storage Connectors: GCS, BigQuery and Kafka # With the added support for Google Cloud, we added also two new storage connectors : Google Cloud Storage and Google BigQuery . Users can use these connectors to create external feature groups or write out training data. Additionally, to make it easier for users to get started with Spark Streaming applications, we added a Kafka connector , which let\u2019s you easily read a Kafka topic into a Spark Streaming Dataframe. Optimized Default Hudi Options # By default, Hudi tends to over-partition input, and therefore the layout of Feature Groups. The default parallelism is 200, to ensure each Spark partition stays within the 2GB limit for inputs up to 500GB. The new default is the following for all insert/upsert operations: extra_write_options = { 'hoodie.bulkinsert.shuffle.parallelism' : '5' , 'hoodie.insert.shuffle.parallelism' : '5' , 'hoodie.upsert.shuffle.parallelism' : '5' } In most of the cases, you will not have to change this default. For very large inputs you should bump this up accordingly, by passing it to the write_options argument of the Feature Group .insert() method: fg . insert ( df , write_options = extra_write_options ) We recommend having shuffle parallelism hoodie.[insert|upsert|bulkinsert].shuffle.parallelism such that its at least input_data_size/500MB. Feature View passed features # With the introduction of the Feature View abstraction , we added APIs to allow users to overwrite features with so-called passed features when calling fv.get_feature_vector() : # get a single vector feature_view . get_feature_vector ( entry = { \"pk1\" : 1 , \"pk2\" : 2 }, passed_features = { \"feature_a\" : \"value_a\" } ) # get multiple vectors feature_view . get_feature_vectors ( entry = [ { \"pk1\" : 1 , \"pk2\" : 2 }, { \"pk1\" : 3 , \"pk2\" : 4 }, { \"pk1\" : 5 , \"pk2\" : 6 } ], passed_features = [ { \"feature_a\" : \"value_a\" }, { \"feature_a\" : \"value_b\" }, { \"feature_a\" : \"value_c\" }, ] ) This is useful, if some of the features values are only known at prediction time and cannot be computed and cached in the online feature store, you can provide those values as passed_features option. The get_feature_vector method is going to use the passed values to construct the final feature vector to submit to the model, it will also apply any transformations to the passed features attached to the respective features. Reading Training Data directly from HopsFS in Python environments # In Hopsworks 3.0 we added support for reading training data from Hopsworks directly into external Python environments. Previously, users had to write the training data to external storage like S3 in order to access it from external environment.","title":3.0},{"location":"user_guides/migration/30_migration/#30-migration-guide","text":"","title":"3.0 Migration Guide"},{"location":"user_guides/migration/30_migration/#breaking-changes","text":"","title":"Breaking Changes"},{"location":"user_guides/migration/30_migration/#feature-view","text":"Feature View is a new core abstraction introduced in version 3.0. The Feature View extends and replaces the old Training Dataset. Feature views are now the gateway for users to access feature data from the feature store. Pre-3.0 3.0 td = fs . create_training_dataset () First create the feature view using a Query object: fv = fs . create_feature_view () You can then create training data from the feature view by writing it as new files with .create_ methods: td_df = fv . create_training_data () fv . create_train_test_split () fv . create_train_validation_test_split () Or you can directly create the training data in memory by below methods: fv . training_data () fv . train_test_split () fv . train_validation_test_split () This means that the process for generating training data changes slightly and training data is grouped by feature views. This has the following advantages: It will be easier to create new versions of training data for re-training of models in the future. You do not have to keep references to training data in your model serving code, instead you use the Feature View object. The feature view offers the same interface for retrieving batch data for batch scoring, so you don\u2019t have to execute SQL against the feature store explicitly anymore. A feature view uses the Query abstraction to define the schema of the view, and therefore, the Query is a mandatory argument, this effectively removes the possibility to create a training dataset directly from a Spark Dataframe.","title":"Feature View"},{"location":"user_guides/migration/30_migration/#required-changes","text":"After the upgrade all existing training datasets will be located under a Feature View with the same name. The versions of the training datasets stay untouched. You can use feature_view.get_training_data to get back existing training datasets. Except that training datasets created directly from Spark Dataframe are not migrated to feature view. In this case, you can use the old APIs for retrieving these training datasets, so that your existing training pipelines are still functional. However, if you have any pipelines, that automatically create training datasets, you will have to adjust these to the above workflow. fs.create_training_dataset() has been removed. To learn more about the new Feature View, have a look at the dedicated concept and guide section in the documentation.","title":"Required changes"},{"location":"user_guides/migration/30_migration/#deequ-based-data-validation-in-favour-of-great-expectations","text":"Unfortunately, the Deequ data validation library is no longer actively maintained which makes it impossible for us to maintain the functionality within Hopsworks. Therefore, we are dropping the entire support for Deequ as validation engine in Hopsworks 3.0 in favour of Great Expectations (GE). This has the following advantages: Great Expectations has become the defacto standard for data validation within the community. Hopsworks is fully compatible with GE native objects, that means you can bring your existing expectation suites without the need for rewriting them. GE is both available for Spark and for Pandas Dataframes, whereas Deequ was only supporting Spark.","title":"Deequ-based Data Validation in favour of Great Expectations"},{"location":"user_guides/migration/30_migration/#required-changes_1","text":"All APIs regarding data validation have been redesigned to accomodate the functionality of GE. This means that you will have to redesign your previous expectations in the form of GE expectation suites that you can attach to Feature Groups. Please refer to the data validation guide for a full specification of the functionality.","title":"Required changes"},{"location":"user_guides/migration/30_migration/#limitations","text":"GE is a Python library and therefore we can support synchronous data validation only in Python and PySpark kernels and not on Java/Scala Spark kernels. However, you have the possibility to launch a job asynchronously after writing with Java/Scala in order to perform data validation.","title":"Limitations"},{"location":"user_guides/migration/30_migration/#deprecated-features","text":"These changes or new features introduce changes in APIs which might break your pipelines in the future. We try to keep old APIs around until the next major release in order to give you some time to adapt your pipelines, however, this is not always possible, and these methods might be removed in any upcoming release, so we recommend addressing these changes as soon as possible. For this reason, we list some of the changes as breaking change, even though they are still backwards compatible.","title":"Deprecated Features"},{"location":"user_guides/migration/30_migration/#on-demand-feature-groups-are-now-called-external-feature-groups","text":"Most data engineers but also many data scientists have a background where they at least partially where exposed to database terminology. Therefore, we decided to rename On-Demand Feature Groups to simply External Feature Groups. We think this makes the abstraction clearer, as practitioners are usually familiar with the concept of Extern Tables in a database. This lead to a change in HSFS APIs: Pre-3.0 3.0 fs . create_on_demand_feature_group () fs . get_on_demand_feature_group () fs . get_on_demand_feature_groups () fs . create_external_feature_group () fs . get_external_feature_group () fs . get_external_feature_groups () Note, pre-3.0 methods are marked as deprecated and still available in the library for backwards compatibility.","title":"On-Demand Feature Groups are now called External Feature Groups"},{"location":"user_guides/migration/30_migration/#streaming-api-for-writing-becomes-the-python-default","text":"Hopsworks provides three write APIs to the Feature Store to accommodate the different use cases: Batch Write: This was the default mode prior to version 3.0. It involves writing a DataFrame in batch either to the offline feature store, or the online one, or both. This mode is still the default when you are writing Spark DataFrames on Hopsworks. External connectors: This mode allows users to mount external tables existing in DataWarehouses like Snowflake, Redshift and BigQuery as feature groups in Hopsworks. In this case the data is not moved and remains on the external data storage. Stream Write: This mode was introduced in version 2.0 and expanded in version 3.0. This mode has a \"Kappa-style\" architecture, where the DataFrame gets streamed into a Kafka topic and, as explained later in the post, the data is picked up from Kafka by Hopsworks and written into the desired stores. In Hopsworks 3.0 this is the default mode for Python clients. With 3.0 the stream API becomes the default for Feature Groups created from pure Python environments with Pandas Dataframes. This has the following advantages: Reduced write amplification: Instead of uploading data to Hopsworks and subsequently starting a Spark job to upsert the data on offline storage and writing it to Kafka for the online storage upsert, the data is directly written to Kafka and from there it\u2019s being upserted directly to offline and/or online. Fresher features: Since new data gets written to Kafka directly without prior upload, the data ends up in the online feature store with subsecond latency, which is a massive improvement given it is written from Python without any Streaming framework. Batching of offline upserts: You can control now yourself how often the Spark application that performs the upsert on the offline feature store is running. Either you run it synchronously with every new Dataframe ingestion, or you batch multiple Dataframes by launching the job less regularly.","title":"Streaming API for writing becomes the Python Default"},{"location":"user_guides/migration/30_migration/#required-changes_2","text":"Your existing feature groups will not be affected by this change, that means all existing feature groups will continue to use the old upload path for ingestion. However, we strongly recommend creating new versions of your existing feature groups that use ingest to using Python, in order to leverage the above advantages.","title":"Required changes"},{"location":"user_guides/migration/30_migration/#built-in-transformation-functions-dont-have-to-be-registered-explicitly-for-every-project","text":"In Hopsworks 2.5 users had to register the built-in transformation functions (min-max scaler, standard scaler, label encoder and robust scaler) explicitly for every project by calling fs.register_builtin_transformation_functions() . This is no longer necessary, as all new projects will have the functions registered by default.","title":"Built-in transformation functions don\u2019t have to be registered explicitly for every project"},{"location":"user_guides/migration/30_migration/#hive-installation-extra-deprecated-in-favour-of-python-extra","text":"In the past when using HSFS in pure Python environments without PySpark, users had to install the hive extra when installing the PyPi package. This extra got deprecated and users now have to install an extra called python to reflect the environment: Pre-3.0 3.0 pip install hsfs [ hive ] pip install hsfs [ python ]","title":"Hive installation extra deprecated in favour of Python extra"},{"location":"user_guides/migration/30_migration/#more-restrictive-feature-types","text":"With Hopsworks 3.0 we made feature types more strict and therefore made ingestion pipelines more robust. Both Spark and Pandas are quite forgiving when it comes to types, which often led to schema incompatibilities when ingesting to a feature group. In this release we narrowed down the allowed Python types, and defined a clear mapping to Spark and Online Feature Store types. Please refer to the feature type guide in the documentation for the exact mapping.","title":"More restrictive feature types"},{"location":"user_guides/migration/30_migration/#required-changes_3","text":"The most common Python/Pandas types are still supported, we recommend you double check your feature groups with the type mapping above.","title":"Required Changes"},{"location":"user_guides/migration/30_migration/#deprecation-of-save-methods-in-favour-of-insert-together-with-get_or_create_","text":"The .save() methods to create feature store entities has been deprecated in favour of .insert() . That means if there is no metadata for an entity in the feature store, a call to .insert() will create it. Together with the new .get_or_create_ APIs this will avoid that users have to change their code between creating entities and deploying the same code into production. Pre-3.0 3.0 try: fg = fs.get_feature_group ( ... ) fg.insert ( df ) except RESTError as e: fg = fs.create_feature_group ( ... ) fg.save ( df ) fg = fs.get_or_create_feature_group ( ... ) fg.insert ( df )","title":"Deprecation of .save methods in favour of .insert together with .get_or_create_"},{"location":"user_guides/migration/30_migration/#hops-python-library-superseded-by-hopsworks-library","text":"The hops python library is now deprecated and is superseded by the hopsworks python library. hopsworks is essentially a reimplementation of hops , but with an object-oriented API, similar in style with hsfs . For guides on how to use the API follow the Projects guides . Furthermore, the functionality provided by the model and serving module in hops , is now ported to the hsml python library. To create models and serving follow the MLOps guides .","title":"hops python library superseded by Hopsworks library"},{"location":"user_guides/migration/30_migration/#new-feature-highlights","text":"This list is meant to serve as a starting point to explore the new features of the Hopsworks 3.0 release, which can significantly improve your workflows.","title":"New Feature Highlights"},{"location":"user_guides/migration/30_migration/#added-new-storage-connectors-gcs-bigquery-and-kafka","text":"With the added support for Google Cloud, we added also two new storage connectors : Google Cloud Storage and Google BigQuery . Users can use these connectors to create external feature groups or write out training data. Additionally, to make it easier for users to get started with Spark Streaming applications, we added a Kafka connector , which let\u2019s you easily read a Kafka topic into a Spark Streaming Dataframe.","title":"Added new Storage Connectors: GCS, BigQuery and Kafka"},{"location":"user_guides/migration/30_migration/#optimized-default-hudi-options","text":"By default, Hudi tends to over-partition input, and therefore the layout of Feature Groups. The default parallelism is 200, to ensure each Spark partition stays within the 2GB limit for inputs up to 500GB. The new default is the following for all insert/upsert operations: extra_write_options = { 'hoodie.bulkinsert.shuffle.parallelism' : '5' , 'hoodie.insert.shuffle.parallelism' : '5' , 'hoodie.upsert.shuffle.parallelism' : '5' } In most of the cases, you will not have to change this default. For very large inputs you should bump this up accordingly, by passing it to the write_options argument of the Feature Group .insert() method: fg . insert ( df , write_options = extra_write_options ) We recommend having shuffle parallelism hoodie.[insert|upsert|bulkinsert].shuffle.parallelism such that its at least input_data_size/500MB.","title":"Optimized Default Hudi Options"},{"location":"user_guides/migration/30_migration/#feature-view-passed-features","text":"With the introduction of the Feature View abstraction , we added APIs to allow users to overwrite features with so-called passed features when calling fv.get_feature_vector() : # get a single vector feature_view . get_feature_vector ( entry = { \"pk1\" : 1 , \"pk2\" : 2 }, passed_features = { \"feature_a\" : \"value_a\" } ) # get multiple vectors feature_view . get_feature_vectors ( entry = [ { \"pk1\" : 1 , \"pk2\" : 2 }, { \"pk1\" : 3 , \"pk2\" : 4 }, { \"pk1\" : 5 , \"pk2\" : 6 } ], passed_features = [ { \"feature_a\" : \"value_a\" }, { \"feature_a\" : \"value_b\" }, { \"feature_a\" : \"value_c\" }, ] ) This is useful, if some of the features values are only known at prediction time and cannot be computed and cached in the online feature store, you can provide those values as passed_features option. The get_feature_vector method is going to use the passed values to construct the final feature vector to submit to the model, it will also apply any transformations to the passed features attached to the respective features.","title":"Feature View passed features"},{"location":"user_guides/migration/30_migration/#reading-training-data-directly-from-hopsfs-in-python-environments","text":"In Hopsworks 3.0 we added support for reading training data from Hopsworks directly into external Python environments. Previously, users had to write the training data to external storage like S3 in order to access it from external environment.","title":"Reading Training Data directly from HopsFS in Python environments"},{"location":"user_guides/mlops/","text":"Model Registry & Serving Guides # This section serves to provide guides and examples for the common usage of abstractions and functionality of Models and Deployments through the Hopsworks UI and APIs. Model Registry Model Serving Vector Database","title":"Model Registry & Serving Guides"},{"location":"user_guides/mlops/#model-registry-serving-guides","text":"This section serves to provide guides and examples for the common usage of abstractions and functionality of Models and Deployments through the Hopsworks UI and APIs. Model Registry Model Serving Vector Database","title":"Model Registry &amp; Serving Guides"},{"location":"user_guides/mlops/registry/","text":"Model Registry Guides # Hopsworks Model Registry is a centralized repository, within an organization, to manage machine learning models. A model is the product of training a machine learning algorithm with training data. This section provides guides for creating models and publish them to the Model Registry to make them available for download for batch predictions, or deployed to serve realtime applications. Exporting a model # Follow these framework-specific guides to export a Model to the Model Registry. TensorFlow Scikit-learn Other frameworks Model Schema # A Model schema describes the input and outputs for a model. It provides a functional description of the model which makes it simpler to get started working with it. For example if the model inputs a tensor, the model schema can define the shape and data type of the tensor. Input Example # An Input example provides an instance of a valid model input. Input examples are stored with the model as separate artifacts.","title":"Model Registry Guides"},{"location":"user_guides/mlops/registry/#model-registry-guides","text":"Hopsworks Model Registry is a centralized repository, within an organization, to manage machine learning models. A model is the product of training a machine learning algorithm with training data. This section provides guides for creating models and publish them to the Model Registry to make them available for download for batch predictions, or deployed to serve realtime applications.","title":"Model Registry Guides"},{"location":"user_guides/mlops/registry/#exporting-a-model","text":"Follow these framework-specific guides to export a Model to the Model Registry. TensorFlow Scikit-learn Other frameworks","title":"Exporting a model"},{"location":"user_guides/mlops/registry/#model-schema","text":"A Model schema describes the input and outputs for a model. It provides a functional description of the model which makes it simpler to get started working with it. For example if the model inputs a tensor, the model schema can define the shape and data type of the tensor.","title":"Model Schema"},{"location":"user_guides/mlops/registry/#input-example","text":"An Input example provides an instance of a valid model input. Input examples are stored with the model as separate artifacts.","title":"Input Example"},{"location":"user_guides/mlops/registry/input_example/","text":"How To Attach An Input Example # Introduction # In this guide you will learn how to attach an input example to a model. An input example is simply an instance of a valid model input. Attaching an input example to your model will give other users a better understanding of what data it expects. Code # Step 1: Connect to Hopsworks # import hopsworks project = hopsworks . login () # get Hopsworks Model Registry handle mr = project . get_model_registry () Step 2: Generate an input example # Generate an input example which corresponds to a valid input to your model. Currently we support pandas.DataFrame, pandas.Series, numpy.ndarray, list to be passed as input example. import numpy as np input_example = np . random . randint ( 0 , high = 256 , size = 784 , dtype = np . uint8 ) Step 3: Set input_example parameter # Set the input_example parameter in the create_model function and call save() to attaching it to the model and register it in the registry. model = mr . tensorflow . create_model ( name = \"mnist\" , input_example = input_example ) model . save ( \"./model\" ) Conclusion # In this guide you learned how to attach an input example to your model.","title":"Input Example"},{"location":"user_guides/mlops/registry/input_example/#how-to-attach-an-input-example","text":"","title":"How To Attach An Input Example"},{"location":"user_guides/mlops/registry/input_example/#introduction","text":"In this guide you will learn how to attach an input example to a model. An input example is simply an instance of a valid model input. Attaching an input example to your model will give other users a better understanding of what data it expects.","title":"Introduction"},{"location":"user_guides/mlops/registry/input_example/#code","text":"","title":"Code"},{"location":"user_guides/mlops/registry/input_example/#step-1-connect-to-hopsworks","text":"import hopsworks project = hopsworks . login () # get Hopsworks Model Registry handle mr = project . get_model_registry ()","title":"Step 1: Connect to Hopsworks"},{"location":"user_guides/mlops/registry/input_example/#step-2-generate-an-input-example","text":"Generate an input example which corresponds to a valid input to your model. Currently we support pandas.DataFrame, pandas.Series, numpy.ndarray, list to be passed as input example. import numpy as np input_example = np . random . randint ( 0 , high = 256 , size = 784 , dtype = np . uint8 )","title":"Step 2: Generate an input example"},{"location":"user_guides/mlops/registry/input_example/#step-3-set-input_example-parameter","text":"Set the input_example parameter in the create_model function and call save() to attaching it to the model and register it in the registry. model = mr . tensorflow . create_model ( name = \"mnist\" , input_example = input_example ) model . save ( \"./model\" )","title":"Step 3: Set input_example parameter"},{"location":"user_guides/mlops/registry/input_example/#conclusion","text":"In this guide you learned how to attach an input example to your model.","title":"Conclusion"},{"location":"user_guides/mlops/registry/model_schema/","text":"How To Attach A Model Schema # Introduction # In this guide you will learn how to attach a model schema to your model. A model schema, describes the type and shape of inputs and outputs (predictions) for your model. Attaching a model schema to your model will give other users a better understanding of what data it expects. Code # Step 1: Connect to Hopsworks # import hopsworks project = hopsworks . login () # get Hopsworks Model Registry handle mr = project . get_model_registry () Step 2: Create ModelSchema # Create a ModelSchema for your inputs and outputs by passing in an example that your model is trained on and a valid prediction. Currently, we support pandas.DataFrame, pandas.Series, numpy.ndarray, list . # Import a Schema and ModelSchema definition from hsml.utils.model_schema import ModelSchema from hsml.utils.schema import Schema # Model inputs for MNIST dataset inputs = [{ 'type' : 'uint8' , 'shape' : [ 28 , 28 , 1 ], 'description' : 'grayscale representation of 28x28 MNIST images' }] # Build the input schema input_schema = Schema ( inputs ) # Model outputs outputs = [{ 'type' : 'float32' , 'shape' : [ 10 ]}] # Build the output schema output_schema = Schema ( outputs ) # Create ModelSchema object model_schema = ModelSchema ( input_schema = input_schema , output_schema = output_schema ) Step 3: Set model_schema parameter # Set the model_schema parameter in the create_model function and call save() to attaching it to the model and register it in the registry. model = mr . tensorflow . create_model ( name = \"mnist\" , model_schema = model_schema ) model . save ( \"./model\" ) Conclusion # In this guide you learned how to attach an input example to your model.","title":"Model Schema"},{"location":"user_guides/mlops/registry/model_schema/#how-to-attach-a-model-schema","text":"","title":"How To Attach A Model Schema"},{"location":"user_guides/mlops/registry/model_schema/#introduction","text":"In this guide you will learn how to attach a model schema to your model. A model schema, describes the type and shape of inputs and outputs (predictions) for your model. Attaching a model schema to your model will give other users a better understanding of what data it expects.","title":"Introduction"},{"location":"user_guides/mlops/registry/model_schema/#code","text":"","title":"Code"},{"location":"user_guides/mlops/registry/model_schema/#step-1-connect-to-hopsworks","text":"import hopsworks project = hopsworks . login () # get Hopsworks Model Registry handle mr = project . get_model_registry ()","title":"Step 1: Connect to Hopsworks"},{"location":"user_guides/mlops/registry/model_schema/#step-2-create-modelschema","text":"Create a ModelSchema for your inputs and outputs by passing in an example that your model is trained on and a valid prediction. Currently, we support pandas.DataFrame, pandas.Series, numpy.ndarray, list . # Import a Schema and ModelSchema definition from hsml.utils.model_schema import ModelSchema from hsml.utils.schema import Schema # Model inputs for MNIST dataset inputs = [{ 'type' : 'uint8' , 'shape' : [ 28 , 28 , 1 ], 'description' : 'grayscale representation of 28x28 MNIST images' }] # Build the input schema input_schema = Schema ( inputs ) # Model outputs outputs = [{ 'type' : 'float32' , 'shape' : [ 10 ]}] # Build the output schema output_schema = Schema ( outputs ) # Create ModelSchema object model_schema = ModelSchema ( input_schema = input_schema , output_schema = output_schema )","title":"Step 2: Create ModelSchema"},{"location":"user_guides/mlops/registry/model_schema/#step-3-set-model_schema-parameter","text":"Set the model_schema parameter in the create_model function and call save() to attaching it to the model and register it in the registry. model = mr . tensorflow . create_model ( name = \"mnist\" , model_schema = model_schema ) model . save ( \"./model\" )","title":"Step 3: Set model_schema parameter"},{"location":"user_guides/mlops/registry/model_schema/#conclusion","text":"In this guide you learned how to attach an input example to your model.","title":"Conclusion"},{"location":"user_guides/mlops/registry/frameworks/python/","text":"How To Export a Python Model # Introduction # In this guide you will learn how to export a generic Python model and register it in the Model Registry. Code # Step 1: Connect to Hopsworks # import hopsworks project = hopsworks . login () # get Hopsworks Model Registry handle mr = project . get_model_registry () Step 2: Train # Define your XGBoost model and run the training loop. # Define a model model = XGBClassifier () # Train model model . fit ( X_train , y_train ) Step 3: Export to local path # Export the XGBoost model to a directory on the local filesystem. model_file = \"model.json\" model . save_model ( model_file ) Step 4: Register model in registry # Use the ModelRegistry.python.create_model(..) function to register a model as a Python model. Define a name, and attach optional metrics for your model, then invoke the save() function with the parameter being the path to the local directory where the model was exported to. # Model evaluation metrics metrics = { 'accuracy' : 0.92 } py_model = mr . python . create_model ( \"py_model\" , metrics = metrics ) py_model . save ( model_dir ) Conclusion # In this guide you learned how to export a Python model to the Model Registry. You can also try attaching an Input Example and a Model Schema to your model to document the shape and type of the data the model was trained on.","title":"Python"},{"location":"user_guides/mlops/registry/frameworks/python/#how-to-export-a-python-model","text":"","title":"How To Export a Python Model"},{"location":"user_guides/mlops/registry/frameworks/python/#introduction","text":"In this guide you will learn how to export a generic Python model and register it in the Model Registry.","title":"Introduction"},{"location":"user_guides/mlops/registry/frameworks/python/#code","text":"","title":"Code"},{"location":"user_guides/mlops/registry/frameworks/python/#step-1-connect-to-hopsworks","text":"import hopsworks project = hopsworks . login () # get Hopsworks Model Registry handle mr = project . get_model_registry ()","title":"Step 1: Connect to Hopsworks"},{"location":"user_guides/mlops/registry/frameworks/python/#step-2-train","text":"Define your XGBoost model and run the training loop. # Define a model model = XGBClassifier () # Train model model . fit ( X_train , y_train )","title":"Step 2: Train"},{"location":"user_guides/mlops/registry/frameworks/python/#step-3-export-to-local-path","text":"Export the XGBoost model to a directory on the local filesystem. model_file = \"model.json\" model . save_model ( model_file )","title":"Step 3: Export to local path"},{"location":"user_guides/mlops/registry/frameworks/python/#step-4-register-model-in-registry","text":"Use the ModelRegistry.python.create_model(..) function to register a model as a Python model. Define a name, and attach optional metrics for your model, then invoke the save() function with the parameter being the path to the local directory where the model was exported to. # Model evaluation metrics metrics = { 'accuracy' : 0.92 } py_model = mr . python . create_model ( \"py_model\" , metrics = metrics ) py_model . save ( model_dir )","title":"Step 4: Register model in registry"},{"location":"user_guides/mlops/registry/frameworks/python/#conclusion","text":"In this guide you learned how to export a Python model to the Model Registry. You can also try attaching an Input Example and a Model Schema to your model to document the shape and type of the data the model was trained on.","title":"Conclusion"},{"location":"user_guides/mlops/registry/frameworks/skl/","text":"How To Export a Scikit-learn Model # Introduction # In this guide you will learn how to export a Scikit-learn model and register it in the Model Registry. Code # Step 1: Connect to Hopsworks # import hopsworks project = hopsworks . login () # get Hopsworks Model Registry handle mr = project . get_model_registry () Step 2: Train # Define your Scikit-learn model and run the training loop. # Define a model iris_knn = KNeighborsClassifier ( .. ) iris_knn . fit ( .. ) Step 3: Export to local path # Export the Scikit-learn model to a directory on the local filesystem. model_file = \"skl_knn.pkl\" joblib . dump ( iris_knn , model_file ) Step 4: Register model in registry # Use the ModelRegistry.sklearn.create_model(..) function to register a model as a Scikit-learn model. Define a name, and attach optional metrics for your model, then invoke the save() function with the parameter being the path to the local directory where the model was exported to. # Model evaluation metrics metrics = { 'accuracy' : 0.92 } skl_model = mr . sklearn . create_model ( \"skl_model\" , metrics = metrics ) skl_model . save ( model_file ) Conclusion # In this guide you learned how to export a Scikit-learn model to the Model Registry. You can also try attaching an Input Example and a Model Schema to your model to document the shape and type of the data the model was trained on.","title":"Scikit-learn"},{"location":"user_guides/mlops/registry/frameworks/skl/#how-to-export-a-scikit-learn-model","text":"","title":"How To Export a Scikit-learn Model"},{"location":"user_guides/mlops/registry/frameworks/skl/#introduction","text":"In this guide you will learn how to export a Scikit-learn model and register it in the Model Registry.","title":"Introduction"},{"location":"user_guides/mlops/registry/frameworks/skl/#code","text":"","title":"Code"},{"location":"user_guides/mlops/registry/frameworks/skl/#step-1-connect-to-hopsworks","text":"import hopsworks project = hopsworks . login () # get Hopsworks Model Registry handle mr = project . get_model_registry ()","title":"Step 1: Connect to Hopsworks"},{"location":"user_guides/mlops/registry/frameworks/skl/#step-2-train","text":"Define your Scikit-learn model and run the training loop. # Define a model iris_knn = KNeighborsClassifier ( .. ) iris_knn . fit ( .. )","title":"Step 2: Train"},{"location":"user_guides/mlops/registry/frameworks/skl/#step-3-export-to-local-path","text":"Export the Scikit-learn model to a directory on the local filesystem. model_file = \"skl_knn.pkl\" joblib . dump ( iris_knn , model_file )","title":"Step 3: Export to local path"},{"location":"user_guides/mlops/registry/frameworks/skl/#step-4-register-model-in-registry","text":"Use the ModelRegistry.sklearn.create_model(..) function to register a model as a Scikit-learn model. Define a name, and attach optional metrics for your model, then invoke the save() function with the parameter being the path to the local directory where the model was exported to. # Model evaluation metrics metrics = { 'accuracy' : 0.92 } skl_model = mr . sklearn . create_model ( \"skl_model\" , metrics = metrics ) skl_model . save ( model_file )","title":"Step 4: Register model in registry"},{"location":"user_guides/mlops/registry/frameworks/skl/#conclusion","text":"In this guide you learned how to export a Scikit-learn model to the Model Registry. You can also try attaching an Input Example and a Model Schema to your model to document the shape and type of the data the model was trained on.","title":"Conclusion"},{"location":"user_guides/mlops/registry/frameworks/tf/","text":"How To Export a TensorFlow Model # Introduction # In this guide you will learn how to export a TensorFlow model and register it in the Model Registry. Save in SavedModel format Make sure the model is saved in the SavedModel format to be able to deploy it on TensorFlow Serving. Code # Step 1: Connect to Hopsworks # import hopsworks project = hopsworks . login () # get Hopsworks Model Registry handle mr = project . get_model_registry () Step 2: Train # Define your TensorFlow model and run the training loop. # Define a model model = tf . keras . Sequential () # Add layers model . add ( .. ) # Compile the model. model . compile ( .. ) # Train the model model . fit ( .. ) Step 3: Export to local path # Export the TensorFlow model to a directory on the local filesystem. model_dir = \"./model\" tf . saved_model . save ( model , model_dir ) Step 4: Register model in registry # Use the ModelRegistry.tensorflow.create_model(..) function to register a model as a TensorFlow model. Define a name, and attach optional metrics for your model, then invoke the save() function with the parameter being the path to the local directory where the model was exported to. # Model evaluation metrics metrics = { 'accuracy' : 0.92 } tf_model = mr . tensorflow . create_model ( \"tf_model\" , metrics = metrics ) tf_model . save ( model_dir ) Conclusion # In this guide you learned how to export a TensorFlow model to the Model Registry. You can also try attaching an Input Example and a Model Schema to your model to document the shape and type of the data the model was trained on.","title":"TensorFlow"},{"location":"user_guides/mlops/registry/frameworks/tf/#how-to-export-a-tensorflow-model","text":"","title":"How To Export a TensorFlow Model"},{"location":"user_guides/mlops/registry/frameworks/tf/#introduction","text":"In this guide you will learn how to export a TensorFlow model and register it in the Model Registry. Save in SavedModel format Make sure the model is saved in the SavedModel format to be able to deploy it on TensorFlow Serving.","title":"Introduction"},{"location":"user_guides/mlops/registry/frameworks/tf/#code","text":"","title":"Code"},{"location":"user_guides/mlops/registry/frameworks/tf/#step-1-connect-to-hopsworks","text":"import hopsworks project = hopsworks . login () # get Hopsworks Model Registry handle mr = project . get_model_registry ()","title":"Step 1: Connect to Hopsworks"},{"location":"user_guides/mlops/registry/frameworks/tf/#step-2-train","text":"Define your TensorFlow model and run the training loop. # Define a model model = tf . keras . Sequential () # Add layers model . add ( .. ) # Compile the model. model . compile ( .. ) # Train the model model . fit ( .. )","title":"Step 2: Train"},{"location":"user_guides/mlops/registry/frameworks/tf/#step-3-export-to-local-path","text":"Export the TensorFlow model to a directory on the local filesystem. model_dir = \"./model\" tf . saved_model . save ( model , model_dir )","title":"Step 3: Export to local path"},{"location":"user_guides/mlops/registry/frameworks/tf/#step-4-register-model-in-registry","text":"Use the ModelRegistry.tensorflow.create_model(..) function to register a model as a TensorFlow model. Define a name, and attach optional metrics for your model, then invoke the save() function with the parameter being the path to the local directory where the model was exported to. # Model evaluation metrics metrics = { 'accuracy' : 0.92 } tf_model = mr . tensorflow . create_model ( \"tf_model\" , metrics = metrics ) tf_model . save ( model_dir )","title":"Step 4: Register model in registry"},{"location":"user_guides/mlops/registry/frameworks/tf/#conclusion","text":"In this guide you learned how to export a TensorFlow model to the Model Registry. You can also try attaching an Input Example and a Model Schema to your model to document the shape and type of the data the model was trained on.","title":"Conclusion"},{"location":"user_guides/mlops/serving/","text":"Model Serving Guide # Deployment # Assuming you have already created a model in the Model Registry, a deployment can now be created to prepare a model artifact for this model and make it accessible for running predictions behind a REST endpoint. Follow the Deployment Creation Guide to create a Deployment for your model. Predictor # Predictors are responsible for running a model server that loads a trained model, handles inference requests and returns predictions, see the Predictor Guide . Transformer # Transformers are used to apply transformations on the model inputs before sending them to the predictor for making predictions using the model, see the Transformer Guide . Inference Batcher # Configure the predictor to batch inference requests, see the Inference Batcher Guide . Inference Logger # Configure the predictor to log inference requests and predictions, see the Inference Logger Guide .","title":"Model Serving Guide"},{"location":"user_guides/mlops/serving/#model-serving-guide","text":"","title":"Model Serving Guide"},{"location":"user_guides/mlops/serving/#deployment","text":"Assuming you have already created a model in the Model Registry, a deployment can now be created to prepare a model artifact for this model and make it accessible for running predictions behind a REST endpoint. Follow the Deployment Creation Guide to create a Deployment for your model.","title":"Deployment"},{"location":"user_guides/mlops/serving/#predictor","text":"Predictors are responsible for running a model server that loads a trained model, handles inference requests and returns predictions, see the Predictor Guide .","title":"Predictor"},{"location":"user_guides/mlops/serving/#transformer","text":"Transformers are used to apply transformations on the model inputs before sending them to the predictor for making predictions using the model, see the Transformer Guide .","title":"Transformer"},{"location":"user_guides/mlops/serving/#inference-batcher","text":"Configure the predictor to batch inference requests, see the Inference Batcher Guide .","title":"Inference Batcher"},{"location":"user_guides/mlops/serving/#inference-logger","text":"Configure the predictor to log inference requests and predictions, see the Inference Logger Guide .","title":"Inference Logger"},{"location":"user_guides/mlops/serving/deployment-state/","text":"How To Inspect A Deployment State # Introduction # In this guide, you will learn how to inspect the state of a deployment. A state can be seen as a snapshot of the current inner workings of a deployment. The following is the state transition diagram for deployments. State transitions of deployments States are composed of a status and a condition . While a status represents a high-level view of the state, conditions contain more detailed information closely related to infrastructure terms. GUI # Step 1: Inspect deployment status # If you have at least one deployment already created, navigate to the deployments page by clicking on the Deployments tab on the navigation menu on the left. Deployments navigation tab Once in the deployments page, find the deployment you want to inspect. Next to the actions buttons, you can find an indicator showing the current status of the deployment. This indicator changes its color based on the status. To inspect the condition of the deployment, click on the name of the deployment to open the deployment overview page. Step 2: Inspect condition # Once in the deployment overview page, you can find the aforementioned status indicator at the top of page. Below it, a one-line message is shown with a more detailed description of the deployment status. This message is built using the current condition of the deployment. Deployments status condition Step 3: Check n\u00ba of running instances # Additionally, you can find the n\u00ba of instances currently running by scrolling down to the resource allocation section. Resource allocation for a deployment Scale-to-zero capabilities If scale-to-zero capabilities are enabled, you can see how the n\u00ba of instances of a running deployment goes to zero and the status changes to idle . To enable scale-to-zero in a deployment, see Resource Allocation Guide Code # Step 1: Connect to Hopsworks # import hopsworks project = hopsworks . login () # get Hopsworks Model Serving handle ms = project . get_model_serving () Step 2: Retrieve an existing deployment # deployment = ms . get_deployment ( \"mydeployment\" ) Step 3: Inspect deployment state # state = deployment . get_state () state . describe () Step 4: Check n\u00ba of running instances # # n\u00ba of predictor instances deployment . resources . describe () # n\u00ba of transformer instances deployment . transformer . resources . describe () API Reference # Deployment PredictorState Deployment status # The status of a deployment is a high-level description of its current state. Show deployment status Status Description CREATED Deployment has never been started STARTING Deployment is starting RUNNING Deployment is ready and running. Predictions are served without additional latencies. IDLE Deployment is ready, but idle. Higher latencies (i.e., cold-start) are expected in the first incoming inference requests FAILED Deployment is in a failed state, which can be due to multiple reasons. More details can be found in the condition UPDATING Deployment is applying updates to the running instances STOPPING Deployment is stopping STOPPED Deployment has been stopped Deployment conditions # A condition contains more specific information about the status of the deployment. They are mainly useful to track the progress of starting or stopping deployments. Status conditions contain three pieces of information: type, status and reason. While the type describes the purpose of the condition, the status represents its progress. Additionally, a reason field is provided with a more descriptive message of the status. Show deployment conditions Type Status Description STOPPED Unknown Deployment is stopping. True Deployment is stopped. Therefore, no instances are running and no resources are allocated. SCHEDULED Unknown Deployment is being scheduled False Deployment failed to be scheduled. This is commonly due to insufficient resources to satisfy the deployment requirements True Deployment has been scheduled successfully. At this point, resources have been already allocated for the deployment. INITIALIZED Unknown Deployment is initializing. This step involves initialization tasks such as pulling docker images or mounting data volumes False Deployment failed to initialized True Deployment has been initialized successfully. At this point, the docker images have been pulled and data volumes mounted STARTED Unknown Deployment is starting. In this step, the model server is started and predictor / transformer scripts (if provided) are executed False Deployment failed to start. This can be due to errors in the predictor / transformer script, missing dependencies or model server incompatibilities. True Deployment has been started successfully. At this point, the model server has been started and the predictor / transformer scripts (if provided) executed. READY Unknown Connectivity is being set up. False Connectivity failed to be set up, mainly due to networking issues. True Connectivity has been set up and the deployment is ready The following are two diagrams with the state transitions of conditions in starting and stopping deployments, respectively. Condition transitions in starting deployments Condition transitions in stopping deployments","title":"Deployment state"},{"location":"user_guides/mlops/serving/deployment-state/#how-to-inspect-a-deployment-state","text":"","title":"How To Inspect A Deployment State"},{"location":"user_guides/mlops/serving/deployment-state/#introduction","text":"In this guide, you will learn how to inspect the state of a deployment. A state can be seen as a snapshot of the current inner workings of a deployment. The following is the state transition diagram for deployments. State transitions of deployments States are composed of a status and a condition . While a status represents a high-level view of the state, conditions contain more detailed information closely related to infrastructure terms.","title":"Introduction"},{"location":"user_guides/mlops/serving/deployment-state/#gui","text":"","title":"GUI"},{"location":"user_guides/mlops/serving/deployment-state/#step-1-inspect-deployment-status","text":"If you have at least one deployment already created, navigate to the deployments page by clicking on the Deployments tab on the navigation menu on the left. Deployments navigation tab Once in the deployments page, find the deployment you want to inspect. Next to the actions buttons, you can find an indicator showing the current status of the deployment. This indicator changes its color based on the status. To inspect the condition of the deployment, click on the name of the deployment to open the deployment overview page.","title":"Step 1: Inspect deployment status"},{"location":"user_guides/mlops/serving/deployment-state/#step-2-inspect-condition","text":"Once in the deployment overview page, you can find the aforementioned status indicator at the top of page. Below it, a one-line message is shown with a more detailed description of the deployment status. This message is built using the current condition of the deployment. Deployments status condition","title":"Step 2: Inspect condition"},{"location":"user_guides/mlops/serving/deployment-state/#step-3-check-no-of-running-instances","text":"Additionally, you can find the n\u00ba of instances currently running by scrolling down to the resource allocation section. Resource allocation for a deployment Scale-to-zero capabilities If scale-to-zero capabilities are enabled, you can see how the n\u00ba of instances of a running deployment goes to zero and the status changes to idle . To enable scale-to-zero in a deployment, see Resource Allocation Guide","title":"Step 3: Check n\u00ba of running instances"},{"location":"user_guides/mlops/serving/deployment-state/#code","text":"","title":"Code"},{"location":"user_guides/mlops/serving/deployment-state/#step-1-connect-to-hopsworks","text":"import hopsworks project = hopsworks . login () # get Hopsworks Model Serving handle ms = project . get_model_serving ()","title":"Step 1: Connect to Hopsworks"},{"location":"user_guides/mlops/serving/deployment-state/#step-2-retrieve-an-existing-deployment","text":"deployment = ms . get_deployment ( \"mydeployment\" )","title":"Step 2: Retrieve an existing deployment"},{"location":"user_guides/mlops/serving/deployment-state/#step-3-inspect-deployment-state","text":"state = deployment . get_state () state . describe ()","title":"Step 3: Inspect deployment state"},{"location":"user_guides/mlops/serving/deployment-state/#step-4-check-no-of-running-instances","text":"# n\u00ba of predictor instances deployment . resources . describe () # n\u00ba of transformer instances deployment . transformer . resources . describe ()","title":"Step 4: Check n\u00ba of running instances"},{"location":"user_guides/mlops/serving/deployment-state/#api-reference","text":"Deployment PredictorState","title":"API Reference"},{"location":"user_guides/mlops/serving/deployment-state/#deployment-status","text":"The status of a deployment is a high-level description of its current state. Show deployment status Status Description CREATED Deployment has never been started STARTING Deployment is starting RUNNING Deployment is ready and running. Predictions are served without additional latencies. IDLE Deployment is ready, but idle. Higher latencies (i.e., cold-start) are expected in the first incoming inference requests FAILED Deployment is in a failed state, which can be due to multiple reasons. More details can be found in the condition UPDATING Deployment is applying updates to the running instances STOPPING Deployment is stopping STOPPED Deployment has been stopped","title":"Deployment status"},{"location":"user_guides/mlops/serving/deployment-state/#deployment-conditions","text":"A condition contains more specific information about the status of the deployment. They are mainly useful to track the progress of starting or stopping deployments. Status conditions contain three pieces of information: type, status and reason. While the type describes the purpose of the condition, the status represents its progress. Additionally, a reason field is provided with a more descriptive message of the status. Show deployment conditions Type Status Description STOPPED Unknown Deployment is stopping. True Deployment is stopped. Therefore, no instances are running and no resources are allocated. SCHEDULED Unknown Deployment is being scheduled False Deployment failed to be scheduled. This is commonly due to insufficient resources to satisfy the deployment requirements True Deployment has been scheduled successfully. At this point, resources have been already allocated for the deployment. INITIALIZED Unknown Deployment is initializing. This step involves initialization tasks such as pulling docker images or mounting data volumes False Deployment failed to initialized True Deployment has been initialized successfully. At this point, the docker images have been pulled and data volumes mounted STARTED Unknown Deployment is starting. In this step, the model server is started and predictor / transformer scripts (if provided) are executed False Deployment failed to start. This can be due to errors in the predictor / transformer script, missing dependencies or model server incompatibilities. True Deployment has been started successfully. At this point, the model server has been started and the predictor / transformer scripts (if provided) executed. READY Unknown Connectivity is being set up. False Connectivity failed to be set up, mainly due to networking issues. True Connectivity has been set up and the deployment is ready The following are two diagrams with the state transitions of conditions in starting and stopping deployments, respectively. Condition transitions in starting deployments Condition transitions in stopping deployments","title":"Deployment conditions"},{"location":"user_guides/mlops/serving/deployment/","text":"How To Create A Deployment # Introduction # In this guide, you will learn how to create a new deployment for a trained model. Warning This guide assumes that a model has already been trained and saved into the Model Registry. To learn how to create a model in the Model Registry, see Model Registry Guide Deployments are used to unify the different components involved in making one or more trained models online and accessible to compute predictions on demand. In each deployment, there are three main components to consider: Model artifact Predictor Transformer GUI # Step 1: Create a deployment # If you have at least one model already trained and saved in the Model Registry, navigate to the deployments page by clicking on the Deployments tab on the navigation menu on the left. Deployments navigation tab Once in the deployments page, click on New deployment if there are not existing deployments or on Create new deployment at the top-right corner to open the deployment creation form. Step 2: Basic deployment configuration # A simplified creation form will appear including the most common deployment fields among all the configuration possible. We provide default values for the rest of the fields, adjusted to the type of deployment you want to create. In the simplified form, select the model framework used to train your model (i.e., TensorFlow Serving or Python ). Then, select the model you want to deploy from the list of available models under pick a model . After selecting the model, the rest of fields are filled automatically. We pick the last model version and model artifact version available in the Model Registry. Moreover, we infer the deployment name from the model name. Deployment name validation rules A valid deployment name can only contain characters a-z, A-Z and 0-9. Predictor script for Python models For Python models, you can select a custom predictor script to load and run the trained model by clicking on From project or Upload new file , to choose an existing script in the project file system or upload a new script, respectively. If you prefer, change the name of the deployment, model version or artifact version . Then, click on Create new deployment to create the deployment for your model. Simplified deployment creation forms for TensorFlow models (left) and Python models (right) Step 3 (Optional): Advanced configuration # Optionally, you can access and adjust other parameters of the deployment configuration by clicking on Advanced options . Advanced options. Go to advanced deployment creation form You will be redirected to a full-page deployment creation form where you can see all the default configuration values we selected for your deployment and adjust them according to your use case. Apart from the aforementioned simplified configuration, in this form you can setup the following components: Deployment advanced options Predictor Transformer Inference logger Inference batcher Resources Once you are done with the changes, click on Create new deployment at the bottom of the page to create the deployment for your model. Step 4: Deployment creation # Wait for the deployment creation process to finish. Deployment creation in progress Step 5: Deployment overview # Once the deployment is created, you will be redirected to the list of all your existing deployments in the project. You can use the filters on the top of the page to easily locate your new deployment. List of deployments After that, click on the new deployment to access the overview page. Deployment overview Code # Step 1: Connect to Hopsworks # import hopsworks project = hopsworks . login () # get Hopsworks Model Registry handle mr = project . get_model_registry () Step 2: Create deployment # Retrieve the trained model you want to deploy. my_model = mr . get_model ( \"my_model\" , version = 1 ) Option A: Using the model object # my_deployment = my_model . deploy () Option B: Using the Model Serving handle # # get Hopsworks Model Serving handle ms = project . get_model_serving () my_predictor = ms . create_predictor ( my_model ) my_deployment = my_predictor . deploy () # or my_deployment = ms . create_deployment ( my_predictor ) my_deployment . save () API Reference # Model Serving Model Artifact # A model artifact is a package containing all of the necessary files for the deployment of a model. It includes the model file(s) and/or custom scripts for loading the model (predictor script) or transforming the model inputs at inference time (the transformer script). When a new deployment is created, a model artifact is generated in two cases: the artifact version in the predictor is set to CREATE (see Artifact Version ) no model artifact with the same files has been created before. Predictor # Predictors are responsible for running the model server that loads the trained model, listens to inference requests and returns prediction results. To learn more about predictors, see the Predictor Guide Note Currently, only one predictor is supported in a deployment. Support for multiple predictors (the inference graphs) is coming soon. Info Model artifacts are assigned an incremental version number, being 0 the version reserved for model artifacts that do not contain predictor or transformer scripts (i.e., shared artifacts containing only the model files). Transformer # Transformers are used to apply transformations on the model inputs before sending them to the predictor for making predictions using the model. To learn more about transformers, see the Transformer Guide . Warning Transformers are only supported in KServe deployments. Inference logger # Inference loggers are deployment components that log inference requests into a Kafka topic for later analysis. To learn about the different logging modes, see the Inference Logger Guide Inference batcher # Inference batcher are deployment component that apply batching to the incoming inference requests for a better throughput-latency trade-off. To learn about the different configuration available for the inference batcher, see the Inference Batcher Guide . Resources # Resources include the number of replicas for the deployment as well as the resources (i.e., memory, CPU, GPU) to be allocated per replica. To learn about the different combinations available, see the Resources Guide . Conclusion # In this guide you learned how to create a deployment.","title":"Deployment creation"},{"location":"user_guides/mlops/serving/deployment/#how-to-create-a-deployment","text":"","title":"How To Create A Deployment"},{"location":"user_guides/mlops/serving/deployment/#introduction","text":"In this guide, you will learn how to create a new deployment for a trained model. Warning This guide assumes that a model has already been trained and saved into the Model Registry. To learn how to create a model in the Model Registry, see Model Registry Guide Deployments are used to unify the different components involved in making one or more trained models online and accessible to compute predictions on demand. In each deployment, there are three main components to consider: Model artifact Predictor Transformer","title":"Introduction"},{"location":"user_guides/mlops/serving/deployment/#gui","text":"","title":"GUI"},{"location":"user_guides/mlops/serving/deployment/#step-1-create-a-deployment","text":"If you have at least one model already trained and saved in the Model Registry, navigate to the deployments page by clicking on the Deployments tab on the navigation menu on the left. Deployments navigation tab Once in the deployments page, click on New deployment if there are not existing deployments or on Create new deployment at the top-right corner to open the deployment creation form.","title":"Step 1: Create a deployment"},{"location":"user_guides/mlops/serving/deployment/#step-2-basic-deployment-configuration","text":"A simplified creation form will appear including the most common deployment fields among all the configuration possible. We provide default values for the rest of the fields, adjusted to the type of deployment you want to create. In the simplified form, select the model framework used to train your model (i.e., TensorFlow Serving or Python ). Then, select the model you want to deploy from the list of available models under pick a model . After selecting the model, the rest of fields are filled automatically. We pick the last model version and model artifact version available in the Model Registry. Moreover, we infer the deployment name from the model name. Deployment name validation rules A valid deployment name can only contain characters a-z, A-Z and 0-9. Predictor script for Python models For Python models, you can select a custom predictor script to load and run the trained model by clicking on From project or Upload new file , to choose an existing script in the project file system or upload a new script, respectively. If you prefer, change the name of the deployment, model version or artifact version . Then, click on Create new deployment to create the deployment for your model. Simplified deployment creation forms for TensorFlow models (left) and Python models (right)","title":"Step 2: Basic deployment configuration"},{"location":"user_guides/mlops/serving/deployment/#step-3-optional-advanced-configuration","text":"Optionally, you can access and adjust other parameters of the deployment configuration by clicking on Advanced options . Advanced options. Go to advanced deployment creation form You will be redirected to a full-page deployment creation form where you can see all the default configuration values we selected for your deployment and adjust them according to your use case. Apart from the aforementioned simplified configuration, in this form you can setup the following components: Deployment advanced options Predictor Transformer Inference logger Inference batcher Resources Once you are done with the changes, click on Create new deployment at the bottom of the page to create the deployment for your model.","title":"Step 3 (Optional): Advanced configuration"},{"location":"user_guides/mlops/serving/deployment/#step-4-deployment-creation","text":"Wait for the deployment creation process to finish. Deployment creation in progress","title":"Step 4: Deployment creation"},{"location":"user_guides/mlops/serving/deployment/#step-5-deployment-overview","text":"Once the deployment is created, you will be redirected to the list of all your existing deployments in the project. You can use the filters on the top of the page to easily locate your new deployment. List of deployments After that, click on the new deployment to access the overview page. Deployment overview","title":"Step 5: Deployment overview"},{"location":"user_guides/mlops/serving/deployment/#code","text":"","title":"Code"},{"location":"user_guides/mlops/serving/deployment/#step-1-connect-to-hopsworks","text":"import hopsworks project = hopsworks . login () # get Hopsworks Model Registry handle mr = project . get_model_registry ()","title":"Step 1: Connect to Hopsworks"},{"location":"user_guides/mlops/serving/deployment/#step-2-create-deployment","text":"Retrieve the trained model you want to deploy. my_model = mr . get_model ( \"my_model\" , version = 1 )","title":"Step 2: Create deployment"},{"location":"user_guides/mlops/serving/deployment/#option-a-using-the-model-object","text":"my_deployment = my_model . deploy ()","title":"Option A: Using the model object"},{"location":"user_guides/mlops/serving/deployment/#option-b-using-the-model-serving-handle","text":"# get Hopsworks Model Serving handle ms = project . get_model_serving () my_predictor = ms . create_predictor ( my_model ) my_deployment = my_predictor . deploy () # or my_deployment = ms . create_deployment ( my_predictor ) my_deployment . save ()","title":"Option B: Using the Model Serving handle"},{"location":"user_guides/mlops/serving/deployment/#api-reference","text":"Model Serving","title":"API Reference"},{"location":"user_guides/mlops/serving/deployment/#model-artifact","text":"A model artifact is a package containing all of the necessary files for the deployment of a model. It includes the model file(s) and/or custom scripts for loading the model (predictor script) or transforming the model inputs at inference time (the transformer script). When a new deployment is created, a model artifact is generated in two cases: the artifact version in the predictor is set to CREATE (see Artifact Version ) no model artifact with the same files has been created before.","title":"Model Artifact"},{"location":"user_guides/mlops/serving/deployment/#predictor","text":"Predictors are responsible for running the model server that loads the trained model, listens to inference requests and returns prediction results. To learn more about predictors, see the Predictor Guide Note Currently, only one predictor is supported in a deployment. Support for multiple predictors (the inference graphs) is coming soon. Info Model artifacts are assigned an incremental version number, being 0 the version reserved for model artifacts that do not contain predictor or transformer scripts (i.e., shared artifacts containing only the model files).","title":"Predictor"},{"location":"user_guides/mlops/serving/deployment/#transformer","text":"Transformers are used to apply transformations on the model inputs before sending them to the predictor for making predictions using the model. To learn more about transformers, see the Transformer Guide . Warning Transformers are only supported in KServe deployments.","title":"Transformer"},{"location":"user_guides/mlops/serving/deployment/#inference-logger","text":"Inference loggers are deployment components that log inference requests into a Kafka topic for later analysis. To learn about the different logging modes, see the Inference Logger Guide","title":"Inference logger"},{"location":"user_guides/mlops/serving/deployment/#inference-batcher","text":"Inference batcher are deployment component that apply batching to the incoming inference requests for a better throughput-latency trade-off. To learn about the different configuration available for the inference batcher, see the Inference Batcher Guide .","title":"Inference batcher"},{"location":"user_guides/mlops/serving/deployment/#resources","text":"Resources include the number of replicas for the deployment as well as the resources (i.e., memory, CPU, GPU) to be allocated per replica. To learn about the different combinations available, see the Resources Guide .","title":"Resources"},{"location":"user_guides/mlops/serving/deployment/#conclusion","text":"In this guide you learned how to create a deployment.","title":"Conclusion"},{"location":"user_guides/mlops/serving/inference-batcher/","text":"How To Configure Inference Batcher # Introduction # Inference batching can be enabled to increase inference request throughput at the cost of higher latencies. The configuration of the inference batcher depends on the serving tool and the model server used in the deployment. See the compatibility matrix . GUI # Step 1: Create new deployment # If you have at least one model already trained and saved in the Model Registry, navigate to the deployments page by clicking on the Deployments tab on the navigation menu on the left. Deployments navigation tab Once in the deployments page, click on New deployment if there are not existing deployments or on Create new deployment at the top-right corner to open the deployment creation form. Step 2: Go to advanced options # A simplified creation form will appear including the most common deployment fields among all the configuration possible. Inference batching is part of the advanced options of a deployment. To navigate to the advanced creation form, click on Advanced options . Advanced options. Go to advanced deployment creation form Step 3: Configure inference batching # To enable inference batching, click on the Request batching checkbox. Inference batching configuration (default values) If your deployment uses KServe, you can optionally set three additional parameters for the inference batcher: maximum batch size, maximum latency (ms) and timeout (s). Once you are done with the changes, click on Create new deployment at the bottom of the page to create the deployment for your model. Code # Step 1: Connect to Hopsworks # import hopsworks project = hopsworks . login () # get Hopsworks Model Registry handle mr = project . get_model_registry () # get Hopsworks Model Serving handle ms = project . get_model_serving () Step 2: Define an inference logger # from hsml.inference_batcher import InferenceBatcher my_batcher = InferenceBatcher ( enabled = True , # optional max_batch_size = 32 , max_latency = 5000 , # milliseconds timeout = 5 # seconds ) Step 3: Create a deployment with the inference batcher # my_model = mr . get_model ( \"my_model\" , version = 1 ) my_predictor = ms . create_predictor ( my_model , inference_batcher = my_batcher ) my_predictor . deploy () # or my_deployment = ms . create_deployment ( my_predictor ) my_deployment . save () API Reference # Inference Batcher Compatibility matrix # Show supported inference batcher configuration Serving tool Model server Inference batching Fine-grained configuration Docker Flask \u274c - TensorFlow Serving \u2705 \u274c Kubernetes Flask \u274c - TensorFlow Serving \u2705 \u274c KServe Flask \u2705 \u2705 TensorFlow Serving \u2705 \u2705","title":"Inference Batcher"},{"location":"user_guides/mlops/serving/inference-batcher/#how-to-configure-inference-batcher","text":"","title":"How To Configure Inference Batcher"},{"location":"user_guides/mlops/serving/inference-batcher/#introduction","text":"Inference batching can be enabled to increase inference request throughput at the cost of higher latencies. The configuration of the inference batcher depends on the serving tool and the model server used in the deployment. See the compatibility matrix .","title":"Introduction"},{"location":"user_guides/mlops/serving/inference-batcher/#gui","text":"","title":"GUI"},{"location":"user_guides/mlops/serving/inference-batcher/#step-1-create-new-deployment","text":"If you have at least one model already trained and saved in the Model Registry, navigate to the deployments page by clicking on the Deployments tab on the navigation menu on the left. Deployments navigation tab Once in the deployments page, click on New deployment if there are not existing deployments or on Create new deployment at the top-right corner to open the deployment creation form.","title":"Step 1: Create new deployment"},{"location":"user_guides/mlops/serving/inference-batcher/#step-2-go-to-advanced-options","text":"A simplified creation form will appear including the most common deployment fields among all the configuration possible. Inference batching is part of the advanced options of a deployment. To navigate to the advanced creation form, click on Advanced options . Advanced options. Go to advanced deployment creation form","title":"Step 2: Go to advanced options"},{"location":"user_guides/mlops/serving/inference-batcher/#step-3-configure-inference-batching","text":"To enable inference batching, click on the Request batching checkbox. Inference batching configuration (default values) If your deployment uses KServe, you can optionally set three additional parameters for the inference batcher: maximum batch size, maximum latency (ms) and timeout (s). Once you are done with the changes, click on Create new deployment at the bottom of the page to create the deployment for your model.","title":"Step 3: Configure inference batching"},{"location":"user_guides/mlops/serving/inference-batcher/#code","text":"","title":"Code"},{"location":"user_guides/mlops/serving/inference-batcher/#step-1-connect-to-hopsworks","text":"import hopsworks project = hopsworks . login () # get Hopsworks Model Registry handle mr = project . get_model_registry () # get Hopsworks Model Serving handle ms = project . get_model_serving ()","title":"Step 1: Connect to Hopsworks"},{"location":"user_guides/mlops/serving/inference-batcher/#step-2-define-an-inference-logger","text":"from hsml.inference_batcher import InferenceBatcher my_batcher = InferenceBatcher ( enabled = True , # optional max_batch_size = 32 , max_latency = 5000 , # milliseconds timeout = 5 # seconds )","title":"Step 2: Define an inference logger"},{"location":"user_guides/mlops/serving/inference-batcher/#step-3-create-a-deployment-with-the-inference-batcher","text":"my_model = mr . get_model ( \"my_model\" , version = 1 ) my_predictor = ms . create_predictor ( my_model , inference_batcher = my_batcher ) my_predictor . deploy () # or my_deployment = ms . create_deployment ( my_predictor ) my_deployment . save ()","title":"Step 3: Create a deployment with the inference batcher"},{"location":"user_guides/mlops/serving/inference-batcher/#api-reference","text":"Inference Batcher","title":"API Reference"},{"location":"user_guides/mlops/serving/inference-batcher/#compatibility-matrix","text":"Show supported inference batcher configuration Serving tool Model server Inference batching Fine-grained configuration Docker Flask \u274c - TensorFlow Serving \u2705 \u274c Kubernetes Flask \u274c - TensorFlow Serving \u2705 \u274c KServe Flask \u2705 \u2705 TensorFlow Serving \u2705 \u2705","title":"Compatibility matrix"},{"location":"user_guides/mlops/serving/inference-logger/","text":"How To Configure Inference Logging # Introduction # Once a model is deployed and starts making predictions as inference requests arrive, logging model inputs and predictions becomes essential to monitor the health of the model and take action if the model's performance degrades over time. Hopsworks supports logging both inference requests and predictions as events to a Kafka topic for analysis. Topic schemas vary depending on the serving tool. See below GUI # Step 1: Create new deployment # If you have at least one model already trained and saved in the Model Registry, navigate to the deployments page by clicking on the Deployments tab on the navigation menu on the left. Deployments navigation tab Once in the deployments page, click on New deployment if there are not existing deployments or on Create new deployment at the top-right corner to open the deployment creation form. Step 2: Go to advanced options # A simplified creation form will appear including the most common deployment fields among all the configuration possible. Inference logging is part of the advanced options of a deployment. To navigate to the advanced creation form, click on Advanced options . Advanced options. Go to advanced deployment creation form Step 3: Configure inference logging # To enable inference logging, choose CREATE as Kafka topic name to create a new topic, or select an existing topic. If you prefer, you can disable inference logging by selecting NONE . If you decide to create a new topic, select the number of partitions and number of replicas for your topic, or use the default values. Inference logging configuration with a new kafka topic If the deployment is created with KServe enabled, you can specify which inference logs you want to send to the Kafka topic (i.e., MODEL_INPUTS , PREDICTIONS or both) Once you are done with the changes, click on Create new deployment at the bottom of the page to create the deployment for your model. Code # Step 1: Connect to Hopsworks # import hopsworks project = hopsworks . login () # get Hopsworks Model Registry handle mr = project . get_model_registry () # get Hopsworks Model Serving handle ms = project . get_model_serving () Step 2: Define an inference logger # from hsml.inference_logger import InferenceLogger from hsml.kafka_topic import KafkaTopic new_topic = KafkaTopic ( name = \"CREATE\" , # optional num_partitions = 1 , num_replicas = 1 ) my_logger = InferenceLogger ( kafka_topic = new_topic , mode = \"ALL\" ) Use dict for simpler code Similarly, you can create the same logger with: my_logger = InferenceLogger ( kafka_topic = { \"name\" : \"CREATE\" }, mode = \"ALL\" ) Step 3: Create a deployment with the inference logger # my_model = mr . get_model ( \"my_model\" , version = 1 ) my_predictor = ms . create_predictor ( my_model , inference_logger = my_logger ) my_predictor . deploy () # or my_deployment = ms . create_deployment ( my_predictor ) my_deployment . save () API Reference # Inference Logger Topic schema # The schema of Kafka events varies depending on the serving tool. In KServe deployments, model inputs and predictions are logged in separate events, but sharing the same requestId field. In non-KServe deployments, the same event contains both the model input and prediction related to the same inference request. Show kafka topic schemas KServe Docker / Kubernetes { \"fields\" : [ { \"name\" : \"servingId\" , \"type\" : \"int\" }, { \"name\" : \"modelName\" , \"type\" : \"string\" }, { \"name\" : \"modelVersion\" , \"type\" : \"int\" }, { \"name\" : \"requestTimestamp\" , \"type\" : \"long\" }, { \"name\" : \"responseHttpCode\" , \"type\" : \"int\" }, { \"name\" : \"inferenceId\" , \"type\" : \"string\" }, { \"name\" : \"messageType\" , \"type\" : \"string\" }, { \"name\" : \"payload\" , \"type\" : \"string\" } ], \"name\" : \"inferencelog\" , \"type\" : \"record\" } { \"fields\" : [ { \"name\" : \"modelId\" , \"type\" : \"int\" }, { \"name\" : \"modelName\" , \"type\" : \"string\" }, { \"name\" : \"modelVersion\" , \"type\" : \"int\" }, { \"name\" : \"requestTimestamp\" , \"type\" : \"long\" }, { \"name\" : \"responseHttpCode\" , \"type\" : \"int\" }, { \"name\" : \"inferenceRequest\" , \"type\" : \"string\" }, { \"name\" : \"inferenceResponse\" , \"type\" : \"string\" }, { \"name\" : \"modelServer\" , \"type\" : \"string\" }, { \"name\" : \"servingTool\" , \"type\" : \"string\" } ], \"name\" : \"inferencelog\" , \"type\" : \"record\" }","title":"Inference Logger"},{"location":"user_guides/mlops/serving/inference-logger/#how-to-configure-inference-logging","text":"","title":"How To Configure Inference Logging"},{"location":"user_guides/mlops/serving/inference-logger/#introduction","text":"Once a model is deployed and starts making predictions as inference requests arrive, logging model inputs and predictions becomes essential to monitor the health of the model and take action if the model's performance degrades over time. Hopsworks supports logging both inference requests and predictions as events to a Kafka topic for analysis. Topic schemas vary depending on the serving tool. See below","title":"Introduction"},{"location":"user_guides/mlops/serving/inference-logger/#gui","text":"","title":"GUI"},{"location":"user_guides/mlops/serving/inference-logger/#step-1-create-new-deployment","text":"If you have at least one model already trained and saved in the Model Registry, navigate to the deployments page by clicking on the Deployments tab on the navigation menu on the left. Deployments navigation tab Once in the deployments page, click on New deployment if there are not existing deployments or on Create new deployment at the top-right corner to open the deployment creation form.","title":"Step 1: Create new deployment"},{"location":"user_guides/mlops/serving/inference-logger/#step-2-go-to-advanced-options","text":"A simplified creation form will appear including the most common deployment fields among all the configuration possible. Inference logging is part of the advanced options of a deployment. To navigate to the advanced creation form, click on Advanced options . Advanced options. Go to advanced deployment creation form","title":"Step 2: Go to advanced options"},{"location":"user_guides/mlops/serving/inference-logger/#step-3-configure-inference-logging","text":"To enable inference logging, choose CREATE as Kafka topic name to create a new topic, or select an existing topic. If you prefer, you can disable inference logging by selecting NONE . If you decide to create a new topic, select the number of partitions and number of replicas for your topic, or use the default values. Inference logging configuration with a new kafka topic If the deployment is created with KServe enabled, you can specify which inference logs you want to send to the Kafka topic (i.e., MODEL_INPUTS , PREDICTIONS or both) Once you are done with the changes, click on Create new deployment at the bottom of the page to create the deployment for your model.","title":"Step 3: Configure inference logging"},{"location":"user_guides/mlops/serving/inference-logger/#code","text":"","title":"Code"},{"location":"user_guides/mlops/serving/inference-logger/#step-1-connect-to-hopsworks","text":"import hopsworks project = hopsworks . login () # get Hopsworks Model Registry handle mr = project . get_model_registry () # get Hopsworks Model Serving handle ms = project . get_model_serving ()","title":"Step 1: Connect to Hopsworks"},{"location":"user_guides/mlops/serving/inference-logger/#step-2-define-an-inference-logger","text":"from hsml.inference_logger import InferenceLogger from hsml.kafka_topic import KafkaTopic new_topic = KafkaTopic ( name = \"CREATE\" , # optional num_partitions = 1 , num_replicas = 1 ) my_logger = InferenceLogger ( kafka_topic = new_topic , mode = \"ALL\" ) Use dict for simpler code Similarly, you can create the same logger with: my_logger = InferenceLogger ( kafka_topic = { \"name\" : \"CREATE\" }, mode = \"ALL\" )","title":"Step 2: Define an inference logger"},{"location":"user_guides/mlops/serving/inference-logger/#step-3-create-a-deployment-with-the-inference-logger","text":"my_model = mr . get_model ( \"my_model\" , version = 1 ) my_predictor = ms . create_predictor ( my_model , inference_logger = my_logger ) my_predictor . deploy () # or my_deployment = ms . create_deployment ( my_predictor ) my_deployment . save ()","title":"Step 3: Create a deployment with the inference logger"},{"location":"user_guides/mlops/serving/inference-logger/#api-reference","text":"Inference Logger","title":"API Reference"},{"location":"user_guides/mlops/serving/inference-logger/#topic-schema","text":"The schema of Kafka events varies depending on the serving tool. In KServe deployments, model inputs and predictions are logged in separate events, but sharing the same requestId field. In non-KServe deployments, the same event contains both the model input and prediction related to the same inference request. Show kafka topic schemas KServe Docker / Kubernetes { \"fields\" : [ { \"name\" : \"servingId\" , \"type\" : \"int\" }, { \"name\" : \"modelName\" , \"type\" : \"string\" }, { \"name\" : \"modelVersion\" , \"type\" : \"int\" }, { \"name\" : \"requestTimestamp\" , \"type\" : \"long\" }, { \"name\" : \"responseHttpCode\" , \"type\" : \"int\" }, { \"name\" : \"inferenceId\" , \"type\" : \"string\" }, { \"name\" : \"messageType\" , \"type\" : \"string\" }, { \"name\" : \"payload\" , \"type\" : \"string\" } ], \"name\" : \"inferencelog\" , \"type\" : \"record\" } { \"fields\" : [ { \"name\" : \"modelId\" , \"type\" : \"int\" }, { \"name\" : \"modelName\" , \"type\" : \"string\" }, { \"name\" : \"modelVersion\" , \"type\" : \"int\" }, { \"name\" : \"requestTimestamp\" , \"type\" : \"long\" }, { \"name\" : \"responseHttpCode\" , \"type\" : \"int\" }, { \"name\" : \"inferenceRequest\" , \"type\" : \"string\" }, { \"name\" : \"inferenceResponse\" , \"type\" : \"string\" }, { \"name\" : \"modelServer\" , \"type\" : \"string\" }, { \"name\" : \"servingTool\" , \"type\" : \"string\" } ], \"name\" : \"inferencelog\" , \"type\" : \"record\" }","title":"Topic schema"},{"location":"user_guides/mlops/serving/predictor/","text":"How To Configure A Predictor # Introduction # In this guide, you will learn how to configure a predictor for a trained model. Warning This guide assumes that a model has already been trained and saved into the Model Registry. To learn how to create a model in the Model Registry, see Model Registry Guide Predictors are the main component of deployments. They are responsible for running a model server that loads a trained model, handles inference requests and returns predictions. They can be configured to use different model servers, serving tools, log specific inference data or scale differently. In each predictor, you can configure the following components: Model server Serving tool Custom script Transformer Inference Logger Inference Batcher Resources GUI # Step 1: Create new deployment # If you have at least one model already trained and saved in the Model Registry, navigate to the deployments page by clicking on the Deployments tab on the navigation menu on the left. Deployments navigation tab Once in the deployments page, click on New deployment if there are not existing deployments or on Create new deployment at the top-right corner to open the deployment creation form. Step 2: Choose a model server # A simplified creation form will appear, including the most common deployment fields among all the configuration possible. These fields include the model server and custom script (for python models). Simplified deployment creation forms for TensorFlow models (left) and Python models (right) Moreover, you can optionally select a predictor script (see Step 3 ), enable KServe (see Step 4 ) or change other advanced configuration (see Step 5 ). Otherwise, click on Create new deployment to create the deployment for your model. Step 3 (Optional): Select a predictor script # For python models, if you want to use your own predictor script click on From project and navigate through the file system to find it, or click on Upload new file to upload a predictor script now. Select a predictor script in the simplified deployment form Step 4 (Optional): Enable KServe # Other configuration such as the serving tool, is part of the advanced options of a deployment. To navigate to the advanced creation form, click on Advanced options . Advanced options. Go to advanced deployment creation form Here, you change the serving tool for your deployment by enabling or disabling the KServe checkbox. KServe checkbox in the advanced deployment form Step 5 (Optional): Other advanced options # Additionally, you can adjust the default values of the rest of components: Predictor components Transformer Inference logger Inference batcher Resources Once you are done with the changes, click on Create new deployment at the bottom of the page to create the deployment for your model. Code # Step 1: Connect to Hopsworks # import hopsworks project = hopsworks . login () # get Hopsworks Model Registry handle mr = project . get_model_registry () # get Hopsworks Model Serving handle ms = project . get_model_serving () Step 2 (Optional): Implement predictor script # Python class Predict ( object ): def __init__ ( self ): \"\"\" Initialization code goes here: - Download the model artifact - Load the model \"\"\" pass def predict ( self , inputs ): \"\"\" Serve predictions using the trained model\"\"\" pass Jupyter magic In a jupyter notebook, you can add %%writefile my_predictor.py at the top of the cell to save it as a local file. Step 3 (Optional): Upload the script to your project # You can also use the UI to upload your predictor script. See above uploaded_file_path = dataset_api . upload ( \"my_predictor.py\" , \"Resources\" , overwrite = True ) predictor_script_path = os . path . join ( \"/Projects\" , project . name , uploaded_file_path ) Step 4: Define predictor # my_model = mr . get_model ( \"my_model\" , version = 1 ) my_predictor = ms . create_predictor ( my_model , # optional model_server = \"PYTHON\" , serving_tool = \"KSERVE\" , script_file = predictor_script_path ) Step 3: Create a deployment with the predictor # my_deployment = my_predictor . deploy () # or my_deployment = ms . create_deployment ( my_predictor ) my_deployment . save () API Reference # Predictor Model Server # Hopsworks Model Serving currently supports deploying models with a Flask server for python-based models or TensorFlow Serving for TensorFlow / Keras models. Support for TorchServe for running PyTorch models is coming soon. Today, you can deploy PyTorch models as python-based models. Show supported model servers Model Server Supported ML Frameworks Flask \u2705 python-based (scikit-learn, xgboost, pytorch...) TensorFlow Serving \u2705 keras, tensorflow TorchServe \u274c pytorch Serving tool # In Hopsworks, model servers can be deployed in three different ways: directly on Docker, on Kubernetes deployments or using KServe inference services. Although the same models can be deployed in either of our two serving tools (Python or KServe), the use of KServe is highly recommended. The following is a comparitive table showing the features supported by each of them. Show serving tools comparison Feature / requirement Docker Kubernetes (enterprise) KServe (enterprise) Autoscaling (scale-out) \u274c \u2705 \u2705 Resource allocation \u2796 fixed \u2796 min. resources \u2705 min / max. resources Inference logging \u2796 simple \u2796 simple \u2705 fine-grained Inference batching \u2796 partially \u2796 partially \u2705 Scale-to-zero \u274c \u274c \u2705 after 30s of inactivity) Transformers \u274c \u274c \u2705 Low-latency predictions \u274c \u274c \u2705 Multiple models \u274c \u274c \u2796 (python-based) Custom predictor required (python-only) \u2705 \u2705 \u274c Custom script # Depending on the model server and serving tool used in the deployment, you can provide your own python script to load the model and make predictions. Show supported custom predictors Serving tool Model server Custom predictor script Docker Flask \u2705 (required) TensorFlow Serving \u274c Kubernetes Flask \u2705 (required) TensorFlow Serving \u274c KServe Flask \u2705 (only required for artifacts with multiple models) TensorFlow Serving \u274c Environment variables # A number of different environment variables is available in the predictor to ease its implementation. Show environment variables Name Description ARTIFACT_FILES_PATH Local path to the model artifact files DEPLOYMENT_NAME Name of the current deployment MODEL_NAME Name of the model being served by the current deployment MODEL_VERSION Version of the model being served by the current deployment ARTIFACT_VERSION Version of the model artifact being served by the current deployment Transformer # Transformers are used to apply transformations on the model inputs before sending them to the predictor for making predictions using the model. To learn more about transformers, see the Transformer Guide . Warning Transformers are only supported in KServe deployments. Inference logger # Inference loggers are deployment components that log inference requests into a Kafka topic for later analysis. To learn about the different logging modes, see the Inference Logger Guide Inference batcher # Inference batcher are deployment component that apply batching to the incoming inference requests for a better throughput-latency trade-off. To learn about the different configuration available for the inference batcher, see the Inference Batcher Guide . Resources # Resources include the number of replicas for the deployment as well as the resources (i.e., memory, CPU, GPU) to be allocated per replica. To learn about the different combinations available, see the Resources Guide . Conclusion # In this guide you learned how to configure a predictor.","title":"Predictor"},{"location":"user_guides/mlops/serving/predictor/#how-to-configure-a-predictor","text":"","title":"How To Configure A Predictor"},{"location":"user_guides/mlops/serving/predictor/#introduction","text":"In this guide, you will learn how to configure a predictor for a trained model. Warning This guide assumes that a model has already been trained and saved into the Model Registry. To learn how to create a model in the Model Registry, see Model Registry Guide Predictors are the main component of deployments. They are responsible for running a model server that loads a trained model, handles inference requests and returns predictions. They can be configured to use different model servers, serving tools, log specific inference data or scale differently. In each predictor, you can configure the following components: Model server Serving tool Custom script Transformer Inference Logger Inference Batcher Resources","title":"Introduction"},{"location":"user_guides/mlops/serving/predictor/#gui","text":"","title":"GUI"},{"location":"user_guides/mlops/serving/predictor/#step-1-create-new-deployment","text":"If you have at least one model already trained and saved in the Model Registry, navigate to the deployments page by clicking on the Deployments tab on the navigation menu on the left. Deployments navigation tab Once in the deployments page, click on New deployment if there are not existing deployments or on Create new deployment at the top-right corner to open the deployment creation form.","title":"Step 1: Create new deployment"},{"location":"user_guides/mlops/serving/predictor/#step-2-choose-a-model-server","text":"A simplified creation form will appear, including the most common deployment fields among all the configuration possible. These fields include the model server and custom script (for python models). Simplified deployment creation forms for TensorFlow models (left) and Python models (right) Moreover, you can optionally select a predictor script (see Step 3 ), enable KServe (see Step 4 ) or change other advanced configuration (see Step 5 ). Otherwise, click on Create new deployment to create the deployment for your model.","title":"Step 2: Choose a model server"},{"location":"user_guides/mlops/serving/predictor/#step-3-optional-select-a-predictor-script","text":"For python models, if you want to use your own predictor script click on From project and navigate through the file system to find it, or click on Upload new file to upload a predictor script now. Select a predictor script in the simplified deployment form","title":"Step 3 (Optional): Select a predictor script"},{"location":"user_guides/mlops/serving/predictor/#step-4-optional-enable-kserve","text":"Other configuration such as the serving tool, is part of the advanced options of a deployment. To navigate to the advanced creation form, click on Advanced options . Advanced options. Go to advanced deployment creation form Here, you change the serving tool for your deployment by enabling or disabling the KServe checkbox. KServe checkbox in the advanced deployment form","title":"Step 4 (Optional): Enable KServe"},{"location":"user_guides/mlops/serving/predictor/#step-5-optional-other-advanced-options","text":"Additionally, you can adjust the default values of the rest of components: Predictor components Transformer Inference logger Inference batcher Resources Once you are done with the changes, click on Create new deployment at the bottom of the page to create the deployment for your model.","title":"Step 5 (Optional): Other advanced options"},{"location":"user_guides/mlops/serving/predictor/#code","text":"","title":"Code"},{"location":"user_guides/mlops/serving/predictor/#step-1-connect-to-hopsworks","text":"import hopsworks project = hopsworks . login () # get Hopsworks Model Registry handle mr = project . get_model_registry () # get Hopsworks Model Serving handle ms = project . get_model_serving ()","title":"Step 1: Connect to Hopsworks"},{"location":"user_guides/mlops/serving/predictor/#step-2-optional-implement-predictor-script","text":"Python class Predict ( object ): def __init__ ( self ): \"\"\" Initialization code goes here: - Download the model artifact - Load the model \"\"\" pass def predict ( self , inputs ): \"\"\" Serve predictions using the trained model\"\"\" pass Jupyter magic In a jupyter notebook, you can add %%writefile my_predictor.py at the top of the cell to save it as a local file.","title":"Step 2 (Optional): Implement predictor script"},{"location":"user_guides/mlops/serving/predictor/#step-3-optional-upload-the-script-to-your-project","text":"You can also use the UI to upload your predictor script. See above uploaded_file_path = dataset_api . upload ( \"my_predictor.py\" , \"Resources\" , overwrite = True ) predictor_script_path = os . path . join ( \"/Projects\" , project . name , uploaded_file_path )","title":"Step 3 (Optional): Upload the script to your project"},{"location":"user_guides/mlops/serving/predictor/#step-4-define-predictor","text":"my_model = mr . get_model ( \"my_model\" , version = 1 ) my_predictor = ms . create_predictor ( my_model , # optional model_server = \"PYTHON\" , serving_tool = \"KSERVE\" , script_file = predictor_script_path )","title":"Step 4: Define predictor"},{"location":"user_guides/mlops/serving/predictor/#step-3-create-a-deployment-with-the-predictor","text":"my_deployment = my_predictor . deploy () # or my_deployment = ms . create_deployment ( my_predictor ) my_deployment . save ()","title":"Step 3: Create a deployment with the predictor"},{"location":"user_guides/mlops/serving/predictor/#api-reference","text":"Predictor","title":"API Reference"},{"location":"user_guides/mlops/serving/predictor/#model-server","text":"Hopsworks Model Serving currently supports deploying models with a Flask server for python-based models or TensorFlow Serving for TensorFlow / Keras models. Support for TorchServe for running PyTorch models is coming soon. Today, you can deploy PyTorch models as python-based models. Show supported model servers Model Server Supported ML Frameworks Flask \u2705 python-based (scikit-learn, xgboost, pytorch...) TensorFlow Serving \u2705 keras, tensorflow TorchServe \u274c pytorch","title":"Model Server"},{"location":"user_guides/mlops/serving/predictor/#serving-tool","text":"In Hopsworks, model servers can be deployed in three different ways: directly on Docker, on Kubernetes deployments or using KServe inference services. Although the same models can be deployed in either of our two serving tools (Python or KServe), the use of KServe is highly recommended. The following is a comparitive table showing the features supported by each of them. Show serving tools comparison Feature / requirement Docker Kubernetes (enterprise) KServe (enterprise) Autoscaling (scale-out) \u274c \u2705 \u2705 Resource allocation \u2796 fixed \u2796 min. resources \u2705 min / max. resources Inference logging \u2796 simple \u2796 simple \u2705 fine-grained Inference batching \u2796 partially \u2796 partially \u2705 Scale-to-zero \u274c \u274c \u2705 after 30s of inactivity) Transformers \u274c \u274c \u2705 Low-latency predictions \u274c \u274c \u2705 Multiple models \u274c \u274c \u2796 (python-based) Custom predictor required (python-only) \u2705 \u2705 \u274c","title":"Serving tool"},{"location":"user_guides/mlops/serving/predictor/#custom-script","text":"Depending on the model server and serving tool used in the deployment, you can provide your own python script to load the model and make predictions. Show supported custom predictors Serving tool Model server Custom predictor script Docker Flask \u2705 (required) TensorFlow Serving \u274c Kubernetes Flask \u2705 (required) TensorFlow Serving \u274c KServe Flask \u2705 (only required for artifacts with multiple models) TensorFlow Serving \u274c","title":"Custom script"},{"location":"user_guides/mlops/serving/predictor/#environment-variables","text":"A number of different environment variables is available in the predictor to ease its implementation. Show environment variables Name Description ARTIFACT_FILES_PATH Local path to the model artifact files DEPLOYMENT_NAME Name of the current deployment MODEL_NAME Name of the model being served by the current deployment MODEL_VERSION Version of the model being served by the current deployment ARTIFACT_VERSION Version of the model artifact being served by the current deployment","title":"Environment variables"},{"location":"user_guides/mlops/serving/predictor/#transformer","text":"Transformers are used to apply transformations on the model inputs before sending them to the predictor for making predictions using the model. To learn more about transformers, see the Transformer Guide . Warning Transformers are only supported in KServe deployments.","title":"Transformer"},{"location":"user_guides/mlops/serving/predictor/#inference-logger","text":"Inference loggers are deployment components that log inference requests into a Kafka topic for later analysis. To learn about the different logging modes, see the Inference Logger Guide","title":"Inference logger"},{"location":"user_guides/mlops/serving/predictor/#inference-batcher","text":"Inference batcher are deployment component that apply batching to the incoming inference requests for a better throughput-latency trade-off. To learn about the different configuration available for the inference batcher, see the Inference Batcher Guide .","title":"Inference batcher"},{"location":"user_guides/mlops/serving/predictor/#resources","text":"Resources include the number of replicas for the deployment as well as the resources (i.e., memory, CPU, GPU) to be allocated per replica. To learn about the different combinations available, see the Resources Guide .","title":"Resources"},{"location":"user_guides/mlops/serving/predictor/#conclusion","text":"In this guide you learned how to configure a predictor.","title":"Conclusion"},{"location":"user_guides/mlops/serving/resources/","text":"How To Allocate Resources For A Deployment # Introduction # Depending on the serving tool used to deploy a trained model, resource allocation can be configured at different levels. While deployments on Docker containers only support a fixed number of resources (CPU and memory), using Kubernetes or KServe allows a better exploitation of the resources available in the platform, by enabling you to specify how many CPUs, GPUs, and memory are allocated to a deployment. See the compatibility matrix . GUI # Step 1: Create new deployment # If you have at least one model already trained and saved in the Model Registry, navigate to the deployments page by clicking on the Deployments tab on the navigation menu on the left. Deployments navigation tab Once in the deployments page, click on New deployment if there are not existing deployments or on Create new deployment at the top-right corner to open the deployment creation form. Step 2: Go to advanced options # A simplified creation form will appear including the most common deployment fields among all the configuration possible. Resource allocation is part of the advanced options of a deployment. To navigate to the advanced creation form, click on Advanced options . Advanced options. Go to advanced deployment creation form Step 3: Configure resource allocation # In the Resource allocation section of the form, you can optionally set the resources to be allocated to the predictor and/or the transformer (if available). Moreover, you can choose the minimum number of replicas for each of these components. Scale-to-zero capabilities Deployments with KServe enabled can scale to zero by choosing 0 as the number of instances. Resource allocation for the predictor and transformer Once you are done with the changes, click on Create new deployment at the bottom of the page to create the deployment for your model. Code # Step 1: Connect to Hopsworks # import hopsworks project = hopsworks . login () # get Hopsworks Model Registry handle mr = project . get_model_registry () # get Hopsworks Model Serving handle ms = project . get_model_serving () Step 2: Define the predictor resource configuration # from hsml.resources import PredictorResources , Resources minimum_res = Resources ( cores = 1 , memory = 128 , gpus = 1 ) maximum_res = Resources ( cores = 2 , memory = 256 , gpus = 1 ) predictor_res = PredictorResources ( num_instances = 1 , requests = minimum_res , limits = maximum_res ) Step 3 (Optional): Define the transformer resource configuration # from hsml.resources import TransformerResources minimum_res = Resources ( cores = 1 , memory = 128 , gpus = 1 ) maximum_res = Resources ( cores = 2 , memory = 256 , gpus = 1 ) transformer_res = TransformerResources ( num_instances = 2 , requests = minimum_res , limits = maximum_res ) Step 4: Create a deployment with the resource configuration # my_model = mr . get_model ( \"my_model\" , version = 1 ) my_predictor = ms . create_predictor ( my_model , resources = predictor_res , # transformer=Transformer(script_file, # resources=transformer_res) ) my_predictor . deploy () # or my_deployment = ms . create_deployment ( my_predictor ) my_deployment . save () API Reference # Resource Allocation Compatibility matrix # Show supported resource allocation configuration Serving tool Component Resources Docker Predictor Fixed Transformer \u274c Kubernetes Predictor Minimum resources Transformer \u274c KServe Predictor Minimum / maximum resources Transformer Minimum / maximum resources","title":"Resource Allocation"},{"location":"user_guides/mlops/serving/resources/#how-to-allocate-resources-for-a-deployment","text":"","title":"How To Allocate Resources For A Deployment"},{"location":"user_guides/mlops/serving/resources/#introduction","text":"Depending on the serving tool used to deploy a trained model, resource allocation can be configured at different levels. While deployments on Docker containers only support a fixed number of resources (CPU and memory), using Kubernetes or KServe allows a better exploitation of the resources available in the platform, by enabling you to specify how many CPUs, GPUs, and memory are allocated to a deployment. See the compatibility matrix .","title":"Introduction"},{"location":"user_guides/mlops/serving/resources/#gui","text":"","title":"GUI"},{"location":"user_guides/mlops/serving/resources/#step-1-create-new-deployment","text":"If you have at least one model already trained and saved in the Model Registry, navigate to the deployments page by clicking on the Deployments tab on the navigation menu on the left. Deployments navigation tab Once in the deployments page, click on New deployment if there are not existing deployments or on Create new deployment at the top-right corner to open the deployment creation form.","title":"Step 1: Create new deployment"},{"location":"user_guides/mlops/serving/resources/#step-2-go-to-advanced-options","text":"A simplified creation form will appear including the most common deployment fields among all the configuration possible. Resource allocation is part of the advanced options of a deployment. To navigate to the advanced creation form, click on Advanced options . Advanced options. Go to advanced deployment creation form","title":"Step 2: Go to advanced options"},{"location":"user_guides/mlops/serving/resources/#step-3-configure-resource-allocation","text":"In the Resource allocation section of the form, you can optionally set the resources to be allocated to the predictor and/or the transformer (if available). Moreover, you can choose the minimum number of replicas for each of these components. Scale-to-zero capabilities Deployments with KServe enabled can scale to zero by choosing 0 as the number of instances. Resource allocation for the predictor and transformer Once you are done with the changes, click on Create new deployment at the bottom of the page to create the deployment for your model.","title":"Step 3: Configure resource allocation"},{"location":"user_guides/mlops/serving/resources/#code","text":"","title":"Code"},{"location":"user_guides/mlops/serving/resources/#step-1-connect-to-hopsworks","text":"import hopsworks project = hopsworks . login () # get Hopsworks Model Registry handle mr = project . get_model_registry () # get Hopsworks Model Serving handle ms = project . get_model_serving ()","title":"Step 1: Connect to Hopsworks"},{"location":"user_guides/mlops/serving/resources/#step-2-define-the-predictor-resource-configuration","text":"from hsml.resources import PredictorResources , Resources minimum_res = Resources ( cores = 1 , memory = 128 , gpus = 1 ) maximum_res = Resources ( cores = 2 , memory = 256 , gpus = 1 ) predictor_res = PredictorResources ( num_instances = 1 , requests = minimum_res , limits = maximum_res )","title":"Step 2: Define the predictor resource configuration"},{"location":"user_guides/mlops/serving/resources/#step-3-optional-define-the-transformer-resource-configuration","text":"from hsml.resources import TransformerResources minimum_res = Resources ( cores = 1 , memory = 128 , gpus = 1 ) maximum_res = Resources ( cores = 2 , memory = 256 , gpus = 1 ) transformer_res = TransformerResources ( num_instances = 2 , requests = minimum_res , limits = maximum_res )","title":"Step 3 (Optional): Define the transformer resource configuration"},{"location":"user_guides/mlops/serving/resources/#step-4-create-a-deployment-with-the-resource-configuration","text":"my_model = mr . get_model ( \"my_model\" , version = 1 ) my_predictor = ms . create_predictor ( my_model , resources = predictor_res , # transformer=Transformer(script_file, # resources=transformer_res) ) my_predictor . deploy () # or my_deployment = ms . create_deployment ( my_predictor ) my_deployment . save ()","title":"Step 4: Create a deployment with the resource configuration"},{"location":"user_guides/mlops/serving/resources/#api-reference","text":"Resource Allocation","title":"API Reference"},{"location":"user_guides/mlops/serving/resources/#compatibility-matrix","text":"Show supported resource allocation configuration Serving tool Component Resources Docker Predictor Fixed Transformer \u274c Kubernetes Predictor Minimum resources Transformer \u274c KServe Predictor Minimum / maximum resources Transformer Minimum / maximum resources","title":"Compatibility matrix"},{"location":"user_guides/mlops/serving/transformer/","text":"How To Configure A Transformer # Introduction # In this guide, you will learn how to configure a transformer in a deployment. Transformers are used to apply transformations on the model inputs before sending them to the predictor for making predictions using the model. They run on a built-in Flask server provided by Hopsworks and require a custom python script implementing the Transformer class . Warning Transformers are only supported in deployments using KServe as serving tool. A transformer has two configurable components: Custom script Resources See examples of transformer scripts in the serving example notebooks . GUI # Step 1: Create new deployment # If you have at least one model already trained and saved in the Model Registry, navigate to the deployments page by clicking on the Deployments tab on the navigation menu on the left. Deployments navigation tab Once in the deployments page, click on New deployment if there are not existing deployments or on Create new deployment at the top-right corner to open the deployment creation form. Step 2: Go to advanced options # A simplified creation form will appear including the most common deployment fields among all the configuration possible. Transformers are part of the advanced options of a deployment. To navigate to the advanced creation form, click on Advanced options . Advanced options. Go to advanced deployment creation form Step 3: Select a transformer script # Transformers require KServe as the serving platform for the deployment. Make sure that KServe is enabled for this deployment by activating the corresponding checkbox. Enable KServe in the advanced deployment form Then, if the transformer script is already located in Hopsworks, click on From project and navigate through the file system to find your script. Otherwise, you can click on Upload new file to upload the transformer script now. Choose a transformer script in the advanced deployment form After selecting the transformer script, you can optionally configure resource allocation for your transformer (see Step 4 ). Otherwise, click on Create new deployment to create the deployment for your model. Step 4 (Optional): Configure resource allocation # At the end of the page, you can configure the resources to be allocated for the transformer, as well as the minimum and maximum number of replicas to be deployed. Scale-to-zero capabilities Deployments with KServe enabled can scale to zero by choosing 0 as the number of instances. Resource allocation for the transformer Once you are done with the changes, click on Create new deployment at the bottom of the page to create the deployment for your model. Code # Step 1: Connect to Hopsworks # import hopsworks project = hopsworks . login () # get Dataset API instance dataset_api = project . get_dataset_api () # get Hopsworks Model Serving handle ms = project . get_model_serving () Step 2: Implement transformer script # Python class Transformer ( object ): def __init__ ( self ): \"\"\" Initialization code goes here \"\"\" pass def preprocess ( self , inputs ): \"\"\" Transform the requests inputs here. The object returned by this method will be used as model input to make predictions. \"\"\" return inputs def postprocess ( self , outputs ): \"\"\" Transform the predictions computed by the model before returning a response \"\"\" return outputs Jupyter magic In a jupyter notebook, you can add %%writefile my_transformer.py at the top of the cell to save it as a local file. Step 3: Upload the script to your project # You can also use the UI to upload your transformer script. See above uploaded_file_path = dataset_api . upload ( \"my_transformer.py\" , \"Resources\" , overwrite = True ) transformer_script_path = os . path . join ( \"/Projects\" , project . name , uploaded_file_path ) Step 4: Define a transformer # my_transformer = ms . create_transformer ( script_file = uploaded_file_path ) # or from hsml.transformer import Transformer my_transformer = Transformer ( script_file ) Step 5: Create a deployment with the transformer # my_predictor = ms . create_predictor ( transformer = my_transformer ) my_deployment = my_predictor . deploy () # or my_deployment = ms . create_deployment ( my_predictor , transformer = my_transformer ) my_deployment . save () API Reference # Transformer Resources # Resources include the number of replicas for the deployment as well as the resources (i.e., memory, CPU, GPU) to be allocated per replica. To learn about the different combinations available, see the Resources Guide . Environment variables # A number of different environment variables is available in the transformer to ease its implementation. Show environment variables Name Description ARTIFACT_FILES_PATH Local path to the model artifact files DEPLOYMENT_NAME Name of the current deployment MODEL_NAME Name of the model being served by the current deployment MODEL_VERSION Version of the model being served by the current deployment ARTIFACT_VERSION Version of the model artifact being served by the current deployment Conclusion # In this guide you learned how to configure a transformer.","title":"Transformer"},{"location":"user_guides/mlops/serving/transformer/#how-to-configure-a-transformer","text":"","title":"How To Configure A Transformer"},{"location":"user_guides/mlops/serving/transformer/#introduction","text":"In this guide, you will learn how to configure a transformer in a deployment. Transformers are used to apply transformations on the model inputs before sending them to the predictor for making predictions using the model. They run on a built-in Flask server provided by Hopsworks and require a custom python script implementing the Transformer class . Warning Transformers are only supported in deployments using KServe as serving tool. A transformer has two configurable components: Custom script Resources See examples of transformer scripts in the serving example notebooks .","title":"Introduction"},{"location":"user_guides/mlops/serving/transformer/#gui","text":"","title":"GUI"},{"location":"user_guides/mlops/serving/transformer/#step-1-create-new-deployment","text":"If you have at least one model already trained and saved in the Model Registry, navigate to the deployments page by clicking on the Deployments tab on the navigation menu on the left. Deployments navigation tab Once in the deployments page, click on New deployment if there are not existing deployments or on Create new deployment at the top-right corner to open the deployment creation form.","title":"Step 1: Create new deployment"},{"location":"user_guides/mlops/serving/transformer/#step-2-go-to-advanced-options","text":"A simplified creation form will appear including the most common deployment fields among all the configuration possible. Transformers are part of the advanced options of a deployment. To navigate to the advanced creation form, click on Advanced options . Advanced options. Go to advanced deployment creation form","title":"Step 2: Go to advanced options"},{"location":"user_guides/mlops/serving/transformer/#step-3-select-a-transformer-script","text":"Transformers require KServe as the serving platform for the deployment. Make sure that KServe is enabled for this deployment by activating the corresponding checkbox. Enable KServe in the advanced deployment form Then, if the transformer script is already located in Hopsworks, click on From project and navigate through the file system to find your script. Otherwise, you can click on Upload new file to upload the transformer script now. Choose a transformer script in the advanced deployment form After selecting the transformer script, you can optionally configure resource allocation for your transformer (see Step 4 ). Otherwise, click on Create new deployment to create the deployment for your model.","title":"Step 3: Select a transformer script"},{"location":"user_guides/mlops/serving/transformer/#step-4-optional-configure-resource-allocation","text":"At the end of the page, you can configure the resources to be allocated for the transformer, as well as the minimum and maximum number of replicas to be deployed. Scale-to-zero capabilities Deployments with KServe enabled can scale to zero by choosing 0 as the number of instances. Resource allocation for the transformer Once you are done with the changes, click on Create new deployment at the bottom of the page to create the deployment for your model.","title":"Step 4 (Optional): Configure resource allocation"},{"location":"user_guides/mlops/serving/transformer/#code","text":"","title":"Code"},{"location":"user_guides/mlops/serving/transformer/#step-1-connect-to-hopsworks","text":"import hopsworks project = hopsworks . login () # get Dataset API instance dataset_api = project . get_dataset_api () # get Hopsworks Model Serving handle ms = project . get_model_serving ()","title":"Step 1: Connect to Hopsworks"},{"location":"user_guides/mlops/serving/transformer/#step-2-implement-transformer-script","text":"Python class Transformer ( object ): def __init__ ( self ): \"\"\" Initialization code goes here \"\"\" pass def preprocess ( self , inputs ): \"\"\" Transform the requests inputs here. The object returned by this method will be used as model input to make predictions. \"\"\" return inputs def postprocess ( self , outputs ): \"\"\" Transform the predictions computed by the model before returning a response \"\"\" return outputs Jupyter magic In a jupyter notebook, you can add %%writefile my_transformer.py at the top of the cell to save it as a local file.","title":"Step 2: Implement transformer script"},{"location":"user_guides/mlops/serving/transformer/#step-3-upload-the-script-to-your-project","text":"You can also use the UI to upload your transformer script. See above uploaded_file_path = dataset_api . upload ( \"my_transformer.py\" , \"Resources\" , overwrite = True ) transformer_script_path = os . path . join ( \"/Projects\" , project . name , uploaded_file_path )","title":"Step 3: Upload the script to your project"},{"location":"user_guides/mlops/serving/transformer/#step-4-define-a-transformer","text":"my_transformer = ms . create_transformer ( script_file = uploaded_file_path ) # or from hsml.transformer import Transformer my_transformer = Transformer ( script_file )","title":"Step 4: Define a transformer"},{"location":"user_guides/mlops/serving/transformer/#step-5-create-a-deployment-with-the-transformer","text":"my_predictor = ms . create_predictor ( transformer = my_transformer ) my_deployment = my_predictor . deploy () # or my_deployment = ms . create_deployment ( my_predictor , transformer = my_transformer ) my_deployment . save ()","title":"Step 5: Create a deployment with the transformer"},{"location":"user_guides/mlops/serving/transformer/#api-reference","text":"Transformer","title":"API Reference"},{"location":"user_guides/mlops/serving/transformer/#resources","text":"Resources include the number of replicas for the deployment as well as the resources (i.e., memory, CPU, GPU) to be allocated per replica. To learn about the different combinations available, see the Resources Guide .","title":"Resources"},{"location":"user_guides/mlops/serving/transformer/#environment-variables","text":"A number of different environment variables is available in the transformer to ease its implementation. Show environment variables Name Description ARTIFACT_FILES_PATH Local path to the model artifact files DEPLOYMENT_NAME Name of the current deployment MODEL_NAME Name of the model being served by the current deployment MODEL_VERSION Version of the model being served by the current deployment ARTIFACT_VERSION Version of the model artifact being served by the current deployment","title":"Environment variables"},{"location":"user_guides/mlops/serving/transformer/#conclusion","text":"In this guide you learned how to configure a transformer.","title":"Conclusion"},{"location":"user_guides/mlops/serving/troubleshooting/","text":"How To Troubleshoot A Deployment # Introduction # In this guide, you will learn how to troubleshoot a deployment that is having issues to serve a trained model. But before that, it is important to understand how deployment states are defined and the possible transitions between conditions. When a deployment is starting, it follows an ordered sequence of states before becoming ready for serving predictions. Similarly, it follows an ordered sequence of states when being stopped, although with fewer steps. GUI # Step 1: Inspect deployment status # If you have at least one deployment already created, navigate to the deployments page by clicking on the Deployments tab on the navigation menu on the left. Deployments navigation tab Once in the deployments page, find the deployment you want to inspect. Next to the actions buttons, you can find an indicator showing the current status of the deployment. For a more descriptive representation, this indicator changes its color based on the status. To inspect the condition of the deployment, click on the name of the deployment to open the deployment overview page. Step 2: Inspect condition # At the top of page, you can find the same status indicator mentioned in the previous step. Below it, a one-line message is shown with a more detailed description of the deployment status. This message is built using the current status condition of the deployment. Oftentimes, the status and the one-line description are enough to understand the current state of a deployment. For instance, when the cluster lacks enough allocatable resources to meet the deployment requirements, a meaningful error message will be shown with the root cause. Condition of a deployment that cannot be scheduled However, when the deployment fails to start futher details might be needed depending on the source of failure. For example, failures in the initialization or starting steps will show a less relevant message. In those cases, you can explore the deployments logs in search of the cause of the problem. Condition of a deployment that fails to start Step 3: Explore transient logs # Each deployment is composed of several components depending on its configuration and the model being served. Transient logs refer to component-specific logs that are directly retrieved from the component itself. Therefore, these logs can only be retrieved as long as the deployment components are reachable. Transient logs are informative and fast to retrieve, facilitating the troubleshooting of deployment components at a glance Transient logs are convenient when access to the most recent logs of a deployment is needed. Info When a deployment is in idle state, there are no components running (i.e., scaled to zero) and, thus, no transient logs are available. Note In the current version of Hopsworks, transient logs can only be accessed using the Hopsworks Machine Learning Python library. See an example here . Step 4: Explore historical logs # Transient logs are continuously collected and stored in OpenSearch, where they become historical logs accessible using the integrated OpenSearch Dashboards. Therefore, historical logs contain the same information than transient logs. However, there might be cases where transient logs could not be collected in time for a specific component and, thus, not included in the historical logs. Historical logs are persisted transient logs that can be queried, filtered and sorted using OpenSearch Dashboards, facilitating a more sophisticated exploration of past records. Historical logs are convenient when a deployment fails occasionally, either at inference time or without a clear reason. In this case, narrowing the inspection of component-specific logs at a concrete point in time and searching for keywords can be helpful. To access the OpenSearch Dashboards, click on the See logs button at the top of the deployment overview page. Access to historical logs of a deployment Note In case you are not familiar with the interface, you may find the official documentation useful. Once in the OpenSearch Dashboards, you can search for keywords, apply multiple filters and sort the records by timestamp. Show available filters Filter Description component Name of the deployment component (i.e., predictor or transformer) container_name Name of the container within a component (i.e., kserve-container, storage-initializer, inference-logger) serving_name Name of the deployment model_name Name of the model being served model_version Version of the model being served timestamp Timestamp when the record was reported Code # Step 1: Connect to Hopsworks # import hopsworks project = hopsworks . login () # get Hopsworks Model Serving handle ms = project . get_model_serving () Step 2: Retrieve an existing deployment # deployment = ms . get_deployment ( \"mydeployment\" ) Step 3: Get current deployment state # state = deployment . get_state () state . describe () Step 4: Explore transient logs # deployment . get_logs ( component = \"predictor|transformer\" , tail = 10 ) API Reference # Deployment PredictorState","title":"Troubleshooting"},{"location":"user_guides/mlops/serving/troubleshooting/#how-to-troubleshoot-a-deployment","text":"","title":"How To Troubleshoot A Deployment"},{"location":"user_guides/mlops/serving/troubleshooting/#introduction","text":"In this guide, you will learn how to troubleshoot a deployment that is having issues to serve a trained model. But before that, it is important to understand how deployment states are defined and the possible transitions between conditions. When a deployment is starting, it follows an ordered sequence of states before becoming ready for serving predictions. Similarly, it follows an ordered sequence of states when being stopped, although with fewer steps.","title":"Introduction"},{"location":"user_guides/mlops/serving/troubleshooting/#gui","text":"","title":"GUI"},{"location":"user_guides/mlops/serving/troubleshooting/#step-1-inspect-deployment-status","text":"If you have at least one deployment already created, navigate to the deployments page by clicking on the Deployments tab on the navigation menu on the left. Deployments navigation tab Once in the deployments page, find the deployment you want to inspect. Next to the actions buttons, you can find an indicator showing the current status of the deployment. For a more descriptive representation, this indicator changes its color based on the status. To inspect the condition of the deployment, click on the name of the deployment to open the deployment overview page.","title":"Step 1: Inspect deployment status"},{"location":"user_guides/mlops/serving/troubleshooting/#step-2-inspect-condition","text":"At the top of page, you can find the same status indicator mentioned in the previous step. Below it, a one-line message is shown with a more detailed description of the deployment status. This message is built using the current status condition of the deployment. Oftentimes, the status and the one-line description are enough to understand the current state of a deployment. For instance, when the cluster lacks enough allocatable resources to meet the deployment requirements, a meaningful error message will be shown with the root cause. Condition of a deployment that cannot be scheduled However, when the deployment fails to start futher details might be needed depending on the source of failure. For example, failures in the initialization or starting steps will show a less relevant message. In those cases, you can explore the deployments logs in search of the cause of the problem. Condition of a deployment that fails to start","title":"Step 2: Inspect condition"},{"location":"user_guides/mlops/serving/troubleshooting/#step-3-explore-transient-logs","text":"Each deployment is composed of several components depending on its configuration and the model being served. Transient logs refer to component-specific logs that are directly retrieved from the component itself. Therefore, these logs can only be retrieved as long as the deployment components are reachable. Transient logs are informative and fast to retrieve, facilitating the troubleshooting of deployment components at a glance Transient logs are convenient when access to the most recent logs of a deployment is needed. Info When a deployment is in idle state, there are no components running (i.e., scaled to zero) and, thus, no transient logs are available. Note In the current version of Hopsworks, transient logs can only be accessed using the Hopsworks Machine Learning Python library. See an example here .","title":"Step 3: Explore transient logs"},{"location":"user_guides/mlops/serving/troubleshooting/#step-4-explore-historical-logs","text":"Transient logs are continuously collected and stored in OpenSearch, where they become historical logs accessible using the integrated OpenSearch Dashboards. Therefore, historical logs contain the same information than transient logs. However, there might be cases where transient logs could not be collected in time for a specific component and, thus, not included in the historical logs. Historical logs are persisted transient logs that can be queried, filtered and sorted using OpenSearch Dashboards, facilitating a more sophisticated exploration of past records. Historical logs are convenient when a deployment fails occasionally, either at inference time or without a clear reason. In this case, narrowing the inspection of component-specific logs at a concrete point in time and searching for keywords can be helpful. To access the OpenSearch Dashboards, click on the See logs button at the top of the deployment overview page. Access to historical logs of a deployment Note In case you are not familiar with the interface, you may find the official documentation useful. Once in the OpenSearch Dashboards, you can search for keywords, apply multiple filters and sort the records by timestamp. Show available filters Filter Description component Name of the deployment component (i.e., predictor or transformer) container_name Name of the container within a component (i.e., kserve-container, storage-initializer, inference-logger) serving_name Name of the deployment model_name Name of the model being served model_version Version of the model being served timestamp Timestamp when the record was reported","title":"Step 4: Explore historical logs"},{"location":"user_guides/mlops/serving/troubleshooting/#code","text":"","title":"Code"},{"location":"user_guides/mlops/serving/troubleshooting/#step-1-connect-to-hopsworks","text":"import hopsworks project = hopsworks . login () # get Hopsworks Model Serving handle ms = project . get_model_serving ()","title":"Step 1: Connect to Hopsworks"},{"location":"user_guides/mlops/serving/troubleshooting/#step-2-retrieve-an-existing-deployment","text":"deployment = ms . get_deployment ( \"mydeployment\" )","title":"Step 2: Retrieve an existing deployment"},{"location":"user_guides/mlops/serving/troubleshooting/#step-3-get-current-deployment-state","text":"state = deployment . get_state () state . describe ()","title":"Step 3: Get current deployment state"},{"location":"user_guides/mlops/serving/troubleshooting/#step-4-explore-transient-logs","text":"deployment . get_logs ( component = \"predictor|transformer\" , tail = 10 )","title":"Step 4: Explore transient logs"},{"location":"user_guides/mlops/serving/troubleshooting/#api-reference","text":"Deployment PredictorState","title":"API Reference"},{"location":"user_guides/mlops/vector_database/","text":"How To Use OpenSearch k-NN plugin # Introduction # The k-NN plugin enables users to search for the k-nearest neighbors to a query point across an index of vectors. To determine the neighbors, you can specify the space (the distance function) you want to use to measure the distance between points. Use cases include recommendations (for example, an \u201cother songs you might like\u201d feature in a music application), image recognition, and fraud detection. Limited to internal Jobs and Notebooks Currently it's only possible to configure the opensearch-py client in a job or jupyter notebook running inside the Hopsworks cluster. Code # In this guide, you will learn how to create a simple recommendation application, using the k-NN plugin in OpenSearch. Step 1: Get the OpenSearch API # import hopsworks project = hopsworks . login () opensearch_api = project . get_opensearch_api () Step 2: Configure the opensearch-py client # from opensearchpy import OpenSearch client = OpenSearch ( ** opensearch_api . get_default_py_config ()) Step 3: Create an index # Create an index to use by calling opensearch_api.get_project_index(..) . knn_index_name = opensearch_api . get_project_index ( \"demo_knn_index\" ) index_body = { \"settings\" : { \"knn\" : True , \"knn.algo_param.ef_search\" : 100 , }, \"mappings\" : { \"properties\" : { \"my_vector1\" : { \"type\" : \"knn_vector\" , \"dimension\" : 2 } } } } response = client . indices . create ( knn_index_name , body = index_body ) print ( response ) Step 4: Bulk ingestion of vectors # Ingest 10 vectors in a bulk fashion to the index. These vectors represent the list of vectors to calculate the similarity for. from opensearchpy.helpers import bulk import random actions = [ { \"_index\" : knn_index_name , \"_id\" : count , \"_source\" : { \"my_vector1\" : [ random . uniform ( 0 , 10 ), random . uniform ( 0 , 10 )], } } for count in range ( 0 , 10 ) ] bulk ( client , actions , ) Step 5: Score vector similarity # Score the vector [2.5, 3] and find the 3 most similar vectors. # Define the search request query = { \"size\" : 3 , \"query\" : { \"knn\" : { \"my_vector1\" : { \"vector\" : [ 2.5 , 3 ], \"k\" : 3 } } } } # Perform the similarity search response = client . search ( body = query , index = knn_index_name ) # Pretty print response import pprint pp = pprint . PrettyPrinter () pp . pprint ( response ) Output from the above script shows the score for each of the three most similar vectors that have been indexed. [4.798869166444522, 4.069064892468535] is the most similar vector to [2.5, 3] with a score of 0.1346312 . 2022 -05-30 09 :55:50,529 INFO: POST https://10.0.2.15:9200/my_project_demo_knn_index/_search [ status:200 request:0.017s ] { '_shards' : { 'failed' : 0 , 'skipped' : 0 , 'successful' : 1 , 'total' : 1 } , 'hits' : { 'hits' : [{ '_id' : '9' , '_index' : 'my_project_demo_knn_index' , '_score' : 0 .1346312, '_source' : { 'my_vector1' : [ 4 .798869166444522, 4 .069064892468535 ]} , '_type' : '_doc' } , { '_id' : '0' , '_index' : 'my_project_demo_knn_index' , '_score' : 0 .040784083, '_source' : { 'my_vector1' : [ 6 .267438489652193, 6 .0538134453735175 ]} , '_type' : '_doc' } , { '_id' : '7' , '_index' : 'my_project_demo_knn_index' , '_score' : 0 .03222388, '_source' : { 'my_vector1' : [ 7 .973873201006634, 2 .7361877621502115 ]} , '_type' : '_doc' }] , 'max_score' : 0 .1346312, 'total' : { 'relation' : 'eq' , 'value' : 3 }} , 'timed_out' : False, 'took' : 9 } API Reference # k-NN plugin OpenSearch Conclusion # In this guide you learned how to create a simple recommendation application.","title":"Vector Database"},{"location":"user_guides/mlops/vector_database/#how-to-use-opensearch-k-nn-plugin","text":"","title":"How To Use OpenSearch k-NN plugin"},{"location":"user_guides/mlops/vector_database/#introduction","text":"The k-NN plugin enables users to search for the k-nearest neighbors to a query point across an index of vectors. To determine the neighbors, you can specify the space (the distance function) you want to use to measure the distance between points. Use cases include recommendations (for example, an \u201cother songs you might like\u201d feature in a music application), image recognition, and fraud detection. Limited to internal Jobs and Notebooks Currently it's only possible to configure the opensearch-py client in a job or jupyter notebook running inside the Hopsworks cluster.","title":"Introduction"},{"location":"user_guides/mlops/vector_database/#code","text":"In this guide, you will learn how to create a simple recommendation application, using the k-NN plugin in OpenSearch.","title":"Code"},{"location":"user_guides/mlops/vector_database/#step-1-get-the-opensearch-api","text":"import hopsworks project = hopsworks . login () opensearch_api = project . get_opensearch_api ()","title":"Step 1: Get the OpenSearch API"},{"location":"user_guides/mlops/vector_database/#step-2-configure-the-opensearch-py-client","text":"from opensearchpy import OpenSearch client = OpenSearch ( ** opensearch_api . get_default_py_config ())","title":"Step 2: Configure the opensearch-py client"},{"location":"user_guides/mlops/vector_database/#step-3-create-an-index","text":"Create an index to use by calling opensearch_api.get_project_index(..) . knn_index_name = opensearch_api . get_project_index ( \"demo_knn_index\" ) index_body = { \"settings\" : { \"knn\" : True , \"knn.algo_param.ef_search\" : 100 , }, \"mappings\" : { \"properties\" : { \"my_vector1\" : { \"type\" : \"knn_vector\" , \"dimension\" : 2 } } } } response = client . indices . create ( knn_index_name , body = index_body ) print ( response )","title":"Step 3: Create an index"},{"location":"user_guides/mlops/vector_database/#step-4-bulk-ingestion-of-vectors","text":"Ingest 10 vectors in a bulk fashion to the index. These vectors represent the list of vectors to calculate the similarity for. from opensearchpy.helpers import bulk import random actions = [ { \"_index\" : knn_index_name , \"_id\" : count , \"_source\" : { \"my_vector1\" : [ random . uniform ( 0 , 10 ), random . uniform ( 0 , 10 )], } } for count in range ( 0 , 10 ) ] bulk ( client , actions , )","title":"Step 4: Bulk ingestion of vectors"},{"location":"user_guides/mlops/vector_database/#step-5-score-vector-similarity","text":"Score the vector [2.5, 3] and find the 3 most similar vectors. # Define the search request query = { \"size\" : 3 , \"query\" : { \"knn\" : { \"my_vector1\" : { \"vector\" : [ 2.5 , 3 ], \"k\" : 3 } } } } # Perform the similarity search response = client . search ( body = query , index = knn_index_name ) # Pretty print response import pprint pp = pprint . PrettyPrinter () pp . pprint ( response ) Output from the above script shows the score for each of the three most similar vectors that have been indexed. [4.798869166444522, 4.069064892468535] is the most similar vector to [2.5, 3] with a score of 0.1346312 . 2022 -05-30 09 :55:50,529 INFO: POST https://10.0.2.15:9200/my_project_demo_knn_index/_search [ status:200 request:0.017s ] { '_shards' : { 'failed' : 0 , 'skipped' : 0 , 'successful' : 1 , 'total' : 1 } , 'hits' : { 'hits' : [{ '_id' : '9' , '_index' : 'my_project_demo_knn_index' , '_score' : 0 .1346312, '_source' : { 'my_vector1' : [ 4 .798869166444522, 4 .069064892468535 ]} , '_type' : '_doc' } , { '_id' : '0' , '_index' : 'my_project_demo_knn_index' , '_score' : 0 .040784083, '_source' : { 'my_vector1' : [ 6 .267438489652193, 6 .0538134453735175 ]} , '_type' : '_doc' } , { '_id' : '7' , '_index' : 'my_project_demo_knn_index' , '_score' : 0 .03222388, '_source' : { 'my_vector1' : [ 7 .973873201006634, 2 .7361877621502115 ]} , '_type' : '_doc' }] , 'max_score' : 0 .1346312, 'total' : { 'relation' : 'eq' , 'value' : 3 }} , 'timed_out' : False, 'took' : 9 }","title":"Step 5: Score vector similarity"},{"location":"user_guides/mlops/vector_database/#api-reference","text":"k-NN plugin OpenSearch","title":"API Reference"},{"location":"user_guides/mlops/vector_database/#conclusion","text":"In this guide you learned how to create a simple recommendation application.","title":"Conclusion"},{"location":"user_guides/projects/","text":"Projects Guides # This section serves to provide guides and examples for the common usage of services in a Project through the Hopsworks UI and APIs. Projects Authentication API Keys Jupyter Jobs Git Python Environment Kafka OpenSearch Secrets","title":"Projects Guides"},{"location":"user_guides/projects/#projects-guides","text":"This section serves to provide guides and examples for the common usage of services in a Project through the Hopsworks UI and APIs. Projects Authentication API Keys Jupyter Jobs Git Python Environment Kafka OpenSearch Secrets","title":"Projects Guides"},{"location":"user_guides/projects/airflow/airflow/","text":"Orchestrate Jobs using Apache Airflow # Introduction # Hopsworks jobs can be orchestrated using Apache Airflow . You can define a Airflow DAG (Directed Acyclic Graph) containing the dependencies between Hopsworks jobs. You can then schedule the DAG to be executed at a specific schedule using a cron expression. Airflow DAGs are defined as Python files. Within the Python file, different operators can be used to trigger different actions. Hopsworks provides an operator to execute jobs on Hopsworks and a sensor to wait for a specific job to finish. Use Apache Airflow in Hopsworks # Hopsworks deployments include a deployment of Apache Airflow. You can access it from the Hopsworks UI by clicking on the Airflow button on the left menu. Airfow is configured to enforce Role Based Access Control (RBAC) to the Airflow DAGs. Admin users on Hopsworks have access to all the DAGs in the deployment. Regular users can access all the DAGs of the projects they are a member of. Access Control Airflow does not have any knowledge of the Hopsworks project you are currently working on. As such, when opening the Airflow UI, you will see all the DAGs all of the projects you are a member of. Hopsworks DAG Builder # Airflow DAG Builder You can create a new Airflow DAG to orchestrate jobs using the Hopsworks DAG builder tool. Click on New Workflow to create a new Airflow DAG. You should provide a name for the DAG as well as a schedule interval. You can define the schedule using the dropdown menus or by providing a cron expression. You can add to the DAG Hopsworks operators and sensors: Operator : The operator is used to trigger a job execution. When configuring the operator you select the job you want to execute and you can optionally provide execution arguments. You can decide whether or not the operator should wait for the execution to be completed. If you select the wait option, the operator will block and Airflow will not execute any parallel task. If you select the wait option the Airflow task fails if the job fails. If you want to execute tasks in parallel, you should not select the wait option but instead use the sensor. When configuring the operator, you can can also provide which other Airflow tasks it depends on. If you add a dependency, the task will be executed only after the upstream tasks have been executed successfully. Sensor : The sensor can be used to wait for executions to be completed. Similarly to the wait option of the operator, the sensor blocks until the job execution is completed. The sensor can be used to launch several jobs in parallel and wait for their execution to be completed. Please note that the sensor is defined at the job level rather than the execution level. The sensor will wait for the most recent execution to be completed and it will fail the Airflow task if the execution was not successful. You can then create the DAG and Hopsworks will generate the Python file. Write your own DAG # If you prefer to code the DAGs or you want to edit a DAG built with the builder tool, you can do so. The Airflow DAGs are stored in the Airflow dataset which you can access using the file browser in the project settings. When writing the code for the DAG you can invoke the operator as follows: HopsworksLaunchOperator ( dag = dag , task_id = \"profiles_fg_0\" , project_name = \"airflow_doc\" , job_name = \"profiles_fg\" , job_arguments = \"\" , wait_for_completion = True ) You should provide the name of the Airflow task ( task_id ) and the Hopsworks job information ( project_name , job_name , job_arguments ). You can set the wait_for_completion flag to True if you want the operator to block and wait for the job execution to be finished. Similarly, you can invoke the sensor as shown below. You should provide the name of the Airflow task ( task_id ) and the Hopsworks job information ( project_name , job_name ) HopsworksJobSuccessSensor ( dag = dag , task_id = 'wait_for_profiles_fg' , project_name = \"airflow_doc\" , job_name = 'profiles_fg' ) When writing the DAG file, you should also add the access_control parameter to the DAG configuration. The access_control parameter specicifies which projects have access to the DAG and which actions the project members can perform on it. If you do not specify the access_control option, project members will not be able to see the DAG in the Airflow UI. Admin access The access_control configuration does not apply to Hopsworks admin users which have full access to all the DAGs even if they are not member of the project. dag = DAG ( dag_id = \"example_dag\" , default_args = args , access_control = { \"project_name\" : { \"can_dag_read\" , \"can_dag_edit\" }, }, schedule_interval = \"0 4 * * *\" ) Project Name You should replace the project_name in the snippet above with the name of your own project Manage Airflow DAGs using Git # You can leverage the Git integration to track your Airflow DAGs in a git repository. Airflow will only consider the DAG files which are stored in the Airflow Dataset in Hopsworks. After cloning the git repository in Hopsworks, you can automate the process of copying the DAG file in the Airflow Dataset using the copy method of the Hopsworks API.","title":"Airflow"},{"location":"user_guides/projects/airflow/airflow/#orchestrate-jobs-using-apache-airflow","text":"","title":"Orchestrate Jobs using Apache Airflow"},{"location":"user_guides/projects/airflow/airflow/#introduction","text":"Hopsworks jobs can be orchestrated using Apache Airflow . You can define a Airflow DAG (Directed Acyclic Graph) containing the dependencies between Hopsworks jobs. You can then schedule the DAG to be executed at a specific schedule using a cron expression. Airflow DAGs are defined as Python files. Within the Python file, different operators can be used to trigger different actions. Hopsworks provides an operator to execute jobs on Hopsworks and a sensor to wait for a specific job to finish.","title":"Introduction"},{"location":"user_guides/projects/airflow/airflow/#use-apache-airflow-in-hopsworks","text":"Hopsworks deployments include a deployment of Apache Airflow. You can access it from the Hopsworks UI by clicking on the Airflow button on the left menu. Airfow is configured to enforce Role Based Access Control (RBAC) to the Airflow DAGs. Admin users on Hopsworks have access to all the DAGs in the deployment. Regular users can access all the DAGs of the projects they are a member of. Access Control Airflow does not have any knowledge of the Hopsworks project you are currently working on. As such, when opening the Airflow UI, you will see all the DAGs all of the projects you are a member of.","title":"Use Apache Airflow in Hopsworks"},{"location":"user_guides/projects/airflow/airflow/#hopsworks-dag-builder","text":"Airflow DAG Builder You can create a new Airflow DAG to orchestrate jobs using the Hopsworks DAG builder tool. Click on New Workflow to create a new Airflow DAG. You should provide a name for the DAG as well as a schedule interval. You can define the schedule using the dropdown menus or by providing a cron expression. You can add to the DAG Hopsworks operators and sensors: Operator : The operator is used to trigger a job execution. When configuring the operator you select the job you want to execute and you can optionally provide execution arguments. You can decide whether or not the operator should wait for the execution to be completed. If you select the wait option, the operator will block and Airflow will not execute any parallel task. If you select the wait option the Airflow task fails if the job fails. If you want to execute tasks in parallel, you should not select the wait option but instead use the sensor. When configuring the operator, you can can also provide which other Airflow tasks it depends on. If you add a dependency, the task will be executed only after the upstream tasks have been executed successfully. Sensor : The sensor can be used to wait for executions to be completed. Similarly to the wait option of the operator, the sensor blocks until the job execution is completed. The sensor can be used to launch several jobs in parallel and wait for their execution to be completed. Please note that the sensor is defined at the job level rather than the execution level. The sensor will wait for the most recent execution to be completed and it will fail the Airflow task if the execution was not successful. You can then create the DAG and Hopsworks will generate the Python file.","title":"Hopsworks DAG Builder"},{"location":"user_guides/projects/airflow/airflow/#write-your-own-dag","text":"If you prefer to code the DAGs or you want to edit a DAG built with the builder tool, you can do so. The Airflow DAGs are stored in the Airflow dataset which you can access using the file browser in the project settings. When writing the code for the DAG you can invoke the operator as follows: HopsworksLaunchOperator ( dag = dag , task_id = \"profiles_fg_0\" , project_name = \"airflow_doc\" , job_name = \"profiles_fg\" , job_arguments = \"\" , wait_for_completion = True ) You should provide the name of the Airflow task ( task_id ) and the Hopsworks job information ( project_name , job_name , job_arguments ). You can set the wait_for_completion flag to True if you want the operator to block and wait for the job execution to be finished. Similarly, you can invoke the sensor as shown below. You should provide the name of the Airflow task ( task_id ) and the Hopsworks job information ( project_name , job_name ) HopsworksJobSuccessSensor ( dag = dag , task_id = 'wait_for_profiles_fg' , project_name = \"airflow_doc\" , job_name = 'profiles_fg' ) When writing the DAG file, you should also add the access_control parameter to the DAG configuration. The access_control parameter specicifies which projects have access to the DAG and which actions the project members can perform on it. If you do not specify the access_control option, project members will not be able to see the DAG in the Airflow UI. Admin access The access_control configuration does not apply to Hopsworks admin users which have full access to all the DAGs even if they are not member of the project. dag = DAG ( dag_id = \"example_dag\" , default_args = args , access_control = { \"project_name\" : { \"can_dag_read\" , \"can_dag_edit\" }, }, schedule_interval = \"0 4 * * *\" ) Project Name You should replace the project_name in the snippet above with the name of your own project","title":"Write your own DAG"},{"location":"user_guides/projects/airflow/airflow/#manage-airflow-dags-using-git","text":"You can leverage the Git integration to track your Airflow DAGs in a git repository. Airflow will only consider the DAG files which are stored in the Airflow Dataset in Hopsworks. After cloning the git repository in Hopsworks, you can automate the process of copying the DAG file in the Airflow Dataset using the copy method of the Hopsworks API.","title":"Manage Airflow DAGs using Git"},{"location":"user_guides/projects/api_key/create_api_key/","text":"How To Create An API Key # Introduction # An API key allows a user or a program to make API calls without having to authenticate with a username and password. To access an endpoint using an API key, a client should send the access token using the ApiKey authentication scheme. The API Key can now be used when connecting to your Hopsworks instance using the hopsworks , hsfs or hsml python library or set in the ApiKey header for the REST API. GET /resource HTTP/1.1 Host: server.hopsworks.ai Authorization: ApiKey <api_key> UI # In this guide, you will learn how to create an API key. Step 1: Navigate to API Keys # In the Account Settings page you can find the API section showing a list of all API keys. List of API Keys Step 2: Create an API Key # Click New Api key , select the required scopes and create it by clicking Create Api Key . Copy the value and save it in a secure location, such as a password manager. Create new API Key Login with API Key using SDK # In this guide you learned how to create an API Key. You can now use the API Key to login using the hopsworks python SDK.","title":"Create API Key"},{"location":"user_guides/projects/api_key/create_api_key/#how-to-create-an-api-key","text":"","title":"How To Create An API Key"},{"location":"user_guides/projects/api_key/create_api_key/#introduction","text":"An API key allows a user or a program to make API calls without having to authenticate with a username and password. To access an endpoint using an API key, a client should send the access token using the ApiKey authentication scheme. The API Key can now be used when connecting to your Hopsworks instance using the hopsworks , hsfs or hsml python library or set in the ApiKey header for the REST API. GET /resource HTTP/1.1 Host: server.hopsworks.ai Authorization: ApiKey <api_key>","title":"Introduction"},{"location":"user_guides/projects/api_key/create_api_key/#ui","text":"In this guide, you will learn how to create an API key.","title":"UI"},{"location":"user_guides/projects/api_key/create_api_key/#step-1-navigate-to-api-keys","text":"In the Account Settings page you can find the API section showing a list of all API keys. List of API Keys","title":"Step 1: Navigate to API Keys"},{"location":"user_guides/projects/api_key/create_api_key/#step-2-create-an-api-key","text":"Click New Api key , select the required scopes and create it by clicking Create Api Key . Copy the value and save it in a secure location, such as a password manager. Create new API Key","title":"Step 2: Create an API Key"},{"location":"user_guides/projects/api_key/create_api_key/#login-with-api-key-using-sdk","text":"In this guide you learned how to create an API Key. You can now use the API Key to login using the hopsworks python SDK.","title":"Login with API Key using SDK"},{"location":"user_guides/projects/auth/krb/","text":"Login using Kerberos # Introduction # Hopsworks supports different methods of authentication. Here we will look at authentication using Kerberos. Prerequisites # A Hopsworks cluster with Kerberos authentication. See Configure Kerberos on how to configure Kerberos on your cluster. Step 1: Log in with Kerberos # If Kerberos is configured you will see a Log in using alternative on the login page. Choose Kerberos and click on Go to Hopsworks to login. Log in using Kerberos If password login is disabled you only see the Log in using Kerberos/SSO alternative. Click on Go to Hopsworks to login. Kerberos only authentication To be able to authenticate with Kerberos you need to configure your browser to use Kerberos. Note that without a properly configured browser, the Kerberos token is not sent to the server and so SSO will not work. If Kerberos is not configured properly you will see Wrong credentials message when trying to log in. Missing Kerberos ticket Step 2: Give consent # When logging in with Kerberos for the first time Hopsworks will retrieve and save consented claims (firstname, lastname and email), about the logged in end-user. If you have multiple email addresses registered in Kerberos you can choose one to use with Hopsworks. If you do not want your information to be saved in Hopsworks you can click Cancel . This will redirect you back to the login page. Give consent After clicking on Register you will be redirected to the landing page: Landing page In the landing page, you will find two buttons. Use these buttons to either create a demo project or a new project . Conclusion # In this guide you learned how to log in to Hopsworks using Kerberos.","title":"Kerberos Authentication"},{"location":"user_guides/projects/auth/krb/#login-using-kerberos","text":"","title":"Login using Kerberos"},{"location":"user_guides/projects/auth/krb/#introduction","text":"Hopsworks supports different methods of authentication. Here we will look at authentication using Kerberos.","title":"Introduction"},{"location":"user_guides/projects/auth/krb/#prerequisites","text":"A Hopsworks cluster with Kerberos authentication. See Configure Kerberos on how to configure Kerberos on your cluster.","title":"Prerequisites"},{"location":"user_guides/projects/auth/krb/#step-1-log-in-with-kerberos","text":"If Kerberos is configured you will see a Log in using alternative on the login page. Choose Kerberos and click on Go to Hopsworks to login. Log in using Kerberos If password login is disabled you only see the Log in using Kerberos/SSO alternative. Click on Go to Hopsworks to login. Kerberos only authentication To be able to authenticate with Kerberos you need to configure your browser to use Kerberos. Note that without a properly configured browser, the Kerberos token is not sent to the server and so SSO will not work. If Kerberos is not configured properly you will see Wrong credentials message when trying to log in. Missing Kerberos ticket","title":"Step 1: Log in with Kerberos"},{"location":"user_guides/projects/auth/krb/#step-2-give-consent","text":"When logging in with Kerberos for the first time Hopsworks will retrieve and save consented claims (firstname, lastname and email), about the logged in end-user. If you have multiple email addresses registered in Kerberos you can choose one to use with Hopsworks. If you do not want your information to be saved in Hopsworks you can click Cancel . This will redirect you back to the login page. Give consent After clicking on Register you will be redirected to the landing page: Landing page In the landing page, you will find two buttons. Use these buttons to either create a demo project or a new project .","title":"Step 2: Give consent"},{"location":"user_guides/projects/auth/krb/#conclusion","text":"In this guide you learned how to log in to Hopsworks using Kerberos.","title":"Conclusion"},{"location":"user_guides/projects/auth/ldap/","text":"Login using LDAP # Introduction # Hopsworks supports different methods of authentication. Here we will look at authentication using LDAP. Prerequisites # A Hopsworks cluster with LDAP authentication. See Configure LDAP on how to configure LDAP on your cluster. Step 1: Log in with LDAP # If LDAP is configured you will see a Log in using alternative on the login page. Choose LDAP and type in your username and password then click on Login . Note that you need to use your LDAP credentials. Log in using LDAP Step 2: Give consent # When logging in with LDAP for the first time Hopsworks will retrieve and save consented claims (firstname, lastname and email), about the logged in end-user. If you have multiple email addresses registered in LDAP you can choose one to use with Hopsworks. If you do not want your information to be saved in Hopsworks you can click Cancel . This will redirect you back to the login page. Give consent After clicking on Register you will be redirected to the landing page: Landing page In the landing page, you will find two buttons. Use these buttons to either create a demo project or a new project . Conclusion # In this guide you learned how to log in to Hopsworks using LDAP.","title":"LDAP Authentication"},{"location":"user_guides/projects/auth/ldap/#login-using-ldap","text":"","title":"Login using LDAP"},{"location":"user_guides/projects/auth/ldap/#introduction","text":"Hopsworks supports different methods of authentication. Here we will look at authentication using LDAP.","title":"Introduction"},{"location":"user_guides/projects/auth/ldap/#prerequisites","text":"A Hopsworks cluster with LDAP authentication. See Configure LDAP on how to configure LDAP on your cluster.","title":"Prerequisites"},{"location":"user_guides/projects/auth/ldap/#step-1-log-in-with-ldap","text":"If LDAP is configured you will see a Log in using alternative on the login page. Choose LDAP and type in your username and password then click on Login . Note that you need to use your LDAP credentials. Log in using LDAP","title":"Step 1: Log in with LDAP"},{"location":"user_guides/projects/auth/ldap/#step-2-give-consent","text":"When logging in with LDAP for the first time Hopsworks will retrieve and save consented claims (firstname, lastname and email), about the logged in end-user. If you have multiple email addresses registered in LDAP you can choose one to use with Hopsworks. If you do not want your information to be saved in Hopsworks you can click Cancel . This will redirect you back to the login page. Give consent After clicking on Register you will be redirected to the landing page: Landing page In the landing page, you will find two buttons. Use these buttons to either create a demo project or a new project .","title":"Step 2: Give consent"},{"location":"user_guides/projects/auth/ldap/#conclusion","text":"In this guide you learned how to log in to Hopsworks using LDAP.","title":"Conclusion"},{"location":"user_guides/projects/auth/login/","text":"Log in To Hopsworks # Introduction # Hopsworks supports different methods of authentication. Here we will look at authentication using username and password. Prerequisites # An account on a Hopsworks cluster. Step 1: Log in with email and password # After your account is validated by an administrator you can use your email and password to login. Login with password Step 2: Two-factor authentication # If two-factor authentication is enabled you will be presented with a two-factor authentication window after you enter your password. Use your authenticator app (example. Google Authenticator ) on your phone to get a one-time password. One time password Upon successful login, you will arrive at the landing page: Landing page In the landing page, you will find two buttons. Use these buttons to either create a demo project or a new project . Conclusion # In this guide you learned how to log in to Hopsworks.","title":"Login"},{"location":"user_guides/projects/auth/login/#log-in-to-hopsworks","text":"","title":"Log in To Hopsworks"},{"location":"user_guides/projects/auth/login/#introduction","text":"Hopsworks supports different methods of authentication. Here we will look at authentication using username and password.","title":"Introduction"},{"location":"user_guides/projects/auth/login/#prerequisites","text":"An account on a Hopsworks cluster.","title":"Prerequisites"},{"location":"user_guides/projects/auth/login/#step-1-log-in-with-email-and-password","text":"After your account is validated by an administrator you can use your email and password to login. Login with password","title":"Step 1: Log in with email and password"},{"location":"user_guides/projects/auth/login/#step-2-two-factor-authentication","text":"If two-factor authentication is enabled you will be presented with a two-factor authentication window after you enter your password. Use your authenticator app (example. Google Authenticator ) on your phone to get a one-time password. One time password Upon successful login, you will arrive at the landing page: Landing page In the landing page, you will find two buttons. Use these buttons to either create a demo project or a new project .","title":"Step 2: Two-factor authentication"},{"location":"user_guides/projects/auth/login/#conclusion","text":"In this guide you learned how to log in to Hopsworks.","title":"Conclusion"},{"location":"user_guides/projects/auth/oauth/","text":"Login Using A Third-party Identity Provider # Introduction # Hopsworks supports different methods of authentication. Here we will look at authentication using Third-party Identity Provider. Prerequisites # A Hopsworks cluster with OAuth authentication. See Configure OAuth2 on how to configure OAuth on your cluster. Step 1: Log in with OAuth # If OAuth is configured a Login with button will appear in the login page. Use this button to log in to Hopsworks using your OAuth credentials. Login with OAuth2 Step 2: Give consent # When logging in with OAuth for the first time Hopsworks will retrieve and save consented claims (firstname, lastname and email), about the logged in end-user. Give consent After clicking on Register you will be redirected to the landing page: Landing page In the landing page, you will find two buttons. Use these buttons to either create a demo project or a new project . Conclusion # In this guide you learned how to log in to Hopsworks using Third-party Identity Provider.","title":"OAuth2 Authentication"},{"location":"user_guides/projects/auth/oauth/#login-using-a-third-party-identity-provider","text":"","title":"Login Using A Third-party Identity Provider"},{"location":"user_guides/projects/auth/oauth/#introduction","text":"Hopsworks supports different methods of authentication. Here we will look at authentication using Third-party Identity Provider.","title":"Introduction"},{"location":"user_guides/projects/auth/oauth/#prerequisites","text":"A Hopsworks cluster with OAuth authentication. See Configure OAuth2 on how to configure OAuth on your cluster.","title":"Prerequisites"},{"location":"user_guides/projects/auth/oauth/#step-1-log-in-with-oauth","text":"If OAuth is configured a Login with button will appear in the login page. Use this button to log in to Hopsworks using your OAuth credentials. Login with OAuth2","title":"Step 1: Log in with OAuth"},{"location":"user_guides/projects/auth/oauth/#step-2-give-consent","text":"When logging in with OAuth for the first time Hopsworks will retrieve and save consented claims (firstname, lastname and email), about the logged in end-user. Give consent After clicking on Register you will be redirected to the landing page: Landing page In the landing page, you will find two buttons. Use these buttons to either create a demo project or a new project .","title":"Step 2: Give consent"},{"location":"user_guides/projects/auth/oauth/#conclusion","text":"In this guide you learned how to log in to Hopsworks using Third-party Identity Provider.","title":"Conclusion"},{"location":"user_guides/projects/auth/profile/","text":"Update Your Profile and Credentials # Introduction # A profile is required to access Hopsworks. A profile is created when a user registers and can be updated via Account settings. Prerequisites # An account on a Hopsworks cluster. Updating profile and credentials is not supported if you are using Third-party Identity Providers like Kerberos, LDAP, or OAuth to authenticate to Hopsworks. Step 1: Go to your Account settings # After you have logged in, in the upper right-hand corner of the screen, you will see your name. Click on your name, then click on the menu item Account settings . The account settings page will open with profile tab selected. In this tab you can change your first and last name. You cannot change your email address and will need to create a new account if you wish to change your email address. Update profile Step 2: Update credential # To update your credential go to the Authentication tab as shown in the image below. Update credential Step 3: Enable/Reset Two-factor Authentication # You can also change your two-factor setting in the Authentication tab. Two-factor authentication is only available if it is enabled from the cluster administration page. Enable Two-factor Authentication After enabling or resetting two-factor you will be presented with a QR Code. You will then need to scan the QR code to add it on your phone's authenticator application (example. Google Authenticator ). If you miss this step, you will have to recover your smartphone credentials at a later stage. Register Two-factor Authentication Use the one time password generated by your authenticator app to confirm the registration. Conclusion # In this guide you learned how to edit your profile and credentials.","title":"Update Profile"},{"location":"user_guides/projects/auth/profile/#update-your-profile-and-credentials","text":"","title":"Update Your Profile and Credentials"},{"location":"user_guides/projects/auth/profile/#introduction","text":"A profile is required to access Hopsworks. A profile is created when a user registers and can be updated via Account settings.","title":"Introduction"},{"location":"user_guides/projects/auth/profile/#prerequisites","text":"An account on a Hopsworks cluster. Updating profile and credentials is not supported if you are using Third-party Identity Providers like Kerberos, LDAP, or OAuth to authenticate to Hopsworks.","title":"Prerequisites"},{"location":"user_guides/projects/auth/profile/#step-1-go-to-your-account-settings","text":"After you have logged in, in the upper right-hand corner of the screen, you will see your name. Click on your name, then click on the menu item Account settings . The account settings page will open with profile tab selected. In this tab you can change your first and last name. You cannot change your email address and will need to create a new account if you wish to change your email address. Update profile","title":"Step 1: Go to your Account settings"},{"location":"user_guides/projects/auth/profile/#step-2-update-credential","text":"To update your credential go to the Authentication tab as shown in the image below. Update credential","title":"Step 2: Update credential"},{"location":"user_guides/projects/auth/profile/#step-3-enablereset-two-factor-authentication","text":"You can also change your two-factor setting in the Authentication tab. Two-factor authentication is only available if it is enabled from the cluster administration page. Enable Two-factor Authentication After enabling or resetting two-factor you will be presented with a QR Code. You will then need to scan the QR code to add it on your phone's authenticator application (example. Google Authenticator ). If you miss this step, you will have to recover your smartphone credentials at a later stage. Register Two-factor Authentication Use the one time password generated by your authenticator app to confirm the registration.","title":"Step 3: Enable/Reset Two-factor Authentication"},{"location":"user_guides/projects/auth/profile/#conclusion","text":"In this guide you learned how to edit your profile and credentials.","title":"Conclusion"},{"location":"user_guides/projects/auth/recovery/","text":"Password Recovery # Introduction # This topic describes how to recover a forgotten password. Prerequisites # An account on a Hopsworks cluster. Step 1: Request password reset # If you forget your password start by clicking on Forgot password on the login page. Enter your email and click on the Send reset link button. Password reset Step 2: Use the password reset link # A password reset link will be sent to the email address you entered if the email is found in the system. Click on the reset link to set your new password. Conclusion # In this guide you learned how to recover your password.","title":"Password Recovery"},{"location":"user_guides/projects/auth/recovery/#password-recovery","text":"","title":"Password Recovery"},{"location":"user_guides/projects/auth/recovery/#introduction","text":"This topic describes how to recover a forgotten password.","title":"Introduction"},{"location":"user_guides/projects/auth/recovery/#prerequisites","text":"An account on a Hopsworks cluster.","title":"Prerequisites"},{"location":"user_guides/projects/auth/recovery/#step-1-request-password-reset","text":"If you forget your password start by clicking on Forgot password on the login page. Enter your email and click on the Send reset link button. Password reset","title":"Step 1: Request password reset"},{"location":"user_guides/projects/auth/recovery/#step-2-use-the-password-reset-link","text":"A password reset link will be sent to the email address you entered if the email is found in the system. Click on the reset link to set your new password.","title":"Step 2: Use the password reset link"},{"location":"user_guides/projects/auth/recovery/#conclusion","text":"In this guide you learned how to recover your password.","title":"Conclusion"},{"location":"user_guides/projects/auth/registration/","text":"Register A New Account On Hopsworks # Introduction # Hopsworks supports different methods of authentication. To use username and password as the method of authentication, you first need to register. Prerequisites # Registration enabled Hopsworks cluster. The process for registering a new account is as follows Step 1: Register a new account # Click on the Register button on the login page and register your email address and details. Register new account Step 2: Enable Two-Factor Authentication # If two-factor authentication is required you will be presented with a page like in the figure below. Scan the QR code or type the code in bold to register your account in your authenticator app (example. Google Authenticator ). Add two-factor authentication Step 3: Validate your email address # Validate your email address by clicking on the link in the validation email you received. After your account is created an administrator needs to validate your account before you can log in. Account created Conclusion # In this guide you learned how to create an account in Hopsworks.","title":"Registration"},{"location":"user_guides/projects/auth/registration/#register-a-new-account-on-hopsworks","text":"","title":"Register A New Account On Hopsworks"},{"location":"user_guides/projects/auth/registration/#introduction","text":"Hopsworks supports different methods of authentication. To use username and password as the method of authentication, you first need to register.","title":"Introduction"},{"location":"user_guides/projects/auth/registration/#prerequisites","text":"Registration enabled Hopsworks cluster. The process for registering a new account is as follows","title":"Prerequisites"},{"location":"user_guides/projects/auth/registration/#step-1-register-a-new-account","text":"Click on the Register button on the login page and register your email address and details. Register new account","title":"Step 1: Register a new account"},{"location":"user_guides/projects/auth/registration/#step-2-enable-two-factor-authentication","text":"If two-factor authentication is required you will be presented with a page like in the figure below. Scan the QR code or type the code in bold to register your account in your authenticator app (example. Google Authenticator ). Add two-factor authentication","title":"Step 2: Enable Two-Factor Authentication"},{"location":"user_guides/projects/auth/registration/#step-3-validate-your-email-address","text":"Validate your email address by clicking on the link in the validation email you received. After your account is created an administrator needs to validate your account before you can log in. Account created","title":"Step 3: Validate your email address"},{"location":"user_guides/projects/auth/registration/#conclusion","text":"In this guide you learned how to create an account in Hopsworks.","title":"Conclusion"},{"location":"user_guides/projects/git/clone_repo/","text":"How To Clone a Git Repository # Introduction # Repositories are cloned and managed within the scope of a project. The content of the repository will reside on the Hopsworks File System. The content of the repository can be edited from Jupyter notebooks and can for example be used to configure Jobs. Repositories can be managed from the Git section in the project settings. The Git overview in the project settings provides a list of repositories currently cloned within the project, the location of their content as well which branch and commit their HEAD is currently at. Prerequisites # For cloning a private repository, you should configure a Git Provider with your git credentials. You can clone a GitHub and GitLab public repository without configuring the provider. However, for BitBucket you always need to configure the username and token to clone a repository. UI # Step 1: Navigate to repositories # In the left-hand sidebar found in your project click on Project settings , and then navigate to the Git section. This page lists all the cloned git repositories under Repositories , while operations performed on those repositories, e.g push / pull / commit are listed under Git Executions . Git repository overview Step 2: Clone a repository # To clone a new repository, click on the Clone repository button on the Git overview page. Git clone You should first choose the git provider e.g., GitHub, GitLab or BitBucket. If you are cloning a private repository, remember to configure the username and token for the provder first in Git Provider . The clone dialog also asks you to specify the URL of the repository to clone. The supported protocol is HTTPS. As an example, if the repository is hosted on GitHub, the URL should look like: https://github.com/logicalclocks/hops-examples.git . Then specify which branch you want to clone. By default the main branch will be used, however a different branch or commit can be specified by selecting Clone from a specific branch . You can select the folder, within your project, in which the repository should be cloned. By default, the repository is going to be cloned within the Jupyter dataset. However, by clicking on the location button, a different location can be selected. Finally, click on the Clone repository button to trigger the cloning of the repository. Step 3: Track progress of the clone # The progress of the git clone can be tracked under Git Executions . Track progress of clone Step 4: Browse repository files # In the File browser page you can now browse the files of the cloned repository. In the figure below, the repository is located in Jupyter/hops-examples directory. Browse repository files Code # You can also clone a repository through the hopsworks git API in python. Step 1: Get the git API # import hopsworks project = hopsworks . login () git_api = project . get_git_api () Step 2: Clone the repository # REPO_URL = \"https://github.com/logicalclocks/hops-examples.git\" # git repository HOPSWORKS_FOLDER = \"Jupyter\" # path in Hopsworks filesystem to clone to PROVIDER = \"GitHub\" BRANCH = \"master\" # optional branch to clone examples_repo = git_api . clone ( REPO_URL , HOPSWORKS_FOLDER , PROVIDER , branch = BRANCH ) API Reference # Api reference for git repositories is available here: GitRepo A notebook for managing git can be found here . Errors and Troubleshooting # Invalid credentials # This might happen when the credentials entered for the provider are incorrect. Try the following: Confirm that the settings for the provider ( in Account Settings > Git providers) are correct. You must enter both your Git provider username and token. Confirm that you have selected the correct Git provider when cloning the repository. Ensure your personal access token has the correct repository access rights. Ensure your personal access token has not expired. Timeout errors # Cloning a large repo or checking out a large branch may hit timeout errors. You can try again later if the system was under heavy load at the time. Symlink errors # Git repositories with symlinks are not yet supported, therefore cloning repositories with symlinks will fail. You can create a separate branch to remove the symlinks, and clone from this branch. Conclusion # In this guide you learned how to clone a Git repository. You can now start Jupyter from the cloned git repository path to work with the files.","title":"Clone Repository"},{"location":"user_guides/projects/git/clone_repo/#how-to-clone-a-git-repository","text":"","title":"How To Clone a Git Repository"},{"location":"user_guides/projects/git/clone_repo/#introduction","text":"Repositories are cloned and managed within the scope of a project. The content of the repository will reside on the Hopsworks File System. The content of the repository can be edited from Jupyter notebooks and can for example be used to configure Jobs. Repositories can be managed from the Git section in the project settings. The Git overview in the project settings provides a list of repositories currently cloned within the project, the location of their content as well which branch and commit their HEAD is currently at.","title":"Introduction"},{"location":"user_guides/projects/git/clone_repo/#prerequisites","text":"For cloning a private repository, you should configure a Git Provider with your git credentials. You can clone a GitHub and GitLab public repository without configuring the provider. However, for BitBucket you always need to configure the username and token to clone a repository.","title":"Prerequisites"},{"location":"user_guides/projects/git/clone_repo/#ui","text":"","title":"UI"},{"location":"user_guides/projects/git/clone_repo/#step-1-navigate-to-repositories","text":"In the left-hand sidebar found in your project click on Project settings , and then navigate to the Git section. This page lists all the cloned git repositories under Repositories , while operations performed on those repositories, e.g push / pull / commit are listed under Git Executions . Git repository overview","title":"Step 1: Navigate to repositories"},{"location":"user_guides/projects/git/clone_repo/#step-2-clone-a-repository","text":"To clone a new repository, click on the Clone repository button on the Git overview page. Git clone You should first choose the git provider e.g., GitHub, GitLab or BitBucket. If you are cloning a private repository, remember to configure the username and token for the provder first in Git Provider . The clone dialog also asks you to specify the URL of the repository to clone. The supported protocol is HTTPS. As an example, if the repository is hosted on GitHub, the URL should look like: https://github.com/logicalclocks/hops-examples.git . Then specify which branch you want to clone. By default the main branch will be used, however a different branch or commit can be specified by selecting Clone from a specific branch . You can select the folder, within your project, in which the repository should be cloned. By default, the repository is going to be cloned within the Jupyter dataset. However, by clicking on the location button, a different location can be selected. Finally, click on the Clone repository button to trigger the cloning of the repository.","title":"Step 2: Clone a repository"},{"location":"user_guides/projects/git/clone_repo/#step-3-track-progress-of-the-clone","text":"The progress of the git clone can be tracked under Git Executions . Track progress of clone","title":"Step 3: Track progress of the clone"},{"location":"user_guides/projects/git/clone_repo/#step-4-browse-repository-files","text":"In the File browser page you can now browse the files of the cloned repository. In the figure below, the repository is located in Jupyter/hops-examples directory. Browse repository files","title":"Step 4: Browse repository files"},{"location":"user_guides/projects/git/clone_repo/#code","text":"You can also clone a repository through the hopsworks git API in python.","title":"Code"},{"location":"user_guides/projects/git/clone_repo/#step-1-get-the-git-api","text":"import hopsworks project = hopsworks . login () git_api = project . get_git_api ()","title":"Step 1: Get the git API"},{"location":"user_guides/projects/git/clone_repo/#step-2-clone-the-repository","text":"REPO_URL = \"https://github.com/logicalclocks/hops-examples.git\" # git repository HOPSWORKS_FOLDER = \"Jupyter\" # path in Hopsworks filesystem to clone to PROVIDER = \"GitHub\" BRANCH = \"master\" # optional branch to clone examples_repo = git_api . clone ( REPO_URL , HOPSWORKS_FOLDER , PROVIDER , branch = BRANCH )","title":"Step 2: Clone the repository"},{"location":"user_guides/projects/git/clone_repo/#api-reference","text":"Api reference for git repositories is available here: GitRepo A notebook for managing git can be found here .","title":"API Reference"},{"location":"user_guides/projects/git/clone_repo/#errors-and-troubleshooting","text":"","title":"Errors and Troubleshooting"},{"location":"user_guides/projects/git/clone_repo/#invalid-credentials","text":"This might happen when the credentials entered for the provider are incorrect. Try the following: Confirm that the settings for the provider ( in Account Settings > Git providers) are correct. You must enter both your Git provider username and token. Confirm that you have selected the correct Git provider when cloning the repository. Ensure your personal access token has the correct repository access rights. Ensure your personal access token has not expired.","title":"Invalid credentials"},{"location":"user_guides/projects/git/clone_repo/#timeout-errors","text":"Cloning a large repo or checking out a large branch may hit timeout errors. You can try again later if the system was under heavy load at the time.","title":"Timeout errors"},{"location":"user_guides/projects/git/clone_repo/#symlink-errors","text":"Git repositories with symlinks are not yet supported, therefore cloning repositories with symlinks will fail. You can create a separate branch to remove the symlinks, and clone from this branch.","title":"Symlink errors"},{"location":"user_guides/projects/git/clone_repo/#conclusion","text":"In this guide you learned how to clone a Git repository. You can now start Jupyter from the cloned git repository path to work with the files.","title":"Conclusion"},{"location":"user_guides/projects/git/configure_git_provider/","text":"How To Configure a Git Provider # Introduction # When you perform Git operations on Hopsworks that need to interact with the remote repository, Hopsworks relies on the Git HTTPS protocol to perform those operations. Authentication with the remote repository happens through a token generated by the Git repository hosting service (GitHub, GitLab, BitBucket). Token permissions The token permissions should grant access to public and private repositories including read and write access to repository contents and commit statuses. If you are using the new GitHub access tokens, make sure you choose the correct Resource owner when generating the token for the repositories you will want to clone. For the Repository permissions of the new GitHub fine-grained token, you should atleast give read and write access to Commit statuses and Contents . UI # Documentation on how to generate a token for the supported Git hosting services is available here: GitHub GitLab BitBucket Step 1: Navigate to Git Providers # You can access the Git Providers page of your Hopsworks cluster by clicking on your name, in the top right corner, and choosing Account Settings from the dropdown menu. The Git providers section displays which providers have been already configured and can be used to clone new repositories. Git provider configuration list Step 2: Configure a provider # Click on Edit Configuration to change a provider username or token, or to configure a new provider. Tick the checkbox next to the provider you want to configure and insert the username and the token to use for that provider. Git provider configuration Click Create Configuration to save the configuration. Step 3: Provider is configured # The configured provider should now be marked as configured. Git provider configured Code # You can also configure a git provider using the hopsworks git API in python. Step 1: Get the git API # import hopsworks project = hopsworks . login () git_api = project . get_git_api () Step 2: Configure git provider # PROVIDER = \"GitHub\" GITHUB_USER = \"my_user\" API_TOKEN = \"my_token\" git_api . set_provider ( PROVIDER , GITHUB_USER , API_TOKEN ) API Reference # GitProvider Conclusion # In this guide you learned how configure your git provider credentials. You can now use the credentials to clone a repository from the configured provider.","title":"Configure Git Provider"},{"location":"user_guides/projects/git/configure_git_provider/#how-to-configure-a-git-provider","text":"","title":"How To Configure a Git Provider"},{"location":"user_guides/projects/git/configure_git_provider/#introduction","text":"When you perform Git operations on Hopsworks that need to interact with the remote repository, Hopsworks relies on the Git HTTPS protocol to perform those operations. Authentication with the remote repository happens through a token generated by the Git repository hosting service (GitHub, GitLab, BitBucket). Token permissions The token permissions should grant access to public and private repositories including read and write access to repository contents and commit statuses. If you are using the new GitHub access tokens, make sure you choose the correct Resource owner when generating the token for the repositories you will want to clone. For the Repository permissions of the new GitHub fine-grained token, you should atleast give read and write access to Commit statuses and Contents .","title":"Introduction"},{"location":"user_guides/projects/git/configure_git_provider/#ui","text":"Documentation on how to generate a token for the supported Git hosting services is available here: GitHub GitLab BitBucket","title":"UI"},{"location":"user_guides/projects/git/configure_git_provider/#step-1-navigate-to-git-providers","text":"You can access the Git Providers page of your Hopsworks cluster by clicking on your name, in the top right corner, and choosing Account Settings from the dropdown menu. The Git providers section displays which providers have been already configured and can be used to clone new repositories. Git provider configuration list","title":"Step 1: Navigate to Git Providers"},{"location":"user_guides/projects/git/configure_git_provider/#step-2-configure-a-provider","text":"Click on Edit Configuration to change a provider username or token, or to configure a new provider. Tick the checkbox next to the provider you want to configure and insert the username and the token to use for that provider. Git provider configuration Click Create Configuration to save the configuration.","title":"Step 2: Configure a provider"},{"location":"user_guides/projects/git/configure_git_provider/#step-3-provider-is-configured","text":"The configured provider should now be marked as configured. Git provider configured","title":"Step 3: Provider is configured"},{"location":"user_guides/projects/git/configure_git_provider/#code","text":"You can also configure a git provider using the hopsworks git API in python.","title":"Code"},{"location":"user_guides/projects/git/configure_git_provider/#step-1-get-the-git-api","text":"import hopsworks project = hopsworks . login () git_api = project . get_git_api ()","title":"Step 1: Get the git API"},{"location":"user_guides/projects/git/configure_git_provider/#step-2-configure-git-provider","text":"PROVIDER = \"GitHub\" GITHUB_USER = \"my_user\" API_TOKEN = \"my_token\" git_api . set_provider ( PROVIDER , GITHUB_USER , API_TOKEN )","title":"Step 2: Configure git provider"},{"location":"user_guides/projects/git/configure_git_provider/#api-reference","text":"GitProvider","title":"API Reference"},{"location":"user_guides/projects/git/configure_git_provider/#conclusion","text":"In this guide you learned how configure your git provider credentials. You can now use the credentials to clone a repository from the configured provider.","title":"Conclusion"},{"location":"user_guides/projects/git/repository_actions/","text":"Repository actions # Introduction # This section explains the git operations or commands you can perform on hopsworks git repositories. These commands include commit, pull, push, create branches and many more. Repository permissions Git repositories are private. Only the owner of the repository can perform git actions on the repository such as commit, push, pull e.t.c. UI # The operations to perform on the cloned repository can be found in the dropdown as shown below. Repository actions Note that some repository actions will require the username and token to be configured first depending on the provider. For example to be able to perform a push action in any repository, you must configure the provider for the repository first. To be able to perform a pull action for the for a GitLab repository, you must configure the GitLab provider first. You will see the dialog below in the case you need to configure the provider first to perform the repository action. Configure provider prompt Read only repositories # In read only repositories, the following actions are disabled: commit, push and file checkout. The read only property can be enabled or disabled in the Cluster settings > Configuration, by updating the enable_read_only_git_repositories variable to true or false. Note that you need administrator privileges to update this property. Code # You can also perform the repository actions using the hopsworks git API in python. Step 1: Get the git API # import hopsworks project = hopsworks . login () git_api = project . get_git_api () Step 2: Get the git repository # git_repo = git_api . get_repo ( REPOSITORY_NAME ) Step 3: Perform the git repository action e.g commit # git_repo = git_api . commit ( \"Test commit\" ) API Reference # Api reference for repository actions is available here: GitRepo","title":"Repository Actions"},{"location":"user_guides/projects/git/repository_actions/#repository-actions","text":"","title":"Repository actions"},{"location":"user_guides/projects/git/repository_actions/#introduction","text":"This section explains the git operations or commands you can perform on hopsworks git repositories. These commands include commit, pull, push, create branches and many more. Repository permissions Git repositories are private. Only the owner of the repository can perform git actions on the repository such as commit, push, pull e.t.c.","title":"Introduction"},{"location":"user_guides/projects/git/repository_actions/#ui","text":"The operations to perform on the cloned repository can be found in the dropdown as shown below. Repository actions Note that some repository actions will require the username and token to be configured first depending on the provider. For example to be able to perform a push action in any repository, you must configure the provider for the repository first. To be able to perform a pull action for the for a GitLab repository, you must configure the GitLab provider first. You will see the dialog below in the case you need to configure the provider first to perform the repository action. Configure provider prompt","title":"UI"},{"location":"user_guides/projects/git/repository_actions/#read-only-repositories","text":"In read only repositories, the following actions are disabled: commit, push and file checkout. The read only property can be enabled or disabled in the Cluster settings > Configuration, by updating the enable_read_only_git_repositories variable to true or false. Note that you need administrator privileges to update this property.","title":"Read only repositories"},{"location":"user_guides/projects/git/repository_actions/#code","text":"You can also perform the repository actions using the hopsworks git API in python.","title":"Code"},{"location":"user_guides/projects/git/repository_actions/#step-1-get-the-git-api","text":"import hopsworks project = hopsworks . login () git_api = project . get_git_api ()","title":"Step 1: Get the git API"},{"location":"user_guides/projects/git/repository_actions/#step-2-get-the-git-repository","text":"git_repo = git_api . get_repo ( REPOSITORY_NAME )","title":"Step 2: Get the git repository"},{"location":"user_guides/projects/git/repository_actions/#step-3-perform-the-git-repository-action-eg-commit","text":"git_repo = git_api . commit ( \"Test commit\" )","title":"Step 3: Perform the git repository action e.g commit"},{"location":"user_guides/projects/git/repository_actions/#api-reference","text":"Api reference for repository actions is available here: GitRepo","title":"API Reference"},{"location":"user_guides/projects/iam_role/iam_role_chaining/","text":"How To Use AWS IAM Roles on EC2 instances # Introduction # When deploying Hopsworks on EC2 instances you might need to assume different roles to access resources on AWS. These roles can be configured in AWS and mapped to a project in Hopsworks. Prerequisites # Before you begin this guide you'll need the following: A Hopsworks cluster running on EC2. Role chaining setup in AWS. Configure role mappings in Hopsworks. For a guide on how to configure this see AWS IAM Role Chaining . UI # In this guide, you will learn how to use a mapped IAM role in your project. Step 1: Navigate to your project's IAM Role Chaining tab # In the Project Settings page you can find the IAM Role Chaining section showing a list of all IAM roles mapped to your project. Role Chaining Step 2: Use the IAM role # You can now use the IAM roles listed in your project when creating a storage connector with Temporary Credentials . Conclusion # In this guide you learned how to use IAM roles on a cluster deployed on an EC2 instances.","title":"AWS IAM Roles"},{"location":"user_guides/projects/iam_role/iam_role_chaining/#how-to-use-aws-iam-roles-on-ec2-instances","text":"","title":"How To Use AWS IAM Roles on EC2 instances"},{"location":"user_guides/projects/iam_role/iam_role_chaining/#introduction","text":"When deploying Hopsworks on EC2 instances you might need to assume different roles to access resources on AWS. These roles can be configured in AWS and mapped to a project in Hopsworks.","title":"Introduction"},{"location":"user_guides/projects/iam_role/iam_role_chaining/#prerequisites","text":"Before you begin this guide you'll need the following: A Hopsworks cluster running on EC2. Role chaining setup in AWS. Configure role mappings in Hopsworks. For a guide on how to configure this see AWS IAM Role Chaining .","title":"Prerequisites"},{"location":"user_guides/projects/iam_role/iam_role_chaining/#ui","text":"In this guide, you will learn how to use a mapped IAM role in your project.","title":"UI"},{"location":"user_guides/projects/iam_role/iam_role_chaining/#step-1-navigate-to-your-projects-iam-role-chaining-tab","text":"In the Project Settings page you can find the IAM Role Chaining section showing a list of all IAM roles mapped to your project. Role Chaining","title":"Step 1: Navigate to your project's IAM Role Chaining tab"},{"location":"user_guides/projects/iam_role/iam_role_chaining/#step-2-use-the-iam-role","text":"You can now use the IAM roles listed in your project when creating a storage connector with Temporary Credentials .","title":"Step 2: Use the IAM role"},{"location":"user_guides/projects/iam_role/iam_role_chaining/#conclusion","text":"In this guide you learned how to use IAM roles on a cluster deployed on an EC2 instances.","title":"Conclusion"},{"location":"user_guides/projects/jobs/pyspark_job/","text":"How To Run A PySpark Job # Introduction # All members of a project in Hopsworks can launch the following types of applications through a project's Jobs service: Python ( Hopsworks Enterprise only ) Apache Spark Launching a job of any type is very similar process, what mostly differs between job types is the various configuration parameters each job type comes with. After following this guide you will be able to create a PySpark job. The PySpark program can either be a .py script or a .ipynb file. Instantiate the SparkSession For a .py file, remember to instantiate the SparkSession i.e spark=SparkSession.builder.getOrCreate() For a .ipynb file, the SparkSession is already available as spark when the job is started. UI # Step 1: Jobs overview # The image below shows the Jobs overview page in Hopsworks and is accessed by clicking Jobs in the sidebar. Jobs overview Step 2: Create new job dialog # To configure a job, click Advanced options , which will open up the advanced configuration page for the job. Create new job dialog Step 3: Set the script # Next step is to select the program to run. You can either select From project , if the file was previously uploaded to Hopsworks, or Upload new file which lets you select a file from your local filesystem as demonstrated below. Configure program Step 4: Set the job type # Next step is to set the job type to SPARK to indicate it should be executed as a spark job. Then specify advanced configuration or click Create New Job to create the job. Set the job type Step 5 (optional): Advanced configuration # Resource allocation for the Spark driver and executors can be configured, also the number of executors and whether dynamic execution should be enabled. Driver memory : Number of cores to allocate for the Spark driver Driver virtual cores : Number of MBs to allocate for the Spark driver Executor memory : Number of cores to allocate for each Spark executor Executor virtual cores : Number of MBs to allocate for each Spark executor Dynamic/Static : Run the Spark application in static or dynamic allocation mode (see spark docs for details). Resource configuration for the Spark kernels Additional files or dependencies required for the Spark job can be configured. Additional archives : List of archives to be extracted into the working directory of each executor. Additional jars : List of jars to be placed in the working directory of each executor. Additional python dependencies : List of python files and archives to be placed on each executor and added to PATH. Additional files : List of files to be placed in the working directory of each executor. File configuration for the Spark kernels Line-separates properties to be set for the Spark application. For example, changing the configuration variables for the Kryo Serializer or setting environment variables for the driver, you can set the properties as shown below. Additional Spark configuration Step 6: Execute the job # Now click the Run button to start the execution of the job, and then click on Executions to see the list of all executions. Start job execution Step 7: Application logs # To monitor logs while the execution is running, click Spark UI to open the Spark UI in a separate tab. Once the execution is finished, you can click on Logs to see the full logs for execution. Access Spark logs Code # Step 1: Upload the PySpark program # This snippet assumes the program to run is in the current working directory and named script.py . It will upload the python script to the Resources dataset in your project. import hopsworks project = hopsworks . login () dataset_api = project . get_dataset_api () uploaded_file_path = dataset_api . upload ( \"script.py\" , \"Resources\" ) Step 2: Create PySpark job # In this snippet we get the JobsApi object to get the default job configuration for a PYSPARK job, set the python script to run and create the Job object. jobs_api = project . get_jobs_api () spark_config = jobs_api . get_configuration ( \"PYSPARK\" ) spark_config [ 'appPath' ] = uploaded_file_path job = jobs_api . create_job ( \"pyspark_job\" , spark_config ) Step 3: Execute the job # In this snippet we execute the job synchronously, that is wait until it reaches a terminal state, and then download and print the logs. execution = job . run ( await_termination = True ) out , err = execution . download_logs () f_out = open ( out , \"r\" ) print ( f_out . read ()) f_err = open ( err , \"r\" ) print ( f_err . read ()) API Reference # Jobs Executions Conclusion # In this guide you learned how to create and run a PySpark job.","title":"Run PySpark Job"},{"location":"user_guides/projects/jobs/pyspark_job/#how-to-run-a-pyspark-job","text":"","title":"How To Run A PySpark Job"},{"location":"user_guides/projects/jobs/pyspark_job/#introduction","text":"All members of a project in Hopsworks can launch the following types of applications through a project's Jobs service: Python ( Hopsworks Enterprise only ) Apache Spark Launching a job of any type is very similar process, what mostly differs between job types is the various configuration parameters each job type comes with. After following this guide you will be able to create a PySpark job. The PySpark program can either be a .py script or a .ipynb file. Instantiate the SparkSession For a .py file, remember to instantiate the SparkSession i.e spark=SparkSession.builder.getOrCreate() For a .ipynb file, the SparkSession is already available as spark when the job is started.","title":"Introduction"},{"location":"user_guides/projects/jobs/pyspark_job/#ui","text":"","title":"UI"},{"location":"user_guides/projects/jobs/pyspark_job/#step-1-jobs-overview","text":"The image below shows the Jobs overview page in Hopsworks and is accessed by clicking Jobs in the sidebar. Jobs overview","title":"Step 1: Jobs overview"},{"location":"user_guides/projects/jobs/pyspark_job/#step-2-create-new-job-dialog","text":"To configure a job, click Advanced options , which will open up the advanced configuration page for the job. Create new job dialog","title":"Step 2: Create new job dialog"},{"location":"user_guides/projects/jobs/pyspark_job/#step-3-set-the-script","text":"Next step is to select the program to run. You can either select From project , if the file was previously uploaded to Hopsworks, or Upload new file which lets you select a file from your local filesystem as demonstrated below. Configure program","title":"Step 3: Set the script"},{"location":"user_guides/projects/jobs/pyspark_job/#step-4-set-the-job-type","text":"Next step is to set the job type to SPARK to indicate it should be executed as a spark job. Then specify advanced configuration or click Create New Job to create the job. Set the job type","title":"Step 4: Set the job type"},{"location":"user_guides/projects/jobs/pyspark_job/#step-5-optional-advanced-configuration","text":"Resource allocation for the Spark driver and executors can be configured, also the number of executors and whether dynamic execution should be enabled. Driver memory : Number of cores to allocate for the Spark driver Driver virtual cores : Number of MBs to allocate for the Spark driver Executor memory : Number of cores to allocate for each Spark executor Executor virtual cores : Number of MBs to allocate for each Spark executor Dynamic/Static : Run the Spark application in static or dynamic allocation mode (see spark docs for details). Resource configuration for the Spark kernels Additional files or dependencies required for the Spark job can be configured. Additional archives : List of archives to be extracted into the working directory of each executor. Additional jars : List of jars to be placed in the working directory of each executor. Additional python dependencies : List of python files and archives to be placed on each executor and added to PATH. Additional files : List of files to be placed in the working directory of each executor. File configuration for the Spark kernels Line-separates properties to be set for the Spark application. For example, changing the configuration variables for the Kryo Serializer or setting environment variables for the driver, you can set the properties as shown below. Additional Spark configuration","title":"Step 5 (optional): Advanced configuration"},{"location":"user_guides/projects/jobs/pyspark_job/#step-6-execute-the-job","text":"Now click the Run button to start the execution of the job, and then click on Executions to see the list of all executions. Start job execution","title":"Step 6: Execute the job"},{"location":"user_guides/projects/jobs/pyspark_job/#step-7-application-logs","text":"To monitor logs while the execution is running, click Spark UI to open the Spark UI in a separate tab. Once the execution is finished, you can click on Logs to see the full logs for execution. Access Spark logs","title":"Step 7: Application logs"},{"location":"user_guides/projects/jobs/pyspark_job/#code","text":"","title":"Code"},{"location":"user_guides/projects/jobs/pyspark_job/#step-1-upload-the-pyspark-program","text":"This snippet assumes the program to run is in the current working directory and named script.py . It will upload the python script to the Resources dataset in your project. import hopsworks project = hopsworks . login () dataset_api = project . get_dataset_api () uploaded_file_path = dataset_api . upload ( \"script.py\" , \"Resources\" )","title":"Step 1: Upload the PySpark program"},{"location":"user_guides/projects/jobs/pyspark_job/#step-2-create-pyspark-job","text":"In this snippet we get the JobsApi object to get the default job configuration for a PYSPARK job, set the python script to run and create the Job object. jobs_api = project . get_jobs_api () spark_config = jobs_api . get_configuration ( \"PYSPARK\" ) spark_config [ 'appPath' ] = uploaded_file_path job = jobs_api . create_job ( \"pyspark_job\" , spark_config )","title":"Step 2: Create PySpark job"},{"location":"user_guides/projects/jobs/pyspark_job/#step-3-execute-the-job","text":"In this snippet we execute the job synchronously, that is wait until it reaches a terminal state, and then download and print the logs. execution = job . run ( await_termination = True ) out , err = execution . download_logs () f_out = open ( out , \"r\" ) print ( f_out . read ()) f_err = open ( err , \"r\" ) print ( f_err . read ())","title":"Step 3: Execute the job"},{"location":"user_guides/projects/jobs/pyspark_job/#api-reference","text":"Jobs Executions","title":"API Reference"},{"location":"user_guides/projects/jobs/pyspark_job/#conclusion","text":"In this guide you learned how to create and run a PySpark job.","title":"Conclusion"},{"location":"user_guides/projects/jobs/python_job/","text":"How To Run A Python Job # Introduction # All members of a project in Hopsworks can launch the following types of applications through a project's Jobs service: Python ( Hopsworks Enterprise only ) Apache Spark Launching a job of any type is very similar process, what mostly differs between job types is the various configuration parameters each job type comes with. After following this guide you will be able to create a Python job. Kubernetes integration required Python Jobs are only available if Hopsworks has been integrated with a Kubernetes cluster. Hopsworks can be integrated with Amazon EKS , Azure AKS and on-premise Kubernetes clusters. UI # Step 1: Jobs overview # The image below shows the Jobs overview page in Hopsworks and is accessed by clicking Jobs in the sidebar. Jobs overview Step 2: Create new job dialog # By default, the dialog will create a Spark job. To instead configure a Python job, click Advanced options , which will open up the advanced configuration page for the job. Create new job dialog Step 3: Set the script # Next step is to select the python script to run. You can either select From project , if the file was previously uploaded to Hopsworks, or Upload new file which lets you select a file from your local filesystem as demonstrated below. Configure program Step 4: Set the job type # Next step is to set the job type to PYTHON to indicate it should be executed as a simple python script. Then click Create New Job to create the job. Set the job type Step 5 (optional): Additional configuration # It is possible to also set following configuration settings for a PYTHON job. Container memory : The amount of memory in MB to be allocated to the Python script Container cores : The number of cores to be allocated for the Python script Additional files : List of files that will be locally accessible by the application Set the job type Step 6: Execute the job # Now click the Run button to start the execution of the job, and then click on Executions to see the list of all executions. Once the execution is finished, click on Logs to see the logs for the execution. Start job execution Code # Step 1: Upload the Python script # This snippet assumes the python script is in the current working directory and named script.py . It will upload the python script to the Resources dataset in your project. import hopsworks project = hopsworks . login () dataset_api = project . get_dataset_api () uploaded_file_path = dataset_api . upload ( \"script.py\" , \"Resources\" ) Step 2: Create Python job # In this snippet we get the JobsApi object to get the default job configuration for a PYTHON job, set the python script to run and create the Job object. jobs_api = project . get_jobs_api () py_job_config = jobs_api . get_configuration ( \"PYTHON\" ) py_job_config [ 'appPath' ] = uploaded_file_path job = jobs_api . create_job ( \"py_job\" , py_job_config ) Step 3: Execute the job # In this snippet we execute the job synchronously, that is wait until it reaches a terminal state, and then download and print the logs. # Run the job execution = job . run ( await_termination = True ) # Download logs out , err = execution . download_logs () f_out = open ( out , \"r\" ) print ( f_out . read ()) f_err = open ( err , \"r\" ) print ( f_err . read ()) API Reference # Jobs Executions Conclusion # In this guide you learned how to create and run a Python job.","title":"Run Python Job"},{"location":"user_guides/projects/jobs/python_job/#how-to-run-a-python-job","text":"","title":"How To Run A Python Job"},{"location":"user_guides/projects/jobs/python_job/#introduction","text":"All members of a project in Hopsworks can launch the following types of applications through a project's Jobs service: Python ( Hopsworks Enterprise only ) Apache Spark Launching a job of any type is very similar process, what mostly differs between job types is the various configuration parameters each job type comes with. After following this guide you will be able to create a Python job. Kubernetes integration required Python Jobs are only available if Hopsworks has been integrated with a Kubernetes cluster. Hopsworks can be integrated with Amazon EKS , Azure AKS and on-premise Kubernetes clusters.","title":"Introduction"},{"location":"user_guides/projects/jobs/python_job/#ui","text":"","title":"UI"},{"location":"user_guides/projects/jobs/python_job/#step-1-jobs-overview","text":"The image below shows the Jobs overview page in Hopsworks and is accessed by clicking Jobs in the sidebar. Jobs overview","title":"Step 1: Jobs overview"},{"location":"user_guides/projects/jobs/python_job/#step-2-create-new-job-dialog","text":"By default, the dialog will create a Spark job. To instead configure a Python job, click Advanced options , which will open up the advanced configuration page for the job. Create new job dialog","title":"Step 2: Create new job dialog"},{"location":"user_guides/projects/jobs/python_job/#step-3-set-the-script","text":"Next step is to select the python script to run. You can either select From project , if the file was previously uploaded to Hopsworks, or Upload new file which lets you select a file from your local filesystem as demonstrated below. Configure program","title":"Step 3: Set the script"},{"location":"user_guides/projects/jobs/python_job/#step-4-set-the-job-type","text":"Next step is to set the job type to PYTHON to indicate it should be executed as a simple python script. Then click Create New Job to create the job. Set the job type","title":"Step 4: Set the job type"},{"location":"user_guides/projects/jobs/python_job/#step-5-optional-additional-configuration","text":"It is possible to also set following configuration settings for a PYTHON job. Container memory : The amount of memory in MB to be allocated to the Python script Container cores : The number of cores to be allocated for the Python script Additional files : List of files that will be locally accessible by the application Set the job type","title":"Step 5 (optional): Additional configuration"},{"location":"user_guides/projects/jobs/python_job/#step-6-execute-the-job","text":"Now click the Run button to start the execution of the job, and then click on Executions to see the list of all executions. Once the execution is finished, click on Logs to see the logs for the execution. Start job execution","title":"Step 6: Execute the job"},{"location":"user_guides/projects/jobs/python_job/#code","text":"","title":"Code"},{"location":"user_guides/projects/jobs/python_job/#step-1-upload-the-python-script","text":"This snippet assumes the python script is in the current working directory and named script.py . It will upload the python script to the Resources dataset in your project. import hopsworks project = hopsworks . login () dataset_api = project . get_dataset_api () uploaded_file_path = dataset_api . upload ( \"script.py\" , \"Resources\" )","title":"Step 1: Upload the Python script"},{"location":"user_guides/projects/jobs/python_job/#step-2-create-python-job","text":"In this snippet we get the JobsApi object to get the default job configuration for a PYTHON job, set the python script to run and create the Job object. jobs_api = project . get_jobs_api () py_job_config = jobs_api . get_configuration ( \"PYTHON\" ) py_job_config [ 'appPath' ] = uploaded_file_path job = jobs_api . create_job ( \"py_job\" , py_job_config )","title":"Step 2: Create Python job"},{"location":"user_guides/projects/jobs/python_job/#step-3-execute-the-job","text":"In this snippet we execute the job synchronously, that is wait until it reaches a terminal state, and then download and print the logs. # Run the job execution = job . run ( await_termination = True ) # Download logs out , err = execution . download_logs () f_out = open ( out , \"r\" ) print ( f_out . read ()) f_err = open ( err , \"r\" ) print ( f_err . read ())","title":"Step 3: Execute the job"},{"location":"user_guides/projects/jobs/python_job/#api-reference","text":"Jobs Executions","title":"API Reference"},{"location":"user_guides/projects/jobs/python_job/#conclusion","text":"In this guide you learned how to create and run a Python job.","title":"Conclusion"},{"location":"user_guides/projects/jobs/schedule_job/","text":"How To Schedule a Job # Introduction # Hopsworks jobs can be scheduled to run at regular intervals using the scheduling function provided by Hopsworks. Each job can be configured to have a single schedule. Schedules can be defined using the drop down menus in the UI or a Quartz cron expression. Schedule frequency The Hopsworks scheduler runs every minute. As such, the scheduling frequency should be of at least 1 minute. Parallel executions If a job execution needs to be scheduled, the scheduler will first check that there are no active executions for that job. If there is an execution running, the scheduler will postpone the execution until the running one is done. UI # Scheduling Jobs # You can define a schedule for a job during the creation of the job itself or after the job has been created from the job overview UI. Schedule a Job The add schedule prompt requires you to select a frequency either through the drop down menus or by using a cron expression. You can also provide a start time to specify from when the schedule should have effect. The start time can also be in the past. If that's the case, the scheduler will backfill the executions from the specified start time. As mentioned above, the execution backfilling will happen one execution at the time. You can optionally provide an end date time to specify until when the scheduling should continue. The end time can also be in the past. In the job overview, you can see the current scheduling configuration, whether or not it is enabled and when the next execution is planned for. All times will be considered as UTC time. Job scheduling overview Job argument # When a job execution is triggered by the scheduler, a -start_time argument is added to the job arguments. The -start_time value will be the time of the scheduled execution in UTC in the ISO-8601 format (e.g.: -start_time 2023-08-19T18:00:00Z ). The -start_time value passed as argument represents the time when the execution was scheduled, not when the execution was started. For example, if the scheduled execution time was in the past (e.g. in the case of backfilling), the -start_time passed to the execution is the time in the past, not the current time when the execution is running. Similarly, if the scheduler was not running for a period of time, when it comes back online, it will start the executions it missed to schedule while offline. Even in that case, the -start_time value will contain the time at which the execution was supposed to be started, not the current time. Disable / Enable a schedule # You can decide to pause the scheduling of a job and avoid new executions to be started. You can later on re-enable the same scheduling configuration, and the scheduler will run the executions that were skipped while the schedule was disabled, if any, sequentially. In this way you will backfill the executions in between. You can skip the backfilling of the executions by editing the scheduling configuration and bringing forward the schedule start time for the job. Delete a scheduling # You can remove the schedule for a job using the UI and by clicking on the trash icon on the schedule section of the job overview. If you re-schedule a job after having deleted the previous schedule, even with the same options, it will not take into account previous scheduled executions.","title":"Scheduling"},{"location":"user_guides/projects/jobs/schedule_job/#how-to-schedule-a-job","text":"","title":"How To Schedule a Job"},{"location":"user_guides/projects/jobs/schedule_job/#introduction","text":"Hopsworks jobs can be scheduled to run at regular intervals using the scheduling function provided by Hopsworks. Each job can be configured to have a single schedule. Schedules can be defined using the drop down menus in the UI or a Quartz cron expression. Schedule frequency The Hopsworks scheduler runs every minute. As such, the scheduling frequency should be of at least 1 minute. Parallel executions If a job execution needs to be scheduled, the scheduler will first check that there are no active executions for that job. If there is an execution running, the scheduler will postpone the execution until the running one is done.","title":"Introduction"},{"location":"user_guides/projects/jobs/schedule_job/#ui","text":"","title":"UI"},{"location":"user_guides/projects/jobs/schedule_job/#scheduling-jobs","text":"You can define a schedule for a job during the creation of the job itself or after the job has been created from the job overview UI. Schedule a Job The add schedule prompt requires you to select a frequency either through the drop down menus or by using a cron expression. You can also provide a start time to specify from when the schedule should have effect. The start time can also be in the past. If that's the case, the scheduler will backfill the executions from the specified start time. As mentioned above, the execution backfilling will happen one execution at the time. You can optionally provide an end date time to specify until when the scheduling should continue. The end time can also be in the past. In the job overview, you can see the current scheduling configuration, whether or not it is enabled and when the next execution is planned for. All times will be considered as UTC time. Job scheduling overview","title":"Scheduling Jobs"},{"location":"user_guides/projects/jobs/schedule_job/#job-argument","text":"When a job execution is triggered by the scheduler, a -start_time argument is added to the job arguments. The -start_time value will be the time of the scheduled execution in UTC in the ISO-8601 format (e.g.: -start_time 2023-08-19T18:00:00Z ). The -start_time value passed as argument represents the time when the execution was scheduled, not when the execution was started. For example, if the scheduled execution time was in the past (e.g. in the case of backfilling), the -start_time passed to the execution is the time in the past, not the current time when the execution is running. Similarly, if the scheduler was not running for a period of time, when it comes back online, it will start the executions it missed to schedule while offline. Even in that case, the -start_time value will contain the time at which the execution was supposed to be started, not the current time.","title":"Job argument"},{"location":"user_guides/projects/jobs/schedule_job/#disable-enable-a-schedule","text":"You can decide to pause the scheduling of a job and avoid new executions to be started. You can later on re-enable the same scheduling configuration, and the scheduler will run the executions that were skipped while the schedule was disabled, if any, sequentially. In this way you will backfill the executions in between. You can skip the backfilling of the executions by editing the scheduling configuration and bringing forward the schedule start time for the job.","title":"Disable / Enable a schedule"},{"location":"user_guides/projects/jobs/schedule_job/#delete-a-scheduling","text":"You can remove the schedule for a job using the UI and by clicking on the trash icon on the schedule section of the job overview. If you re-schedule a job after having deleted the previous schedule, even with the same options, it will not take into account previous scheduled executions.","title":"Delete a scheduling"},{"location":"user_guides/projects/jobs/spark_job/","text":"How To Run A Spark Job # Introduction # All members of a project in Hopsworks can launch the following types of applications through a project's Jobs service: Python ( Hopsworks Enterprise only ) Apache Spark Launching a job of any type is very similar process, what mostly differs between job types is the various configuration parameters each job type comes with. After following this guide you will be able to create a Spark job. UI # Step 1: Jobs overview # The image below shows the Jobs overview page in Hopsworks and is accessed by clicking Jobs in the sidebar. Jobs overview Step 2: Create new job dialog # To configure a job, click Advanced options , which will open up the advanced configuration page for the job. Create new job dialog Step 3: Set the jar # Next step is to select the program to run. You can either select From project , if the file was previously uploaded to Hopsworks, or Upload new file which lets you select a file from your local filesystem as demonstrated below. After that set the name for the job. Configure program Step 4: Set the job type # Next step is to set the job type to SPARK to indicate it should be executed as a spark job. Then specify additional configuration or click Create New Job to create the job. Set the job type Step 5: Set the main class # Next step is to set the main class for the application. Then specify advanced configuration or click Create New Job to create the job. Set the main class Step 6 (optional): Advanced configuration # Resource allocation for the Spark driver and executors can be configured, also the number of executors and whether dynamic execution should be enabled. Driver memory : Number of cores to allocate for the Spark driver Driver virtual cores : Number of MBs to allocate for the Spark driver Executor memory : Number of cores to allocate for each Spark executor Executor virtual cores : Number of MBs to allocate for each Spark executor Dynamic/Static : Run the Spark application in static or dynamic allocation mode (see spark docs for details). Resource configuration for the Spark kernels Additional files or dependencies required for the Spark job can be configured. Additional archives : List of archives to be extracted into the working directory of each executor. Additional jars : List of jars to be placed in the working directory of each executor. Additional python dependencies : List of python files and archives to be placed on each executor and added to PATH. Additional files : List of files to be placed in the working directory of each executor. File configuration for the Spark kernels Line-separates properties to be set for the Spark application. For example, changing the configuration variables for the Kryo Serializer or setting environment variables for the driver, you can set the properties as shown below. Additional Spark configuration Step 7: Execute the job # Now click the Run button to start the execution of the job, and then click on Executions to see the list of all executions. Start job execution Step 8: Application logs # To monitor logs while the execution is running, click Spark UI to open the Spark UI in a separate tab. Once the execution is finished, you can click on Logs to see the full logs for execution. Access Spark logs Code # Step 1: Upload the Spark jar # This snippet assumes the Spark program is in the current working directory and named sparkpi.jar . It will upload the jar to the Resources dataset in your project. import hopsworks project = hopsworks . login () dataset_api = project . get_dataset_api () uploaded_file_path = dataset_api . upload ( \"sparkpi.jar\" , \"Resources\" ) Step 2: Create Spark job # In this snippet we get the JobsApi object to get the default job configuration for a SPARK job, set the python script to run and create the Job object. jobs_api = project . get_jobs_api () spark_config = jobs_api . get_configuration ( \"SPARK\" ) spark_config [ 'appPath' ] = uploaded_file_path spark_config [ 'mainClass' ] = 'org.apache.spark.examples.SparkPi' job = jobs_api . create_job ( \"pyspark_job\" , spark_config ) Step 3: Execute the job # In this snippet we execute the job synchronously, that is wait until it reaches a terminal state, and then download and print the logs. execution = job . run ( await_termination = True ) out , err = execution . download_logs () f_out = open ( out , \"r\" ) print ( f_out . read ()) f_err = open ( err , \"r\" ) print ( f_err . read ()) API Reference # Jobs Executions Conclusion # In this guide you learned how to create and run a Spark job.","title":"Run Spark Job"},{"location":"user_guides/projects/jobs/spark_job/#how-to-run-a-spark-job","text":"","title":"How To Run A Spark Job"},{"location":"user_guides/projects/jobs/spark_job/#introduction","text":"All members of a project in Hopsworks can launch the following types of applications through a project's Jobs service: Python ( Hopsworks Enterprise only ) Apache Spark Launching a job of any type is very similar process, what mostly differs between job types is the various configuration parameters each job type comes with. After following this guide you will be able to create a Spark job.","title":"Introduction"},{"location":"user_guides/projects/jobs/spark_job/#ui","text":"","title":"UI"},{"location":"user_guides/projects/jobs/spark_job/#step-1-jobs-overview","text":"The image below shows the Jobs overview page in Hopsworks and is accessed by clicking Jobs in the sidebar. Jobs overview","title":"Step 1: Jobs overview"},{"location":"user_guides/projects/jobs/spark_job/#step-2-create-new-job-dialog","text":"To configure a job, click Advanced options , which will open up the advanced configuration page for the job. Create new job dialog","title":"Step 2: Create new job dialog"},{"location":"user_guides/projects/jobs/spark_job/#step-3-set-the-jar","text":"Next step is to select the program to run. You can either select From project , if the file was previously uploaded to Hopsworks, or Upload new file which lets you select a file from your local filesystem as demonstrated below. After that set the name for the job. Configure program","title":"Step 3: Set the jar"},{"location":"user_guides/projects/jobs/spark_job/#step-4-set-the-job-type","text":"Next step is to set the job type to SPARK to indicate it should be executed as a spark job. Then specify additional configuration or click Create New Job to create the job. Set the job type","title":"Step 4: Set the job type"},{"location":"user_guides/projects/jobs/spark_job/#step-5-set-the-main-class","text":"Next step is to set the main class for the application. Then specify advanced configuration or click Create New Job to create the job. Set the main class","title":"Step 5: Set the main class"},{"location":"user_guides/projects/jobs/spark_job/#step-6-optional-advanced-configuration","text":"Resource allocation for the Spark driver and executors can be configured, also the number of executors and whether dynamic execution should be enabled. Driver memory : Number of cores to allocate for the Spark driver Driver virtual cores : Number of MBs to allocate for the Spark driver Executor memory : Number of cores to allocate for each Spark executor Executor virtual cores : Number of MBs to allocate for each Spark executor Dynamic/Static : Run the Spark application in static or dynamic allocation mode (see spark docs for details). Resource configuration for the Spark kernels Additional files or dependencies required for the Spark job can be configured. Additional archives : List of archives to be extracted into the working directory of each executor. Additional jars : List of jars to be placed in the working directory of each executor. Additional python dependencies : List of python files and archives to be placed on each executor and added to PATH. Additional files : List of files to be placed in the working directory of each executor. File configuration for the Spark kernels Line-separates properties to be set for the Spark application. For example, changing the configuration variables for the Kryo Serializer or setting environment variables for the driver, you can set the properties as shown below. Additional Spark configuration","title":"Step 6 (optional): Advanced configuration"},{"location":"user_guides/projects/jobs/spark_job/#step-7-execute-the-job","text":"Now click the Run button to start the execution of the job, and then click on Executions to see the list of all executions. Start job execution","title":"Step 7: Execute the job"},{"location":"user_guides/projects/jobs/spark_job/#step-8-application-logs","text":"To monitor logs while the execution is running, click Spark UI to open the Spark UI in a separate tab. Once the execution is finished, you can click on Logs to see the full logs for execution. Access Spark logs","title":"Step 8: Application logs"},{"location":"user_guides/projects/jobs/spark_job/#code","text":"","title":"Code"},{"location":"user_guides/projects/jobs/spark_job/#step-1-upload-the-spark-jar","text":"This snippet assumes the Spark program is in the current working directory and named sparkpi.jar . It will upload the jar to the Resources dataset in your project. import hopsworks project = hopsworks . login () dataset_api = project . get_dataset_api () uploaded_file_path = dataset_api . upload ( \"sparkpi.jar\" , \"Resources\" )","title":"Step 1: Upload the Spark jar"},{"location":"user_guides/projects/jobs/spark_job/#step-2-create-spark-job","text":"In this snippet we get the JobsApi object to get the default job configuration for a SPARK job, set the python script to run and create the Job object. jobs_api = project . get_jobs_api () spark_config = jobs_api . get_configuration ( \"SPARK\" ) spark_config [ 'appPath' ] = uploaded_file_path spark_config [ 'mainClass' ] = 'org.apache.spark.examples.SparkPi' job = jobs_api . create_job ( \"pyspark_job\" , spark_config )","title":"Step 2: Create Spark job"},{"location":"user_guides/projects/jobs/spark_job/#step-3-execute-the-job","text":"In this snippet we execute the job synchronously, that is wait until it reaches a terminal state, and then download and print the logs. execution = job . run ( await_termination = True ) out , err = execution . download_logs () f_out = open ( out , \"r\" ) print ( f_out . read ()) f_err = open ( err , \"r\" ) print ( f_err . read ())","title":"Step 3: Execute the job"},{"location":"user_guides/projects/jobs/spark_job/#api-reference","text":"Jobs Executions","title":"API Reference"},{"location":"user_guides/projects/jobs/spark_job/#conclusion","text":"In this guide you learned how to create and run a Spark job.","title":"Conclusion"},{"location":"user_guides/projects/jupyter/python_notebook/","text":"How To Run A Python Notebook # Introduction # Jupyter is provided as a service in Hopsworks, providing the same user experience and features as if run on your laptop. Supports JupyterLab and the classic Jupyter front-end Configured with Python3, Spark, PySpark and SparkR kernels Important If Hopsworks is not configured to run Jupyter on Kubernetes then the Python kernel is disabled by default. In this case the Python kernel can be enabled by setting the configuration variable enable_jupyter_python_kernel_non_kubernetes to True. Follow this guide for instructions on how to set a configuration variable. Step 1: Jupyter dashboard # The image below shows the Jupyter service page in Hopsworks and is accessed by clicking Jupyter in the sidebar. Jupyter dashboard in Hopsworks From this page, you can configure various options and settings to start Jupyter with as described in the sections below. Step 2 (Optional): Configure resources # Next step is to configure Jupyter, Click edit configuration to get to the configuration page and select Python . Container cores : Number of cores to allocate for the Jupyter instance Container memory : Number of MBs to allocate for the Jupyter instance Configured resource pool is shared by all running kernels. If a kernel crashes while executing a cell, try increasing the Container memory. Resource configuration for the Python kernel Click Save to save the new configuration. Step 3 (Optional): Configure max runtime and root path # Before starting the server there are two additional configurations that can be set next to the Run Jupyter button. The runtime of the Jupyter instance can be configured, this is useful to ensure that idle instances will not be hanging around and keep allocating resources. If a limited runtime is not desirable, this can be disabled by setting no limit . Configure maximum runtime The root path from which to start the Jupyter instance can be configured. By default it starts by setting the /Jupyter folder as the root. Configure root folder Step 4: Start Jupyter # Start the Jupyter instance by clicking the Run Jupyter button. Starting Jupyter and running a Python notebook Conclusion # In this guide you learned how to configure and run a Python application in Jupyter. You can now follow this guide to install a library that can be used in a notebook.","title":"Run Python Notebook"},{"location":"user_guides/projects/jupyter/python_notebook/#how-to-run-a-python-notebook","text":"","title":"How To Run A Python Notebook"},{"location":"user_guides/projects/jupyter/python_notebook/#introduction","text":"Jupyter is provided as a service in Hopsworks, providing the same user experience and features as if run on your laptop. Supports JupyterLab and the classic Jupyter front-end Configured with Python3, Spark, PySpark and SparkR kernels Important If Hopsworks is not configured to run Jupyter on Kubernetes then the Python kernel is disabled by default. In this case the Python kernel can be enabled by setting the configuration variable enable_jupyter_python_kernel_non_kubernetes to True. Follow this guide for instructions on how to set a configuration variable.","title":"Introduction"},{"location":"user_guides/projects/jupyter/python_notebook/#step-1-jupyter-dashboard","text":"The image below shows the Jupyter service page in Hopsworks and is accessed by clicking Jupyter in the sidebar. Jupyter dashboard in Hopsworks From this page, you can configure various options and settings to start Jupyter with as described in the sections below.","title":"Step 1: Jupyter dashboard"},{"location":"user_guides/projects/jupyter/python_notebook/#step-2-optional-configure-resources","text":"Next step is to configure Jupyter, Click edit configuration to get to the configuration page and select Python . Container cores : Number of cores to allocate for the Jupyter instance Container memory : Number of MBs to allocate for the Jupyter instance Configured resource pool is shared by all running kernels. If a kernel crashes while executing a cell, try increasing the Container memory. Resource configuration for the Python kernel Click Save to save the new configuration.","title":"Step 2 (Optional): Configure resources"},{"location":"user_guides/projects/jupyter/python_notebook/#step-3-optional-configure-max-runtime-and-root-path","text":"Before starting the server there are two additional configurations that can be set next to the Run Jupyter button. The runtime of the Jupyter instance can be configured, this is useful to ensure that idle instances will not be hanging around and keep allocating resources. If a limited runtime is not desirable, this can be disabled by setting no limit . Configure maximum runtime The root path from which to start the Jupyter instance can be configured. By default it starts by setting the /Jupyter folder as the root. Configure root folder","title":"Step 3 (Optional): Configure max runtime and root path"},{"location":"user_guides/projects/jupyter/python_notebook/#step-4-start-jupyter","text":"Start the Jupyter instance by clicking the Run Jupyter button. Starting Jupyter and running a Python notebook","title":"Step 4: Start Jupyter"},{"location":"user_guides/projects/jupyter/python_notebook/#conclusion","text":"In this guide you learned how to configure and run a Python application in Jupyter. You can now follow this guide to install a library that can be used in a notebook.","title":"Conclusion"},{"location":"user_guides/projects/jupyter/remote_filesystem_driver/","text":"Configuring remote filesystem driver # Introduction # We provide two ways to access and persist files in HopsFs from a jupyter notebook: hdfscontentsmanager : With hdfscontentsmanager you interact with the project datasets using the dataset api. When you start a notebook using the hdfscontentsmanager you will only see the files in the configured root path. hopsfsmount : With hopsfsmount all the project datasets are available in the jupyter notebook as a local filesystem. This means you can use native Python file I/O operations (copy, move, create, open, etc.) to interact with the project datasets. When you open the jupyter notebook you will see all the project datasets. Configuring the driver # To configure the driver you need to have admin role and set the jupyter_remote_fs_driver to either hdfscontentsmanager or hopsfsmount . The default driver is hdfscontentsmanager . Conclusion # In this guide you learned about the filesystem drivers for jupyter notebooks and how to configure them.","title":"Remote Filesystem Driver"},{"location":"user_guides/projects/jupyter/remote_filesystem_driver/#configuring-remote-filesystem-driver","text":"","title":"Configuring remote filesystem driver"},{"location":"user_guides/projects/jupyter/remote_filesystem_driver/#introduction","text":"We provide two ways to access and persist files in HopsFs from a jupyter notebook: hdfscontentsmanager : With hdfscontentsmanager you interact with the project datasets using the dataset api. When you start a notebook using the hdfscontentsmanager you will only see the files in the configured root path. hopsfsmount : With hopsfsmount all the project datasets are available in the jupyter notebook as a local filesystem. This means you can use native Python file I/O operations (copy, move, create, open, etc.) to interact with the project datasets. When you open the jupyter notebook you will see all the project datasets.","title":"Introduction"},{"location":"user_guides/projects/jupyter/remote_filesystem_driver/#configuring-the-driver","text":"To configure the driver you need to have admin role and set the jupyter_remote_fs_driver to either hdfscontentsmanager or hopsfsmount . The default driver is hdfscontentsmanager .","title":"Configuring the driver"},{"location":"user_guides/projects/jupyter/remote_filesystem_driver/#conclusion","text":"In this guide you learned about the filesystem drivers for jupyter notebooks and how to configure them.","title":"Conclusion"},{"location":"user_guides/projects/jupyter/spark_notebook/","text":"How To Run A Spark Notebook # Introduction # Jupyter is provided as a service in Hopsworks, providing the same user experience and features as if run on your laptop. Supports JupyterLab and the classic Jupyter front-end Configured with Python3, Spark, PySpark and SparkR kernels Step 1: Jupyter dashboard # The image below shows the Jupyter service page in Hopsworks and is accessed by clicking Jupyter in the sidebar. Jupyter dashboard in Hopsworks From this page, you can configure various options and settings to start Jupyter with as described in the sections below. Step 2 (Optional): Configure spark # Next step is to configure the Spark properties to be used in Jupyter, Click edit configuration to get to the configuration page and select Spark . Resource and compute # Resource allocation for the Spark driver and executors can be configured, also the number of executors and whether dynamic execution should be enabled. Driver memory : Number of cores to allocate for the Spark driver Driver virtual cores : Number of MBs to allocate for the Spark driver Executor memory : Number of cores to allocate for each Spark executor Executor virtual cores : Number of MBs to allocate for each Spark executor Dynamic/Static : Run the Spark application in static or dynamic allocation mode (see spark docs for details). Resource configuration for the Spark kernels Attach files or dependencies # Additional files or dependencies required for the Spark job can be configured. Additional archives : List of zip or .tgz files that will be locally accessible by the application Additional jars : List of .jar files to add to the CLASSPATH of the application Additional python dependencies : List of .py, .zip or .egg files that will be locally accessible by the application Additional files : List of files that will be locally accessible by the application File configuration for the Spark kernels Line-separates properties to be set for the Spark application. For example, changing the configuration variables for the Kryo Serializer or setting environment variables for the driver, you can set the properties as shown below. Additional Spark configuration Click Save to save the new configuration. Step 3 (Optional): Configure max runtime and root path # Before starting the server there are two additional configurations that can be set next to the Run Jupyter button. The runtime of the Jupyter instance can be configured, this is useful to ensure that idle instances will not be hanging around and keep allocating resources. If a limited runtime is not desirable, this can be disabled by setting no limit . Configure maximum runtime The root path from which to start the Jupyter instance can be configured. By default it starts by setting the /Jupyter folder as the root. Configure root folder Step 4: Start Jupyter # Start the Jupyter instance by clicking the Run Jupyter button. Starting Jupyter and running a Spark notebook Step 5: Access Spark UI # Navigate back to Hopsworks and a Spark session will have appeared, click on the Spark UI button to go to the Spark UI. Access Spark UI and see application logs Conclusion # In this guide you learned how to configure and run a Spark application in Jupyter. You can now follow this guide to install a library that can be used in a notebook.","title":"Run Spark Notebook"},{"location":"user_guides/projects/jupyter/spark_notebook/#how-to-run-a-spark-notebook","text":"","title":"How To Run A Spark Notebook"},{"location":"user_guides/projects/jupyter/spark_notebook/#introduction","text":"Jupyter is provided as a service in Hopsworks, providing the same user experience and features as if run on your laptop. Supports JupyterLab and the classic Jupyter front-end Configured with Python3, Spark, PySpark and SparkR kernels","title":"Introduction"},{"location":"user_guides/projects/jupyter/spark_notebook/#step-1-jupyter-dashboard","text":"The image below shows the Jupyter service page in Hopsworks and is accessed by clicking Jupyter in the sidebar. Jupyter dashboard in Hopsworks From this page, you can configure various options and settings to start Jupyter with as described in the sections below.","title":"Step 1: Jupyter dashboard"},{"location":"user_guides/projects/jupyter/spark_notebook/#step-2-optional-configure-spark","text":"Next step is to configure the Spark properties to be used in Jupyter, Click edit configuration to get to the configuration page and select Spark .","title":"Step 2 (Optional): Configure spark"},{"location":"user_guides/projects/jupyter/spark_notebook/#resource-and-compute","text":"Resource allocation for the Spark driver and executors can be configured, also the number of executors and whether dynamic execution should be enabled. Driver memory : Number of cores to allocate for the Spark driver Driver virtual cores : Number of MBs to allocate for the Spark driver Executor memory : Number of cores to allocate for each Spark executor Executor virtual cores : Number of MBs to allocate for each Spark executor Dynamic/Static : Run the Spark application in static or dynamic allocation mode (see spark docs for details). Resource configuration for the Spark kernels","title":"Resource and compute"},{"location":"user_guides/projects/jupyter/spark_notebook/#attach-files-or-dependencies","text":"Additional files or dependencies required for the Spark job can be configured. Additional archives : List of zip or .tgz files that will be locally accessible by the application Additional jars : List of .jar files to add to the CLASSPATH of the application Additional python dependencies : List of .py, .zip or .egg files that will be locally accessible by the application Additional files : List of files that will be locally accessible by the application File configuration for the Spark kernels Line-separates properties to be set for the Spark application. For example, changing the configuration variables for the Kryo Serializer or setting environment variables for the driver, you can set the properties as shown below. Additional Spark configuration Click Save to save the new configuration.","title":"Attach files or dependencies"},{"location":"user_guides/projects/jupyter/spark_notebook/#step-3-optional-configure-max-runtime-and-root-path","text":"Before starting the server there are two additional configurations that can be set next to the Run Jupyter button. The runtime of the Jupyter instance can be configured, this is useful to ensure that idle instances will not be hanging around and keep allocating resources. If a limited runtime is not desirable, this can be disabled by setting no limit . Configure maximum runtime The root path from which to start the Jupyter instance can be configured. By default it starts by setting the /Jupyter folder as the root. Configure root folder","title":"Step 3 (Optional): Configure max runtime and root path"},{"location":"user_guides/projects/jupyter/spark_notebook/#step-4-start-jupyter","text":"Start the Jupyter instance by clicking the Run Jupyter button. Starting Jupyter and running a Spark notebook","title":"Step 4: Start Jupyter"},{"location":"user_guides/projects/jupyter/spark_notebook/#step-5-access-spark-ui","text":"Navigate back to Hopsworks and a Spark session will have appeared, click on the Spark UI button to go to the Spark UI. Access Spark UI and see application logs","title":"Step 5: Access Spark UI"},{"location":"user_guides/projects/jupyter/spark_notebook/#conclusion","text":"In this guide you learned how to configure and run a Spark application in Jupyter. You can now follow this guide to install a library that can be used in a notebook.","title":"Conclusion"},{"location":"user_guides/projects/kafka/consume_messages/","text":"How To Consume Message From A Topic # Introduction # A Consumer is a process which reads messages from a Kafka topic. In Hopsworks, all user roles are capable of performing 'Read' and 'Describe' actions on Kafka topics within projects that they are a member of or are shared with them. Prerequisites # This guide requires that you have previously produced messages to a kafka topic. Code # In this guide, you will learn how to consume messages from a kafka topic. Step 1: Get the Kafka API # import hopsworks project = hopsworks . login () kafka_api = project . get_kafka_api () Step 2: Configure confluent-kafka client # consumer_config = kafka_api . get_default_config () consumer_config [ 'default.topic.config' ] = { 'auto.offset.reset' : 'earliest' } from confluent_kafka import Consumer consumer = Consumer ( consumer_config ) Step 3: Consume messages from a topic # # Subscribe to topic consumer . subscribe ([ \"my_topic\" ]) for i in range ( 0 , 10 ): msg = consumer . poll ( timeout = 10.0 ) print ( msg . value ()) API Reference # KafkaTopic Conclusion # In this guide you learned how to consume messages from a Kafka Topic.","title":"Consume messages"},{"location":"user_guides/projects/kafka/consume_messages/#how-to-consume-message-from-a-topic","text":"","title":"How To Consume Message From A Topic"},{"location":"user_guides/projects/kafka/consume_messages/#introduction","text":"A Consumer is a process which reads messages from a Kafka topic. In Hopsworks, all user roles are capable of performing 'Read' and 'Describe' actions on Kafka topics within projects that they are a member of or are shared with them.","title":"Introduction"},{"location":"user_guides/projects/kafka/consume_messages/#prerequisites","text":"This guide requires that you have previously produced messages to a kafka topic.","title":"Prerequisites"},{"location":"user_guides/projects/kafka/consume_messages/#code","text":"In this guide, you will learn how to consume messages from a kafka topic.","title":"Code"},{"location":"user_guides/projects/kafka/consume_messages/#step-1-get-the-kafka-api","text":"import hopsworks project = hopsworks . login () kafka_api = project . get_kafka_api ()","title":"Step 1: Get the Kafka API"},{"location":"user_guides/projects/kafka/consume_messages/#step-2-configure-confluent-kafka-client","text":"consumer_config = kafka_api . get_default_config () consumer_config [ 'default.topic.config' ] = { 'auto.offset.reset' : 'earliest' } from confluent_kafka import Consumer consumer = Consumer ( consumer_config )","title":"Step 2: Configure confluent-kafka client"},{"location":"user_guides/projects/kafka/consume_messages/#step-3-consume-messages-from-a-topic","text":"# Subscribe to topic consumer . subscribe ([ \"my_topic\" ]) for i in range ( 0 , 10 ): msg = consumer . poll ( timeout = 10.0 ) print ( msg . value ())","title":"Step 3: Consume messages from a topic"},{"location":"user_guides/projects/kafka/consume_messages/#api-reference","text":"KafkaTopic","title":"API Reference"},{"location":"user_guides/projects/kafka/consume_messages/#conclusion","text":"In this guide you learned how to consume messages from a Kafka Topic.","title":"Conclusion"},{"location":"user_guides/projects/kafka/create_schema/","text":"How To Create A Kafka Schema # Introduction # Code # In this guide, you will learn how to create a Kafka Avro Schema in the Hopsworks Schema Registry. Step 1: Get the Kafka API # import hopsworks project = hopsworks . login () kafka_api = project . get_kafka_api () Step 2: Define the schema # Define the Avro Schema, see types for the format of the schema. schema = { \"type\" : \"record\" , \"name\" : \"tutorial\" , \"fields\" : [ { \"name\" : \"id\" , \"type\" : \"int\" }, { \"name\" : \"data\" , \"type\" : \"string\" } ] } Step 3: Create the schema # Create the schema in the Schema Registry. SCHEMA_NAME = \"schema_example\" my_schema = kafka_api . create_schema ( SCHEMA_NAME , schema ) API Reference # KafkaSchema Conclusion # In this guide you learned how to create a Kafka Schema.","title":"Create Schema"},{"location":"user_guides/projects/kafka/create_schema/#how-to-create-a-kafka-schema","text":"","title":"How To Create A Kafka Schema"},{"location":"user_guides/projects/kafka/create_schema/#introduction","text":"","title":"Introduction"},{"location":"user_guides/projects/kafka/create_schema/#code","text":"In this guide, you will learn how to create a Kafka Avro Schema in the Hopsworks Schema Registry.","title":"Code"},{"location":"user_guides/projects/kafka/create_schema/#step-1-get-the-kafka-api","text":"import hopsworks project = hopsworks . login () kafka_api = project . get_kafka_api ()","title":"Step 1: Get the Kafka API"},{"location":"user_guides/projects/kafka/create_schema/#step-2-define-the-schema","text":"Define the Avro Schema, see types for the format of the schema. schema = { \"type\" : \"record\" , \"name\" : \"tutorial\" , \"fields\" : [ { \"name\" : \"id\" , \"type\" : \"int\" }, { \"name\" : \"data\" , \"type\" : \"string\" } ] }","title":"Step 2: Define the schema"},{"location":"user_guides/projects/kafka/create_schema/#step-3-create-the-schema","text":"Create the schema in the Schema Registry. SCHEMA_NAME = \"schema_example\" my_schema = kafka_api . create_schema ( SCHEMA_NAME , schema )","title":"Step 3: Create the schema"},{"location":"user_guides/projects/kafka/create_schema/#api-reference","text":"KafkaSchema","title":"API Reference"},{"location":"user_guides/projects/kafka/create_schema/#conclusion","text":"In this guide you learned how to create a Kafka Schema.","title":"Conclusion"},{"location":"user_guides/projects/kafka/create_topic/","text":"How To Create A Kafka Topic # Introduction # A Topic is a queue to which records are stored and published. Producer applications write data to topics and consumer applications read from topics. Prerequisites # This guide requires that you have 'Data owner' role and have previously created a Kafka Schema to be used for the topic. Code # In this guide, you will learn how to create a Kafka Topic. Step 1: Get the Kafka API # import hopsworks project = hopsworks . login () kafka_api = project . get_kafka_api () Step 2: Define the schema # TOPIC_NAME = \"topic_example\" SCHEMA_NAME = \"schema_example\" my_topic = kafka_api . create_topic ( TOPIC_NAME , SCHEMA_NAME , 1 , replicas = 1 , partitions = 1 ) API Reference # KafkaTopic Conclusion # In this guide you learned how to create a Kafka Topic.","title":"Create Topic"},{"location":"user_guides/projects/kafka/create_topic/#how-to-create-a-kafka-topic","text":"","title":"How To Create A Kafka Topic"},{"location":"user_guides/projects/kafka/create_topic/#introduction","text":"A Topic is a queue to which records are stored and published. Producer applications write data to topics and consumer applications read from topics.","title":"Introduction"},{"location":"user_guides/projects/kafka/create_topic/#prerequisites","text":"This guide requires that you have 'Data owner' role and have previously created a Kafka Schema to be used for the topic.","title":"Prerequisites"},{"location":"user_guides/projects/kafka/create_topic/#code","text":"In this guide, you will learn how to create a Kafka Topic.","title":"Code"},{"location":"user_guides/projects/kafka/create_topic/#step-1-get-the-kafka-api","text":"import hopsworks project = hopsworks . login () kafka_api = project . get_kafka_api ()","title":"Step 1: Get the Kafka API"},{"location":"user_guides/projects/kafka/create_topic/#step-2-define-the-schema","text":"TOPIC_NAME = \"topic_example\" SCHEMA_NAME = \"schema_example\" my_topic = kafka_api . create_topic ( TOPIC_NAME , SCHEMA_NAME , 1 , replicas = 1 , partitions = 1 )","title":"Step 2: Define the schema"},{"location":"user_guides/projects/kafka/create_topic/#api-reference","text":"KafkaTopic","title":"API Reference"},{"location":"user_guides/projects/kafka/create_topic/#conclusion","text":"In this guide you learned how to create a Kafka Topic.","title":"Conclusion"},{"location":"user_guides/projects/kafka/produce_messages/","text":"How To Produce To A Topic # Introduction # A Producer is a process which produces messages to a Kafka topic. In Hopsworks, only users with the 'Data owner' role are capable of performing the 'Write' action on Kafka topics within the project that they are a member of. Prerequisites # This guide requires that you have 'Data owner' role and have previously created a Kafka Topic . Code # In this guide, you will learn how to produce messages to a kafka topic. Step 1: Get the Kafka API # import hopsworks project = hopsworks . login () kafka_api = project . get_kafka_api () Step 2: Configure confluent-kafka client # producer_config = kafka_api . get_default_config () from confluent_kafka import Producer producer = Producer ( producer_config ) Step 3: Produce messages to topic # import uuid import json # Send a few messages for i in range ( 0 , 10 ): producer . produce ( \"my_topic\" , json . dumps ({ \"id\" : i , \"data\" : str ( uuid . uuid1 ())}), \"key\" ) # Trigger the sending of all messages to the brokers, 10 sec timeout producer . flush ( 10 ) API Reference # KafkaTopic Conclusion # In this guide you learned how to produce messages to a Kafka Topic. Next step is to create a Consumer to read the messages from the topic.","title":"Produce messages"},{"location":"user_guides/projects/kafka/produce_messages/#how-to-produce-to-a-topic","text":"","title":"How To Produce To A Topic"},{"location":"user_guides/projects/kafka/produce_messages/#introduction","text":"A Producer is a process which produces messages to a Kafka topic. In Hopsworks, only users with the 'Data owner' role are capable of performing the 'Write' action on Kafka topics within the project that they are a member of.","title":"Introduction"},{"location":"user_guides/projects/kafka/produce_messages/#prerequisites","text":"This guide requires that you have 'Data owner' role and have previously created a Kafka Topic .","title":"Prerequisites"},{"location":"user_guides/projects/kafka/produce_messages/#code","text":"In this guide, you will learn how to produce messages to a kafka topic.","title":"Code"},{"location":"user_guides/projects/kafka/produce_messages/#step-1-get-the-kafka-api","text":"import hopsworks project = hopsworks . login () kafka_api = project . get_kafka_api ()","title":"Step 1: Get the Kafka API"},{"location":"user_guides/projects/kafka/produce_messages/#step-2-configure-confluent-kafka-client","text":"producer_config = kafka_api . get_default_config () from confluent_kafka import Producer producer = Producer ( producer_config )","title":"Step 2: Configure confluent-kafka client"},{"location":"user_guides/projects/kafka/produce_messages/#step-3-produce-messages-to-topic","text":"import uuid import json # Send a few messages for i in range ( 0 , 10 ): producer . produce ( \"my_topic\" , json . dumps ({ \"id\" : i , \"data\" : str ( uuid . uuid1 ())}), \"key\" ) # Trigger the sending of all messages to the brokers, 10 sec timeout producer . flush ( 10 )","title":"Step 3: Produce messages to topic"},{"location":"user_guides/projects/kafka/produce_messages/#api-reference","text":"KafkaTopic","title":"API Reference"},{"location":"user_guides/projects/kafka/produce_messages/#conclusion","text":"In this guide you learned how to produce messages to a Kafka Topic. Next step is to create a Consumer to read the messages from the topic.","title":"Conclusion"},{"location":"user_guides/projects/opensearch/connect/","text":"How To Connect To OpenSearch # Introduction # Text here Limited to internal Jobs and Notebooks Currently it's only possible to configure the opensearch-py client in a job or jupyter notebook running inside the Hopsworks cluster. Code # In this guide, you will learn how to connect to the OpenSearch cluster using an opensearch-py client. Step 1: Get the OpenSearch API # import hopsworks project = hopsworks . login () opensearch_api = project . get_opensearch_api () Step 2: Configure the opensearch-py client # from opensearchpy import OpenSearch client = OpenSearch ( ** opensearch_api . get_default_py_config ()) API Reference # OpenSearch Conclusion # In this guide you learned how to connect to the OpenSearch cluster. You can now use the client to interact directly with the OpenSearch cluster, such as vector database .","title":"Connect"},{"location":"user_guides/projects/opensearch/connect/#how-to-connect-to-opensearch","text":"","title":"How To Connect To OpenSearch"},{"location":"user_guides/projects/opensearch/connect/#introduction","text":"Text here Limited to internal Jobs and Notebooks Currently it's only possible to configure the opensearch-py client in a job or jupyter notebook running inside the Hopsworks cluster.","title":"Introduction"},{"location":"user_guides/projects/opensearch/connect/#code","text":"In this guide, you will learn how to connect to the OpenSearch cluster using an opensearch-py client.","title":"Code"},{"location":"user_guides/projects/opensearch/connect/#step-1-get-the-opensearch-api","text":"import hopsworks project = hopsworks . login () opensearch_api = project . get_opensearch_api ()","title":"Step 1: Get the OpenSearch API"},{"location":"user_guides/projects/opensearch/connect/#step-2-configure-the-opensearch-py-client","text":"from opensearchpy import OpenSearch client = OpenSearch ( ** opensearch_api . get_default_py_config ())","title":"Step 2: Configure the opensearch-py client"},{"location":"user_guides/projects/opensearch/connect/#api-reference","text":"OpenSearch","title":"API Reference"},{"location":"user_guides/projects/opensearch/connect/#conclusion","text":"In this guide you learned how to connect to the OpenSearch cluster. You can now use the client to interact directly with the OpenSearch cluster, such as vector database .","title":"Conclusion"},{"location":"user_guides/projects/project/add_members/","text":"How To Add Members To A Project # Introduction # In this guide, you will learn how to add new members to your project as well as the different roles within a project. Step 1: Members list # On the Project settings page, you can find the General section, which lists the members of the project. List of project members Step 2: Add a new member # Next click Add members and a dialog where users can be invited will appear. Select the users to invite. Add new member dialog Subsequently, the selected project members can be assigned to 2 different roles, depending on the privileges necessary for him/her to fulfill their needs. Data owner # Data owners hold the highest authority in the project, having full control of its contents. They are allowed to: - Share a project - Manage the project and its members - Work with all feature store abstractions (such as Feature groups, Feature views, Storage connectors, etc.) It is worth mentioning that the project's creator (aka. author ) is a special type of Data owner . He is the only user capable of deleting the project and it is impossible to change his role to Data scientist . Data scientist # Data scientists can be viewed as the users of data. They are allowed to: - Create feature views/training datasets using existing features - Manage the feature views/training datasets they have created Step 3: Member invited # The invited user will now appear in the list of members and will have access to the project. List of project members Conclusion # In this guide, you learned how to add a new member and the types of roles this member can be a part of.","title":"Add Members"},{"location":"user_guides/projects/project/add_members/#how-to-add-members-to-a-project","text":"","title":"How To Add Members To A Project"},{"location":"user_guides/projects/project/add_members/#introduction","text":"In this guide, you will learn how to add new members to your project as well as the different roles within a project.","title":"Introduction"},{"location":"user_guides/projects/project/add_members/#step-1-members-list","text":"On the Project settings page, you can find the General section, which lists the members of the project. List of project members","title":"Step 1: Members list"},{"location":"user_guides/projects/project/add_members/#step-2-add-a-new-member","text":"Next click Add members and a dialog where users can be invited will appear. Select the users to invite. Add new member dialog Subsequently, the selected project members can be assigned to 2 different roles, depending on the privileges necessary for him/her to fulfill their needs.","title":"Step 2: Add a new member"},{"location":"user_guides/projects/project/add_members/#data-owner","text":"Data owners hold the highest authority in the project, having full control of its contents. They are allowed to: - Share a project - Manage the project and its members - Work with all feature store abstractions (such as Feature groups, Feature views, Storage connectors, etc.) It is worth mentioning that the project's creator (aka. author ) is a special type of Data owner . He is the only user capable of deleting the project and it is impossible to change his role to Data scientist .","title":"Data owner"},{"location":"user_guides/projects/project/add_members/#data-scientist","text":"Data scientists can be viewed as the users of data. They are allowed to: - Create feature views/training datasets using existing features - Manage the feature views/training datasets they have created","title":"Data scientist"},{"location":"user_guides/projects/project/add_members/#step-3-member-invited","text":"The invited user will now appear in the list of members and will have access to the project. List of project members","title":"Step 3: Member invited"},{"location":"user_guides/projects/project/add_members/#conclusion","text":"In this guide, you learned how to add a new member and the types of roles this member can be a part of.","title":"Conclusion"},{"location":"user_guides/projects/project/create_project/","text":"How To Create A Project # Introduction # In this guide, you will learn how to create a new project. Project name validation rules A valid project name can only contain characters a-z, A-Z, 0-9 and special characters \u2018_\u2019 and \u2018.\u2019 but not \u2018__\u2019 (double underscore). There is also a number of reserved project names that can not be used. GUI # Step 1: Create a project # If you log in to the platform and do not have any projects, you are presented with the following view. To run the Feature Store tour click Run a demo project , to create a new project click Create new project . For this guide click Create new project to continue. Landing page Step 2: Project creation form # In the creation form in which you enter the project name, an optional description and set of members to invite to the project. Project creation form Step 3: Project creation # Then wait for the project creation process to finish. List of created API Keys Step 4: Project overview # Once the project is created the overview page for it will appear. List of created API Keys Code # Step 1: Connect to Hopsworks # import hopsworks connection = hopsworks . connection () Step 2: Create project # project = connection . create_project ( \"my_project\" ) API Reference # Projects Reserved project names # PROJECTS, HOPS-SYSTEM, HOPSWORKS, INFORMATION_SCHEMA, AIRFLOW, GLASSFISH_TIMERS, GRAFANA, HOPS, METASTORE, MYSQL, NDBINFO, RONDB_REPLICATION, PERFORMANCE_SCHEMA, SQOOP, SYS, GLASSFISH_TIMERS, GRAFANA, HOPS, METASTORE, MYSQL, NDBINFO, PERFORMANCE_SCHEMA, SQOOP, SYS, BIGINT, BINARY, BOOLEAN, BOTH, BY, CASE, CAST, CHAR, COLUMN, CONF, CREATE, CROSS, CUBE, CURRENT, CURRENT_DATE, CURRENT_TIMESTAMP, CURSOR, DATABASE, DATE, DECIMAL, DELETE, DESCRIBE, DISTINCT, DOUBLE, DROP, ELSE, END, EXCHANGE, EXISTS, EXTENDED, EXTERNAL, FALSE, FETCH, FLOAT, FOLLOWING, FOR, FROM, FULL, FUNCTION, GRANT, GROUP, GROUPING, HAVING, IF, IMPORT, IN, INNER, INSERT, INT, INTERSECT, INTERVAL, INTO, IS, JOIN, LATERAL, LEFT, LESS, LIKE, LOCAL, MACRO, MAP, MORE, NONE, NOT, NULL, OF, ON, OR, ORDER, OUT, OUTER, OVER, PARTIALSCAN, PARTITION, PERCENT, PRECEDING, PRESERVE, PROCEDURE, RANGE, READS, REDUCE, REVOKE, RIGHT, ROLLUP, ROW, ROWS, SELECT, SET, SMALLINT, TABLE, TABLESAMPLE, THEN, TIMESTAMP, TO, TRANSFORM, TRIGGER, TRUE, TRUNCATE, UNBOUNDED, UNION, UNIQUEJOIN, UPDATE, USER, USING, UTC_TMESTAMP, VALUES, VARCHAR, WHEN, WHERE, WINDOW, WITH, COMMIT, ONLY, REGEXP, RLIKE, ROLLBACK, START, CACHE, CONSTRAINT, FOREIGN, PRIMARY, REFERENCES, DAYOFWEEK, EXTRACT, FLOOR, INTEGER, PRECISION, VIEWS, TIME, NUMERIC, SYNC, BASE, PYTHON37, FILEBEAT. And any word containing _FEATURESTORE. Conclusion # In this guide you learned how to create a project.","title":"Create Project"},{"location":"user_guides/projects/project/create_project/#how-to-create-a-project","text":"","title":"How To Create A Project"},{"location":"user_guides/projects/project/create_project/#introduction","text":"In this guide, you will learn how to create a new project. Project name validation rules A valid project name can only contain characters a-z, A-Z, 0-9 and special characters \u2018_\u2019 and \u2018.\u2019 but not \u2018__\u2019 (double underscore). There is also a number of reserved project names that can not be used.","title":"Introduction"},{"location":"user_guides/projects/project/create_project/#gui","text":"","title":"GUI"},{"location":"user_guides/projects/project/create_project/#step-1-create-a-project","text":"If you log in to the platform and do not have any projects, you are presented with the following view. To run the Feature Store tour click Run a demo project , to create a new project click Create new project . For this guide click Create new project to continue. Landing page","title":"Step 1: Create a project"},{"location":"user_guides/projects/project/create_project/#step-2-project-creation-form","text":"In the creation form in which you enter the project name, an optional description and set of members to invite to the project. Project creation form","title":"Step 2: Project creation form"},{"location":"user_guides/projects/project/create_project/#step-3-project-creation","text":"Then wait for the project creation process to finish. List of created API Keys","title":"Step 3: Project creation"},{"location":"user_guides/projects/project/create_project/#step-4-project-overview","text":"Once the project is created the overview page for it will appear. List of created API Keys","title":"Step 4: Project overview"},{"location":"user_guides/projects/project/create_project/#code","text":"","title":"Code"},{"location":"user_guides/projects/project/create_project/#step-1-connect-to-hopsworks","text":"import hopsworks connection = hopsworks . connection ()","title":"Step 1: Connect to Hopsworks"},{"location":"user_guides/projects/project/create_project/#step-2-create-project","text":"project = connection . create_project ( \"my_project\" )","title":"Step 2: Create project"},{"location":"user_guides/projects/project/create_project/#api-reference","text":"Projects","title":"API Reference"},{"location":"user_guides/projects/project/create_project/#reserved-project-names","text":"PROJECTS, HOPS-SYSTEM, HOPSWORKS, INFORMATION_SCHEMA, AIRFLOW, GLASSFISH_TIMERS, GRAFANA, HOPS, METASTORE, MYSQL, NDBINFO, RONDB_REPLICATION, PERFORMANCE_SCHEMA, SQOOP, SYS, GLASSFISH_TIMERS, GRAFANA, HOPS, METASTORE, MYSQL, NDBINFO, PERFORMANCE_SCHEMA, SQOOP, SYS, BIGINT, BINARY, BOOLEAN, BOTH, BY, CASE, CAST, CHAR, COLUMN, CONF, CREATE, CROSS, CUBE, CURRENT, CURRENT_DATE, CURRENT_TIMESTAMP, CURSOR, DATABASE, DATE, DECIMAL, DELETE, DESCRIBE, DISTINCT, DOUBLE, DROP, ELSE, END, EXCHANGE, EXISTS, EXTENDED, EXTERNAL, FALSE, FETCH, FLOAT, FOLLOWING, FOR, FROM, FULL, FUNCTION, GRANT, GROUP, GROUPING, HAVING, IF, IMPORT, IN, INNER, INSERT, INT, INTERSECT, INTERVAL, INTO, IS, JOIN, LATERAL, LEFT, LESS, LIKE, LOCAL, MACRO, MAP, MORE, NONE, NOT, NULL, OF, ON, OR, ORDER, OUT, OUTER, OVER, PARTIALSCAN, PARTITION, PERCENT, PRECEDING, PRESERVE, PROCEDURE, RANGE, READS, REDUCE, REVOKE, RIGHT, ROLLUP, ROW, ROWS, SELECT, SET, SMALLINT, TABLE, TABLESAMPLE, THEN, TIMESTAMP, TO, TRANSFORM, TRIGGER, TRUE, TRUNCATE, UNBOUNDED, UNION, UNIQUEJOIN, UPDATE, USER, USING, UTC_TMESTAMP, VALUES, VARCHAR, WHEN, WHERE, WINDOW, WITH, COMMIT, ONLY, REGEXP, RLIKE, ROLLBACK, START, CACHE, CONSTRAINT, FOREIGN, PRIMARY, REFERENCES, DAYOFWEEK, EXTRACT, FLOOR, INTEGER, PRECISION, VIEWS, TIME, NUMERIC, SYNC, BASE, PYTHON37, FILEBEAT. And any word containing _FEATURESTORE.","title":"Reserved project names"},{"location":"user_guides/projects/project/create_project/#conclusion","text":"In this guide you learned how to create a project.","title":"Conclusion"},{"location":"user_guides/projects/python/custom_commands/","text":"Adding extra configuration with generic bash commands # Introduction # Hopsworks comes with a prepackaged Python environment that contains libraries for data engineering, machine learning, and more general data science development. Hopsworks also offers the ability to install additional packages using different options e.g., Pypi, conda channel, and public or private git repository among others. Some Python libraries require the installation of some OS-Level libraries. In some cases, you may need to add more complex configuration to your environment. This demands writing your own commands and executing them on top of the existing environment. In this guide, you will learn how to run custom bash commands that can be used to add more complex configuration to your environment e.g., installing OS-Level packages or configuring an oracle database. Running bash commands # In this section, we will see how you can run custom bash commands in Hopsworks to configure your Python environment. In Hopsworks, we maintain a docker image built on top of Ubuntu Linux distribution. You can run generic bash commands on top of the project environment from the UI or REST API. Setting up the bash script and artifacts from the UI # To use the UI, navigate to the Python environment in the Project settings. In the Python environment page, navigate to custom commands. From the UI, you can write the bash commands in the textbox provided. These bash commands will be uploaded and executed when building your new environment. You can include build artifacts e.g., binaries that you would like to execute or include when building the environment. See Figure 1. Figure 1: You can write custom commands and upload build artifacts from the UI Code # You can also run the custom commands using the REST API. From the REST API, you should provide the path, in HOPSFS, to the bash script and the artifacts(comma seperated string of paths in HopsFs). The REST API endpoint for running custom commands is: hopsworks-api/api/project/<projectId>/python/environments/<pythonVersion>/commands/custom and the body should look like this: { \"commandsFile\" : \"<pathToYourBashScriptInHopsFS>\" , \"artifacts\" : \"<commaSeperatedListOfPathsToArtifactsInHopsFS>\" } What to include in the bash script # There are few important things to be aware of when writing the bash script: The first line of your bash script should always be #!/bin/bash (known as shebang) so that the script can be interpreted and executed using the Bash shell. You can use apt , apt-get and deb commands to install packages. You should always run these commands with sudo . In some cases, these commands will ask for user input, therefore you should provide the input of what the command expects, e.g., sudo apt -y install , otherwise the build will fail. We have already configured apt-get to be non-interactive The build artifacts will be copied to srv/hops/build . You can use them in your script via this path. This path is also available via the environmental variable BUILD_PATH . If you want to use many artifacts it is advisable to create a zip file and upload it to HopsFS in one of your project datasets. You can then include the zip file as one of the artifacts. The conda environment is located in /srv/hops/anaconda/envs/theenv . You can install or uninstall packages in the conda environment using pip like: /srv/hops/anaconda/envs/theenv/bin/pip install spotify==0.10.2 . If the command requires some input, write the command together with the expected input otherwise the build will fail. Conclusion # In this guide you have learned how to add extra configuration to your python environment by running custom commands.","title":"Custom Commands"},{"location":"user_guides/projects/python/custom_commands/#adding-extra-configuration-with-generic-bash-commands","text":"","title":"Adding extra configuration with generic bash commands"},{"location":"user_guides/projects/python/custom_commands/#introduction","text":"Hopsworks comes with a prepackaged Python environment that contains libraries for data engineering, machine learning, and more general data science development. Hopsworks also offers the ability to install additional packages using different options e.g., Pypi, conda channel, and public or private git repository among others. Some Python libraries require the installation of some OS-Level libraries. In some cases, you may need to add more complex configuration to your environment. This demands writing your own commands and executing them on top of the existing environment. In this guide, you will learn how to run custom bash commands that can be used to add more complex configuration to your environment e.g., installing OS-Level packages or configuring an oracle database.","title":"Introduction"},{"location":"user_guides/projects/python/custom_commands/#running-bash-commands","text":"In this section, we will see how you can run custom bash commands in Hopsworks to configure your Python environment. In Hopsworks, we maintain a docker image built on top of Ubuntu Linux distribution. You can run generic bash commands on top of the project environment from the UI or REST API.","title":"Running bash commands"},{"location":"user_guides/projects/python/custom_commands/#setting-up-the-bash-script-and-artifacts-from-the-ui","text":"To use the UI, navigate to the Python environment in the Project settings. In the Python environment page, navigate to custom commands. From the UI, you can write the bash commands in the textbox provided. These bash commands will be uploaded and executed when building your new environment. You can include build artifacts e.g., binaries that you would like to execute or include when building the environment. See Figure 1. Figure 1: You can write custom commands and upload build artifacts from the UI","title":"Setting up the bash script and artifacts from the UI"},{"location":"user_guides/projects/python/custom_commands/#code","text":"You can also run the custom commands using the REST API. From the REST API, you should provide the path, in HOPSFS, to the bash script and the artifacts(comma seperated string of paths in HopsFs). The REST API endpoint for running custom commands is: hopsworks-api/api/project/<projectId>/python/environments/<pythonVersion>/commands/custom and the body should look like this: { \"commandsFile\" : \"<pathToYourBashScriptInHopsFS>\" , \"artifacts\" : \"<commaSeperatedListOfPathsToArtifactsInHopsFS>\" }","title":"Code"},{"location":"user_guides/projects/python/custom_commands/#what-to-include-in-the-bash-script","text":"There are few important things to be aware of when writing the bash script: The first line of your bash script should always be #!/bin/bash (known as shebang) so that the script can be interpreted and executed using the Bash shell. You can use apt , apt-get and deb commands to install packages. You should always run these commands with sudo . In some cases, these commands will ask for user input, therefore you should provide the input of what the command expects, e.g., sudo apt -y install , otherwise the build will fail. We have already configured apt-get to be non-interactive The build artifacts will be copied to srv/hops/build . You can use them in your script via this path. This path is also available via the environmental variable BUILD_PATH . If you want to use many artifacts it is advisable to create a zip file and upload it to HopsFS in one of your project datasets. You can then include the zip file as one of the artifacts. The conda environment is located in /srv/hops/anaconda/envs/theenv . You can install or uninstall packages in the conda environment using pip like: /srv/hops/anaconda/envs/theenv/bin/pip install spotify==0.10.2 . If the command requires some input, write the command together with the expected input otherwise the build will fail.","title":"What to include in the bash script"},{"location":"user_guides/projects/python/custom_commands/#conclusion","text":"In this guide you have learned how to add extra configuration to your python environment by running custom commands.","title":"Conclusion"},{"location":"user_guides/projects/python/environment_history/","text":"Python Environment History # The Hopsworks installation ships with a Miniconda environment that comes preinstalled with the most popular libraries you can find in a data scientist toolkit, including TensorFlow, PyTorch and sci-kit-learn. The environment may be managed using the Hopsworks Python service to install or manage libraries which may then be used in Jupyter or the Jobs service in the platform. The Python virtual environment is shared by different members of the project. When a member of the project introduces a change to the environment i.e., installs/uninstalls a library, a new environment is created and it becomes a defacto environment for everyone in the project. It is therefore important to track how the environment has been changing over time i.e., what libraries were installed, uninstalled, upgraded, or downgraded when the environment was created and who introduced the changes. In this guide, you will learn how you can track the changes of your Python environment. Viewing python environment history in the UI # The Python environment evolves over time as libraries are installed, uninstalled, upgraded, and downgraded. To assist in tracking the changes in the environment, you can see the environment history in the UI. You can view what changes were introduced at each point a new environment was created. Hopsworks will keep a version of a YAML file for each environment so that if you want to restore an older environment you can use it. To see the differences between environments click on the button as shown in figure 1. You will then see the difference between the environment and the previous environment it was created from. Figure 1: You can see the difference between the two environments by clicking on the button pointed. If you had built the environment using custom commands you can go back to see what commands were run during the build as shown in figure 2. Figure 2: You can see custom commands that were used to create the environment by clicking on the button pointed. Conclusion # In this guide, you have learned how you can track the changes of your Python environment.","title":"Python Environment History"},{"location":"user_guides/projects/python/environment_history/#python-environment-history","text":"The Hopsworks installation ships with a Miniconda environment that comes preinstalled with the most popular libraries you can find in a data scientist toolkit, including TensorFlow, PyTorch and sci-kit-learn. The environment may be managed using the Hopsworks Python service to install or manage libraries which may then be used in Jupyter or the Jobs service in the platform. The Python virtual environment is shared by different members of the project. When a member of the project introduces a change to the environment i.e., installs/uninstalls a library, a new environment is created and it becomes a defacto environment for everyone in the project. It is therefore important to track how the environment has been changing over time i.e., what libraries were installed, uninstalled, upgraded, or downgraded when the environment was created and who introduced the changes. In this guide, you will learn how you can track the changes of your Python environment.","title":"Python Environment History"},{"location":"user_guides/projects/python/environment_history/#viewing-python-environment-history-in-the-ui","text":"The Python environment evolves over time as libraries are installed, uninstalled, upgraded, and downgraded. To assist in tracking the changes in the environment, you can see the environment history in the UI. You can view what changes were introduced at each point a new environment was created. Hopsworks will keep a version of a YAML file for each environment so that if you want to restore an older environment you can use it. To see the differences between environments click on the button as shown in figure 1. You will then see the difference between the environment and the previous environment it was created from. Figure 1: You can see the difference between the two environments by clicking on the button pointed. If you had built the environment using custom commands you can go back to see what commands were run during the build as shown in figure 2. Figure 2: You can see custom commands that were used to create the environment by clicking on the button pointed.","title":"Viewing python environment history in the UI"},{"location":"user_guides/projects/python/environment_history/#conclusion","text":"In this guide, you have learned how you can track the changes of your Python environment.","title":"Conclusion"},{"location":"user_guides/projects/python/python_env_export/","text":"How To Export Python Environment # Introduction # The python environment in a project can be exported to an environment.yml file. It can be useful to export it and then recreate it outside of Hopsworks, or just have a snapshot of all the installed libraries and their versions. In this guide, you will learn how to export the python environment for a project. Step 1: Go to environment # Under the Project settings section you can find the Python libraries setting. Step 2: Click Export env # An existing Anaconda environment can be exported as a yml file, clicking the Export env will download the environment.yml file in your browser. Export environment Conclusion # In this guide you learned how to export your python environment.","title":"Export environment"},{"location":"user_guides/projects/python/python_env_export/#how-to-export-python-environment","text":"","title":"How To Export Python Environment"},{"location":"user_guides/projects/python/python_env_export/#introduction","text":"The python environment in a project can be exported to an environment.yml file. It can be useful to export it and then recreate it outside of Hopsworks, or just have a snapshot of all the installed libraries and their versions. In this guide, you will learn how to export the python environment for a project.","title":"Introduction"},{"location":"user_guides/projects/python/python_env_export/#step-1-go-to-environment","text":"Under the Project settings section you can find the Python libraries setting.","title":"Step 1: Go to environment"},{"location":"user_guides/projects/python/python_env_export/#step-2-click-export-env","text":"An existing Anaconda environment can be exported as a yml file, clicking the Export env will download the environment.yml file in your browser. Export environment","title":"Step 2: Click Export env"},{"location":"user_guides/projects/python/python_env_export/#conclusion","text":"In this guide you learned how to export your python environment.","title":"Conclusion"},{"location":"user_guides/projects/python/python_env_recreate/","text":"How To Recreate Python Environment # Introduction # Sometimes it may be desirable to recreate the python environment to start from the same state the python environment was created with. In this guide, you will learn how to recreate the python environment. Keep in mind There may be Jobs or Jupyter notebooks that depend on additional libraries that have been installed. It is recommended to first export the environment to save a snapshot of all libraries currently installed and their versions. Step 1: Remove the environment # Under the Project settings section you can find the Python libraries setting. First click Remove env . Remove environment Step 2: Create new environment # After removing the environment, simply recreate it by clicking Create Environment . Create environment Conclusion # In this guide you learned how to recreate your python environment.","title":"Recreate environment"},{"location":"user_guides/projects/python/python_env_recreate/#how-to-recreate-python-environment","text":"","title":"How To Recreate Python Environment"},{"location":"user_guides/projects/python/python_env_recreate/#introduction","text":"Sometimes it may be desirable to recreate the python environment to start from the same state the python environment was created with. In this guide, you will learn how to recreate the python environment. Keep in mind There may be Jobs or Jupyter notebooks that depend on additional libraries that have been installed. It is recommended to first export the environment to save a snapshot of all libraries currently installed and their versions.","title":"Introduction"},{"location":"user_guides/projects/python/python_env_recreate/#step-1-remove-the-environment","text":"Under the Project settings section you can find the Python libraries setting. First click Remove env . Remove environment","title":"Step 1: Remove the environment"},{"location":"user_guides/projects/python/python_env_recreate/#step-2-create-new-environment","text":"After removing the environment, simply recreate it by clicking Create Environment . Create environment","title":"Step 2: Create new environment"},{"location":"user_guides/projects/python/python_env_recreate/#conclusion","text":"In this guide you learned how to recreate your python environment.","title":"Conclusion"},{"location":"user_guides/projects/python/python_install/","text":"How To Install Python Libraries # Introduction # The prepackaged python environment in Hopsworks contains a large number of libraries for data engineering, machine learning and more general data science development. But in some cases users want to install additional packages for their applications. In this guide, you will learn how to install Python packages using these different options. PyPi, using pip package manager A conda channel, using conda package manager Packages saved in certain file formats, currently we support .whl or .egg A public or private git repository A requirements.txt file to install multiple libraries at the same time using pip An environment.yml file to install multiple libraries at the same time using conda and pip Under the Project settings section you can find the Python libraries setting. Notice If your libraries require installing some extra OS-Level packages, refer to the guide custom commands guide on how to install OS-Level packages. Name and version # Enter the name and, optionally, the desired version to install. Installing library by name and version Search # Enter the search term and select a library and version to install. Installing library using search Distribution (.whl, .egg..) # Install a python package by uploading the corresponding package file and selecting it in the file browser. Installing library from file Git source # To install from a git repository simply provide the repository URL. The URL you should provide is the same as you would enter on the command line using pip install git+{repo_url} . In the case of a private git repository, also select whether it is a GitHub or GitLab repository and the preconfigured access token for the repository. Note : If you are installing from a git repository which is not GitHub or GitLab simply supply the access token in the URL. Keep in mind that in this case the access token may be visible in logs for other users in the same project to see. Installing library from git repo Conclusion # In this guide you learned how to install python libraries. Now you can use the library in a Jupyter notebook or a Job .","title":"Install Library"},{"location":"user_guides/projects/python/python_install/#how-to-install-python-libraries","text":"","title":"How To Install Python Libraries"},{"location":"user_guides/projects/python/python_install/#introduction","text":"The prepackaged python environment in Hopsworks contains a large number of libraries for data engineering, machine learning and more general data science development. But in some cases users want to install additional packages for their applications. In this guide, you will learn how to install Python packages using these different options. PyPi, using pip package manager A conda channel, using conda package manager Packages saved in certain file formats, currently we support .whl or .egg A public or private git repository A requirements.txt file to install multiple libraries at the same time using pip An environment.yml file to install multiple libraries at the same time using conda and pip Under the Project settings section you can find the Python libraries setting. Notice If your libraries require installing some extra OS-Level packages, refer to the guide custom commands guide on how to install OS-Level packages.","title":"Introduction"},{"location":"user_guides/projects/python/python_install/#name-and-version","text":"Enter the name and, optionally, the desired version to install. Installing library by name and version","title":"Name and version"},{"location":"user_guides/projects/python/python_install/#search","text":"Enter the search term and select a library and version to install. Installing library using search","title":"Search"},{"location":"user_guides/projects/python/python_install/#distribution-whl-egg","text":"Install a python package by uploading the corresponding package file and selecting it in the file browser. Installing library from file","title":"Distribution (.whl, .egg..)"},{"location":"user_guides/projects/python/python_install/#git-source","text":"To install from a git repository simply provide the repository URL. The URL you should provide is the same as you would enter on the command line using pip install git+{repo_url} . In the case of a private git repository, also select whether it is a GitHub or GitLab repository and the preconfigured access token for the repository. Note : If you are installing from a git repository which is not GitHub or GitLab simply supply the access token in the URL. Keep in mind that in this case the access token may be visible in logs for other users in the same project to see. Installing library from git repo","title":"Git source"},{"location":"user_guides/projects/python/python_install/#conclusion","text":"In this guide you learned how to install python libraries. Now you can use the library in a Jupyter notebook or a Job .","title":"Conclusion"},{"location":"user_guides/projects/secrets/create_secret/","text":"How To Create A Secret # Introduction # A Secret is a key-value pair used to store encrypted information accessible only to the owner of the secret. Also if you wish to, you can share the same secret API key with all the members of a Project. UI # Step 1: Navigate to Secrets # In the Account Settings page you can find the Secrets section showing a list of all secrets. List of secrets Step 2: Create a Secret # Click New Secret to bring up the dialog for secret creation. Enter a name for the secret to be used for lookup, and the secret value. If the secret should be private to this user, select Private , to share the secret with all members of a project select Project and enter the project name. Create new secret dialog Step 3: Secret created # Click New Secret to bring up the dialog for secret creation. Enter a name for the secret to be used for lookup, and the secret value. If the secret should be private to this user, select Private , to share the secret with all members of a project select Project and enter the project name. Secret is now created Code # Step 1: Get secrets API # import hopsworks connection = hopsworks . connection () secrets_api = connection . get_secrets_api () Step 2: Create secret # secret = secrets_api . create_secret ( \"my_secret\" , \"Fk3MoPlQXCQvPo\" ) API Reference # Secrets Conclusion # In this guide you learned how to create a secret.","title":"Create Secret"},{"location":"user_guides/projects/secrets/create_secret/#how-to-create-a-secret","text":"","title":"How To Create A Secret"},{"location":"user_guides/projects/secrets/create_secret/#introduction","text":"A Secret is a key-value pair used to store encrypted information accessible only to the owner of the secret. Also if you wish to, you can share the same secret API key with all the members of a Project.","title":"Introduction"},{"location":"user_guides/projects/secrets/create_secret/#ui","text":"","title":"UI"},{"location":"user_guides/projects/secrets/create_secret/#step-1-navigate-to-secrets","text":"In the Account Settings page you can find the Secrets section showing a list of all secrets. List of secrets","title":"Step 1: Navigate to Secrets"},{"location":"user_guides/projects/secrets/create_secret/#step-2-create-a-secret","text":"Click New Secret to bring up the dialog for secret creation. Enter a name for the secret to be used for lookup, and the secret value. If the secret should be private to this user, select Private , to share the secret with all members of a project select Project and enter the project name. Create new secret dialog","title":"Step 2: Create a Secret"},{"location":"user_guides/projects/secrets/create_secret/#step-3-secret-created","text":"Click New Secret to bring up the dialog for secret creation. Enter a name for the secret to be used for lookup, and the secret value. If the secret should be private to this user, select Private , to share the secret with all members of a project select Project and enter the project name. Secret is now created","title":"Step 3: Secret created"},{"location":"user_guides/projects/secrets/create_secret/#code","text":"","title":"Code"},{"location":"user_guides/projects/secrets/create_secret/#step-1-get-secrets-api","text":"import hopsworks connection = hopsworks . connection () secrets_api = connection . get_secrets_api ()","title":"Step 1: Get secrets API"},{"location":"user_guides/projects/secrets/create_secret/#step-2-create-secret","text":"secret = secrets_api . create_secret ( \"my_secret\" , \"Fk3MoPlQXCQvPo\" )","title":"Step 2: Create secret"},{"location":"user_guides/projects/secrets/create_secret/#api-reference","text":"Secrets","title":"API Reference"},{"location":"user_guides/projects/secrets/create_secret/#conclusion","text":"In this guide you learned how to create a secret.","title":"Conclusion"}]}