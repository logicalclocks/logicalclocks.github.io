{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Enterprise Data Feature Engineering Frameworks Pandas Flink Spark SQL Storage connectors JDBC BigQuery Object Store Snowflake RedShift Enterprise Feature Store Govern &amp; Monitor Serve Share &amp; Re-use Create Project Based Collaboration Write API Feature Groups External Feature Groups Read API Feature Views Training Data Feature Vectors Search, Versioning, Statistics, Monitoring Provenance &amp; Lineage Azure AWS Google Cloud On-premise Enterprise AI MLOps Experiments &amp; Model Training Model Registry Model Serving Operational ML Analytical ML BI Tools Vector DBOpenSearch <p>Hopsworks is a data platform for ML with a Python-centric Feature Store and MLOps capabilities. Hopsworks is a modular platform. You can use it as a standalone Feature Store, you can use it to manage, govern, and serve your models, and you can even use it to develop and operate feature, training and inference pipelines. Hopsworks brings collaboration for ML teams, providing a secure, governed platform for developing, managing, and sharing ML assets - features, models, training data, batch scoring data, logs, and more. </p>"},{"location":"#python-centric-feature-store","title":"Python-Centric Feature Store","text":"<p>Hopsworks is widely used as a standalone Feature Store. Hopsworks breaks the monolithic model development pipeline into separate feature and training pipelines, enabling both feature reuse and better tested ML assets. You can develop features by building feature pipelines in any Python (or Spark or Flink) environment, either inside or outside Hopsworks. You can use the Python frameworks you are familiar with to build production feature pipelines. You can compute aggregations in Pandas, validate feature data with Great Expectations, reduce your data dimensionality with embeddings and PCA, test your feature logic and features end-to-end with PyTest, and transform your categorical and numerical features with Scikit-Learn, TensorFlow, and PyTorch. You can orchestrate your feature pipelines with your Python framework of choice, including Hopsworks' own Airflow support.</p>"},{"location":"#the-widest-feature-store-capabilities","title":"The Widest Feature Store Capabilities","text":"<p>Hopsworks Feature Store also supports feature pipelines in PySpark, Spark, Flink, and SQL. Offline features can either be stored in Hopsworks, as Hudi tables on object storage, or in external data lakehouses (Snowflake, Databricks, Redshift, BigQuery, any JDBC-enabled platform) via External Feature Groups. Online features are served by RonDB, developed by Hopsworks as the lowest latency, highest throughput, highest availability data store for your features.</p>"},{"location":"#mlops-on-hopsworks","title":"MLOps on Hopsworks","text":"<p>Hopsworks provides model serving capabilities through KServe, with additional support for feature/prediction logging to Kafka (also part of Hopsworks), and secure, low-latency model deployments via Istio. Hopsworks also has a Model Registry for KServe, with support for versioning both models and model assets (such as KServe transformers). Hopsworks also includes a vector database to provide similarity search capabilities for embeddings, based on OpenSearch.</p>"},{"location":"#project-based-multi-tenancy-and-team-collaboration","title":"Project-based Multi-Tenancy and Team Collaboration","text":"<p>Hopsworks provides projects as a secure sandbox in which teams can collaborate and share ML assets. Hopsworks' unique multi-tenant project model even enables sensitive data to be stored in a shared cluster, while still providing fine-grained sharing capabilities for ML assets across project boundaries.  Projects can be used to structure teams so that they have end-to-end responsibility from raw data to managed features and models. Projects can also be used to create development, staging, and production environments for data teams. All ML assets support versioning, lineage, and provenance provide all Hopsworks users with a complete view of the MLOps life cycle, from feature engineering through model serving. </p>"},{"location":"#development-and-operations","title":"Development and Operations","text":"<p>Hopsworks provides a FTI (feature/training/inference) pipeline architecture for ML systems. Each part of the pipeline is defined in a Hopsworks job which corresponds to a Jupyter notebook, a python script or a jar. The production pipelines are then orchestrated with Airflow which is bundled in Hopsworks. Hopsworks provides several python environments that can be used and customized for each part of the FTI pipeline, for example switching between using PyTorch or TensorFlow in the training pipeline. You can train models on as many GPUs as are installed in a Hopsworks cluster and easily share them among users. You can also run Spark, Spark Streaming, or Flink programs on Hopsworks. JupyterLab is also bundled which can be used to run Python and Spark interactively. </p>"},{"location":"#available-on-any-platform","title":"Available on any Platform","text":"<p>Hopsworks is available to be installed on a kubernetes cluster in the cloud on AWS, Azure, and GCP, and On-Prem (Ubuntu/Redhat compatible), even in air-gapped data centers. Hopsworks is also available as a serverless platform that manages and serves both your features and models.</p>"},{"location":"#join-the-community","title":"Join the community","text":"<ul> <li>Ask questions and give us feedback in the Hopsworks Community</li> <li>Follow us on Twitter</li> <li>Check out all our latest product releases</li> <li>Join our public slack-channel</li> </ul>"},{"location":"#contribute","title":"Contribute","text":"<p>We are building the most complete and modular ML platform available in the market, and we count on your support to continuously improve Hopsworks. Feel free to give us suggestions, report bugs and add features to our library anytime.</p>"},{"location":"#open-source","title":"Open-Source","text":"<p>Hopsworks is available under the AGPL-V3 license. In plain English this means that you are free to use Hopsworks and even build paid services on it, but if you modify the source code, you should also release back your changes and any systems built around it as AGPL-V3.</p> <p>We're the best at what we do, and we strive to keep the same standard for our community! Our many thanks to the contributors of Hopsworks.</p>"},{"location":"concepts/hopsworks/","title":"Hopsworks Platform","text":"<p>Hopsworks is a modular MLOps platform with:</p> <ul> <li>a feature store (available as standalone)</li> <li>model registry and model serving based on KServe</li> <li>vector database based on OpenSearch</li> <li>a data science and data engineering platform</li> </ul> <p></p>"},{"location":"concepts/hopsworks/#standalone-feature-store","title":"Standalone Feature Store","text":"<p>Hopsworks was the first open-source and first enterprise feature store for ML.  You can use Hopsworks as a standalone feature store with the HSFS API.</p>"},{"location":"concepts/hopsworks/#model-management","title":"Model Management","text":"<p>Hopsworks includes support for model management, with model deployments using the KServe framework and a model registry designed for KServe. Hopsworks logs all inference requests to Kafka to enable easy monitoring of deployed models, and provides model metrics with grafana/prometheus.</p>"},{"location":"concepts/hopsworks/#vector-db","title":"Vector DB","text":"<p>Hopsworks provides a vector database (or embedding store) based on OpenSearch kNN (FAISS and nmslib). Hopsworks Vector DB includes out-of-the-box support for authentication, access control, filtering, backup-and-restore, and horizontal scalability. Hopsworks' Feature Store and vector DB are often used together to build scalable recommender systems, such as ranking-and-retrieval for real-time recommendations. </p>"},{"location":"concepts/hopsworks/#governance","title":"Governance","text":"<p>Hopsworks provides a data-mesh architecture for managing ML assets and teams, with multi-tenant projects. Not unlike a GitHub repository, a project is a sandbox containing team members, data, and ML assets. In Hopsworks, all ML assets (features, models, training data) are versioned, taggable, lineage-tracked, and support free-text search. Data can be also be securely shared between projects.</p>"},{"location":"concepts/hopsworks/#data-science-platform","title":"Data Science Platform","text":"<p>You can develop feature engineering, model training and inference pipelines in Hopsworks. There is support for version control (GitHub, GitLab, BitBucket), Jupyter notebooks, a shared distributed file system, many bundled modular project python environments for managing python dependencies without needing to write Dockerfiles, jobs (Python, Spark, Flink), and workflow orchestration with Airflow.</p>"},{"location":"concepts/dev/inside/","title":"Inside Hopsworks","text":"<p>Hopsworks provides a complete self-service development environment for feature engineering and model training. You can develop programs as Jupyter notebooks or jobs, customize the bundled FTI (feature, training and inference pipeline) python environments, you can manage your source code with Git, and you can orchestrate jobs with Airflow.</p> <p></p>"},{"location":"concepts/dev/inside/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<p>Hopsworks provides a Jupyter notebook development environment for programs written in Python, Spark, Flink, and SparkSQL. You can also develop in your IDE (PyCharm, IntelliJ, etc), test locally, and then run your programs as Jobs in Hopsworks. Jupyter notebooks can also be run as Jobs.</p>"},{"location":"concepts/dev/inside/#source-code-control","title":"Source Code Control","text":"<p>Hopsworks provides source code control support using Git (GitHub, GitLab or BitBucket). You can securely checkout code into your project and commit and push updates to your code to your source code repository. </p>"},{"location":"concepts/dev/inside/#fti-pipeline-environments","title":"FTI Pipeline Environments","text":"<p>Hopsworks postulates that building ML systems following the FTI pipeline architecture is best practice. This architecture consists of three independently developed and operated ML pipelines:</p> <ul> <li>Feature pipeline: takes as input raw data that it transforms into features (and labels)</li> <li>Training pipeline: takes as input features (and labels) and outputs a trained model</li> <li>Inference pipeline: takes new feature data and a trained model and makes predictions</li> </ul> <p>In order to facilitate the development of these pipelines Hopsworks bundles several python environments containing necessary dependencies. Each of these environments may then also be customized further by cloning it and installing additional dependencies from PyPi, Conda channels, Wheel files, GitHub repos or a custom Dockerfile. Internal compute such as Jobs and Jupyter is run in one of these environments and changes are applied transparently when you install new libraries using our APIs. That is, there is no need to write a Dockerfile, users install libraries directly in one or more of the environments. You can setup custom development and production environments by creating separate projects or creating multiple clones of an environment within the same project.</p>"},{"location":"concepts/dev/inside/#jobs","title":"Jobs","text":"<p>In Hopsworks, a Job is a schedulable program that is allocated compute and memory resources. You can run a Job in Hopsworks:</p> <ul> <li>From the UI</li> <li>Programmatically with the Hopsworks SDK (Python, Java) or REST API</li> <li>From Airflow programs (either inside our outside Hopsworks)</li> <li>From your IDE using a plugin (PyCharm/IntelliJ plugin)</li> </ul>"},{"location":"concepts/dev/inside/#orchestration","title":"Orchestration","text":"<p>Airflow comes out-of-the box with Hopsworks, but you can also use an external Airflow cluster (with the Hopsworks Job operator) if you have one. Airflow can be used to schedule the execution of Jobs, individually or as part of Airflow DAGs.</p>"},{"location":"concepts/dev/outside/","title":"Outside Hopsworks","text":"<p>You can write programs that use Hopsworks in any Python, Spark, PySpark, or Flink environment. Hopsworks also running SQL queries to compute features in external data warehouses. The Feature Store can also be queried with SQL.</p> <p>There is REST API for Hopsworks that can be used with a valid API key, generated in Hopsworks. However, it is often easier to develop your programs against SDKs available in Python and Java/Scala for HSFS, in Python for HSML, and in Python for the Hopsworks API.</p> <p></p>"},{"location":"concepts/fs/","title":"Architecture","text":""},{"location":"concepts/fs/#what-is-hopsworks-feature-store","title":"What is Hopsworks Feature Store?","text":"<p>Hopsworks and its Feature Store are an open source data-intensive AI platform used for the development and operation of machine learning models at scale. The Hopsworks Feature Store provides the HSFS API to enable clients to write features to feature groups in the feature store, and to read features from feature views - either through a low latency Online API to retrieve pre-computed features for operational models or through a high throughput, latency insensitive Offline API, used to create training data and to retrieve batch data for scoring.</p> <p></p>"},{"location":"concepts/fs/#hsfs-api","title":"HSFS API","text":"<p>The HSFS (Hopsworks Feature Store) API is how you, as a developer, will use the feature store. The HSFS API helps simplify some of the problems that feature stores address including:</p> <ul> <li>consistent features for training and serving</li> <li>centralized, secure access to features</li> <li>point-in-time JOINs of features to create training data with no data leakage</li> <li>easier connection and backfilling of features from external data sources</li> <li>use of external tables as features</li> <li>transparent computation of statistics and usage data for features.</li> </ul>"},{"location":"concepts/fs/#write-to-feature-groups-read-from-feature-views","title":"Write to feature groups, read from feature views.","text":"<p>You write to feature groups with a feature pipeline program. The program can be written in Python, Spark, Flink, or SQL.</p> <p>You read from views on top of the feature groups, called feature views. That is, a feature view does not store feature data, but is a logical grouping of features. Typically, you define a feature view because you want to train/deploy a model with exactly those features in the feature view. Feature views enable the reuse of feature data from different feature groups across different models.</p>"},{"location":"concepts/fs/feature_group/external_fg/","title":"External Feature Groups","text":"<p>External feature groups are offline feature groups where their data is stored in an external table. An external table requires a storage connector, defined with the Connector API (or more typically in the user interface), to enable HSFS to retrieve data from the external table. An external feature group doesn't allow for offline data ingestion or modification; instead, it includes a user-defined SQL string for retrieving data. You can also perform SQL operations, including projections, aggregations, and so on. The SQL query is executed on-demand when HSFS retrieves data from the external Feature Group, for example, when creating training data using features in the external table.</p> <p>In the image below, we can see that HSFS currently supports a large number of data sources, including any JDBC-enabled source, Snowflake, Data Lake, Redshift, BigQuery, S3, ADLS, GCS, and Kafka</p> <p></p>"},{"location":"concepts/fs/feature_group/feature_monitoring/","title":"Feature Monitoring","text":"<p>Feature Monitoring complements data validation capabilities by allowing you to monitor your feature data after it has been ingested into the Feature Store.</p> <p>HSFS supports monitoring features on your Feature Group by:</p> <ul> <li>transparently computing statistics on the whole or a subset of feature data defined by a detection window.</li> <li>comparing statistics against a reference window of feature data, and configuring thresholds to identify anomalous data.</li> <li>configuring alerts based on the statistics comparison results.</li> </ul>"},{"location":"concepts/fs/feature_group/feature_monitoring/#scheduled-statistics","title":"Scheduled Statistics","text":"<p>After creating a Feature Group in HSFS, you can setup statistics monitoring to compute statistics over one or more features on a scheduled basis. Statistics are computed on the whole or a subset of feature data (i.e., detection window) already inserted into the Feature Group.</p>"},{"location":"concepts/fs/feature_group/feature_monitoring/#statistics-comparison","title":"Statistics Comparison","text":"<p>In addition to scheduled statistics, you can enable the comparison of statistics against a reference subset of feature data (i.e., reference window) and define the criteria for this comparison including the statistics metric to compare and a threshold to identify anomalous values.</p> <p>Feature Monitoring Guide</p> <p>More information can be found in the Feature monitoring guide.</p>"},{"location":"concepts/fs/feature_group/feature_pipelines/","title":"Feature Pipelines","text":"<p>A feature pipeline is a program that orchestrates the execution of a dataflow graph of data validation, aggregation, dimensionality reduction, transformation, and other feature engineering steps on input data to create and/or update feature data. With HSFS, you can write feature pipelines in different languages as shown in the figure below. </p> <p></p>"},{"location":"concepts/fs/feature_group/feature_pipelines/#data-sources","title":"Data Sources","text":"<p>Your feature pipeline needs to connect to some (external) data source to read the data to be processed. Python, Spark, and Flink have connectors to a huge number of different data sources, while SQL feature pipelines are often restricted to a single data source (for example, your connector to SnowFlake only runs SQL on SnowFlake). SparkSQL, in contrast, can be used over tables that originate in different  data sources.</p>"},{"location":"concepts/fs/feature_group/feature_pipelines/#data-validation","title":"Data Validation","text":"<p>In order to be able to train and serve models that you can rely on, you need clean, high quality features. Data validation operations include removing bad data, removing or imputing missing values, and identifying problems such as feature shift. HSFS supports Great Expectations to specify data validation rules that are executed in the client before features are written to the Feature Store. The validation results are collected and shown in Hopsworks.</p>"},{"location":"concepts/fs/feature_group/feature_pipelines/#aggregations","title":"Aggregations","text":"<p>Aggregations are used to summarize large datasets into more concise, signal-rich features. Popular aggregations include count(), sum(), mean(), median(), stddev(), min(), and max(). These aggregations produce a single number (a numerical feature) that captures information about a potentially large dataset. Both numerical and categorical features are often transformed before being used to train or serve models.</p>"},{"location":"concepts/fs/feature_group/feature_pipelines/#dimensionality-reduction","title":"Dimensionality Reduction","text":"<p>If input data is impractically large or if it has a significant amount of redundancy, it can often be transformed into a reduced set of features with dimensionality reduction (often called feature extraction). Popular dimensionality algorithms include embedding algorithms, PCA, and TSNE.</p>"},{"location":"concepts/fs/feature_group/feature_pipelines/#transformations","title":"Transformations","text":"<p>Transformations are covered in more detail in training/inference pipelines, as transformations typically happen after the feature store. If you store transformed features in feature groups, the feature data is no longer useful for EDA (as it near to impossible for Data Scientists to understand the transformed values). It also makes it impossible for inference pipelines to log untransformed feature values and predictions for an operational model. There is one use case for storing transformed features in feature groups - when you need to have ultra low latency when reading precomputed features (and online transformations when reading features add too much latency for your use case). The figure below shows to include transformations in your feature pipelines. </p> <p></p>"},{"location":"concepts/fs/feature_group/feature_pipelines/#feature-engineering-in-python","title":"Feature Engineering in Python","text":"<p>Python is the most widely used framework for feature engineering due to its extensive library support for aggregations (Pandas/Polars), data validation (Great Expectations), and dimensionality reduction (embeddings, PCA), and transformations (in Scikit-Learn, TensorFlow, PyTorch). Python also supports open-source feature engineering frameworks used for automated feature engineering, such as featuretools that supports relational and temporal sources.</p>"},{"location":"concepts/fs/feature_group/feature_pipelines/#feature-engineering-in-sparkpyspark","title":"Feature Engineering in Spark/PySpark","text":"<p>Spark is popular as a feature engineering framework as it can scale to process larger volumes of data than Python, and provides native support for aggregations, and it supports many of the same data validation (Great Expectations), and dimensionality reduction algorithms (embeddings, PCA) as Python. Spark also has native support for transformations, which are useful for analytical models (batch scoring), but less useful for operational models, where online transformations are required, and Spark environments are less common. Online model serving environments typically only support online transformations in Python.</p>"},{"location":"concepts/fs/feature_group/feature_pipelines/#feature-engineering-in-sql","title":"Feature Engineering in SQL","text":"<p>SQL has grown in popularity for performing heavy lifting in feature pipelines - computing aggregates on data - when the input data already resides in a data warehouse. Data warehouses also support data validation, for example, through Great Expectations in DBT. However, SQL is not mature as a platform for transformations and dimensionality reductions, where UDFs are applied row-wise.</p> <p>You can do aggregation in SQL for data in your data warehouse or database.</p>"},{"location":"concepts/fs/feature_group/feature_pipelines/#feature-engineering-in-flink","title":"Feature Engineering in Flink","text":"<p>Apache Flink is a powerful and flexible framework for stateful feature computation operations over unbounded and bounded data streams. It is used for feature engineering when you need very fresh features computed in real-time. Flink provides a rich set of operators and functions such as time windows and aggregation operations that can be applied to keyed and/or global window streams. Flink\u2019s stateful operations allow users to maintain and update state across multiple data records or events, which is particularly useful for feature engineering tasks such as sessionization and/or maintaining rolling aggregates over a sliding window of data.</p> <p>Flink feature engineering pipelines are supported in Java/Scala only.</p>"},{"location":"concepts/fs/feature_group/feature_pipelines/#feature-engineering-in-beam","title":"Feature Engineering in Beam","text":"<p>Beam feature engineering pipelines are supported in Java/Scala only. </p>"},{"location":"concepts/fs/feature_group/fg_overview/","title":"Overview","text":"<p>As a programmer, you can consider a feature, in machine learning, to be a variable associated with some entity that contains a value that is useful for helping train a model to solve a prediction problem. That is, the feature is just a variable with predictive power for a machine learning problem, or task.</p> <p>A feature group is a table of features, where each feature group has a primary key, and optionally an event_time column (indicating when the features in that row were observed), and a partition key. Collectively, they are referred to as columns. The partition key determines how to layout the feature group rows on disk such that you can efficiently query the data using queries with the partition key. For example, if your partition key is the day and you have hundreds of days worth of data, with a partition key, you can query the day for only a given day or a range of days, and only the data for those days will be read from disk.</p> <p></p>"},{"location":"concepts/fs/feature_group/fg_overview/#online-and-offline-storage","title":"Online and offline Storage","text":"<p>Feature groups can be stored in a low-latency \"online\" database and/or in low cost, high throughput \"offline\" storage, typically a data lake or data warehouse.</p> <p></p>"},{"location":"concepts/fs/feature_group/fg_overview/#online-storage","title":"Online Storage","text":"<p>The online store stores only the latest values of features for a feature group. It is used to serve pre-computed features to models at runtime.</p>"},{"location":"concepts/fs/feature_group/fg_overview/#offline-storage","title":"Offline Storage","text":"<p>The offline store stores the historical values of features for a feature group so that it may store much more data than the online store. Offline feature groups are used, typically, to create training data for models, but also to retrieve data for batch scoring of models.</p> <p>In most cases, offline data is stored in Hopsworks, but through the implementation of storage connectors, it can reside in an external file system. The externally stored data can be managed by Hopsworks by defining ordinary feature groups or it can be used for reading only by defining External Feature Group.</p>"},{"location":"concepts/fs/feature_group/fg_statistics/","title":"Data Validation/Stats/Alerts","text":"<p>HSFS supports monitoring, validation, and alerting for features:</p> <ul> <li>transparently compute statistics over features on writing to a feature group;</li> <li>validation of data written to feature groups using Great Expectations</li> <li>alerting users when there was a problem writing or update features. </li> </ul>"},{"location":"concepts/fs/feature_group/fg_statistics/#statistics","title":"Statistics","text":"<p>When you create a Feature Group in HSFS, you can configure it to compute statistics over the features inserted into the Feature Group by setting the <code>statistics_config</code> dict parameter, see Feature Group Statistics for details. Every time you write to the Feature Group, new statistics will be computed over all of the data in the Feature Group.</p>"},{"location":"concepts/fs/feature_group/fg_statistics/#data-validation","title":"Data Validation","text":"<p>You can define expectation suites in Great Expectations and associate them with feature groups. When you write to a feature group, the expectations are executed, then you can define a policy on the feature group for what to do if any expectation fails.</p> <p></p>"},{"location":"concepts/fs/feature_group/fg_statistics/#alerting","title":"Alerting","text":"<p>HSFS also supports alerts, that can be triggered when there are problems in your feature pipelines, for example, when a write fails due to an error or a failed expectation. You can send alerts to different alerting endpoints, such as email or Slack, that can be configured in the Hopsworks UI. For example, you can send a slack message if features being written to a feature group are missing some input data.</p>"},{"location":"concepts/fs/feature_group/on_demand_feature/","title":"On-demand features","text":"<p>Features are defined as on-demand when their value cannot be pre-computed beforehand, rather they need to be computed in real-time during inference. This is achieved by implementing the on-demand features as a Python function in a Python module. Also ensure that the same version of the Python module is installed in both the feature and inference pipelines.</p> <p>In the image below shows an example of a housing price model that demonstrates how to implement an on-demand feature, a zip code (or post code) that is computed using longitude/latitude parameters. In your online application, longitude and latitude are provided as parameters to the application, and the same python function used to calculate the zip code in the feature pipeline is used to compute the zip code in the Online Inference pipeline. </p> <p></p>"},{"location":"concepts/fs/feature_group/spine_group/","title":"Spine Group","text":"<p>It is possible to maintain labels or prediction events among the regular features in a regular feature group with a feature pipeline updating the labels at a specific cadence.</p> <p>Often times, however, it is more convenient to provide the training events or entities in a Dataframe when reading feature data from the feature store through a feature view. We call such a Dataframe a Spine as it is the structure around which the training data or batch data is built. In order to retrieve the correct feature values for the entities in the Dataframe, using a point-in-time correct join, some additional metadata apart from the Dataframe schema is necessary. Namely, the information about which columns define the primary key, and which column indicates the event time at which the label was valid. The spine Dataframe together with this additional metadata is what we call a Spine Group.</p> <p>For example, in the following spine, we want to retrieve the features for the three locations, no later than the event time of each of the rainfall measurements, which is our prediction target:</p> location_id event_time rainfall (label) 1 2022-06-01 13:11 44 2 2022-06-01 09:14 5 3 2022-06-01 06:36 2 <p>A Spine Group does not materialize any data to the feature store itself, and always needs to be provided when retrieving features from the offline API. You can think of it as a place holder or a temporary feature group, to be replaced by a Dataframe in point-in-time joins.</p> <p>When using the online API, it is not necessary to provide the spine, since the online feature store contains only the latest feature values, and therefore no point in time join is required, the label is not required, as the inference pipeline is going to compute the prediction and the primary key values are specified when calling the online API.</p>"},{"location":"concepts/fs/feature_group/versioning/","title":"Versioning","text":"<p>See here for information about version of feature views.</p>"},{"location":"concepts/fs/feature_group/versioning/#schema-versioning","title":"Schema Versioning","text":"<p>The schema of feature groups is versioned. If you make a breaking change to the schema of a feature group, you need to increment the version of the feature group, and then backfill the new feature group. A breaking schema change is when you:</p> <ul> <li>drop a column from the schema</li> <li>add a new feature without any default value for the new feature</li> <li>change how a feature is computed, such that, for training models, the data for the old feature is not compatible with the data for the new feature. For example, if you have an embedding as a feature and change the algorithm to compute that embedding, you probably should not mix feature values computed with the old embedding model with feature values computed with the new embedding model.</li> </ul> <p></p>"},{"location":"concepts/fs/feature_group/versioning/#data-versioning-for-feature-groups","title":"Data Versioning for Feature Groups","text":"<p>Data Versioning of a feature group involves tracking updates to the feature group, so that you can recover the state of the feature group at a given point-in-time in the past.</p> <p></p>"},{"location":"concepts/fs/feature_group/write_apis/","title":"Write APIs","text":"<p>You write to feature groups, and read from feature views.</p> <p>There are 3 APIs for writing to feature groups, as shown in the table below:</p> Stream API Batch API Connector API Python X - - Spark X X - Flink X - - External Table - - X"},{"location":"concepts/fs/feature_group/write_apis/#stream-api","title":"Stream API","text":"<p>The Stream API is the only API for Python and Flink clients, and is the preferred API for Spark, as it ensures consistent features between offline and online feature stores. The Stream API first writes data to be ingested to a Kafka topic, and then Hopsworks ensures that the data is synchronized to the Online and Offline Feature Groups through the OnlineFS service and Hudi DeltaStreamer jobs, respectively. The data in the feature groups is guaranteed to arrive at-most-once, through idempotent writes to the online feature group (only the latest values of features are stored there, and duplicates in Kafka only cause idempotent updates) and duplicate removal by Apache Hudi for the offline feature group.</p> <p></p>"},{"location":"concepts/fs/feature_group/write_apis/#batch-api","title":"Batch API","text":"<p>For very large updates to feature groups, such as when you are backfilling large amounts of data to an offline feature group, it is often preferential to write directly to the Hudi tables in Hopsworks, instead of via Kafka - thus reducing write amplification. Spark clients can write directly to Hudi tables on Hopsworks with Hopsworks libraries and certificates using a HDFS API. This requires network connectivity between the Spark clients and the datanodes in Hopsworks.</p> <p></p>"},{"location":"concepts/fs/feature_group/write_apis/#connector-api","title":"Connector API","text":"<p>Hopsworks supports external tables as feature groups. You can mount a table from an external database as an offline feature group using the Connector API - you create an external table using the connector. This enables you to use features from your external data source (Snowflake, Redshift, Delta Lake, etc) as you would any feature in an offline feature group in Hopsworks. You can, for example, join features from different feature groups (external or not) together to create feature views and training data for models.</p> <p></p>"},{"location":"concepts/fs/feature_view/feature_monitoring/","title":"Feature Monitoring","text":"<p>Feature Monitoring complements data validation capabilities by allowing you to monitor your feature data once they have been ingested into the Feature Store.</p> <p>HSFS supports monitoring features on your Feature View by:</p> <ul> <li>transparently computing statistics on the whole or a subset of feature data defined by a detection window.</li> <li>comparing statistics against a reference window of feature data (e.g., training dataset), and configuring thresholds to identify anomalous data.</li> <li>configuring alerts based on the statistics comparison results.</li> </ul>"},{"location":"concepts/fs/feature_view/feature_monitoring/#scheduled-statistics","title":"Scheduled Statistics","text":"<p>After creating a Feature View in HSFS, you can setup statistics monitoring to compute statistics over one or more features on a scheduled basis. Statistics are computed on the whole or a subset of feature data (i.e., detection window) using the Feature View query.</p>"},{"location":"concepts/fs/feature_view/feature_monitoring/#statistics-comparison","title":"Statistics Comparison","text":"<p>In addition to scheduled statistics, you can enable the comparison of statistics against a reference subset of feature data (i.e., reference window), typically a training dataset, and define the criteria for this comparison including the statistics metric to compare and a threshold to identify anomalous values.</p> <p>Feature Monitoring Guide</p> <p>More information can be found in the Feature monitoring guide.</p>"},{"location":"concepts/fs/feature_view/fv_overview/","title":"Overview","text":"<p>A feature view is a logical view over (or interface to) a set of features that may come from different feature groups. You create a feature view by joining together features from existing feature groups. In the illustration below, we can see that features are joined together from the two feature groups: seller_delivery_time_monthly and the seller_reviews_quarterly. You can also see that features in the feature view inherit not only the feature type from their feature groups, but also whether they are the primary key and/or the event_time. The image also includes transformation functions that are applied to individual features. Transformation functions are a part of the feature types included in the feature view. That is, a feature in a feature view is not only defined by its data type (int, string, etc) or its feature type (categorical, numerical, embedding), but also by its transformation. </p> <p></p> <p>Feature views can also include:</p> <ul> <li>the label for the supervised ML problem </li> <li>transformation functions that should be applied to specified features consistently between training and serving</li> <li>the ability to create training data</li> <li>the ability to retrieve a feature vector with the most recent feature values</li> </ul> <p>In the flow chart below, we can see the decisions that can be taken when creating (1) a feature view, and (2) creating training data with the feature view.</p> <p></p> <p>We can see here how the feature view is a representation for a model in the feature store - the same feature view is used to retrieve feature vectors for operational model that was created with training data from this feature view. As such, you can see that the most common use case for creating a feature view is to define the features that will be used in a model. In this way, feature views enable features from different feature groups to be reused across different models, and if features are stored untransformed in feature groups, they become even more reusable, as different feature views can apply different transformations to the same feature.</p>"},{"location":"concepts/fs/feature_view/offline_api/","title":"Offline API","text":"<p>The feature view provides an Offline API for</p> <ul> <li>creating training data</li> <li>creating batch (scoring) data</li> </ul>"},{"location":"concepts/fs/feature_view/offline_api/#training-data","title":"Training Data","text":"<p>Training data is created using a feature view. You can create training data as either:</p> <ul> <li>in-memory Pandas/Polars DataFrames, useful when you have a small amount of training data;</li> <li>materialized training data in files, in a file format of your choice (such as .tfrecord, .csv, or .parquet).</li> </ul> <p>You can apply filters when creating training data from a feature view:</p> <ul> <li>start-time and end-time, for example, to create the train-set from an earlier time range, and the test-set from a later (unseen) time range;</li> <li>feature value features, for example, only train a model on customers from a particular country.</li> </ul> <p>Note that filters are not applied when retrieving feature vectors using feature views, as we only look up features for a specific entity, like a customer. In this case, the application should know that predictions for this customer should be made on the model trained on customers in USA, for example.</p>"},{"location":"concepts/fs/feature_view/offline_api/#point-in-time-correct-training-data","title":"Point-in-time Correct Training Data","text":"<p>When you create training data from features in different feature groups, it is possible that the feature groups are updated at different cadences. For example, maybe one feature group is updated hourly, while another feature group is updated daily. It is very complex to write code that joins features together from such feature groups and ensures there is no data leakage in the resultant training data. HSFS hides this complexity by performing the point-in-time JOIN transparently, similar to the illustration below:</p> <p></p> <p>HSFS uses the event_time columns on both feature groups to determine the most recent (but not newer) feature values that are joined together with the feature values from the feature group containing the label. That is, the features in the feature group containing the label are the observation times for the features in the resulting training data, and we want feature values from the other feature groups that have the most recent timestamps, but not newer than the timestamp in the label-containing feature group.</p>"},{"location":"concepts/fs/feature_view/offline_api/#spine-dataframes","title":"Spine Dataframes","text":"<p>The left side of the point-in-time join is typically the set of training entities/primary key values for which the relevant features need to be retrieved. This left side of the join can also be replaced by a spine group. When using feature groups also so save labels/prediction targets, it can happen that you end up with the same entity multiple times in the training dataset depending on the cadence at which the label group was updated and the length of the event time interval that is being used to generate the training dataset. This can lead to bias in the training dataset and should be avoided. To avoid this kind of situation, users can either narrow down the event time interval during training dataset creation or use a spine in order to precisely define the entities to be included in the training dataset. This is just one example where spines are helpful.</p>"},{"location":"concepts/fs/feature_view/offline_api/#splitting-training-data","title":"Splitting Training Data","text":"<p>You can create random train/validation/test splits of your training data using the HSFS API. You can also time-based splits with the HSFS API.</p>"},{"location":"concepts/fs/feature_view/offline_api/#evaluation-sets","title":"Evaluation Sets","text":"<p>Test data can also be split into evaluation sets to help evaluate a model for potential bias. First, you have to identify the classes of samples that could be at risk of bias, and generate evaluation sets from your unseen test set - one evaluation set for each group of samples at risk of bias. For example, if you have a feature group of users, where one of the features is gender, and you want to evaluate the risk of bias due to gender, you can use filters to generate 3 evaluation sets from your test set - one for male, female, and non-binary. Then you score your model against all 3 evaluation sets to ensure that the prediction performance is comparable and non-biased across all 3 gender.</p>"},{"location":"concepts/fs/feature_view/offline_api/#batch-scoring-data","title":"Batch (Scoring) Data","text":"<p>Batch data for scoring models is created using a feature view. Similar to training data, you can create batch data as either:</p> <ul> <li>in-memory Pandas/Polars DataFrames, useful when you have a small amount of data to score;</li> <li>materialized data in files, in a file format of your choice (such as .tfrecord, .csv, or .parquet)</li> </ul> <p>Batch data requires specification of a <code>start_time</code> for the start of the batch scoring data. You can also specify the <code>end_time</code> (default is the current date).</p> <p></p>"},{"location":"concepts/fs/feature_view/offline_api/#spine-dataframes_1","title":"Spine Dataframes","text":"<p>Similar to training dataset generation, it might be helpful to specify a spine when retrieving features for batch inference. The only difference in this case is that the spine dataframe doesn't need to contain the label, as this will be the output of the inference pipeline. A typical use case is the handling of opt-ins, where certain customers have to be excluded from an inference pipeline due to a missing marketing opt-in.</p>"},{"location":"concepts/fs/feature_view/online_api/","title":"Online API","text":"<p>The Feature View provides an Online API to return an individual feature vector, or a batch of feature vectors, containing the latest feature values. To retrieve a feature vector, a client needs to provide the primary key(s) for the feature groups backing the feature view. For example, if you have <code>customer_profile</code> and <code>customer_purchases</code> Feature Groups both with <code>customer_id</code> as a primary key, and a Feature View made up from features from both Feature Groups, then, you would use <code>customer_id</code> to retrieve a feature vector using the Feature View object.</p>"},{"location":"concepts/fs/feature_view/online_api/#feature-vectors","title":"Feature Vectors","text":"<p>A feature vector is a row of features (without the primary key(s) and event timestamp):</p> <p></p> <p>It may be the case that for any given feature vector, not all features will come pre-engineered from the feature store. Some features will be provided by the client (or at least the raw data to compute the feature will come from the client). We call these 'passed' features and, similar to precomputed features from the feature store, they can also be transformed by the HSFS client in the method:</p> <ul> <li>feature_view.get_feature_vector(entry, passed_features={...})</li> </ul>"},{"location":"concepts/fs/feature_view/statistics/","title":"Statistics","text":"<p>The feature view does not contain any statistics, as it is simply an interface consisting of a number of features and any transformation functions applied to those features.</p> <p>However, training data can have descriptive statistics over it computed by HSFS. Descriptive statistics for training data is important for model monitoring, as it can enable model monitoring. If you compute the same descriptive statistics over windows of input features to models, you can help determine when there is a significant change in the distribution of an input feature, so-called feature shift.</p>"},{"location":"concepts/fs/feature_view/training_inference_pipelines/","title":"Consistent Transformations","text":"<p>A training pipeline is a program that orchestrates the training of a machine learning model. For supervised machine learning, a training pipeline requires both features and labels, and these can typically be retrieved from the feature store as either in-memory Pandas/Polars DataFrames or read as training data files, created from the feature store. An inference pipeline is a program that takes user input, optionally enriches it with features from the feature store, and builds a feature vector (or batch of feature vectors) with with it uses a model to make a prediction.</p>"},{"location":"concepts/fs/feature_view/training_inference_pipelines/#transformations","title":"Transformations","text":"<p>Feature transformations are mathematical operations that change feature values with the goal of improving model convergence or performance properties. Transformation functions take as input a single value (or small number of values), they often require state (such as the mean value of a feature to normalize the input), and they output a single value or a list of values.</p>"},{"location":"concepts/fs/feature_view/training_inference_pipelines/#training-serving-skew","title":"Training Serving Skew","text":"<p>It is crucial that the transformations performed when creating features (for training or serving) are consistent - use the same code - to avoid training/serving skew. In the image below, you can see that transformations happen after the Feature Store, but that the implementation of the transformation functions need to be consistent between the training and inference pipelines. </p> <p></p> <p>There are 3 main approaches to prevent training/serving skew that we support in Hopsworks. These are (1) perform transformations in models, (2) perform transformations in pipelines (sklearn, TF, PyTorch) and use the model registry to save the transformation pipeline so that the same transformation is used in your inference pipeline, and (3) use HSFS transformations, defined as UDFs in Python.</p>"},{"location":"concepts/fs/feature_view/training_inference_pipelines/#transformations-as-pre-processing-layers-in-models","title":"Transformations as Pre-Processing Layers in Models","text":"<p>Transformation functions can be implemented as preprocessing steps within a model. For example, you can write a transformation function as a pre-processing layer in Keras/TensorFlow. When you save the model, the preprocessing steps will also be saved as part of the model. Any state required to compute the transformation, such as the arithmetic mean of a numerical feature in the train set, is also stored with the function, enabling consistent transformations during inference.  When data preprocessing is part of the model, users can just send the untransformed feature values to the model and the model itself will apply any transformation functions as preprocessing layers (such as encoding categorical variables or normalizing numerical variables).</p>"},{"location":"concepts/fs/feature_view/training_inference_pipelines/#transformation-pipelines-in-scikit-learntensorflowpytorch","title":"Transformation Pipelines in Scikit-Learn/TensorFlow/PyTorch","text":"<p>You have to save your transformation pipeline (serialize the object or the parameters) and make sure you apply exactly the same transformations in your inference pipeline. This means you should version the transformations. In Hopsworks, you can store the transformations with your versioned models in the Model Registry, helping you to ensure the same transformation pipeline is applied to both training/serving for the same model version.</p>"},{"location":"concepts/fs/feature_view/training_inference_pipelines/#transformations-as-python-udfs-in-hsfs","title":"Transformations as Python UDFs in HSFS","text":"<p>Hopsworks feature store also supports consistent transformation functions by enabling a Python UDF, that implements a transformation, to be attached a to feature in a feature view. When training data is created with a feature view or when a feature vector is retrieved from a feature view, HSFS ensures that any transformation functions defined over any features will be applied before returning feature values. You can use built-in transformation objects in HSFS or write your own custom transformation functions as Python UDFs. The benefit of this approach is that transformations are applied consistently when creating training data and when retrieving feature data from the online feature store. Transformations no longer need to be included in either your training pipeline or inference pipeline, as they are applied transparently when creating training data and retrieving feature vectors. Hopsworks uses Spark to create training data as files, and any transformation functions for features are executed as Python UDFs in Spark - enabling transformation functions to be applied on large volumes of data and removing potentially CPU-intensive transformations from training pipelines.</p>"},{"location":"concepts/fs/feature_view/versioning/","title":"Versioning","text":"<p>Feature views are interfaces, and if there is a change in the interface (the types of the features, the transformations applied to the features), then you need to change the version, to prevent breaking existing clients.</p> <p>Training datasets are associated with a specific feature view version. Training data also has its own version number (along with the version of its parent feature view). For example, online transformation functions often need training data statistics (e.g., normalizing a numerical feature requires you to divide the feature value by the mean value for that feature in the training dataset). As many training datasets can be created from a feature view, when you initialize the feature view you need to tell it which version of the training data to use - <code>feature_view.init(1)</code> means use version 1 of the training data for this feature view.</p>"},{"location":"concepts/mlops/bi_tools/","title":"BI Tools","text":"<p>The Hopsworks Feature Store is based on an offline data store, queryable via an Apache Hive API, and an online data store, queryable via a MySQL Server API.</p> <p>Given that Feature Groups in Hopsworks have well-defined schemas, features in the Hopsworks Feature Store can be analyzed and reports can be generated from them using any BI Tools that include connectors for MySQL (JDBC) and Apache Hive (2-way TLS required). One platform we use with customers is Apache Superset, as it can be configured alongside Hopsworks to provide BI Tooling capabilities.</p>"},{"location":"concepts/mlops/data_transformations/","title":"Data Transformations","text":"<p>Data transformations are integral to all AI applications. Data transformations produce new features that can enhance the performance of an AI application. However, not all transformations in an AI application are equivalent.</p> <p>Transformations like binning and aggregations typically create reusable features, while transformations like one-hot encoding, scaling and normalization often produce model-specific features. Additionally, in real-time AI systems, some features can only be computed during inference when the request is received, as they need request-time parameters to be computed.</p> <p></p> <p>This classification of features can be used to create a taxonomy for data transformation that would apply to any scalable and modular AI system that aims to reuse features. The taxonomy helps identify which classes of data transformation can cause online-offline skews in AI systems, allowing for their prevention. Hopsworks provides support for a feature view abstraction as well as model-dependent transformations and on-demand transformations to prevent online-offline skew.</p>"},{"location":"concepts/mlops/data_transformations/#data-transformation-taxonomy-for-ai-systems","title":"Data Transformation Taxonomy for AI Systems","text":"<p>Transformation functions in an AI system can be classified into three types based on the nature of the input features they generate: model-independent, model-dependent, and on-demand transformations. </p> <p></p> <p>Model-independent transformations create reusable features that can be utilized across one or more machine-learning models. These transformations include techniques such as grouped aggregations (e.g., minimum, maximum, or average of a variable), windowed aggregations (e.g., the number of clicks per day), and binning to generate categorical variables. Since the data produced by model-independent transformations are reusable, these features can be stored in a feature store.</p> <p>Model-dependent transformations generate features specific to one model. These include transformations that are unique to a particular model or are parameterized by the training dataset, making them model-specific. For instance, text tokenization is a transformation required by all large language models (LLMs) but each LLM has their own (unique) tokenizer. Other transformations, such as encoding categorical variables in a numerical representation or scaling/normalizing/standardizing numerical variables to enhance the performance of gradient-based models, are parameterized by the training dataset. Consequently, the features produced are applicable only to the model trained using that specific training dataset. Since these features are not reusable, there is no need to store them in a feature store. Also, storing encoded features in a feature store leads to write amplification, as every time feature values are written to a feature group, all existing rows in the feature group have to be re-encoded (and creation of a training dataset using a subset or rows in the feature group becomes impossible as they cannot be re-encoded).</p> <p>On-demand transformations are exclusive to real-time AI systems, where predictions must be generated in real time based on incoming prediction requests. On-demand transformations compute on-demand features, which usually require at least one input parameter that is only available in a prediction request for their computation. These transformations can also combine request-time parameters with precomputed features from feature stores. Some examples include generating zip_codes from latitude and longitude received in the prediction request or calculating the time_since_last_transaction from a transaction request. The on-demand features produced can also be computed and backfilled into a feature store when the necessary historical data required for their computation becomes available. Backfilling on-demand features into the feature store eliminates the need to recompute them when creating training data. On-demand transformations are typically also model-independent transformations (model-dependent transformations can be applied after the on-demand transformation).</p> <p>Each of these transformations is employed within specific areas in a modular AI system and can be illustrated using the figure below. </p> <p>Model-independent transformations are utilized exclusively in areas where new and historical data arrives, typically within feature pipelines. Model-dependent transformations are necessary during the creation of training data, in training programs and must also be consistently applied in inference programs prior to making predictions. On-demand transformations are primarily employed in online inference programs, though they can also be integrated into feature engineering programs to backfill data into the feature store.</p> <p>The presence of model-dependent and on-demand transformations across different modules in a modular AI system introduces the potential for online-offline skew. Hopsworks provides support for  model-dependent transformations and on-demand transformations to easily create modular skew-free AI pipelines.</p>"},{"location":"concepts/mlops/data_transformations/#hopsworks-and-the-data-transformation-taxonomy","title":"Hopsworks and the Data Transformation Taxonomy","text":"<p>In Hopsworks, an AI system is typically decomposed into different AI pipelines and usually falls into either a feature pipeline, a training pipeline, or an inference pipeline. </p> <p>Hopsworks stores reusable feature data, created by model-independent transformations within the feature pipeline, into feature groups (tables containing feature data in both offline and online stores). Model-independent transformations in Hopsworks can be performed using a wide range of commonly used data engineering tools and the generated features can be seamlessly inserted into feature groups. The figure below illustrates the different software tools supported by Hopsworks for creating reusable features through model-independent transformations.</p> <p></p> <p>Additionally, Hopsworks provides a simple Python API to create custom transformation functions as either Python or Pandas User-Defined Functions (UDFs). Pandas UDFs enable the vectorized execution of transformation functions, offering significantly higher throughput compared to Python UDFs for large volumes of data. They can also be scaled out across workers in a Spark program, allowing for scalability from gigabytes (GBs) to terabytes (TBs) or more. However, Python UDFs can be much faster for small volumes of data, such as in the case of online inference.</p> <p>Transformation functions defined in Hopsworks can then be attached to feature groups to create on-demand transformation. On-demand transformations in feature groups are executed automatically whenever data is inserted into them to compute and backfill the on-demand features into the feature group. Backfilling on-demand features removes the need to recompute them while creating training and batch data.</p> <p>Hopsworks also provides a powerful abstraction known as feature views, which enables feature reuse and prevents skew between training and inference pipelines. A feature view is a meta-data-only selection of features, created from potentially different feature groups. It includes the input and output schema required for a model. This means that a feature view describes not only the input features but also the output targets, along with any helper columns necessary for training or inference of the model. This allows feature views to create consistent snapshots of data for both training and inference of a model. Additionally feature views, also compute and save statistics for the training datasets they create.</p> <p>Hopsworks supports attaching transformations functions to feature views to create model-dependent transformations that have no online-offline skew. These transformations get access to the same training dataset statistics during both training and inference ensuring their consistency. Additionally, feature views through lineage get access to the on-demand transformation used to create on-demand features if any are selected during the creation of the feature view. This allows for the computation of on-demand features in real-time during online-inference. </p>"},{"location":"concepts/mlops/mlops/","title":"Mlops","text":""},{"location":"concepts/mlops/opensearch/","title":"Vector Database","text":"<p>Hopsworks includes OpenSearch as a multi-tenant service in projects. OpenSearch provides vector database capabilities through its k-NN plugin, that supports the FAISS and nsmlib embedding indexes. Through Hopsworks, OpenSearch also provides enterprise capabilities, including authentication and access control to indexes (an index can be private to a Hopsworks project), filtering, scalability, high availability, and disaster recovery support. To learn how Opensearch empowers vector similar search in Hopsworks, you can see this guide.</p> <p></p>"},{"location":"concepts/mlops/prediction_services/","title":"Prediction Services","text":"<p>A prediction service is an end-to-end analytical or operational machine learning system that takes in data and outputs predictions that are consumed by users of the prediction service.</p> <p>A prediction service consists of the following components:</p> <ul> <li>feature pipeline(s),</li> <li>training pipeline,</li> <li>inference pipeline (for either batch predictions or online predictions)</li> <li>a sink for predictions - either a store or a user-interface.</li> </ul>"},{"location":"concepts/mlops/prediction_services/#analytical-ml","title":"Analytical ML","text":"<p>In the analytical ML figure below, we can see an analytical prediction service, where feature pipelines update the feature store with new feature data, running at some schedule (e.g., hourly, daily), and a batch program also runs on a schedule, reading batch scoring data from the feature store, computing predictions for that data with an embedded model, and writing those predictions to a database sink, from where the predictions are used for (predictive, prescriptive) analytical reports and/or to AI-enable operational services.</p> <p></p>"},{"location":"concepts/mlops/prediction_services/#operational-ml","title":"Operational ML","text":"<p>In the operational ML figure below, we can see an operational prediction service, where feature pipelines update the feature store with new feature data, running at some schedule (e.g., streaming, hourly, daily), and the operational service sends prediction requests to a model deployed on KServe via its secured Istio endpoint. A deployed model on KServer handles the prediction request by first retrieving pre-computed features from the feature store for the given request, and then building a feature vector that is scored by the model. The prediction result is returned to the client (the operational service). KServe logs both the feature values and the prediction results back to Hopsworks for further analysis and to help create new training data.</p> <p></p>"},{"location":"concepts/mlops/prediction_services/#mlops-flywheel","title":"MLOps Flywheel","text":"<p>Once you have built your analytical or operational ML system, the MLOps flywheel is the path to building a self-managing system that automatically collects and processes feature logs, prediction logs, and outcomes to help create new training data for models. This enables a ML flywheel where new training data and insights are generated from your operational or analytical ML service, by feeding logs back into the feature store. More training data enables the training of better models, and with better models, you should hopefully improve you operational/batch services, so that you attract more clients, who in turn produce more data for training models. And, thus, the ML flywheel is bootstrapped and leads to a virtuous cycle of more data leading to better models and more models leading to more users, who produce more data, and so on.</p> <p></p>"},{"location":"concepts/mlops/registry/","title":"Model Registry","text":"<p>Hopsworks Model Registry is designed with specific support for KServe and MLOps, through versioning. It enables developers to publish, test, monitor, govern and share models for collaboration with other teams. The model registry is where developers publish their models during the experimentation phase. The model registry can also be used to share models with the team and stakeholders.</p> <p>Like other project-based multi-tenant services in Hopsworks, a model registry is private to a project. That means you can easily add a development, staging, and production model registry to a cluster, and implement CI/CD processes for transitioning a model from development to staging to production.</p> <p>The model registry for KServe's capability are shown in the diagram below:</p> <p></p> <p>The model registry centralizes model management, enabling models to be securely accessed and governed. Models are more than just the model itself - the registry also stores sample data for testing, configuration information, provenance information, environment variables, links to the code used to generate the model, the model version, and tags/descriptions). When you save a model, you can also save model metrics with the model, enabling users to understand, for example, performance of the model on test (or unseen) data.</p>"},{"location":"concepts/mlops/registry/#model-package","title":"Model Package","text":"<p>A ML model consists of a number of different components in a model package:  - Model Input/Output Schema  - Model artifacts  - Model version information  - Model format (based on the ML framework used to train the model - e.g., .pkl or .tb files)</p> <p>You can also optionally include in your packaged model:  - Sample data (used to test the model  in KServe)  - The source notebook/program/experiment used to create the model</p>"},{"location":"concepts/mlops/serving/","title":"Model Serving","text":"<p>In Hopsworks, you can easily deploy models from the model registry in KServe or in Docker containers (for Hopsworks Community). KServe is the defacto open-source framework for model serving on Kubernetes. You can deploy models in either programs, using the HSML library, or in the UI. A KServe model deployment can include the following components:</p> <code>Transformer</code> <p>A pre-processing and post-processing component that can transform model inputs before predictions are made, and predictions before these are delivered back to the client.</p> <code>Predictor</code> <p>A predictor is a ML model in a Python object that takes a feature vector as input and returns a prediction as output.</p> <code>Inference Logger</code> <p>Hopsworks logs inputs and outputs of transformers and predictors to a Kafka topic that is part of the same project as the model.</p> <code>Inference Batcher</code> <p>Inference requests can be batched to improve throughput (at the cost of slightly higher latency).</p> <code>Istio Model Endpoint</code> <p>You can publish a model over REST(HTTP) or gRPC using a Hopsworks API key. API keys have scopes to ensure the principle of least privilege access control to resources managed by Hopsworks.</p> <p>Models deployed on KServe in Hopsworks can be easily integrated with the Hopsworks Feature Store using either a Transformer or Predictor Python script, that builds the predictor's input feature vector using the application input and pre-computed features from the Feature Store.</p> <p></p> <p>Model Serving Guide</p> <p>More information can be found in the Model Serving guide.</p>"},{"location":"concepts/mlops/training/","title":"Model Training","text":"<p>Hopsworks supports running model training pipelines on any Python environment, whether on an external Python client or on a Hopsworks cluster. The outputs of a training pipeline are typically experiment results, including logs, and possibly a trained model. You can plugin your own experimentation tracking platform or model registry, or you can use Hopsworks.</p>"},{"location":"concepts/mlops/training/#training-pipelines-on-hopsworks","title":"Training Pipelines on Hopsworks","text":"<p>If you train models with Hopsworks, you can setup CI/CD pipelines as shown below, where the experiments are tracked by Hopsworks, and any model created is published to a model registry. Each project has its own private model registry, so when you are working in a development project, you typically publish models to your project's private development registry, and if all model validation tests pass, and the model performance is good enough, the same training pipeline can be submitted via a CI/CD pipeline (e.g., GitHub push request) to a staging project, and the same procedure can be repeated to push the training pipeline to a production project.</p> <p></p> <p>Hopsworks Model Registry and Model Serving capabilities can then be used to build a batch or online prediction service using the model.</p>"},{"location":"concepts/projects/cicd/","title":"CI/CD","text":"<p>You can setup traditional development, staging, and production environment in Hopsworks using Projects. A project enables you provide access control for the different environments - just like a GitHub repository, owners of projects can add and remove members of projects and assign different roles to project members - the \"data owner\" role can write to feature store, while a \"data scientist\" can only read from the feature store and create training data.</p>"},{"location":"concepts/projects/cicd/#dev-staging-prod","title":"Dev, Staging, Prod","text":"<p>You can create dev, staging, and prod projects - either on the same cluster, but mostly commonly, with production on its own cluster:</p> <p></p>"},{"location":"concepts/projects/cicd/#versioning","title":"Versioning","text":"<p>Hopsworks supports the versioning of ML assets, including:</p> <ul> <li>Feature Groups: the version of its schema - breaking schema changes require a new version and backfilling the new version;</li> <li>Feature Views:  the version of its schema, and breaking schema changes only require a new version;</li> <li>Models: the version of a model;</li> <li>Deployments: the version of the deployment of a model - a model with the same version can be found in &gt;1 deployment.</li> </ul>"},{"location":"concepts/projects/cicd/#pytest-for-feature-logic-and-feature-pipeline-tests","title":"Pytest for feature logic and feature pipeline tests","text":"<p>Pytest and Great Expectations can be used for testing feature pipelines. Pytest is used to test feature logic and for end-to-end feature pipeline tests, while Great Expectations is used for data validation tests.  Here, we can see how a feature pipeline test uses sample data to compute features and validate they have been written successfully, first to a development feature store, and then they can be pushed to a staging feature store, before finally being promoted to production. </p>"},{"location":"concepts/projects/governance/","title":"Governance","text":"<p>Hopsworks provides project-level multi-tenancy, a data mesh enabling technology. Think of it as a GitHub repository for your teams and ML assets. More specifically, a project is a sandbox for team members, ML assets (features, training data, models, vector database, model deployments), and optionally feature pipelines and training pipelines. The ML assets can only be accessed by project members, and there is role-based access control (RBAC) for project members within a project.</p> <p></p>"},{"location":"concepts/projects/governance/#devstagingprod-for-data","title":"Dev/Staging/Prod for Data","text":"<p>Projects enable you to define development, staging, and even production projects on the same cluster. Often, companies deploy production projects on dedicated clusters, but development projects and staging projects on a shared cluster. This way, projects can be easily used to implement CI/CD workflows.</p>"},{"location":"concepts/projects/governance/#data-mesh-of-feature-stores","title":"Data Mesh of Feature Stores","text":"<p>Projects enable you to move beyond the traditional dev/staging/prod ownership model for data. Different teams or lines of business can have their own private feature stores, you can mix them with a group-wide feature store, and feature stores can be securely shared between teams/organizations. Effectively, you can have decentralized ownership of feature stores, with domain-specific projects, and each project managing its own feature pipelines. Hopsworks provides data/feature sharing support between these self-service projects. </p>"},{"location":"concepts/projects/governance/#audit-logs-with-rest-api","title":"Audit Logs with REST API","text":"<p>Hopsworks stores audit logs for all calls on its REST API in its file system, HopsFS. The audit log can be used to analyze the historical usage of services by users.</p>"},{"location":"concepts/projects/search/","title":"Tags/Search/Lineage","text":""},{"location":"concepts/projects/search/#search","title":"Search","text":"<p>Hopsworks supports free-text search to discover machine-learning assets:</p> <ul> <li>features</li> <li>feature groups</li> <li>feature views</li> <li>training data</li> </ul> <p>You can use the search bar at the top of your project to free-text search for the names or descriptions of any ML asset. You can also search using keywords or tags that are attached to an ML asset.</p> <p>You can search for assets within a specific project or across all projects in a Hopsworks deployment, including those you are not a member of. This allows for easier discoverability and reusability of assets within an organization.  To avoid users gaining unauthorized access to data, if a search result is in a project you are not a member of, the information displayed is limited to: names, descriptions, tags, asset creator and create date. If the search result is within a project you are a member of, you are also able to inspect recent activities on the asset as well as statistics.</p>"},{"location":"concepts/projects/search/#tags","title":"Tags","text":"<p>A keyword is a single user-defined word attached to an ML asset. Keywords can be used to help it make it easier to find ML assets or understand the context in which they should be used, for example, PII could be used to indicate that the ML asset is based on personally identifiable information.</p> <p>However, it may be preferable to have a stronger governance framework for ML assets than keywords alone. For this, you can define a schematized tag, defining a list of key/value tags along with a type for a value. In the figure below, you can see an example of a schematized tag with two key/value pairs: pii of type boolean (indicating if this feature group contains PII data), and owner of type string (indicating who the owner of the data in this feature group is). Note there is also a keyword defined for this feature group called eu_region, indicating the data has its origins in the EU.</p> <p></p>"},{"location":"concepts/projects/search/#lineage","title":"Lineage","text":"<p>Hopsworks tracks the lineage (or provenance) of ML assets automatically for you. You can see what features are used in which feature view or training dataset. You can see what training dataset was used to train a given model. For assets that are managed outside of Hopsworks, there is support for the explicit definition of lineage dependencies.</p> <p></p>"},{"location":"concepts/projects/storage/","title":"Data Storage/Sharing","text":"<p>Every project in Hopsworks has its own private assets:</p> <ul> <li>a Feature Store (including both Online and Offline Stores)</li> <li>a Filesystem subtree (all directory and files under /Projects//) <li>a Model Registry</li> <li>Model Deployments</li> <li>Kafka topics</li> <li>OpenSearch indexes (including KNN indexes - the vector DB)</li> <li>a Hive Database</li> <p>Access control to these assets is controlled using project membership ACLs (access-control lists). Users in a project who have a Data Owner role have read/write access to these assets.  Users in a project who have a Data Scientist role have mostly read-only access to these assets, with the exception of the ability to write to well-known directories (Resources, Jupyter, Logs). </p> <p>However, it is often desirable to share assets between projects, with read-only, read/write privileges, and to restrict the privileges to specific role (e.g., Data Owners) in the target project. In Hopsworks, you can explicitly share assets between projects without copying the assets. Sharing is managed by ACLs in Hopsworks, see example below: </p>"},{"location":"setup_installation/","title":"Setup and Administration","text":"<p>This section contains installation guides for the Hopsworks Platform using kubernetes, on </p> <ul> <li>AWS</li> <li>Azure</li> <li>GCP</li> <li>On-Prem environments</li> </ul> <p>and common administration instructions.</p> <p>For instructions on installing the Hopsworks Client libraries, see the Client Installation guide.</p>"},{"location":"setup_installation/admin/","title":"Cluster Administration","text":"<p>Hopsworks has a cluster management page that allows you, the administrator, to perform management actions,  monitor and control Hopsworks.</p> <p>To access the cluster management page you should log in into Hopsworks using your administrator account.  In the top right corner, click on your name in the top right corner of the navigation bar and choose Cluster Settings from the dropdown menu.</p>"},{"location":"setup_installation/admin/alert/","title":"Configure Alerts","text":""},{"location":"setup_installation/admin/alert/#introduction","title":"Introduction","text":"<p>Alerts are sent from Hopsworks using Prometheus'  Alert manager. In order to send alerts we first need to configure the Alert manager.</p>"},{"location":"setup_installation/admin/alert/#prerequisites","title":"Prerequisites","text":"<p>Administrator account on a Hopsworks cluster.</p>"},{"location":"setup_installation/admin/alert/#step-1-go-to-alerts-configuration","title":"Step 1: Go to alerts configuration","text":"<p>To configure the Alert manager click on your name in the top right corner of the navigation bar and choose Cluster Settings from the dropdown menu. In the Cluster Settings' Alerts tab you can configure the alert  manager to send alerts via email, slack or pagerduty.</p> Configure alerts"},{"location":"setup_installation/admin/alert/#step-2-configure-email-alerts","title":"Step 2: Configure Email Alerts","text":"<p>To send alerts via email you need to configure an SMTP server. Click on the Configure  button on the left side of the email row and fill out the form that pops up.</p> Configure Email Alerts <ul> <li>Default from: the address used as sender in the alert email.</li> <li>SMTP smarthost: the Simple Mail Transfer Protocol (SMTP) host through which emails are sent.</li> <li>Default hostname (optional): hostname to identify to the SMTP server.</li> <li>Authentication method: how to authenticate to the SMTP server.   CRAM-MD5, LOGIN or PLAIN.</li> </ul> <p>Optionally cluster wide Email alert receivers can be added in Default receiver emails. These receivers will be available to all users when they create event triggered alerts.</p>"},{"location":"setup_installation/admin/alert/#step-3-configure-slack-alerts","title":"Step 3: Configure Slack Alerts","text":"<p>Alerts can also be sent via Slack messages. To be able to send Slack messages you first need to configure a Slack webhook. Click on the Configure button on the left side of the slack row and past in your Slack webhook in Webhook.</p> Configure slack Alerts <p>Optionally cluster wide Slack alert receivers can be added in Slack channel/user. These receivers will be available to all users when they create event triggered alerts.</p>"},{"location":"setup_installation/admin/alert/#step-4-configure-pagerduty-alerts","title":"Step 4: Configure Pagerduty Alerts","text":"<p>Pagerduty is another way you can send alerts from Hopsworks. Click on the Configure button on the left side of  the pagerduty row and fill out the form that pops up. </p> Configure Pagerduty Alerts <p>Fill in Pagerduty URL: the URL to send API requests to.</p> <p>Optionally cluster wide Pagerduty alert receivers can be added in Service key/Routing key. By first choosing the PagerDuty integration type:</p> <ul> <li>global event routing (routing_key): when using PagerDuty integration type <code>Events API v2</code>.</li> <li>service (service_key): when using PagerDuty integration type <code>Prometheus</code>.</li> </ul> <p>Then adding the Service key/Routing key of the receiver(s). PagerDuty provides  documentation on how to integrate with  Prometheus' Alert manager.</p>"},{"location":"setup_installation/admin/alert/#step-5-configure-webhook-alerts","title":"Step 5: Configure Webhook Alerts","text":"<p>You can also use webhooks to send alerts. A Webhook Alert is sent as an HTTP POST command with a JSON-encoded parameter payload. Click on the Configure button on the left side of the webhook row and fill out the form that pops up. </p> Configure Webhook Alerts <p>Fill in the unique URL of your Webhook: the endpoint to send HTTP POST requests to.</p> <p>A global receiver is created when a webhook is configured and can be used by any project in the cluster. </p>"},{"location":"setup_installation/admin/alert/#step-6-advanced-configuration","title":"Step 6: Advanced configuration","text":"<p>If you are familiar with Prometheus' Alert manager  you can also configure alerts by editing the yaml/json file directly by going to the advaced page and clicking the edit button.</p> <p>The advanced page shows the configuration currently loaded on the alert manager. After editing the configuration it takes some time to propagate changes to the alertmanager. </p> <p>The reload button can be used to validate the changes made to the configuration.  It will try to load the new configuration to the alertmanager and show any errors that might prevent the configuration from being loaded. </p> Advanced configuration <p>Warning</p> <p>If you make any changes to the configuration ensure that the changes are valid by reloading the configuration until the changes are loaded and visible in the advanced page. </p> <p>Example: Adding the yaml snippet shown below in the global section of the alert manager configuration will have the same effect as creating the SMTP configuration as shown in section 1 above.</p> <pre><code>global:\n    smtp_smarthost: smtp.gmail.com:587\n    smtp_from: hopsworks@gmail.com\n    smtp_auth_username: hopsworks@gmail.com\n    smtp_auth_password: XXXXXXXXX\n    smtp_auth_identity: hopsworks@gmail.com\n ...\n</code></pre> <p>To test the alerts by creating triggers from Jobs and Feature group validations see Alerts.</p> <p>The yaml syntax in the UI is slightly different in that it does not allow double quotes (it will ignore the values but give no error).  Below is an example configuration, that can be used in the UI, with both email and slack receivers configured for system alerts.</p> <pre><code>global:\n    smtp_smarthost: smtp.gmail.com:587\n    smtp_from: hopsworks@gmail.com\n    smtp_auth_username: hopsworks@gmail.com\n    smtp_auth_password: XXXXXXXXX\n    smtp_auth_identity: hopsworks@gmail.com\n    resolveTimeout: 5m\ntemplates:\n  - /srv/hops/alertmanager/alertmanager-0.17.0.linux-amd64/template/*.tmpl\nroute:\n  receiver: default\n  routes:\n    - receiver: email\n      continue: true\n      match:\n        type: system-alert\n    - receiver: slack\n      continue: true\n      match:\n        type: system-alert\n  groupBy:\n    - alertname\n  groupWait: 10s\n  groupInterval: 10s\nreceivers:\n  - name: default\n  - name: email\n    emailConfigs:\n      - to: someone@logicalclocks.com\n        from: hopsworks@logicalclocks.com\n        smarthost: mail.hello.com\n        text: &gt;-\n          summary: {{ .CommonAnnotations.summary }} description: {{\n          .CommonAnnotations.description }}\n  - name: slack\n    slackConfigs:\n      - apiUrl: &gt;-\n          https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX\n        channel: '#general'\n        text: &gt;-\n          &lt;!channel&gt; summary: {{ .Annotations.summary }} description: {{\n          .Annotations.description }}\n</code></pre>"},{"location":"setup_installation/admin/auth/","title":"Authentication Methods","text":""},{"location":"setup_installation/admin/auth/#introduction","title":"Introduction","text":"<p>Hopsworks can be configured to use different types of authentication methods. In this guide we will look at the  different authentication methods available in Hopsworks.</p>"},{"location":"setup_installation/admin/auth/#prerequisites","title":"Prerequisites","text":"<p>Administrator account on a Hopsworks cluster.</p>"},{"location":"setup_installation/admin/auth/#step-1-go-to-authentication-methods-page","title":"Step 1: Go to Authentication methods page","text":"<p>To configure Authentication methods click on your name in the top right corner of the navigation bar and choose  Cluster Settings from the dropdown menu.</p>"},{"location":"setup_installation/admin/auth/#step-2-configure-authentication-methods","title":"Step 2: Configure Authentication methods","text":"<p>In the Cluster Settings Authentication tab you can configure how users authenticate.</p> <ol> <li> <p>TOTP Two-factor Authentication: can be disabled, optional or mandatory. If set to mandatory all users are     required to set up two-factor authentication when registering. </p> <p>Note</p> <p>If two-factor is set to mandatory on a cluster with preexisting users all users will need to go through lost device recovery step to enable two-factor. So consider setting it to optional first and allow users to  enable it before setting it to mandatory.</p> </li> <li> <p>OAuth2: if your organization already have an identity management system compatible with     OpenID Connect (OIDC) you can configure Hopsworks to use your identity provider     by enabling OAuth as shown in the figure below. After enabling OAuth     you can register your identity provider by clicking on Add Identity Provider button. See    Create client for details.</p> </li> <li>LDAP/Kerberos: if your organization is using LDAP or Kerberos to manage users and services you can configure     Hopsworks to use it as the user management system. You can enable LDAP/Kerberos by clicking on the checkbox,     as shown in the figure below, and choosing LDAP or Kerberos. For more information on how to configure LDAP and Kerberos see     Configure LDAP and Configure Kerberos.</li> </ol> Setup Authentication Methods <p>In the figure above we see a cluster with Two-factor authentication disabled, OAuth enabled with one registered  identity provider and LDAP authentication enabled. </p>"},{"location":"setup_installation/admin/project/","title":"Manage Projects","text":"<p>Hopsworks provides an administrator with a view of the projects in a Hopsworks cluster.</p> <p>A Hopsworks administrator is not automatically a member of all the projects in a cluster. However, they can see which projects exist, who is the project owner, and they can limit the storage quota and compute quota for each project.</p>"},{"location":"setup_installation/admin/project/#prerequisites","title":"Prerequisites","text":"<p>You need to be an administrator on a Hopsworks cluster.</p>"},{"location":"setup_installation/admin/project/#changing-project-quotas","title":"Changing project quotas","text":"<p>You can find the Project management page by clicking on your name, in the top right corner of the navigation bar, and choosing Cluster Settings from the dropdown menu and going to the Project tab.</p> Project page <p>This page will list all the projects in a cluster, their name, owner and when its quota was last updated. By clicking on the edit configuration link of a project you will be able to edit the quotas of that project.</p> Project quotas"},{"location":"setup_installation/admin/project/#storage","title":"Storage","text":"<p>Storage quota represents the amount of data a project can store. The storage quota is broken down in three different areas:</p> <ul> <li>Feature Store: This represents the storage quota for files and directories stored in the <code>_featurestore.db</code> dataset in the project. This dataset contains all the feature group offline data for the project.</li> <li>Hive DB: This represents the storage quota for files and directories stored in the <code>[projectName].db</code> dataset in the project. This is a general purpose Hive database for the project that can be used for analytics.</li> <li>Project: This represents the storage quota for all the data stored on any other dataset.</li> </ul> <p>Each storage quota is divided into space quota, i.e., how much space the files can consume, and namespace quota, i.e., how many files and directories there can be. If Hopsworks is deployed on-premise using hard drives to store the data, i.e., Hopsworks is not configured to store its data in a S3-compliant storage system, the data is replicated across multiple nodes (by default 3) and the space quota takes the replication factor into consideration. As an example, a 100MB file stored with a replication factor of 3, will consume 300MB of space quota.</p> <p>By default, all storage quotas are disabled and not enforced. Administrators can change this default by changing the following configuration in the Configuration UI and/or the cluster definition: <pre><code>hopsworks:\n    featurestore_default_quota: [default quota in bytes, -1 to disable]\n    hdfs_default_quota: [default quota in bytes, -1 to disable]\n    hive_default_quota: [default quota in bytes, -1 to disable]\n</code></pre> The values specified will be set during project creation and administrators will be able to customize each project using this UI.</p>"},{"location":"setup_installation/admin/project/#compute","title":"Compute","text":"<p>Compute quotas represents the amount of compute a project can use to run Spark and Flink applications as well as Tez queries. Quota is expressed as number of seconds a container of size 1 CPU and 1GB of RAM can run for.</p> <p>If the Hopsworks cluster is connected to a Kubernetes cluster, Python jobs, Jupyter notebooks and KServe models are not subject to the compute quota. Currently, Hopsworks does not support defining quotas for compute scheduled on the connected Kubernetes cluster.</p> <p>By default, the compute quota is disabled. Administrators can change this default by changing the following configuration in the Configuration UI and/or the cluster definition: <pre><code>hopsworks:\n    yarn_default_payment_type: [NOLIMIT to disable the quota, PREPAID to enable it]\n    yarn_default_quota: [default quota in seconds]\n</code></pre></p> <p>The values specified will be set during project creation and administrators will be able to customize each project using this UI.</p>"},{"location":"setup_installation/admin/project/#kakfa-topics","title":"Kakfa Topics","text":"<p>Kafka is used within Hopsworks to enable users to write data to the feature store in Real-Time and from a variety of different frameworks. If a user creates a feature group with the stream APIs enabled, then a Kafka topic will be created for that feature group. By default, a project can have up to 100 Kafka topics. Administrators can increase the number of Kafka topics a project is allowed to create by increasing the quota in the project admin UI. </p>"},{"location":"setup_installation/admin/project/#force-deleting-a-project","title":"Force deleting a project","text":"<p>Administrators have the option to force delete a project. This is useful if the project was not created or deleted properly, e.g., because of an error.</p>"},{"location":"setup_installation/admin/project/#controlling-who-can-create-projects","title":"Controlling who can create projects","text":"<p>Every user on Hopsworks can create projects. By default, each user can create up to 10 projects. For production environments, the number of projects should be limited and controlled for resource allocation purposes as well as closer control over the data.  Administrators can control how many projects a user can provision by setting the following configuration in the Configuration UI and/or cluster definition:</p> <pre><code>hopsworks:\n    max_num_proj_per_user: [Maximum number of projects each user can create]\n</code></pre> <p>This value will be set when the user is provisioned. Administrators can grant additional projects to a specific user through the User Administration UI.</p>"},{"location":"setup_installation/admin/roleChaining/","title":"AWS IAM Role Chaining","text":""},{"location":"setup_installation/admin/roleChaining/#introduction","title":"Introduction","text":"<p>When running Hopsworks in Amazon EKS you have several options to give the Hopsworks user access to AWS resources. The simplest is to assign Amazon EKS node IAM role access to the resources. But, this will make these resources accessible by all users. To manage access to resources on a project base you need to use Role chaining. </p> <p>In this document we will see how to configure AWS and Hopsworks to use Role chaining in your Hopsworks projects.</p>"},{"location":"setup_installation/admin/roleChaining/#prerequisites","title":"Prerequisites","text":"<p>Before you begin this guide you'll need the following:</p> <ul> <li>A Hopsworks cluster running on EKS.</li> <li>Enabled IAM OpenID Connect (OIDC) provider for your cluster.</li> <li>Administrator account on the Hopsworks cluster.</li> </ul>"},{"location":"setup_installation/admin/roleChaining/#step-1-create-an-iam-role-and-associate-it-with-a-kubernetes-service-account","title":"Step 1: Create an IAM role and associate it with a Kubernetes service account","text":"<p>To use role chaining the hopsworks instance pods need to be able to impersonate the roles you want to be linked to your project. For this you need to create an IAM role and associate it with your Kubernetes service accounts with assume role permissions and attach it to your hopsworks instance pods.  For more details on how to create an IAM roles for Kubernetes service accounts see the aws documentation. </p> <p>Note</p> <p>To ensure that users can't use the service account role and impersonate the roles by their own means, you need to ensure that the service account is only attached to the hopsworks instance pods.</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Federated\": \"arn:aws:iam::$account_id:oidc-provider/$oidc_provider\"\n      },\n      \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n      \"Condition\": {\n        \"StringEquals\": {\n          \"$oidc_provider:aud\": \"sts.amazonaws.com\",\n          \"$oidc_provider:sub\": \"system:serviceaccount:$namespace:$service_account\"\n        }\n      }\n    }\n  ]\n}\n</code></pre> Example trust policy for a service account. <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AssumeDataRoles\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"sts:AssumeRole\",\n            \"Resource\": [\n                \"arn:aws:iam::123456789011:role/my-role\",\n                \"arn:aws:iam::xxxxxxxxxxxx:role/s3-role\",\n                \"arn:aws:iam::xxxxxxxxxxxx:role/dev-s3-role\",\n                \"arn:aws:iam::xxxxxxxxxxxx:role/redshift\"\n            ]\n        }\n    ]\n}\n</code></pre> Example policy for assuming four roles. <p>The IAM role will need to add a trust policy to allow the service account to assume the role, and permissions to assume the different roles that will be used to access resources.</p> <p>To associate the IAM role with your Kubernetes service account you will need to annotate your service account with the Amazon Resource Name (ARN) of the IAM role that you want the service account to assume.</p> <pre><code>kubectl annotate serviceaccount -n $namespace $service_account eks.amazonaws.com/role-arn=arn:aws:iam::$account_id:role/my-role\n</code></pre>"},{"location":"setup_installation/admin/roleChaining/#step-2-create-the-resource-roles","title":"Step 2: Create the resource roles","text":"<p>For the service account role to be able to impersonate the roles you also need to configure the roles themselves to allow it. This is done by adding the service account role to the role's Trust relationships.</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n        \"Effect\": \"Allow\",\n        \"Principal\": {\n            \"AWS\": \"arn:aws:iam::xxxxxxxxxxxx:role/service-account-role\"\n        },\n        \"Action\": \"sts:AssumeRole\"\n        }\n    ]\n}\n</code></pre> Example trust-policy document."},{"location":"setup_installation/admin/roleChaining/#step-3-create-mappings","title":"Step 3: Create mappings","text":"<p>Now that the service account IAM role can assume the roles we need to configure Hopsworks to delegate access to the roles on a project base.</p> <p>In Hopsworks, click on your name in the top right corner of the navigation bar and choose Cluster Settings from the dropdown menu. In the Cluster Settings' IAM Role Chaining tab you can configure the mappings between projects and IAM roles.</p> Role Chaining <p>Add mappings by clicking on New role chaining. Enter the project name. Select the type of user that can assume the role. Enter the role ARN. And click on Create new role chaining</p> Create Role Chaining <p>Project member can now create connectors using temporary credentials to assume the role you configured. More detail about using temporary credentials can be found here.</p> <p>Project member can see the list of role they can assume by going the Project Settings -&gt; Assuming IAM Roles page.</p>"},{"location":"setup_installation/admin/user/","title":"User Management","text":""},{"location":"setup_installation/admin/user/#introduction","title":"Introduction","text":"<p>Whether you run Hopsworks on-premise, or on the cloud using kubernetes,  you have a Hopsworks cluster which contains all users and projects.</p>"},{"location":"setup_installation/admin/user/#prerequisites","title":"Prerequisites","text":"<p>Administrator account on a Hopsworks cluster.</p>"},{"location":"setup_installation/admin/user/#step-1-go-to-user-management","title":"Step 1: Go to user management","text":"<p>All the users of your Hopsworks instance have access to your cluster with different access rights.  You can find them by clicking on your name in the top right corner of the navigation bar and choosing Cluster  Settings from the dropdown menu and going to the Users tab (You need to have Admin role to get access to the  Cluster Settings page).</p> Active Users"},{"location":"setup_installation/admin/user/#step-2-manage-user-roles","title":"Step 2: Manage user roles","text":"<p>Roles let you manage the access rights of a user to the cluster.</p> <ul> <li>User: users with this role are only allowed to use the cluster by creating a limited number of projects.</li> <li>Admin: users with this role are allowed to manage the cluster. This includes accepting new users to the cluster or    blocking them, managing user quota, configure alerts and setting up authentication methods.  </li> </ul> <p>You can change the role of a user by clicking on the select dropdown that shows the current role of the user.</p>"},{"location":"setup_installation/admin/user/#step-3-validating-and-blocking-users","title":"Step 3: Validating and blocking users","text":"<p>By default, a user who register on Hopsworks using their own credentials are not granted access to the cluster.  First, a user with an admin role needs to validate their account.</p> <p>By clicking on the Review Requests button you can open a user request review popup as shown in the image below.</p> Review user request <p>On the user request review popup you can activate or block users. Users with a validated email address will have a  check mark on their email.  </p> <p>Similarly, if a user is no longer allowed access to the cluster you can block them. To keep consistency with the history of your datasets, a user can not be deleted but only blocked. If necessary a user can be  deleted manually in the cluster using the command line. You can block a user by clicking on the block icon on the right side of the user in the list.</p> Blocked Users <p>Blocked users will appear on the lower section of the page. Click on display blocked users to show all the blocked  users in your cluster. If a user is blocked by mistake you can reactivate it by clicking on the check mark icon  that corresponds to that user in the blocked users list. </p> <p>If there are too many users in your cluster, use the search box (available for blocked users too) to filter users by  name or email. It is also possible to filter activated users by role. For example to see all administrators in you  cluster click on the select dropdown to the right of the search box and choose Admin. </p>"},{"location":"setup_installation/admin/user/#step-4-create-a-new-users","title":"Step 4: Create a new users","text":"<p>If you want to allow users to login without registering you can pre-create them by clicking on New user.</p> Create new user <p>After setting the user's name and email chose the type of user you want to create (Hopsworks, Kerberos or LDAP). To  create a Kerberos or LDAP user you need to get the users UUID from the Kerberos or LDAP server. Hopsworks user  can also be assigned a Role. Kerberos and LDAP users on the other hand can only be assigned a role through group  mapping.</p> <p>A temporary password will be generated and displayed when you click on Create new user. Copy the password and pass  it securely to the user. </p> Copy temporary password"},{"location":"setup_installation/admin/user/#step-5-reset-user-password","title":"Step 5: Reset user password","text":"<p>In the case where a user loses her/his password and can not recover it with the  password recovery, an administrator can reset it for them.</p> <p>On the bottom of the Users page click on the Reset a user password link. A popup window with a dropdown for  searching users by name or email will open. Find the user and click on Reset new password.</p> Reset user password <p>A temporary password will be displayed. Copy the password and pass it to the user securely.</p> Copy temporary password <p>A user with a temporary password will see a warning message when going to Account settings Authentication tab.</p> Change password <p>Note</p> <p>A temporary password should be changed as soon as possible.</p>"},{"location":"setup_installation/admin/variables/","title":"Cluster Configuration","text":""},{"location":"setup_installation/admin/variables/#introduction","title":"Introduction","text":"<p>Whether you run Hopsworks on-premise, or on the cloud using kubernetes, it is possible to change a variety of configurations on the cluster, changing its default behaviour. This section is not going into detail for every setting, since every Hopsworks cluster comes with a robust default setup. However, this guide is to explain where to find the configurations and if necessary, how to change them.</p> <p>Note</p> <p>In most cases you will be only be prompted to change these configurations by a Hopsworks Solutions Engineer or similar.</p>"},{"location":"setup_installation/admin/variables/#prerequisites","title":"Prerequisites","text":"<p>An administrator account on a Hopsworks cluster.</p>"},{"location":"setup_installation/admin/variables/#step-1-the-configuration-page","title":"Step 1: The configuration page","text":"<p>You can find the configuration page by navigating in the UI:</p> <ol> <li>Click on your user name in the top right corner, then select Cluster Settings.</li> <li>Among the cluster settings, you will find a tab Configuration</li> </ol> Configuration settings"},{"location":"setup_installation/admin/variables/#step-2-editing-existing-configurations","title":"Step 2: Editing existing configurations","text":"<p>To edit an existing configuration, simply find the property using the search field, then click the edit button to change the value of the setting or its visibility. Once you have made the change, don't forget to click save to persist the changes.</p>"},{"location":"setup_installation/admin/variables/#visibility","title":"Visibility","text":"<p>The visibility setting indicates whether a setting can be read only by Hops Admins or also by simple Hops Users, that is everyone. Additionally, you can also allow to read the setting even when not authenticated. If the setting contains a password or sensitive information, you can also hide the value so it's not shown in the UI.</p>"},{"location":"setup_installation/admin/variables/#step-3-adding-a-new-configuration","title":"Step 3: Adding a new configuration","text":"<p>In rare cases it might be necessary to add additional configurations.</p> <p>To do so, click on New Variable, where you can then configure the new setting with a key, value and visibility. Once you have set the desired properties, you can persist them by clicking Create Configuration</p> Adding a new configuration property"},{"location":"setup_installation/admin/audit/audit-logs/","title":"Access Audit Logs","text":""},{"location":"setup_installation/admin/audit/audit-logs/#introduction","title":"Introduction","text":"<p>Hopsworks collects audit logs on all URL requests to the application server. These logs are saved in Payara log directory under <code>&lt;payara-log-dir&gt;/audit</code> by default.</p>"},{"location":"setup_installation/admin/audit/audit-logs/#prerequisites","title":"Prerequisites","text":"<p>In order to access the audit logs you need the following: </p> <ul> <li>Administrator account on the Hopsworks cluster.</li> <li>SSH access to the Hopsworks cluster with a user in the <code>glassfish</code> group.</li> </ul>"},{"location":"setup_installation/admin/audit/audit-logs/#step-1-configure-audit-logs","title":"Step 1: Configure Audit logs","text":"<p>Audit logs can be configured from the Cluster Settings Configuration tab. You can access the Configuration page of your Hopsworks cluster by clicking on your name, in the top right corner, and choosing Cluster Settings from the dropdown menu.</p> Audit log configuration <p>Type audit in the search box to see the configuration variables associated with audit logs. To edit a configuration variable, you can click on the edit button (), insert the new value and save changes clicking on the check mark ().</p> <p>Audit logs configuration variables</p> Name Description audit_log_count the number of files to keep when rotating logs (java.util.logging.FileHandler count) audit_log_file_format log file name pattern. (java.util.logging.FileHandler.pattern) audit_log_file_type the output format of the log file. Can be one of java.util.logging.SimpleFormatter (default), io.hops.hopsworks.audit.helper.JSONLogFormatter, or io.hops.hopsworks.audit.helper.HtmlLogFormatter. audit_log_size_limit the maximum number of bytes to write to any one file. (java.util.logging.FileHandler.limit) audit_log_date_format if io.hops.hopsworks.audit.helper.JSONLogFormatter is used as audit log file type, this will set the date format of the output JSON. The format should be java.text.SimpleDateFormat compatible string. <p>Warning</p> <p>Hopsworks application needs to be reloaded for any changes to be applied. For doing that, go to the Payara admin panel (<code>https://&lt;your-domain&gt;:4848</code>), click on Applications on the side menu and reload the hopsworks-ear application.</p>"},{"location":"setup_installation/admin/audit/audit-logs/#step-2-access-the-logs","title":"Step 2: Access the Logs","text":"<p>To access the audit logs, SSH into the instance pod of your Hopsworks cluster and navigate to the path <code>/opt/payara/appserver/glassfish/nodes/&lt;node name&gt;/&lt;instance name&gt;/logs/audit</code>.</p> <p>Audit logs follow the format set in the audit_log_file_type configuration variable.</p> <p>Example of audit logs using JSONLogFormatter</p> <pre><code>{\"className\":\"io.hops.hopsworks.api.user.AuthService\",\"methodName\":\"login\",\"parameters\":\"[admin@hopsworks.ai, org.apache.catalina.connector.ResponseFacade@2de6dd0b, org.apache.catalina.connector.RequestFacade@7a82f674]\",\"outcome\":\"200\",\"caller\":{\"username\":null,\"email\":\"admin@hopsworks.ai\",\"userId\":null},\"clientIp\":\"10.0.2.2\",\"userAgent\":\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36\",\"pathInfo\":\"/auth/login\",\"dateTime\":\"2022-11-09 12:00:08\"}\n</code></pre> <p>Regardless the format, each line in the audit logs can contain the following variables:</p> <p>Audit log variables</p> Name Description className the class called by the request methodName the method called by the request parameters parameters sent from the client outcome response code sent from the server caller the logged in user that made the request. Can be username, email, or userId clientIp the IP address of the client userAgent the browser used by the client pathInfo the URL path called by the client dateTime time of the request"},{"location":"setup_installation/admin/audit/audit-logs/#going-further","title":"Going Further","text":"<p>You can export audit logs to use them outside Hopsworks.</p>"},{"location":"setup_installation/admin/audit/export-audit-logs/","title":"Export Audit Logs","text":""},{"location":"setup_installation/admin/audit/export-audit-logs/#introduction","title":"Introduction","text":"<p>Audit logs can be exported to your storage of preference. In case audit logs have not been configured yet in your Hopsworks cluster, please see Access Audit Logs.</p> <p>Note</p> <pre><code>As an example, in this guide we will show how to export audit logs to BigQuery using the ```bq``` command-line tool.\n</code></pre>"},{"location":"setup_installation/admin/audit/export-audit-logs/#prerequisites","title":"Prerequisites","text":"<p>In order to export audit logs you need SSH access to the Hopsworks cluster.</p>"},{"location":"setup_installation/admin/audit/export-audit-logs/#step-1-create-a-bigquery-table","title":"Step 1: Create a BigQuery Table","text":"<p>Create a dataset and a table in BigQuery.</p> <p>The table schema is shown below.</p> <pre><code>fullname          mode      type        description\npathInfo          NULLABLE  STRING  \nmethodName        NULLABLE  STRING  \ncaller            NULLABLE  RECORD  \ndateTime          NULLABLE  TIMESTAMP   bq-datetime\nuserAgent         NULLABLE  STRING  \nclientIp          NULLABLE  STRING  \noutcome           NULLABLE  STRING  \nparameters        NULLABLE  STRING  \nclassName         NULLABLE  STRING  \ncaller.userId     NULLABLE  STRING  \ncaller.email      NULLABLE  STRING  \ncaller.username   NULLABLE  STRING\n</code></pre>"},{"location":"setup_installation/admin/audit/export-audit-logs/#step-2-export-audit-logs-to-the-bigquery-table","title":"Step 2: Export Audit Logs to the BigQuery Table","text":"<p>Audit logs can be exported in different formats. For instance, to export audit logs in JSON format set <code>audit_log_file_type=io.hops.hopsworks.audit.helper.JSONLogFormatter</code>.</p> <p>Info</p> <pre><code>For more information on how to configure the audit log file type see the ```audit_log_file_type``` configuration variable in [Audit logs](../audit/audit-logs.md#step-1-configure-audit-logs).\n</code></pre> <p>To export the audit logs to the BigQuery table created in the previous step, run the following command.</p> <pre><code>bq load --project_id &lt;projectId&gt; \\\n        --source_format=NEWLINE_DELIMITED_JSON \\\n        &lt;DATASET.TABLE&gt; \\\n        /srv/hops/domains/domain1/logs/audit/server_audit_log0.log\n</code></pre> <p>Tip</p> <pre><code>This command can be configured to run periodically on a given schedule by setting up a cronjob.\n</code></pre>"},{"location":"setup_installation/admin/ha-dr/dr/","title":"Disaster Recovery","text":""},{"location":"setup_installation/admin/ha-dr/dr/#backup","title":"Backup","text":"<p>The state of the Hopsworks cluster is divided into data and metadata and distributed across the different node groups. This section of the guide allows you to take a consistent backup between data in the offline and online feature store as well as the metadata.</p> <p>The following services contain critical state that should be backed up:</p> <ul> <li>RonDB: as mentioned above, the RonDB is used by Hopsworks to store the cluster metadata as well as the data for the online feature store.</li> <li>HopsFS: HopsFS stores the data for the batch feature store as well as checkpoints and logs for feature engineering applications.</li> </ul> <p>Backing up service/application metrics and services/applications logs are out of the scope of this guide. By default metrics and logs are rotated after 7 days. Application logs are available on HopsFS when the application has finished and, as such, are backed up with the rest of HopsFS\u2019 data.</p> <p>Apache Kafka and OpenSearch are additional services maintaining state. The OpenSearch metadata can be reconstructed from the metadata stored on RonDB.</p> <p>Apache Kafka is used in Hopsworks to store the in-flight data that is on its way to the online feature store. In the event of a total loss of the cluster, running jobs with in-flight data will have to be replayed.</p>"},{"location":"setup_installation/admin/ha-dr/dr/#configuration-backup","title":"Configuration Backup","text":"<p>Hopsworks adopts an Infrastructure-as-code philosophy, as such all the configuration files for the different Hopsworks services are generated during the deployment phase. Cluster-specific customizations should be centralized in the cluster definition used to deploy the cluster. As such the cluster definition should be backed up (e.g., by committing it to a git repository) to be able to recreate the same cluster in case it needs to be recreated.</p>"},{"location":"setup_installation/admin/ha-dr/dr/#rondb-backup","title":"RonDB Backup","text":"<p>The RonDB backup is divided into two parts: user and privileges backup and data backup.</p> <p>To take the backup of users and privileges you can run the following command from any of the nodes in the head node group. This command generates a SQL file containing all the user definitions for both the metadata services (Hopsworks, HopsFS, Metastore) as well as the user and permission grants for the online feature store. This command needs to be run as user \u2018mysql\u2019 or with sudo privileges.</p> <pre><code>/srv/hops/mysql/bin/mysqlpump -S /srv/hops/mysql-cluster/mysql.sock --exclude-databases=% --exclude-users=root,mysql.sys,mysql.session,mysql.infoschema --users &gt; users.sql\n</code></pre> <p>The second step is to trigger the backup of the data. This can be achieved by running the following command as user \u2018mysql\u2019 on one of the nodes of the head node group. </p> <pre><code>/srv/hops/mysql-cluster/ndb/scripts/mgm-client.sh -e \"START BACKUP [replace_backup_id] SNAPSHOTEND WAIT COMPLETED\"\n</code></pre> <p>The backup ID is an integer greater or equal than 1. The script uses the following: <code>$(date +'%y%m%d%H%M')</code> instead of an integer as backup id to make it easier to identify backups over time.</p> <p>The command instructs each RonDB datanode to backup the data it is responsible for. The backup will be located locally on each datanode under the following path: </p> <pre><code>/srv/hops/mysql-cluster/ndb/backups/BACKUP - the directory name will be BACKUP-[backup_id] \n</code></pre> <p>A more comprehensive backup script is available here - The script includes the steps above as well as collecting all the partial RonDB backups on a single node. The script is a good starting point and can be adapted to ship the database backup outside the cluster.</p>"},{"location":"setup_installation/admin/ha-dr/dr/#hopsfs-backup","title":"HopsFS Backup","text":"<p>HopsFS is a distributed file system based on Apache HDFS. HopsFS stores its metadata in RonDB, as such metadata backup has already been discussed in the section above. The data is stored in the form of blocks on the different data nodes.  For availability reasons, the blocks are replicated across three different data nodes.</p> <p>Within a node, the blocks are stored by default under the following directory, under the ownership of the \u2018hdfs\u2019 user: </p> <pre><code>/srv/hopsworks-data/hops/hopsdata/hdfs/dn/\n</code></pre> <p>To safely backup all the data, a copy of all the datanodes should be taken. As the data is replicated across the different nodes, excluding a set of nodes might result in data loss.</p> <p>Additionally, as HopsFS blocks are files on the file system and the filesystem can be quite large, the backup is not transactional. Consistency is dictated by the metadata. Blocks being added during the copying process will not be visible when restoring as they are not part of the metadata backup taken prior to cloning the HopsFS blocks.</p> <p>When the HopsFS data blocks are stored in a cloud block storage, for example, Amazon S3, then it is sufficient to only backup the metadata. The blob cloud storage service will ensure durability of the data blocks.</p>"},{"location":"setup_installation/admin/ha-dr/dr/#restore","title":"Restore","text":"<p>As with the backup phase, the restore operation is broken down in different steps.</p>"},{"location":"setup_installation/admin/ha-dr/dr/#cluster-deployment","title":"Cluster deployment","text":"<p>The first step to redeploy the cluster is to redeploy the binaries and configuration. You should reuse the same cluster definition used to deploy the first (original) cluster. This will re-create the same cluster with the same configuration.</p>"},{"location":"setup_installation/admin/ha-dr/dr/#rondb-restore","title":"RonDB restore","text":"<p>The deployment step above created a functioning empty cluster. To restore the cluster, the first step is to restore the metadata and online feature store data stored on RonDB.  To restore the state of RonDB, we first need to restore its schemas and tables, then its data, rebuild the indices, and finally restore the users and grants.</p>"},{"location":"setup_installation/admin/ha-dr/dr/#restore-rondb-schemas-and-tables","title":"Restore RonDB schemas and tables","text":"<p>This command should be executed on one of the nodes in the head node group and is going to recreate the schemas, tables, and internal RonDB metadata. In the command below, you should replace the node_id with the id of the node you are running the command on, backup_id with the id of the backup you want to restore. Finally, you should replace the mgm_node_ip with the address of the node where the RonDB management service is running.</p> <pre><code>/srv/hops/mysql/bin/ndb_restore -n [node_id] -b [backup_id] -m --disable-indexes --ndb-connectstring=[mgm_node_ip]:1186 --backup_path=/srv/hops/mysql-cluster/ndb/backups/BACKUP/BACKUP-[backup_id]\n</code></pre>"},{"location":"setup_installation/admin/ha-dr/dr/#restore-rondb-data","title":"Restore RonDB data","text":"<p>This command should be executed on all the RonDB datanodes. Each command should be customized with the node id of the node you are trying to restore (i.e., replace the node_id). As for the command above you should replace the backup_id and mgm_node_ip.</p> <pre><code>/srv/hops/mysql/bin/ndb_restore -n [node_id] -b [backup_id] -r --ndb-connectstring=[mgm_node_ip]:1186 --backup_path=/srv/hops/mysql-cluster/ndb/backups/BACKUP/BACKUP-[backup_id]\n</code></pre>"},{"location":"setup_installation/admin/ha-dr/dr/#rebuild-the-indices","title":"Rebuild the indices","text":"<p>In the first command we disable the indices for recovery. This last command will take care of enabling them again. This command needs to run only once on one of the nodes of the head node group. As for the commands above, you should replace node_id, backup_id and mgm_node_id.</p> <pre><code>/srv/hops/mysql/bin/ndb_restore -n [node_id] -b [backup_id] --rebuild-indexes --ndb-connectstring=[mgm_node_ip]:1186 --backup_path=/srv/hops/mysql-cluster/ndb/backups/BACKUP/BACKUP-[backup_ip]\n</code></pre>"},{"location":"setup_installation/admin/ha-dr/dr/#restore-users-and-grants","title":"Restore Users and Grants","text":"<p>In the backup phase, we took the backup of the user and grants separately. The last step of the RonDB restore process is to re-create all the users and grants both for Hopsworks services as well as for the online feature store users. This can be achieved by running the following command on one node of the head node group:</p> <pre><code>/srv/hops/mysql-cluster/ndb/scripts/mysql-client.sh source users.sql\n</code></pre>"},{"location":"setup_installation/admin/ha-dr/dr/#hopsfs-restore","title":"HopsFS restore","text":"<p>With the metadata restored, you can now proceed to restore the file system blocks on HopsFS and restart the file system. When starting the datanode, it will advertise it\u2019s ID/ClusterID and Storage ID based on the VERSION file that can be found in this directory:</p> <pre><code>/srv/hopsworks-data/hops/hopsdata/hdfs/dn/current\n</code></pre> <p>It\u2019s important that all the datanodes are restored and they report their block to the namenodes processes running on the head nodes. By default the namenodes in HopsFS will exit \u201cSAFE MODE\u201d (i.e., the mode that allows only read operations) only when the datanodes have reported 99.9% of the blocks the namenodes have in the metadata.  As such, the namenodes will not resume operations until all the file blocks have been restored.</p>"},{"location":"setup_installation/admin/ha-dr/dr/#opensearch-state-rebuild","title":"OpenSearch state rebuild","text":"<p>The OpenSearch state can be rebuilt using the Hopsworks metadata stored on RonDB. The rebuild process is done by using the re-indexing mechanism provided by ePipe. The re-indexing can be triggered by running the following command on the head node where ePipe is running:</p> <pre><code>/srv/hops/epipe/bin/reindex-epipe.sh\n</code></pre> <p>The script is deployed and configured during the platform deployment.</p>"},{"location":"setup_installation/admin/ha-dr/dr/#kafka-topics-rebuild","title":"Kafka topics rebuild","text":"<p>The backup and restore plan doesn\u2019t cover the data in transit in Kafka, for which the jobs producing it will have to be replayed. However, the RonDB backup contains the information necessary to recreate the topics of all the feature groups.  You can run the following command, as super user, to recreate all the topics with the correct partitioning and replication factors:</p> <pre><code>/srv/hops/kafka/bin/kafka-restore.sh\n</code></pre> <p>The script is deployed and configured during the platform deployment.</p>"},{"location":"setup_installation/admin/ha-dr/ha/","title":"High Availability","text":"<p>At a high level a Hopsworks cluster can be divided into 4 groups of nodes. Each node group should be deployed according to the requirements (e.g., 3/5/7 nodes for the head node group) to guarantee the availability of the components. </p> <ul> <li>Head nodes: The head node is responsible for running all the metadata, public API, and user interface services that are required for Hopsworks to provide its functionality. They need to be deployed in an odd number (1, 3, 5) as the head nodes run services like Zookeeper and OpenSearch which enforce consistency through quorum based protocols. The head nodes are also responsible for managing the services running on the remaining group of nodes.</li> <li>Worker nodes: The worker node is responsible for executing the feature engineering pipeline code as well as storing the data for the offline feature store (HopsFS). In an on-prem deployment, the data is stored and replicated on the workers\u2019 local hard drives. By default the data is replicated across 3 workers. In a cloud deployment, HopsFS\u2019 data is persisted in a cloud object store (Amazon S3, Azure Blob Storage, Google Cloud Blob Storage) and the HopsFS datanodes are responsible for persisting, retrieving and caching of blocks from the object store.</li> <li>RonDB Data nodes:   These nodes are responsible for storing the services\u2019 metadata (Hopsworks, HopsFS, Hive Metastore, Airflow) as well as the data for the online feature store.   For high availability, at least two data nodes should be deployed and RonDB is typically configured with a replication factor of 2, as it uses synchronous replication with 2-phase commit, not a quorum-based replication protocol.   More advanced deployment patterns and best practices are covered in the RonDB documentation.</li> <li>Query brokers: The query brokers are the entry point for querying the online feature store. They handle authentication, authorization and execution of the requests for online feature data being submitted from the feature store APIs. At least two query brokers should be deployed to achieve high availability. Query brokers are stateless. Additional query brokers should be deployed to handle additional load and clients.</li> </ul> <p>Example deployment:</p> Example High Available deployment <p>For higher availability, a Hopsworks cluster should be deployed across multiple availability zones, however, a single cluster cannot be deployed across multiple regions. Multiple region deployments are out of the scope of this guide.</p> <p>A different service placement is also possible, e.g., separating RonDB data nodes between metadata and online feature store or adding more replicas of a metadata service without necessarily adding a whole new head node, however, this is outside the scope of this guide.</p>"},{"location":"setup_installation/admin/ha-dr/intro/","title":"Hopsworks High Availability and Disaster Recovery Documentation","text":"<p>The Hopsworks Feature Store is the underlying component powering enterprise ML pipelines as well as serving feature data to model making user facing predictions. Sometimes the Hopsworks cluster can experience hardware failures or power loss, to help you plan for these occasions and avoid Hopsworks Feature Store downtime, we put together this guide. This guide is divided into three sections:</p> <ul> <li>High availability: deployment patterns and best practices to make sure individual component failures do not impact the availability of the Hopsworks cluster.</li> <li>Backup: configuration policies and best practices to make sure you have have fresh copy of the data and metadata in case of necessity</li> <li>Restore: procedures and best practices to restore a previous backup if needed.</li> </ul>"},{"location":"setup_installation/admin/ldap/configure-krb/","title":"Configure Kerberos","text":""},{"location":"setup_installation/admin/ldap/configure-krb/#introduction","title":"Introduction","text":"<p>Kerberos is a network authentication protocol that allow nodes to communicating over a non-secure network to prove their identity to one another in a secure manner. This tutorial shows an administrator how to configure Kerberos authentication.</p> <p>Kerberos need some server configuration before you can enable it from the UI.</p>"},{"location":"setup_installation/admin/ldap/configure-krb/#prerequisites","title":"Prerequisites","text":"<p>A server configured with Kerberos. See Server Configuration for Kerberos for  instruction on how to do this. </p>"},{"location":"setup_installation/admin/ldap/configure-krb/#step-1-enable-kerberos","title":"Step 1: Enable Kerberos","text":"<p>After configuring the server you can configure Authentication methods by clicking on your name in the top right corner of the navigation bar and choosing Cluster Settings from the dropdown menu. In the Authentication tab you can find in Cluster Settings, you can enable Kerberos by clicking on the Kerberos checkbox.</p> <p>If LDAP/Kerberos checkbox is not checked, make sure that you configured your application server and enable it by clicking on the checkbox.</p> Setup Authentication Methods"},{"location":"setup_installation/admin/ldap/configure-krb/#step-2-edit-configuration","title":"Step 2: Edit configuration","text":"<p>Finally, click on edit configuration and fill in the attributes.</p> Configure Kerberos <ul> <li>Account status: the status a user will be assigned when logging in for the first time. If a user is assigned a status   different from Activated an admin needs to manually activate each user from the User management.</li> <li>Group mapping: allows you to specify a mapping between LDAP groups and Hopsworks groups. The mapping is a   semicolon separated string in the form <code>Directory Administrators-&gt;HOPS_ADMIN;IT People-&gt; HOPS_USER</code>. Default    is empty. If no mapping is specified, users need to be assigned a role by an admin before they can log in.</li> <li>User id: the id field in LDAP with a string placeholder. Default <code>uid=%s</code>.</li> <li>User given name: the given name field in LDAP. Default <code>givenName</code>.</li> <li>User surname: the surname field in LDAP. Default <code>sn</code>.</li> <li>User email: the email field in LDAP. Default <code>mail</code>.</li> <li>User search filter: the search filter for user. Default <code>uid=%s</code>.</li> <li>Principal search filter: the search filter for principal name. Default <code>krbPrincipalName=%s</code>.</li> <li>Group search filter: the search filter for groups. Default <code>member=%d</code>.</li> <li>Group target: the target to search for groups in the LDAP directory tree. Default <code>cn</code>.</li> <li>Dynamic group target: the target to search for dynamic groups in the LDAP directory tree. Default <code>memberOf</code>.</li> <li>User dn: specify the distinguished name (DN) of the container or base point where the users are stored. Default is   empty.</li> <li>Group dn: specify the DN of the container or base point where the groups are stored. Default is empty.</li> </ul> <p>All defaults are taken from OpenLDAP.</p> <p>Note</p> <p>Group mapping can be disabled by setting <code>ldap_group_mapping_enabled=false</code> in the Configuration UI. When group mapping is disabled an administrator needs to activate each user from the User Management page.</p> <p>If group mapping is disabled then Account status in the Kerberos configuration above should be set to <code>Verified</code>.</p> <p>The login page will now have the choice to use Kerberos for authentication.</p> Log in using Kerberos <p>Note</p> <p>Make sure that you have Kerberos properly configured on your computer and you are logged in. Kerberos support must also be configured on the browser to use Kerberos for authentication.</p>"},{"location":"setup_installation/admin/ldap/configure-ldap/","title":"Configure LDAP/Kerberos","text":""},{"location":"setup_installation/admin/ldap/configure-ldap/#introduction","title":"Introduction","text":"<p>LDAP (Lightweight Directory Access Protocol) is a software protocol for enabling anyone in a network to gain access to resources such as files and devices. This tutorial shows an administrator how to configure LDAP authentication.</p> <p>LDAP need some server configuration before you can enable it from the UI.</p>"},{"location":"setup_installation/admin/ldap/configure-ldap/#prerequisites","title":"Prerequisites","text":"<p>A server configured with LDAP. See Server Configuration for LDAP for  instruction on how to do this.</p>"},{"location":"setup_installation/admin/ldap/configure-ldap/#step-1-enable-ldap","title":"Step 1: Enable LDAP","text":"<p>After configuring the server you can configure Authentication methods by clicking on your name in the top right  corner of the navigation bar and choosing Cluster Settings from the dropdown menu. In the Authentication tab you can find in Cluster Settings, you can enable LDAP by clicking on the LDAP checkbox.</p> <p>If LDAP/Kerberos checkbox is not checked make sure that you configured your application server and enable it by  clicking on the checkbox.</p> Setup Authentication Methods"},{"location":"setup_installation/admin/ldap/configure-ldap/#step-2-edit-configuration","title":"Step 2: Edit configuration","text":"<p>Finally, click on edit configuration and fill in the attributes.</p> Configure LDAP <ul> <li>Account status: the status a user will be assigned when logging in for the first time. If a use is assigned a status    different from Activated an admin needs to manually activate each user from the User management.</li> <li>Group mapping: allows you to specify a mapping between LDAP groups and Hopsworks groups. The mapping is a    semicolon separated string in the form <code>Directory Administrators-&gt;HOPS_ADMIN;IT People-&gt; HOPS_USER</code>. Default   is empty. If no mapping is specified, users need to be assigned a role by an admin before they can log in.</li> <li>User id: the id field in LDAP with a string placeholder. Default <code>uid=%s</code>.</li> <li>User given name: the given name field in LDAP. Default <code>givenName</code>.</li> <li>User surname: the surname field in LDAP. Default <code>sn</code>.</li> <li>User email: the email field in LDAP. Default <code>mail</code>.</li> <li>User search filter: the search filter for user. Default <code>uid=%s</code>.</li> <li>Group search filter: the search filter for groups. Default <code>member=%d</code>.</li> <li>Group target: the target to search for groups in the LDAP directory tree. Default <code>cn</code>.</li> <li>Dynamic group target: the target to search for dynamic groups in the LDAP directory tree. Default <code>memberOf</code>.</li> <li>User dn: specify the distinguished name (DN) of the container or base point where the users are stored. Default is    empty. </li> <li>Group dn: specify the DN of the container or base point where the groups are stored. Default is empty.</li> </ul> <p>All defaults are taken from OpenLDAP.</p> <p>The login page will now have the choice to use LDAP for authentication.</p> Log in using LDAP <p>Note</p> <p>Group mapping can be disabled by setting <code>ldap_group_mapping_enabled=false</code> in the Configuration UI. When group mapping is disabled an administrator needs to activate each user from the User Management page.</p> <p>If group mapping is disabled then Account status in LDAP configuration above should be set to <code>Verified</code>.</p>"},{"location":"setup_installation/admin/ldap/configure-project-mapping/","title":"Configure LDAP/Kerberos group to project mapping","text":""},{"location":"setup_installation/admin/ldap/configure-project-mapping/#introduction","title":"Introduction","text":"<p>A group to project mapping allows you to add members of your LDAP group to a project without having to add each user manually. A mapping is created by specifying a group from LDAP that will be mapped to a project in Hopsworks and what role the members of that group will be assigned in the project.</p> <p>Once a mapping is created, project membership is managed by LDAP group membership. Any change to group membership in LDAP will be reflected  in Hopsworks i.e. removing a user from the LDAP group will also remove them from the project.</p>"},{"location":"setup_installation/admin/ldap/configure-project-mapping/#prerequisites","title":"Prerequisites","text":"<ol> <li>A server configured with LDAP or Kerberos. See Server Configuration for Kerberos and Server Configuration for LDAP for instructions on how to do this.</li> <li>LDAP group mapping sync enabled. This can be done by setting the variable <code>ldap_group_mapping_sync_enabled=true</code>.  See Cluster Configuration on how to change variable values in Hopsworks.</li> </ol>"},{"location":"setup_installation/admin/ldap/configure-project-mapping/#step-1-create-a-mapping","title":"Step 1: Create a mapping","text":"<p>To create a mapping go to Cluster Settings by clicking on your name in the top right corner of the navigation bar and choosing Cluster Settings from the dropdown menu. In the Project mapping tab, you can create a new mapping by clicking on Create new mapping.</p> Project mapping <p>This will take you to the create mapping page shown below</p> Create mapping <p>Here you can choose multiple Remote groups from your LDAP groups and map them to a project from the Project drop down list. You can also choose the Project role users will be assigned when they are added to the project.</p> <p>Finally, click on Create mapping and go back to mappings. You should see the newly created mapping(s) as shown below.</p> Project mappings <p>Note</p> <p>If there are no groups in the Remote group drop down list check if ldap_groups_search_filter is correct by using the value in <code>ldapsearch</code> replacing <code>%c</code> with <code>*</code>, as shown in the example below.</p> <p><code>ldapsearch -LLL -H ldap:/// -b '&lt;base dn&gt;' -D '&lt;user dn&gt;' -w &lt;password&gt; '(&amp;(objectClass=groupOfNames)(cn=*))'</code></p> <p>This should return all the groups in your LDAP. </p> <p>See Cluster Configuration on how to find and update the value of this variable.</p>"},{"location":"setup_installation/admin/ldap/configure-project-mapping/#step-2-edit-a-mapping","title":"Step 2: Edit a mapping","text":"<p>From the list of mappings click on the edit button (). This will make the row editable and allow you to change the remote group, project name, and project role of a mapping.</p> Edit mapping <p>Warning</p> <p>Updating a mapping's remote group or project name will remove all members of the previous group from the project.</p>"},{"location":"setup_installation/admin/ldap/configure-project-mapping/#step-3-delete-a-mapping","title":"Step 3: Delete a mapping","text":"<p>To delete a mapping click on the delete button.</p> <p>Warning</p> <p>Deleting a mapping will remove all members of that group from the project.</p>"},{"location":"setup_installation/admin/ldap/configure-project-mapping/#step-4-configure-sync-interval","title":"Step 4: Configure sync interval","text":"<p>After configuring all the group mappings users will be added to or removed from the projects in the mapping when they login to Hopsworks. It is also possible to synchronize mappings without requiring users to log out. This can be done by setting <code>ldap_group_mapping_sync_interval</code> to an interval greater or equal to 2 minutes. If <code>ldap_group_mapping_sync_interval</code> is set group mapping sync will run periodically based on the interval and add or remove users from projects.</p>"},{"location":"setup_installation/admin/ldap/configure-server/","title":"Configure Server for LDAP and Kerberos","text":""},{"location":"setup_installation/admin/ldap/configure-server/#introduction","title":"Introduction","text":"<p>LDAP and Kerberos integration need some configuration in the helm charts for your  cluster definition used to deploy your Hopsworks cluster. This tutorial shows an administrator how to configure the application server for LDAP and Kerberos integration.</p>"},{"location":"setup_installation/admin/ldap/configure-server/#prerequisites","title":"Prerequisites","text":"<p>An accessible LDAP domain.  A Kerberos Key Distribution Center (KDC) running on the same domain as Hopsworks (Only for Kerberos).</p>"},{"location":"setup_installation/admin/ldap/configure-server/#step-1-server-configuration-for-ldap","title":"Step 1: Server Configuration for LDAP","text":"<p>The LDAP attributes below are used to configure JNDI external resource in Payara. The JNDI resource will communicate  with your LDAP server to perform the authentication.</p> <pre><code>ldap:\n    enabled: true\n    jndilookupname: \"dc=hopsworks,dc=ai\"\n    provider_url: \"ldap://193.10.66.104:1389\"\n    attr_binary_val: \"entryUUID\"\n    security_auth: \"none\"\n    security_principal: \"\"\n    security_credentials: \"\"\n    referral: \"ignore\"\n    additional_props: \"\"\n</code></pre> <ul> <li>jndilookupname: should contain the LDAP domain.</li> <li>attr_binary_val: is the binary unique identifier that will be used in subsequent logins to identify the user.</li> <li>security_auth: how to authenticate to the LDAP server.</li> <li>security_principal: contains the username of the user that will be used to query LDAP.</li> <li>security_credentials: contains the password of the user that will be used to query LDAP.</li> <li>referral: whether to follow or ignore an alternate location in which an LDAP Request may be processed.</li> </ul> <p>An already deployed instance can be configured to connect to LDAP.  Go to the payara admin UI and create a new JNDI external resource. The name of the resource should be ldap/LdapResource. </p> LDAP Resource <p>This can also be achieved by running the below asadmin command.</p> <pre><code>asadmin create-jndi-resource \\\n --restype javax.naming.ldap.LdapContext \\\n --factoryclass com.sun.jndi.ldap.LdapCtxFactory \\\n --jndilookupname dc\\=hopsworks\\,dc\\=ai \\\n --property java.naming.provider.url=ldap\\\\://193\\.10\\.66\\.104\\\\:1389:\\\n hopsworks.ldap.basedn=dc\\\\\\=hopsworks\\,dc\\\\\\=ai:\\\n java.naming.ldap.attributes.binary=entryUUID:\\\n java.naming.security.authentication=simple:\\\n java.naming.security.principal=&lt;username&gt;:\\\n java.naming.security.credentials=&lt;password&gt;:\\\n java.naming.referral=ignore \\\n ldap/LdapResource\n</code></pre>"},{"location":"setup_installation/admin/ldap/configure-server/#step-2-server-configuration-for-kerberos","title":"Step 2: Server Configuration for Kerberos","text":"<p>The Kerberos attributes are used to configure SPNEGO. SPNEGO is used to establish a secure context between the requester and the application server when using Kerberos  authentication.  </p> <pre><code>kerberos:\n    enabled: true\n    krb_conf_path: \"/etc/krb5.conf\"\n    krb_server_key_tab_path: \"/etc/security/keytabs/service.keytab\"\n    krb_server_key_tab_name: \"service.keytab\"\n    spnego_server_conf: '\\nuseKeyTab=true\\nprincipal=\\\"HTTP/server.hopsworks.ai@HOPSWORKS.AI\\\"\\nstoreKey=true\\nisInitiator=false'\nldap:\n    jndilookupname: \"dc=hopsworks,dc=ai\"\n    provider_url: \"ldap://193.10.66.104:1389\"\n    attr_binary_val: \"objectGUID\"\n    security_auth: \"none\"\n    security_principal: \"\"\n    security_credentials: \"\"\n    referral: \"ignore\"\n    additional_props: \"\"\n</code></pre> <p>Both Kerberos and LDAP attributes need to be specified to configure Kerberos. The LDAP attributes are explained above.</p> <ul> <li>krb_conf_path: contains the path to the krb5.conf used by SPNEGO to get information about the default domain and the    location of the Kerberos KDC. The file is copied by the recipe in to /srv/hops/domains/domain1/config.</li> <li>krb_server_key_tab_path: contains the path to the Kerberos service keytab. The keytab is copied by the recipe in to   /srv/hops/domains/domain/config with the name set in the krb_server_key_tab_name attribute.</li> <li>spnego_server_conf: contains the configuration that will be appended to Payara's (application serve used to host hopsworks)    login.conf. In particular, it should contain useKeyTab=true, and the principal name to be used in the authentication phase.    Initiator should be set to false.</li> </ul>"},{"location":"setup_installation/admin/monitoring/export-metrics/","title":"Exporting Hopsworks metrics","text":""},{"location":"setup_installation/admin/monitoring/export-metrics/#introduction","title":"Introduction","text":"<p>Hopsworks services produce metrics which are centrally gathered by Prometheus and visualized in Grafana. Although the system is self-contained, it is possible for another federated Prometheus instance to scrape these metrics or directly push them to another system. This is useful if you have a centralized monitoring system with already configured alerts.</p>"},{"location":"setup_installation/admin/monitoring/export-metrics/#prerequisites","title":"Prerequisites","text":"<p>In order to configure Prometheus to export metrics you need to have the right to change the remote Prometheus configuration.</p>"},{"location":"setup_installation/admin/monitoring/export-metrics/#exporting-metrics","title":"Exporting metrics","text":"<p>Prometheus can be configured to export metrics to another Prometheus instance (cross-service federation) or to a custom service which knows how to handle them.</p>"},{"location":"setup_installation/admin/monitoring/export-metrics/#prometheus-federation","title":"Prometheus federation","text":"<p>Prometheus servers can be federated to scale better or to just clone all metrics (cross-service federation).</p> <p>In the guide below we assume Prometheus A is the service running in Hopsworks and Prometheus B is the server you want to clone metrics to.</p>"},{"location":"setup_installation/admin/monitoring/export-metrics/#step-1","title":"Step 1","text":"<p>Prometheus B needs to be able to connect to TCP port <code>9090</code> of Prometheus A to scrape metrics. If you have any firewall (or Security Group) in place, allow ingress for that port.</p>"},{"location":"setup_installation/admin/monitoring/export-metrics/#step-2","title":"Step 2","text":"<p>The next step is to expose Prometheus A running inside Hopsworks Kubernetes cluster. If Prometheus B has direct access to Prometheus A then you can skip this step.</p> <p>We will create a Kubernetes Service of type LoadBalancer to expose port <code>9090</code></p> <p>Warning</p> <p>If you need to apply custom annotations, then modify the Manifest below The example below assumes Hopsworks is installed at Namespace hopsworks</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Service\nmetadata:\n  name: prometheus-external\n  namespace: hopsworks\n  labels:\n    app: prometheus\nspec:\n  type: LoadBalancer\n  selector:\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/component: server\n  ports:\n    - protocol: TCP\n      port: 9090\n      targetPort: 9090\nEOF\n</code></pre> <p>Then we need to find the External IP address of the newly created Service</p> <pre><code>export NAMESPACE=hopsworks\nkubectl -n $NAMESPACE get svc prometheus-external -ojsonpath='{.status.loadBalancer.ingress[0].ip}'\n</code></pre> <p>Warning</p> <p>It will take a few seconds until an IP address is assigned to the Service</p> <p>We will use this IP address in Step 2</p>"},{"location":"setup_installation/admin/monitoring/export-metrics/#step-2_1","title":"Step 2","text":"<p>Edit the configuration file of Prometheus B server and append the following Job under <code>scrape_configs</code></p> <p>Note</p> <p>Replace IP_ADDRESS with the IP address from Step 1 or the IP address of Prometheus service if it is directly accessible. The snippet below assumes Hopsworks services runs at Namespace hopsworks</p> <pre><code>- job_name: 'federate'\n  scrape_interval: 15s\n\n  honor_labels: true\n  metrics_path: '/federate'\n\n  params:\n    'match[]':\n      - '{namespace=\"hopsworks\"}'\n\n  static_configs:\n    - targets:\n      - 'IP_ADDRESS:9090'\n</code></pre> <p>The configuration above will scrape for services metrics under the hopsworks Namespace. If you want to additionally scrape user application metrics then append <code>'{job=\"pushgateway\"}'</code> to the matchers, for example:</p> <pre><code>  params:\n    'match[]':\n      - '{namespace=\"hopsworks\"}'\n      - '{job=\"pushgateway\"}'\n</code></pre> <p>Depending on the Prometheus setup you might need to restart Prometheus B service to pick up the new configuration. For more details on federation visit Prometheus documentation</p>"},{"location":"setup_installation/admin/monitoring/export-metrics/#custom-service","title":"Custom service","text":"<p>Prometheus can push metrics to another custom resource via HTTP. The custom service is responsible for handling the received metrics. To push metrics with this method we use the <code>remote_write</code> configuration.</p> <p>We will only give a sample configuration as <code>remote_write</code> is extensively documented in Prometheus documentation In the example below we push metrics to a custom service listening on port 9096 which transforms the metrics and forwards them.</p> <p>In order to configure Prometheus to push metrics to a remote HTTP service we need to customize our Helm chart values file with the following snippet after changing the url accordingly. You can also tweak other configuration parameters to your needs.</p> <pre><code>prometheus:\n  prometheus:\n    server:\n      remoteWrite:\n      - url: \"http://localhost:9096\"\n        queue_config:\n          capacity: 10000\n          max_samples_per_send: 5000\n          batch_send_deadline: 60s\n</code></pre> <p>If the section already exists, then append the <code>remoteWrite</code> section.</p> <p>Run <code>helm install</code> or <code>helm upgrade</code> if it's the first time you install Hopsworks or you want to apply the change to an existing cluster respectively.</p>"},{"location":"setup_installation/admin/monitoring/grafana/","title":"Services Dashboards","text":""},{"location":"setup_installation/admin/monitoring/grafana/#introduction","title":"Introduction","text":"<p>The Hopsworks platform is composed of different services. Hopsworks uses Prometheus to collect health and performance metrics from the different services and Grafana to display them to the Hopsworks administrators. </p> <p>In this guide you will learn how to access the Grafana dashboards to monitor the health of the cluster or to troubleshoot performance issues.</p>"},{"location":"setup_installation/admin/monitoring/grafana/#prerequisites","title":"Prerequisites","text":"<p>To access the services dashboards in Grafana, you need to have an administrator account on the Hopsworks cluster.</p>"},{"location":"setup_installation/admin/monitoring/grafana/#step-1-access-grafana","title":"Step 1: Access Grafana","text":"<p>You can access the admin page of your Hopsworks cluster by clicking on your name, in the top right corner, and choosing Cluster Settings from the dropdown menu.</p> <p>You can then navigate to the Monitoring tab. The Monitoring tab gives you access to several of the observability tools that are already deployed to help you manage the health of the cluster.</p> Monitoring tab <p>Click on the Grafana link to open the Grafana web application and navigate through the dashboards.</p>"},{"location":"setup_installation/admin/monitoring/grafana/#step-2-navigate-through-the-dashboards","title":"Step 2: Navigate through the dashboards","text":"<p>In the Grafana web application, you can click on the Home button on the top left corner and navigate through the available dashboards.</p> <p>Dashboards are organized into three folders:</p> <ul> <li> <p>Hops: This folder contains all the dashboards of the Hopsworks services (e.g. the web application, the file system, resource manager) as well as the dashboards of the hosts (e.g. EC2 instances, virtual machines, servers) on which the cluster is deployed.</p> </li> <li> <p>RonDB: This folder contains all the dashboard related to the database. The Database dashboard contains a general overview of the RonDB cluster, while the remaining dashboards focus on specific items (e.g. thread activity, memory management, etc).</p> </li> <li> <p>Kubernetes: If you have integrated Hopsworks with a Kubernetes cluster, this folder contains the dashboards to monitor the health of the Kubernetes cluster.</p> </li> </ul> Grafana view <p>The default dashboards are read only and cannot be edited. Additional dashboards can be created by logging in to Grafana. You can log in into Grafana using the username and password specified in the cluster definition.</p> <p>Warning</p> <p>By default Hopsworks keeps metrics information only for the past 15 days. This means that, by default, you will not be able to access health and performance metrics which are older than 15 days.</p>"},{"location":"setup_installation/admin/monitoring/grafana/#going-further","title":"Going Further","text":"<p>You can read Grafana Documentation to learn how to use it advancedly.</p>"},{"location":"setup_installation/admin/monitoring/services-logs/","title":"Services Logs","text":""},{"location":"setup_installation/admin/monitoring/services-logs/#introduction","title":"Introduction","text":"<p>The Hopsworks platform is composed of different services running on different nodes. Hopsworks uses Filebeat, Logstash and OpenSearch to collect, parse, index and present the logs to the Hopsworks administrators. </p> <p>In this guide you will learn how to access the Hopsworks logs using OpenSearch Dashboards.</p>"},{"location":"setup_installation/admin/monitoring/services-logs/#prerequisites","title":"Prerequisites","text":"<p>To access the services logs, you need to have an administrator account on the Hopsworks cluster.</p>"},{"location":"setup_installation/admin/monitoring/services-logs/#step-1-access-the-logs","title":"Step 1: Access the Logs","text":"<p>You can access the admin page of your Hopsworks cluster by clicking on your name, in the top right corner, and choosing Cluster Settings from the dropdown menu.</p> <p>You can then navigate to the Monitoring tab. The Monitoring tab gives you access to several of the observability tools that are already deployed to help you manage the health of the cluster.</p> Monitoring tab <p>Click on the Service Logs link to open the OpenSearch Dashboards web application and navigate through the logs.</p>"},{"location":"setup_installation/admin/monitoring/services-logs/#step-2-search-the-logs","title":"Step 2: Search the logs","text":"<p>In the OpenSearch dashboard web application you will see by default all the logs generated by all monitored services in the last 15 minutes. </p> <p>You can filter the logs of a specific service by searching for the term <code>service:[service name]</code>. As shown in the picture below, you can search for the namenode logs by querying <code>service:namenode</code>.</p> <p>Currently only the logs of the following services are collected and indexed: Hopsworks web application (called <code>domain1</code> in the log entries), namenodes, resource managers, datanodes, nodemanagers, Kafka brokers, Hive services and RonDB. These are the core component of the platform, additional logs will be added in the future.</p> OpenSearch Dashboards displaying the logs <p>Warning</p> <p>By default, logs are rotated automatically after 7 days. This means that by default, you will not be able to access logs through OpenSearch Dashboards which are older than 7 days. Depending on the service and on the Hopsworks configuration, you can still access the logs by SSH directly into the machines of the cluster.</p>"},{"location":"setup_installation/admin/monitoring/services-logs/#going-further","title":"Going Further","text":"<p>You can read OpenSearch Dashboards Documentation to learn how to use them advancedly.</p>"},{"location":"setup_installation/admin/oauth2/create-azure-client/","title":"Create An Application in Azure Active Directory.","text":""},{"location":"setup_installation/admin/oauth2/create-azure-client/#introduction","title":"Introduction","text":"<p>This example uses Azure Active Directory as the identity provider, but the same can be done with any identity provider  supporting OAuth2 OpenID Connect protocol.</p>"},{"location":"setup_installation/admin/oauth2/create-azure-client/#prerequisites","title":"Prerequisites","text":"<p>Azure account.</p>"},{"location":"setup_installation/admin/oauth2/create-azure-client/#step-1-register-hopsworks-as-an-application-in-your-identity-provider","title":"Step 1: Register Hopsworks as an application in your identity provider","text":"<p>To use OAuth2 in Hopsworks you first need to create and configure an OAuth client in your identity provider. We will take the example of Azure AD for the remaining of this documentation, but equivalent steps can be taken on other identity providers.</p> <p>Navigate to the Microsoft Azure Portal and authenticate. Navigate to Azure Active Directory. Click on App Registrations. Click on New Registration.</p> <p> Create application </p> <p>Enter a name for the client such as hopsworks_oauth_client. Verify the Supported account type is set to Accounts in this organizational directory only. And Click Register.</p> <p> Name application </p>"},{"location":"setup_installation/admin/oauth2/create-azure-client/#step-2-get-the-necessary-fields-for-client-registration","title":"Step 2: Get the necessary fields for client registration","text":"<p>In the Overview section, copy the Application (client) ID field. We will use it in  Identity Provider registration under the name Client id.</p> <p> Copy client ID </p> <p>Click on Endpoints and copy the OpenId Connect metadata document endpoint excluding the .well-known/openid-configuration part.  We will use it in Identity Provider registration under the name Connection URL.</p> <p> Endpoint </p> <p>Click on Certificates &amp; secrets, then Click on New client secret.</p> <p> New client secret </p> <p>Add a description of the secret. Select an expiration period. And, Click Add.</p> <p> Client secret creation </p> <p>Copy the secret. This will be used in Identity Provider registration under the name  Client Secret.</p> <p> Client secret creation </p> <p>Click on Authentication. Then click on Add a platform</p> <p> Add a platform </p> <p>In Configure platforms click on Web.</p> <p> Configure platform: Web </p> <p>Enter the Redirect URI and click on Configure. The redirect URI is HOPSWORKS-URI/callback with HOPSWORKS-URI the URI of your Hopsworks cluster.</p> <p> Configure platform: Redirect </p>"},{"location":"setup_installation/admin/oauth2/create-client/","title":"Register Identity Provider in Hopsworks","text":""},{"location":"setup_installation/admin/oauth2/create-client/#introduction","title":"Introduction","text":"<p>Before registering your identity provider in Hopsworks you need to create a client application in your identity provider and  acquire a client id and a client secret. An example on how to create a client using Okta and Azure Active Directory  identity providers can be found here and here respectively.</p>"},{"location":"setup_installation/admin/oauth2/create-client/#prerequisites","title":"Prerequisites","text":"<p>Acquired a client id and a client secret from your identity provider.</p>"},{"location":"setup_installation/admin/oauth2/create-client/#step-1-register-a-client","title":"Step 1: Register a client","text":"<p>After acquiring the client id and client secret create the client in Hopsworks by enabling OAuth2 and clicking on add another identity provider in the Authentication configuration page. Then set  base uri of your identity provider in Connection URL give a name to your identity provider (the name will be used  in the login page as an alternative login method) and set the client id and client secret in their respective  fields,  as shown in the figure below.</p> Application overview <ul> <li>Connection URL: (provider Uri) is the base uri of the identity provider's API (URI should contain scheme http:// or    https://). </li> </ul> <p>Additional configuration can be set here:</p> <ul> <li>Verify email: if checked only users with verified email address (in the identity provider) can log in to Hopsworks. </li> <li>Code challenge: if your identity provider requires code challenge for authorization request check    the code challenge check box. This will allow you to choose code challenge method that can be either plain or    S256.</li> <li>Logo URL: optionally a logo URL to an image can be added. The logo will be shown on the login page with the name    as shown in the figure below.</li> <li>Claim names for given name, family name, email and group can also be set here. If left empty the default openid claim names will be used.</li> </ul>"},{"location":"setup_installation/admin/oauth2/create-client/#step-2-add-group-mappings","title":"Step 2: Add Group mappings","text":"<p>Optionally you can add a group mapping from your identity provider to Hopsworks groups, by clicking on your name in the  top right corner of the navigation bar and choosing Cluster Settings from the dropdown menu. In the Cluster  Settings Configuration tab search for oauth_group_mapping and click on the edit button.</p> Set Configuration variables <p>Note</p> <p>Setting <code>oauth_group_mapping</code> to <code>ANY_GROUP-&gt;HOPS_USER</code> will assign the role user to any user from any group in  your identity provider when they log into Hopsworks with OAuth for the first time. You can replace ANY_GROUP with  the group of your choice in the identity provider. You can replace HOPS_USER by HOPS_ADMIN if you want the  users of that group to be admins in Hopsworks. You can do several mappings by separating them with a semicolon.</p> <p>Group mapping can be disabled by setting <code>oauth_group_mapping_enabled=false</code> in the Configuration UI. When group mapping is disabled an administrator needs to activate each user from the User Management page.</p> <p>If group mapping is disabled then <code>oauth_account_status</code> in the Configuration UI should be set to 1 (Verified).</p> <p>Users will now see a new button on the login page. The button has the name you set above for Name and will  redirect to your identity provider.</p> Login with OAuth2 <p>Note</p> <p>When creating a client make sure you can access the provider metadata by making a GET request on the well known  endpoint of the provider. The well-known URL, will typically be the Connection URL plus  <code>.well-known/openid-configuration</code>. For the above client it would be  <code>https://dev-86723251.okta.com/.well-known/openid-configuration</code>.</p>"},{"location":"setup_installation/admin/oauth2/create-okta-client/","title":"Create An Application in Okta","text":""},{"location":"setup_installation/admin/oauth2/create-okta-client/#introduction","title":"Introduction","text":"<p>This example uses an Okta development account to create an application that will represent a Hopsworks client in the  identity provider.</p>"},{"location":"setup_installation/admin/oauth2/create-okta-client/#prerequisites","title":"Prerequisites","text":"<p>Okta development account. To create a developer account go to Okta developer.</p>"},{"location":"setup_installation/admin/oauth2/create-okta-client/#step-1-register-hopsworks-as-an-application-in-your-identity-provider","title":"Step 1: Register Hopsworks as an application in your identity provider","text":"<p>After creating a developer account register a client by going to Applications and click on Create App Integration.</p> Okta Applications <p>This will open a popup as shown in the figure below. Select OIDC as Sign-in-method and Web Application as  Application type and click next.  Create new Application </p> <p>Give your application a name and select Client credential as Grant Type. Then add a Sign-in redirect URI  that is your Hopsworks cluster domain name (including the port number if needed) with path /callback, and a Sign-out  redirect URI that is Hopsworks cluster domain name (including the port number if needed) with no path.</p> New Application <p>If you want to limit who can access your Hopsworks cluster select Limit access to selected groups and  select group(s) you want to give access to. Here we will allow everyone in the organization to access the cluster.</p> Group assignment"},{"location":"setup_installation/admin/oauth2/create-okta-client/#group-mapping","title":"Group mapping","text":"<p>You can also create mappings from groups in Okta to groups in Hopsworks. To achieve this you need to configure Okta to  send Groups with user information. To do this go to Applications and select your application name. In the Sign  On tab click edit OpenID Connect ID Token and select Filter for Groups claim type, then for Groups claim  filter add groups as the claim name, select Match Regex from the dropdown and .* (dot star) as Regex to  match all groups. See Group mapping on how to do the mapping in Hopsworks.</p> Group claim"},{"location":"setup_installation/admin/oauth2/create-okta-client/#step-2-get-the-necessary-fields-for-client-registration","title":"Step 2: Get the necessary fields for client registration","text":"<p>After the application is created go back to Applications and click on the application you just created. Use the Okta domain (Connection URL), client id and client secret generated for your app in the  Identity Provider registration in Hopsworks.</p> Application overview <p>Note</p> <p>When copying the domain in the figure above make sure to add the url scheme (http:// or https://) when using it  in the Connection URL in the Identity Provider registration form.</p>"},{"location":"setup_installation/aws/getting_started/","title":"AWS - Getting started","text":"<p>Kubernetes and Helm are used to install &amp; run Hopsworks and the Feature Store in the cloud. They both integrate seamlessly with third-party platforms such as Databricks, SageMaker and KubeFlow. This guide shows how to set up the Hopsworks platform in your organization's AWS account.</p>"},{"location":"setup_installation/aws/getting_started/#prerequisites","title":"Prerequisites","text":"<p>To follow the instruction on this page you will need the following:</p> <ul> <li>Kubernetes Version: Hopsworks can be deployed on EKS clusters running Kubernetes &gt;= 1.27.0.</li> <li>aws-cli to provision the AWS resources </li> <li>eksctl to interact with the AWS APIs and provision the EKS cluster</li> <li>helm to deploy Hopsworks</li> </ul>"},{"location":"setup_installation/aws/getting_started/#ecr-registry","title":"ECR Registry","text":"<p>Hopsworks allows users to customize the images used by Python jobs, Jupyter Notebooks and (Py)Spark applications running in their projects. The images are stored in ECR. Hopsworks needs access to an ECR repository to push the project images.</p>"},{"location":"setup_installation/aws/getting_started/#permissions","title":"Permissions","text":"<ul> <li> <p>The deployment requires cluster admin access to create ClusterRoles, ServiceAccounts, and ClusterRoleBindings.</p> </li> <li> <p>A namespace is required to deploy the Hopsworks stack. If you don\u2019t have permissions to create a namespace, ask your EKS administrator to provision one.</p> </li> </ul>"},{"location":"setup_installation/aws/getting_started/#eks-deployment","title":"EKS Deployment","text":"<p>The following steps describe how to deploy an EKS cluster and related resources so that it\u2019s compatible with Hopsworks.</p>"},{"location":"setup_installation/aws/getting_started/#step-1-aws-eks-setup","title":"Step 1: AWS EKS Setup","text":""},{"location":"setup_installation/aws/getting_started/#step-11-create-s3-bucket","title":"Step 1.1: Create S3 Bucket","text":"<pre><code>aws s3 mb s3://BUCKET_NAME --region REGION --profile PROFILE\n</code></pre>"},{"location":"setup_installation/aws/getting_started/#step-12-create-ecr-repository","title":"Step 1.2: Create ECR Repository","text":"<p>Create the repository to host the projects images. </p> <pre><code>aws --profile PROFILE ecr create-repository --repository-name NAMESPACE/hopsworks-base --region REGION\n</code></pre>"},{"location":"setup_installation/aws/getting_started/#step-13-create-iam-policies","title":"Step 1.3: Create IAM Policies","text":"<pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"hopsworksaiInstanceProfile\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"S3:PutObject\",\n        \"S3:ListBucket\",\n        \"S3:GetObject\",\n        \"S3:DeleteObject\",\n        \"S3:AbortMultipartUpload\",\n        \"S3:ListBucketMultipartUploads\",\n        \"S3:PutLifecycleConfiguration\",\n        \"S3:GetLifecycleConfiguration\",\n        \"S3:PutBucketVersioning\",\n        \"S3:GetBucketVersioning\",\n        \"S3:ListBucketVersions\",\n        \"S3:DeleteObjectVersion\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::BUCKET_NAME/*\",\n        \"arn:aws:s3:::BUCKET_NAME\"\n      ]\n    },\n    {\n      \"Sid\": \"AllowPushandPullImagesToUserRepo\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ecr:GetDownloadUrlForLayer\",\n        \"ecr:BatchGetImage\",\n        \"ecr:CompleteLayerUpload\",\n        \"ecr:UploadLayerPart\",\n        \"ecr:InitiateLayerUpload\",\n        \"ecr:BatchCheckLayerAvailability\",\n        \"ecr:PutImage\",\n        \"ecr:ListImages\",\n        \"ecr:BatchDeleteImage\",\n        \"ecr:GetLifecyclePolicy\",\n        \"ecr:PutLifecyclePolicy\",\n        \"ecr:TagResource\"\n      ],\n      \"Resource\": [\n        \"arn:aws:ecr:REGION:ECR_AWS_ACCOUNT_ID:repository/*/hopsworks-base\"\n      ]\n    }\n  ]\n}\n</code></pre> <pre><code>aws --profile PROFILE iam create-policy --policy-name POLICY_NAME --policy-document file://policy.json\n</code></pre>"},{"location":"setup_installation/aws/getting_started/#step-14-create-eks-cluster-using-eksctl","title":"Step 1.4: Create EKS cluster using eksctl","text":"<p>When creating the cluster using eksctl the following parameters are required in the cluster configuration YAML file (eksctl.yaml):</p> <ul> <li> <p>amiFamily should either be AmazonLinux2023 or Ubuntu2404</p> </li> <li> <p>Instance type should be Intel based or AMD (i.e not ARM)</p> </li> <li> <p>The following policies are required: IAM policies - eksctl</p> </li> </ul> <pre><code>- arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy\n- arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy\n- arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly\n</code></pre> <p>The following is required if you are using the EKS AWS Load Balancer Controller to grant permissions to the controller to provision the necessary load balancers Welcome: AWS Load Balancer Controller</p> <pre><code>      withAddonPolicies:\n        awsLoadBalancerController: true \n</code></pre> <p>You need to update the CLUSTER NAME and the POLICY ARN generated above</p> <pre><code>apiVersion: eksctl.io/v1alpha5\nkind: ClusterConfig\n\nmetadata:\n  name: CLUSTER_NAME\n  region: REGION\n  version: \"1.29\" \n\niam:\n  withOIDC: true\n\nmanagedNodeGroups:\n  - name: ng-1\n    amiFamily: AmazonLinux2023\n    instanceType: m6i.2xlarge\n    minSize: 1\n    maxSize: 4\n    desiredCapacity: 4\n    volumeSize: 100\n    ssh:\n      allow: true # will use ~/.ssh/id_rsa.pub as the default ssh key\n    iam:\n      attachPolicyARNs:\n        - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy\n        - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy\n        - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly\n        - arn:aws:iam::827555229956:policy/POLICYNAME\n      withAddonPolicies:\n        awsLoadBalancerController: true\naddons:\n  - name: aws-ebs-csi-driver\n    wellKnownPolicies:      # add IAM and service account\n      ebsCSIController: true\n</code></pre> <p>You can create the EKS cluster using the following eksctl command:</p> <pre><code>eksctl create cluster -f eksctl.yaml --profile PROFILE\n</code></pre> <p>Once the creation process is completed, you should be able to access the cluster using the kubectl CLI tool:</p> <pre><code>kubectl get nodes\n</code></pre> <p>You should see the list of nodes provisioned for the cluster.</p>"},{"location":"setup_installation/aws/getting_started/#step-14-install-the-aws-loadbalancer-addon","title":"Step 1.4: Install the AWS LoadBalancer Addon","text":"<p>For Hopsworks to provision the necessary network and application load balancers, we need to install the AWS LoadBalancer plugin (See AWS Documentation ) <pre><code>helm repo add eks https://aws.github.io/eks-charts\nhelm repo update eks\nhelm install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system --set clusterName=CLUSTER_NAME\n</code></pre></p>"},{"location":"setup_installation/aws/getting_started/#step-15-optional-create-gp3-storage-class","title":"Step 1.5: (Optional) Create GP3 Storage Class","text":"<p>By default EKS comes with GP2 as storage class. GP3 is more cost effective, we can use it with Hopsworks by creating the storage class</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: ebs-gp3\nprovisioner: ebs.csi.aws.com\nparameters:\n  csi.storage.k8s.io/fstype: xfs\n  type: gp3\nreclaimPolicy: Delete\nvolumeBindingMode: WaitForFirstConsumer\nallowVolumeExpansion: true\nEOF\n</code></pre>"},{"location":"setup_installation/aws/getting_started/#step-2-hopsworks-deployment","title":"Step 2: Hopsworks Deployment","text":"<p>This section describes the steps required to deploy the Hopsworks stack using Helm.</p>"},{"location":"setup_installation/aws/getting_started/#step-21-add-the-hopsworks-helm-repository","title":"Step 2.1: Add the Hopsworks Helm repository","text":"<ul> <li>Configure Repo</li> </ul> <p>To obtain access to the Hopsworks helm chart repository, please obtain  an evaluation/startup licence here.</p> <p>Once you have the helm chart repository URL, replace the environment variable $HOPSWORKS_REPO in the following command with this URL.</p> <pre><code>helm repo add hopsworks $HOPSWORKS_REPO\nhelm repo update hopsworks\n</code></pre> <ul> <li>Create Hopsworks namespace </li> </ul> <pre><code>kubectl create namespace hopsworks\n</code></pre> <ul> <li>Update values.aws.yml</li> </ul> <pre><code>global:\n  _hopsworks:\n    storageClassName: ebs-gp3\n    cloudProvider: \"AWS\"\n    managedDockerRegistery:\n      enabled: true\n      domain: \"ECR_AWS_ACCOUNT_ID.dkr.ecr.REGION.amazonaws.com\"\n      namespace: \"NAMESPACE\"\n      credHelper:\n        enabled: true\n        secretName: &amp;awsregcred \"awsregcred\"\n    minio:\n      hopsfs:\n        enabled: false\n    externalLoadBalancers:\n      enabled: true\n      class: null\n      annotations:\n        service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing\n\nhopsworks:\n  variables:\n    docker_operations_managed_docker_secrets: *awsregcred\n    docker_operations_image_pull_secrets: \"regcred\"\n  dockerRegistry:\n    preset:\n      usePullPush: false\n      secrets:\n        - \"regcred\"\n        - *awsregcred\n  service:\n    worker:\n      external:\n        http:\n          type: NodePort\n  ingress:\n    enabled: true\n    ingressClassName: alb\n    annotations:\n      alb.ingress.kubernetes.io/scheme: internet-facing\n\nhopsfs:\n  objectStorage:\n    enabled: true\n    provider: \"S3\"\n    s3:\n      bucket: \n        name: \"BUCKET_NAME\"\n      region: \"REGION\"\n\nconsul:\n  consul:\n    server:\n      storageClass: ebs-gp3\n</code></pre> <ul> <li>Run the Helm install </li> </ul> <pre><code>helm install hopsworks hopsworks/hopsworks --namespace hopsworks --values values.aws.yaml --timeout=600s\n</code></pre> <p>Using the kubectl CLI tool, you can track the deployment process. You can use the command below to track which pods are running and which ones are in the process of being provisioned. You can also use the command below to detect any failure.</p> <pre><code>kubectl -n hopsworks get pods\n</code></pre>"},{"location":"setup_installation/aws/getting_started/#step-3-resources-created","title":"Step 3: Resources Created","text":"<p>Using the Helm chart and the values files the following resources are created:</p> <p>Load Balancers: <pre><code>    externalLoadBalancers:\n      enabled: true\n      class: null\n      annotations:\n        service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing\n</code></pre></p> <p>Enabling the external load balancer in the values.yml file provisions the following load balancers for the following services:</p> <ul> <li> <p>arrowflight : This load balancer is used to send queries from external clients to the Hopsworks Query Service</p> </li> <li> <p>kafka : This load balancer is used to send data to the Apache Kafka brokers for ingestion to the online feature store.</p> </li> <li> <p>rdrs: This load balancer is used to query online feature store data using the REST APIs</p> </li> <li> <p>mysql: This load balancer is used to query online feature store data using the MySQL APIs</p> </li> <li> <p>opensearch : This load balancer is used to query the Hopsworks vector database</p> </li> </ul> <p>On EKS using the AWS Load Balancers, the AWS controller deployed above will be responsible to provision the necessary load balancers. You can configure the load balancers using the annotations documented in the AWS Load Balancer controller guide</p> <p>You can enable/disable individual load balancers provisioning using the following values in the values.yml file:</p> <ul> <li> <p>kafka.externalLoadBalancer.enabled</p> </li> <li> <p>opensearch.externalLoadBalancer.enabled</p> </li> <li> <p>rdrs.externalLoadBalancer.enabled</p> </li> <li> <p>mysql.externalLoadBalancer.enabled</p> </li> </ul> <p>Other load balancer providers are also supported by providing the appropriate controller, class and annotations.</p> <p>Ingress:</p> <pre><code>  ingress:\n    enabled: true\n    ingressClassName: alb\n    annotations:\n      alb.ingress.kubernetes.io/scheme: internet-facing\n</code></pre> <p>Hopsworks UI and REST interface is available outside the K8s cluster using an Ingress. On AWS this is implemented by provisioning an application load balancer using the AWS load balancer controller. As per the load balancer above, the controller checks for the following annotations: Annotations - AWS Load Balancer Controller</p> <p>HTTPS is required to access the Hopsworks UI, therefore you need to add the following annotation:</p> <pre><code>alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-west-2:xxxxx:certificate/xxxxxxx\n</code></pre> <p>To configure the TLS certificate the Application Load Balancer should use to terminate the connection. The certificate should be available in the AWS Certificate Manager</p> <p>Cluster Roles and Cluster Role Bindings:</p> <p>By default a set of cluster roles are provisioned, if you don\u2019t have permissions to provision cluster roles or cluster role bindings, you should reach out to your K8s administrator. You should then provide the appropriate resource names as value in the values.yml file.</p>"},{"location":"setup_installation/aws/getting_started/#step-4-next-steps","title":"Step 4: Next steps","text":"<p>Check out our other guides for how to get started with Hopsworks and the Feature Store:</p> <ul> <li>Get started with the Hopsworks Feature Store</li> <li>Follow one of our tutorials</li> <li>Follow one of our Guide</li> </ul>"},{"location":"setup_installation/azure/getting_started/","title":"Azure - Getting started with AKS","text":"<p>Kubernetes and Helm are used to install &amp; run Hopsworks and the Feature Store in the cloud. They both integrate seamlessly with third-party platforms such as Databricks, SageMaker and KubeFlow. This guide shows how to set up the Hopsworks platform in your organization's Azure account.</p>"},{"location":"setup_installation/azure/getting_started/#prerequisites","title":"Prerequisites","text":"<p>To follow the instruction on this page you will need the following:</p> <ul> <li>Kubernetes Version: Hopsworks can be deployed on AKS clusters running Kubernetes &gt;= 1.27.0.</li> <li>An Azure resource group in which the Hopsworks cluster will be deployed. </li> <li>The azure CLI installed and logged in.</li> <li>kubectl (to manage the AKS cluster)</li> <li>helm (to deploy Hopsworks)</li> </ul>"},{"location":"setup_installation/azure/getting_started/#permissions","title":"Permissions","text":"<p>The deployment requires cluster admin access to create ClusterRoles, ServiceAccounts, and ClusterRoleBindings in AKS.</p> <p>A namespace is also required for deploying the Hopsworks stack. If you don\u2019t have permissions to create a namespace, ask your AKS administrator to provision one for you.</p> <p>To run all the commands on this page the user needs to have at least the following permissions on the Azure resource group:</p> <p>You will also need to have a role such as Application Administrator on the Azure Active Directory to be able to create the hopsworks.ai service principal.</p>"},{"location":"setup_installation/azure/getting_started/#step-1-azure-kubernetes-service-aks-setup","title":"Step 1: Azure Kubernetes Service (AKS) Setup","text":""},{"location":"setup_installation/azure/getting_started/#step-11-create-an-azure-blob-storage-account","title":"Step 1.1: Create an Azure Blob Storage Account","text":"<p>Create a storage account to host project data. Ensure that the storage account is in the same region as the AKS cluster for performance and cost reasons:</p> <pre><code>az storage account create --name $STORAGE_ACCOUNT_NAME --resource-group $RESOURCE_GROUP --location $REGION\n</code></pre> <p>Also, create the corresponding container:</p> <pre><code>az storage container create --account-name $STORAGE_ACCOUNT_NAME --name $CONTAINER_NAME\n</code></pre>"},{"location":"setup_installation/azure/getting_started/#step-12-create-an-azure-container-registry-acr","title":"Step 1.2: Create an Azure Container Registry (ACR)","text":"<p>Create an ACR to store the images used by Hopsworks:</p> <pre><code>az acr create --resource-group $RESOURCE_GROUP --name $CONTAINER_REGISTRY_NAME --sku Basic --location $REGION\n\nexport ACR_ID=`az acr show --name $CONTAINER_REGISTRY_NAME --resource-group $RESOURCE_GROUP --query \"id\" --output tsv`\n</code></pre>"},{"location":"setup_installation/azure/getting_started/#step-13-create-a-user-assigned-managed-identity","title":"Step 1.3: Create a User-Assigned Managed Identity","text":"<p>Create a user-assigned managed identity to grant AKS access to the storage account and container registry:</p> <pre><code>az identity create --name $UA_IDENTITY_NAME --resource-group $RESOURCE_GROUP\n\nexport UA_IDENTITY_PRINCIPAL_ID=`az identity show --name $UA_IDENTITY_NAME --resource-group $RESOURCE_GROUP --query principalId --output tsv`\nexport UA_IDENTITY_CLIENT_ID=`az identity show --name $UA_IDENTITY_NAME --resource-group $RESOURCE_GROUP --query clientId --output tsv`\nexport UA_IDENTITY_RESOURCE_ID=`az identity show --name $UA_IDENTITY_NAME --resource-group $RESOURCE_GROUP --query id --output tsv`\n</code></pre>"},{"location":"setup_installation/azure/getting_started/#step-14-grant-permissions-to-the-user-assigned-managed-identity","title":"Step 1.4: Grant permissions to the User-Assigned Managed Identity","text":"<p>Create a custom role definition with the minimum permissions needed to read and write to the storage account:</p> <pre><code>export STORAGE_ID=`az storage account show --name $STORAGE_ACCOUNT_NAME --resource-group $RESOURCE_GROUP --query \"id\" --output tsv`\n\naz role definition create --role-definition '{\n  \"Name\": \"hopsfs-storage-permissions\",\n  \"IsCustom\": true,\n  \"Description\": \"Allow HopsFS to access the storage container\",\n  \"Actions\": [\n    \"Microsoft.Storage/storageAccounts/blobServices/containers/write\",\n    \"Microsoft.Storage/storageAccounts/blobServices/containers/read\",\n    \"Microsoft.Storage/storageAccounts/blobServices/write\",\n    \"Microsoft.Storage/storageAccounts/blobServices/read\",\n    \"Microsoft.Storage/storageAccounts/listKeys/action\"\n  ],\n  \"NotActions\": [],\n  \"DataActions\": [\n    \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/delete\",\n    \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/read\",\n    \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/move/action\",\n    \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/write\"\n  ],\n  \"AssignableScopes\": [\n    \"'$STORAGE_ID'\"\n  ]\n}'\n\naz role assignment create --role hopsfs-storage-permissions --assignee-object-id $UA_IDENTITY_PRINCIPAL_ID --assignee-principal-type ServicePrincipal --scope $STORAGE_ID\n</code></pre>"},{"location":"setup_installation/azure/getting_started/#step-15-create-service-principal-for-hopsworks-services","title":"Step 1.5: Create Service Principal for Hopsworks services","text":"<p>Create a service principal to grant Hopsworks applications with access to the container registry. For example, Hopsworks uses this service principal to push new Python environments created via the Hopsworks UI.</p> <pre><code>export SP_PASSWORD=`az ad sp create-for-rbac --name $SP_NAME --scopes $ACR_ID --role AcrPush --years 1 --query \"password\" --output tsv`\nexport SP_USER_NAME=`az ad sp list --display-name $SP_NAME --query \"[].appId\" --output tsv`\nexport SP_RESOURCE_ID=`az ad sp list --display-name $SP_NAME --query \"[].id\" --output tsv`\n\naz role assignment create --role AcrDelete --assignee-object-id $SP_RESOURCE_ID --assignee-principal-type ServicePrincipal --scope $ACR_ID\n</code></pre>"},{"location":"setup_installation/azure/getting_started/#step-16-create-an-aks-kubernetes-cluster","title":"Step 1.6: Create an AKS Kubernetes Cluster","text":"<p>Provision an AKS cluster with a number of nodes:</p> <pre><code>az aks create --resource-group $RESOURCE_GROUP --name $KUBERNETES_CLUSTER_NAME --network-plugin azure \\\n    --enable-cluster-autoscaler --min-count 1 --max-count 4 --node-count 3 --node-vm-size Standard_D8_v4 \\\n    --attach-acr $CONTAINER_REGISTRY_NAME \\\n    --assign-identity $UA_IDENTITY_RESOURCE_ID --assign-kubelet-identity $UA_IDENTITY_RESOURCE_ID \\\n    --enable-managed-identity --generate-ssh-keys\n</code></pre>"},{"location":"setup_installation/azure/getting_started/#step-2-configure-kubectl","title":"Step 2: Configure kubectl","text":"<pre><code>az aks get-credentials --resource-group $RESOURCE_GROUP --name $KUBERNETES_CLUSTER_NAME --file ~/my-aks-kubeconfig.yaml\nexport KUBECONFIG=~/my-aks-kubeconfig.yaml\nkubectl config current-context\n</code></pre>"},{"location":"setup_installation/azure/getting_started/#step-3-create-secret-for-the-service-principal","title":"Step 3: Create Secret for the Service Principal","text":""},{"location":"setup_installation/azure/getting_started/#step-31-create-hopsworks-namespace","title":"Step 3.1: Create Hopsworks namespace","text":"<pre><code>kubectl create namespace hopsworks\n</code></pre>"},{"location":"setup_installation/azure/getting_started/#step-32-create-secret","title":"Step 3.2: Create secret","text":"<pre><code>kubectl create secret docker-registry azregcred \\\n    --namespace hopsworks \\\n    --docker-server=$CONTAINER_REGISTRY_NAME.azurecr.io \\\n    --docker-username=$SP_USER_NAME \\\n    --docker-password=$SP_PASSWORD\n</code></pre>"},{"location":"setup_installation/azure/getting_started/#step-4-setup-hopsworks-for-deployment","title":"Step 4: Setup Hopsworks for Deployment","text":""},{"location":"setup_installation/azure/getting_started/#step-41-add-the-hopsworks-helm-repository","title":"Step 4.1: Add the Hopsworks Helm repository","text":"<p>To obtain access to the Hopsworks helm chart repository, please obtain  an evaluation/startup licence here.</p> <p>Once you have the helm chart repository URL, replace the environment variable $HOPSWORKS_REPO in the following command with this URL.</p> <pre><code>helm repo add hopsworks $HOPSWORKS_REPO\nhelm repo update hopsworks\n</code></pre>"},{"location":"setup_installation/azure/getting_started/#step-42-create-helm-values-file","title":"Step 4.2: Create helm values file","text":"<p>Below is a simplifield values.azure.yaml file to get started which can be updated for improved performance and further customisation.</p> <pre><code>global:\n  _hopsworks:\n    storageClassName: null\n    cloudProvider: \"AZURE\"\n    managedDockerRegistery:\n      enabled: true\n      domain: \"CONTAINER_REGISTRY_NAME.azurecr.io\"\n      namespace: \"hopsworks\"\n      credHelper:\n        enabled: false\n        secretName: \"\"\n\n    minio:\n      enabled: false\n\nhopsworks:\n  variables:\n    docker_operations_managed_docker_secrets: &amp;azregcred \"azregcred\"\n    docker_operations_image_pull_secrets: *azregcred\n  dockerRegistry:\n    preset:\n      usePullPush: false\n      secrets:\n        - *azregcred\n\nhopsfs:\n  objectStorage:\n    enabled: true\n    provider: \"AZURE\"\n    azure:\n      storage:\n        account: \"STORAGE_ACCOUNT_NAME\"\n        container: \"STORAGE_ACCOUNT_CONTAINER_NAME\"\n        identityClientId: \"UA_IDENTITY_CLIENT_ID\"\n</code></pre>"},{"location":"setup_installation/azure/getting_started/#step-5-deploy-hopsworks","title":"Step 5: Deploy Hopsworks","text":"<p>Deploy Hopsworks in the created namespace.</p> <pre><code>helm install hopsworks hopsworks/hopsworks --namespace hopsworks --values values.azure.yaml --timeout=600s\n</code></pre> <p>Check that Hopsworks is installing on your provisioned AKS cluster.</p> <pre><code>kubectl get pods --namespace=hopsworks\n\nkubectl get svc -n hopsworks -o wide\n</code></pre> <p>Upon completion (circa 20 minutes), setup a load balancer to access Hopsworks:</p> <pre><code>kubectl expose deployment hopsworks --type=LoadBalancer --name=hopsworks-service --namespace &lt;namespace&gt;\n</code></pre>"},{"location":"setup_installation/azure/getting_started/#step-6-next-steps","title":"Step 6: Next steps","text":"<p>Check out our other guides for how to get started with Hopsworks and the Feature Store:</p> <ul> <li>Get started with the Hopsworks Feature Store</li> <li>Follow one of our tutorials</li> <li>Follow one of our Guide</li> </ul>"},{"location":"setup_installation/common/arrow_flight_duckdb/","title":"ArrowFlight Server with DuckDB","text":"<p>By default, Hopsworks uses big data technologies (Spark or Hive) to create training data and read data for Python clients. This is great for large datasets, but for small or moderately sized datasets (think of the size of data that would fit in a Pandas DataFrame in your local Python environment), the overhead of starting a Spark or Hive job and doing distributed data processing can be significant.</p> <p>ArrowFlight Server with DuckDB significantly reduces the time that Python clients need to read feature groups and batch inference data from the Feature Store, as well as creating moderately-sized in-memory training datasets.</p> <p>When the service is enabled, clients will automatically use it for the following operations:</p> <ul> <li>reading Feature Groups</li> <li>reading Queries</li> <li>reading Training Datasets</li> <li>creating In-Memory Training Datasets</li> <li>reading Batch Inference Data</li> </ul> <p>For larger datasets, clients can still make use of the Spark/Hive backend by explicitly setting <code>read_options={\"use_hive\": True}</code>.</p>"},{"location":"setup_installation/common/arrow_flight_duckdb/#service-configuration","title":"Service configuration","text":"<p>Note</p> <p>Supported only on AWS at the moment.</p> <p>The ArrowFlight Server is co-located with RonDB in the Hopsworks cluster. If the ArrowFlight Server is activated, RonDB and ArrowFlight Server can each use up to 50% of the available resources on the node, so they can co-exist without impacting each other. Just like RonDB, the ArrowFlight Server can be replicated across multiple nodes to serve more clients at lower latency. To guarantee high performance, each individual ArrowFlight Server instance processes client requests sequentially. Requests will be queued for up to 10 minutes before they are rejected.</p> <p> Activate ArrowFlight Server with DuckDB on a RonDB cluster </p> <p>To deploy ArrowFlight Server on a cluster:</p> <ol> <li>Select \"RonDB cluster\"</li> <li>Select an instance type with at least 16GB of memory and 4 cores. (*)</li> <li>Tick the checkbox <code>Enable ArrowFlight Server</code>.</li> </ol> <p>(*) The service should have at least the 2x the amount of memory available that a typical Python client would have.   Because RonDB and ArrowFlight Server share the same node we recommend selecting an instance type with at least 4x the   client memory. For example, if the service serves Python clients with typically 4GB of memory,   an instance with at least 16GB of memory should be selected.   An instance with 16GB of memory will be able to read feature groups and training datasets of up to 10-100M rows,   depending on the number of columns and size of the features (~2GB in parquet). The same instance will be able to create   point-in-time correct training datasets with 1-10M rows, also depending on the number and the size of the features.   Larger instances are able to handle larger datasets. The numbers scale roughly linearly with the instance size.</p>"},{"location":"setup_installation/gcp/getting_started/","title":"GCP - Getting started with GKE","text":"<p>Kubernetes and Helm are used to install &amp; run Hopsworks and the Feature Store in the cloud. They both integrate seamlessly with third-party platforms such as Databricks, SageMaker and KubeFlow. This guide shows how to set up the Hopsworks platform in your organization's Google Cloud Platform's (GCP) account.</p>"},{"location":"setup_installation/gcp/getting_started/#prerequisites","title":"Prerequisites","text":"<p>To follow the instruction on this page you will need the following:</p> <ul> <li>Kubernetes Version: Hopsworks can be deployed on GKE clusters running Kubernetes &gt;= 1.27.0.</li> <li>gcloud CLI to provision the GCP resources </li> <li>gke-gcloud-auth-plugin to manage authentication with the GKE cluster</li> <li>helm to deploy Hopsworks</li> </ul>"},{"location":"setup_installation/gcp/getting_started/#permissions","title":"Permissions","text":"<ul> <li> <p>The deployment requires cluster admin access to create ClusterRoles, ServiceAccounts, and ClusterRoleBindings.</p> </li> <li> <p>A namespace is required to deploy the Hopsworks stack. If you don\u2019t have permissions to create a namespace, ask your GKE administrator to provision one.</p> </li> </ul>"},{"location":"setup_installation/gcp/getting_started/#step-1-gcp-gke-setup","title":"Step 1: GCP GKE Setup","text":""},{"location":"setup_installation/gcp/getting_started/#step-11-create-a-google-cloud-storage-gcs-bucket","title":"Step 1.1: Create a Google Cloud Storage (GCS) bucket","text":"<p>Create a bucket to store project data. Ensure the bucket is in the same region as your GKE cluster for performance and cost optimization.</p> <pre><code>gsutil mb -l $region gs://$bucket_name\n</code></pre>"},{"location":"setup_installation/gcp/getting_started/#step-12-create-service-account","title":"Step 1.2: Create Service Account","text":"<p>Create a file named <code>hopsworksai_role.yaml</code> with the following content:</p> <pre><code>title: Hopsworks AI Instances\ndescription: Role that allows Hopsworks AI Instances to access resources\nstage: GA\nincludedPermissions:\n- storage.buckets.get\n- storage.buckets.update\n- storage.multipartUploads.abort\n- storage.multipartUploads.create\n- storage.multipartUploads.list\n- storage.multipartUploads.listParts\n- storage.objects.create\n- storage.objects.delete\n- storage.objects.get\n- storage.objects.list\n- storage.objects.update\n- artifactregistry.repositories.create\n- artifactregistry.repositories.get\n- artifactregistry.repositories.uploadArtifacts\n- artifactregistry.repositories.downloadArtifacts\n- artifactregistry.tags.list\n- artifactregistry.tags.delete\n</code></pre> <p>Execute the following gcloud command to create a custom role from the file. Replace $PROJECT_ID with your GCP project id:</p> <pre><code>gcloud iam roles create hopsworksai_instances \\\n  --project=$PROJECT_ID \\\n  --file=hopsworksai_role.yaml\n</code></pre> <p>Execute the following gcloud command to create a service account for Hopsworks AI instances. Replace $PROJECT_ID with your GCP project id:</p> <pre><code>gcloud iam service-accounts create hopsworksai_instances \\\n  --project=$PROJECT_ID \\\n  --description=\"Service account for Hopsworks AI instances\" \\\n  --display-name=\"Hopsworks AI instances\"\n</code></pre> <p>Execute the following gcloud command to bind the custom role to the service account. Replace all occurrences $PROJECT_ID with your GCP project id:</p> <pre><code>gcloud projects add-iam-policy-binding $PROJECT_ID \\\n  --member=\"serviceAccount:hopsworks-ai-instances@$PROJECT_ID.iam.gserviceaccount.com\" \\\n  --role=\"projects/$PROJECT_ID/roles/hopsworksai_instances\"\n</code></pre>"},{"location":"setup_installation/gcp/getting_started/#step-13-create-a-gke-cluster","title":"Step 1.3: Create a GKE Cluster","text":"<p><pre><code>gcloud container clusters create &lt;cluster-name&gt; \\\n  --zone &lt;zone&gt; \\\n  --machine-type n2-standard-8 \\\n  --num-nodes 1 \\\n  --enable-ip-alias \\\n  --service-account my-service-account@my-project.iam.gserviceaccount.com\n</code></pre> Once the creation process is completed, you should be able to access the cluster using the kubectl CLI tool:</p> <pre><code>kubectl get nodes\n</code></pre>"},{"location":"setup_installation/gcp/getting_started/#step-14-create-gcr-repository","title":"Step 1.4: Create GCR repository","text":"<p>Hopsworks allows users to customize images for Python jobs, Jupyter Notebooks, and (Py)Spark applications. These images should be stored in Google Container Registry (GCR). The GKE cluster needs access to a GCR repository to push project images.</p> <p>Enable Artifact Registry and create a GCR repository to store images:</p> <pre><code>gcloud artifacts repositories create &lt;repo-name&gt; \\\n  --repository-format=docker \\\n  --location=&lt;region&gt;\n</code></pre>"},{"location":"setup_installation/gcp/getting_started/#step-3-setup-hopsworks-for-deployment","title":"Step 3: Setup Hopsworks for Deployment","text":""},{"location":"setup_installation/gcp/getting_started/#step-31-add-the-hopsworks-helm-repository","title":"Step 3.1: Add the Hopsworks Helm repository","text":"<p>To obtain access to the Hopsworks helm chart repository, please obtain  an evaluation/startup licence here.</p> <p>Once you have the helm chart repository URL, replace the environment variable $HOPSWORKS_REPO in the following command with this URL.</p> <pre><code>helm repo add hopsworks $HOPSWORKS_REPO\nhelm repo update hopsworks\n</code></pre>"},{"location":"setup_installation/gcp/getting_started/#step-32-create-hopsworks-namespace","title":"Step 3.2: Create Hopsworks namespace","text":"<pre><code>kubectl create namespace hopsworks\n</code></pre>"},{"location":"setup_installation/gcp/getting_started/#step-33-create-helm-values-file","title":"Step 3.3: Create helm values file","text":"<p>Below is a simplifield values.gcp.yaml file to get started which can be updated for improved performance and further customisation.</p> <pre><code>global:\n  _hopsworks:\n    storageClassName: null\n    cloudProvider: \"GCP\"\n    managedDockerRegistery:\n      enabled: true\n      domain: \"europe-north1-docker.pkg.dev\"\n      namespace: \"PROJECT_ID/hopsworks\"\n      credHelper:\n        enabled: true\n        secretName: &amp;gcpregcred \"gcpregcred\"\n\n    managedObjectStorage:\n      enabled: true\n      s3:\n        bucket: \n          name: &amp;bucket \"hopsworks\"\n        region: &amp;region \"europe-north1\"\n        endpoint: &amp;gcpendpoint \"https://storage.cloud.google.com\"\n        secret:\n          name: &amp;gcpcredentials \"gcp-credentials\"\n          acess_key_id: &amp;gcpaccesskey \"access-key-id\"\n          secret_key_id: &amp;gcpsecretkey \"secret-access-key\"\n    minio:\n      enabled: false\n</code></pre>"},{"location":"setup_installation/gcp/getting_started/#step-4-deploy-hopsworks","title":"Step 4: Deploy Hopsworks","text":"<p>Deploy Hopsworks in the created namespace.</p> <pre><code>helm install hopsworks hopsworks/hopsworks \\\n  --namespace hopsworks \\\n  --values values.gcp.yaml \\\n  --timeout=600s\n</code></pre> <p>Check that Hopsworks is installing on your provisioned AKS cluster.</p> <pre><code>kubectl get pods --namespace=hopsworks\n\nkubectl get svc -n hopsworks -o wide\n</code></pre> <p>Upon completion (circa 20 minutes), setup a load balancer to access Hopsworks:</p> <pre><code>kubectl expose deployment hopsworks --type=LoadBalancer --name=hopsworks-service --namespace &lt;namespace&gt;\n</code></pre>"},{"location":"setup_installation/gcp/getting_started/#step-5-next-steps","title":"Step 5: Next steps","text":"<p>Check out our other guides for how to get started with Hopsworks and the Feature Store:</p> <ul> <li>Get started with the Hopsworks Feature Store</li> <li>Follow one of our tutorials</li> <li>Follow one of our Guide</li> </ul>"},{"location":"setup_installation/on_prem/contact_hopsworks/","title":"Hopsworks On-Premise Installation","text":"<p>It is possible to use Hopsworks on-premises, which means that companies can run their machine learning workloads on their own hardware and infrastructure, rather than relying on a cloud provider. This can provide greater flexibility, control, and cost savings, as well as enabling companies to meet specific compliance and security requirements.</p> <p>Working on-premises with Hopsworks typically involves collaboration with the Hopsworks engineering teams, as each infrastructure is unique and requires a tailored approach to deployment and configuration. The process begins with an assessment of the company's existing infrastructure and requirements, including network topology, security policies, and hardware specifications.</p> <p>For further details about on-premise installations; contact us.</p>"},{"location":"setup_installation/on_prem/external_kafka_cluster/","title":"External Kafka cluster","text":"<p>Hopsworks uses Apache Kafka to ingest data to the feature store. Streaming applications and external clients send data to the Kafka cluster for ingestion to the online and offline feature store. By default, Hopsworks comes with an embedded Kafka cluster managed by Hopsworks itself, however, users can configure Hopsworks to leverage an existing external cluster.  This guide will cover how to configure an Hopsworks cluster to leverage an external Kafka cluster.</p>"},{"location":"setup_installation/on_prem/external_kafka_cluster/#configure-the-external-kafka-cluster-integration","title":"Configure the external Kafka cluster integration","text":"<p>To enable the integration with an external Kafka cluster, you should set the <code>enable_bring_your_own_kafka</code> configuration option to <code>true</code>. This can also be achieved in the cluster definition by setting the following attribute:</p> <pre><code>hopsworks:\n  enable_bring_your_own_kafka: \"true\"\n</code></pre>"},{"location":"setup_installation/on_prem/external_kafka_cluster/#online-feature-store-service-configuration","title":"Online Feature Store service configuration","text":"<p>In addition to the configuration changes above, you should also configure the Online Feature Store service (OnlineFS in short) to connect to the external Kafka cluster. This can be achieved by provisioning the necessary credentials for OnlineFS to subscribe and consume messages from Kafka topics used by the Hopsworks feature store.</p> <p>OnlineFs can be configured to use these credentials by adding the following configurations to the cluster definition used to deploy Hopsworks:</p> <pre><code>  onlinefs:\n    config_dir: \"/home/ubuntu/cluster-definitions/byok\"\n    kafka_cosumers:\n      topic_list: \"comma separated list of kafka topics to subscribe to\"\n</code></pre> <p>In particular, the <code>onlinefs/config_dir</code> should contain the credentials necessary for the Kafka consumers to authenticate.  Additionally the directory should contain a file name <code>onlinefs-kafka.properties</code> with the Kafka consumer configuration. The following is an example of the <code>onlinefs-kafka.properties</code> file:</p> <pre><code>bootstrap.servers=cluster_identifier.us-east-2.aws.confluent.cloud:9092\nsecurity.protocol=SASL_SSL\nsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"username\" password=\"password\";\nsasl.mechanism=PLAIN\n</code></pre> <p>Hopsworks will not provision topics</p> <p>Please note that when using an external Kafka cluster, Hopsworks will not provision the topics for the different projects. Users are responsible for provisioning the necessary topics and configure the projects accordingly (see next section). Users should also specify the list of topics OnlineFS should subscribe to by providing the <code>onlinefs/kafka_consumers/topic_list</code> option in the cluster definition.</p>"},{"location":"setup_installation/on_prem/external_kafka_cluster/#project-configuration","title":"Project configuration","text":""},{"location":"setup_installation/on_prem/external_kafka_cluster/#topic-configuration","title":"Topic configuration","text":"<p>As mentioned above, when configuring Hopsworks to use an external Kafka cluster, Hopsworks will not provision the topics for the different projects. Instead, when creating a project, users will be asked to provide the topic name to use for the feature store operations.</p> <p> Example project creation when using an external Kafka cluster </p>"},{"location":"setup_installation/on_prem/external_kafka_cluster/#storage-connector-configuration","title":"Storage connector configuration","text":"<p>Users should create a Kafka storage connector named <code>kafka_connector</code> which is going to be used by the feature store clients to configure the necessary Kafka producers to send data. The configuration is done for each project to ensure its members have the necessary authentication/authorization. If the storage connector is not found in the project, default values referring to Hopsworks managed Kafka will be used.</p>"},{"location":"tutorials/","title":"Tutorials","text":"<p>We are happy to welcome you to our collection of tutorials dedicated to exploring the fundamentals of Hopsworks and Machine Learning development. In addition to offering different types of use cases and common subjects in the field, it facilitates navigation and use of models in a production environment using Hopsworks Feature Store.</p>"},{"location":"tutorials/#how-to-run-the-tutorials","title":"How to run the tutorials","text":"<p>In order to run the tutorials, you will need a Hopsworks account. To do so, go to app.hopsworks.ai and create one. With a managed account, just run the Jupyter notebook from within Hopsworks. Generally the notebooks contain the information you will need on how to interact with the Hopsworks Platform.</p> <p>The easiest way to get started is by using Google Colab to run the notebooks. However, you can also run them in your local Python environment with Jupyter. You can find the raw notebook files in our tutorials repository.</p>"},{"location":"tutorials/#fraud-tutorial","title":"Fraud Tutorial","text":"<p>This is a quick-start of the Hopsworks Feature Store; using a fraud use case we will load data into the feature store, create two feature groups from which we will make a training dataset and train a model.</p>"},{"location":"tutorials/#batch","title":"Batch","text":"<p>This is a batch use case variant of the fraud tutorial, it will give you a high level view on how to use our python APIs and the UI to navigate the feature groups.</p> Notebooks 1. How to load, engineer and create feature groups 2. How to create training datasets 3. How to train a model from the feature store"},{"location":"tutorials/#online","title":"Online","text":"<p>This is a online use case variant of the fraud tutorial, it is similar to the batch use case, however, in this tutorial you will get introduced to the usage of Feature Groups which are kept in online storage, and how to access single feature vectors from the online storage at low latency. Additionally, the model will be deployed as a model serving instance, to provide a REST endpoint for real time serving.</p> Notebooks 1. How to load, engineer and create feature groups 2. How to create training datasets 3. How to train a model from the feature store and deploying it as a serving instance together with the online feature store"},{"location":"tutorials/#churn-tutorial","title":"Churn Tutorial","text":"<p>This is a churn tutorial with the Hopsworks feature store and model serving to build a prediction service. In this tutorial you will get introduced to the usage of Feature Groups which are kept in online storage, and how to access single feature vectors from the online storage at low latency. Additionally, the model will be deployed as a model serving instance, to provide a REST endpoint for real time serving.</p> Notebooks 1. How to load, engineer and create feature groups 2. How to create training datasets 3. How to train a model from the feature store and deploying it as a serving instance together with the online feature store"},{"location":"tutorials/#integration-tutorials","title":"Integration Tutorials","text":"<p>Hopsworks is easily integrated with many tools, especially from the Python world. In this section you will find examples for some popular libraries and services.</p>"},{"location":"tutorials/#great-expectations","title":"Great Expectations","text":"<p>Great Expectations is a library for data validation. You can use Great Expectations within Hopsworks to validate data which is to be inserted into the feature store, in order to ensure that only high-quality features end up in the feature store.</p> Notebooks 1. A brief introduction to Great Expectations concepts which are relevant for integration with the Hopsworks MLOps platform 2. How to integrate Great Expectations seamlessly with your Hopsworks feature pipelines"},{"location":"tutorials/#weights-and-biases","title":"Weights and Biases","text":"<p>Weights and Biases is a developer tool for machine learning model training that with a couple of lines of code let you keep track of hyperparameters, system metrics, and outputs so you can compare experiments, and easily share your findings with colleagues.</p> <p>This tutorial is a variant of the batch fraud tutorial using Weights and Biases for model training, tracking and as model registry.</p> Notebooks 1. How to load, engineer and create feature groups 2. How to create training datasets 3. How to train a model from the feature store and use Weights and Biases to track the process"},{"location":"user_guides/","title":"How-To Guides","text":"<p>This section serves to provide guides and examples for the common usage of abstractions and functionality of the Hopsworks platform through the Hopsworks UI and APIs.</p> <ul> <li>Client Installation: How to get started with the Hopsworks Client libraries.</li> <li>Feature Store: Learn about the common usage of the core Hopsworks Feature Store abstractions, such as Feature Groups, Feature Views, Data Validation and Storage Connectors. Also, learn from the Client Integrations guides how to connect to the Feature Store from external environments such as a local Python environment, Databricks, or AWS Sagemaker</li> <li>MLOps: Learn about the common usage of Hopsworks MLOps abstractions, such as the Model Registry or Model Serving.</li> <li>Projects: The core abstraction on Hopsworks are Projects. Learn in this section how to manage your projects and the services therein.</li> <li>Migration: Learn how to migrate to newer versions of Hopsworks.</li> </ul>"},{"location":"user_guides/client_installation/","title":"Client Installation Guide","text":""},{"location":"user_guides/client_installation/#hopsworks-python-library","title":"Hopsworks Python library","text":"<p>The Hopsworks Python client library is required to connect to Hopsworks from your local machine or any other Python environment such as Google Colab or AWS Sagemaker. Execute the following command to install the Hopsworks client library in your Python environment:</p> <p>Virtual environment</p> <p>It is recommended to use a virtual python environment instead of the system environment used by your operating system, in order to avoid any side effects regarding interfering dependencies.</p> <p>Windows/Conda Installation</p> <p>On Windows systems you might need to install twofish manually before installing hopsworks, if you don't have the Microsoft Visual C++ Build Tools installed. In that case, it is recommended to use a conda environment and run the following commands:</p> <pre><code>conda install twofish\npip install hopsworks[python]\n</code></pre> <p><pre><code>pip install hopsworks[python]\n</code></pre> Supported versions of Python: 3.8, 3.9, 3.10, 3.11, 3.12 (PyPI \u2197)</p>"},{"location":"user_guides/client_installation/#profiles","title":"Profiles","text":"<p>The Hopsworks library has several profiles that bring additional dependencies and enable additional functionalities:</p> Profile Name Description No Profile This is the base installation. Supports interacting with the feature store metadata, model registry and deployments. It also supports reading and writing from the feature store from PySpark environments. <code>python</code> This profile enables reading and writing from/to the feature store from a Python environment <code>great-expectations</code> This profile installs the Great Expectations Python library and enables data validation on feature pipelines <code>polars</code> This profile installs the Polars library and enables reading and writing Polars DataFrames <p>You can install all the above profiles with the following command:</p> <pre><code>pip install hopsworks[python,great-expectations,polars]\n</code></pre>"},{"location":"user_guides/client_installation/#hsfs-java-library","title":"HSFS Java Library:","text":"<p>If you want to interact with the Hopsworks Feature Store from environments such as Spark, Flink or Beam, you can use the Hopsworks Feature Store (HSFS) Java library.</p> <p>Feature Store Only</p> <p>The Java library only allows interaction with the Feature Store component of the Hopsworks platform. Additionally each environment might restrict the supported API operation. You can see which API operation is supported by which environment here</p> <p>The HSFS library is available on the Hopsworks' Maven repository. If you are using Maven as build tool, you can add the following in your <code>pom.xml</code> file:</p> <pre><code>&lt;repositories&gt;\n    &lt;repository&gt;\n        &lt;id&gt;Hops&lt;/id&gt;\n        &lt;name&gt;Hops Repository&lt;/name&gt;\n        &lt;url&gt;https://archiva.hops.works/repository/Hops/&lt;/url&gt;\n        &lt;releases&gt;\n            &lt;enabled&gt;true&lt;/enabled&gt;\n        &lt;/releases&gt;\n        &lt;snapshots&gt;\n            &lt;enabled&gt;true&lt;/enabled&gt;\n        &lt;/snapshots&gt;\n    &lt;/repository&gt;\n&lt;/repositories&gt;\n</code></pre> <p>The library has different builds targeting different environments: </p>"},{"location":"user_guides/client_installation/#spark","title":"Spark","text":"<p>The <code>artifactId</code> for the Spark build is <code>hsfs-spark-spark{spark.version}</code>, if you are using Maven as build tool, you can add the following dependency:</p> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;com.logicalclocks&lt;/groupId&gt;\n    &lt;artifactId&gt;hsfs-spark-spark3.1&lt;/artifactId&gt;\n    &lt;version&gt;${hsfs.version}&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Hopsworks provides builds for Spark 3.1, 3.3 and 3.5. The builds are also provided as JAR files which can be downloaded from Hopsworks repository</p>"},{"location":"user_guides/client_installation/#flink","title":"Flink","text":"<p>The <code>artifactId</code> for the Flink build is <code>hsfs-flink</code>, if you are using Maven as build tool, you can add the following dependency:</p> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;com.logicalclocks&lt;/groupId&gt;\n    &lt;artifactId&gt;hsfs-flink&lt;/artifactId&gt;\n    &lt;version&gt;${hsfs.version}&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"user_guides/client_installation/#beam","title":"Beam","text":"<p>The <code>artifactId</code> for the Beam build is <code>hsfs-beam</code>, if you are using Maven as build tool, you can add the following dependency:</p> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;com.logicalclocks&lt;/groupId&gt;\n    &lt;artifactId&gt;hsfs-beam&lt;/artifactId&gt;\n    &lt;version&gt;${hsfs.version}&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"user_guides/client_installation/#next-steps","title":"Next Steps","text":"<p>If you are using a local python environment and want to connect to Hopsworks, you can follow the Python Guide section to create an API Key and to get started.</p>"},{"location":"user_guides/client_installation/#other-environments","title":"Other environments","text":"<p>The Hopsworks Feature Store client libraries can also be installed in external environments, such as Databricks, AWS Sagemaker, or Azure Machine Learning. For more information, see Client Integrations.</p>"},{"location":"user_guides/fs/","title":"Feature Store Guides","text":"<p>This section serves to provide guides and examples for the common usage of abstractions and functionality of the Feature Store through the Hopsworks UI and APIs.</p> <ul> <li>Storage Connectors</li> <li>Feature Groups</li> <li>Feature Views</li> <li>Vector Similarity Search</li> <li>Compute Engines</li> <li>Integrations</li> <li>Transformations</li> </ul>"},{"location":"user_guides/fs/compute_engines/","title":"Compute Engines","text":""},{"location":"user_guides/fs/compute_engines/#compute-engines","title":"Compute Engines","text":"<p>In order to execute a feature pipeline to write to the Feature Store, as well as to retrieve data from the Feature Store, you need a compute engine. Hopsworks Feature Store APIs are built around dataframes, that means feature data is inserted into the Feature Store from a Dataframe and likewise when reading data from the Feature Store, it is returned as a Dataframe.</p> <p>As such, Hopsworks supports three computational engines:</p> <ol> <li>Apache Spark: Spark Dataframes and Spark Structured Streaming Dataframes are supported, both from Python environments (PySpark) and from Scala environments.</li> <li>Python: For pure Python environments without dependencies on Spark, Hopsworks supports Pandas Dataframes and Polars Dataframes.</li> <li>Apache Flink: Flink Data Streams are currently supported as an experimental feature from Java/Scala environments.</li> <li>Apache Beam experimental: Beam Data Streams are currently supported as an experimental feature from Java/Scala environments.</li> </ol> <p>Hopsworks supports running compute on the platform itself in the form of Jobs or in Jupyter Notebooks. Alternatively, you can also connect to Hopsworks using Python or Spark from external environments, given that there is network connectivity.</p>"},{"location":"user_guides/fs/compute_engines/#functionality-support","title":"Functionality Support","text":"<p>Hopsworks is aiming to provide functional parity between the computational engines, however, there are certain Hopsworks functionalities which are exclusive to the engines.</p> Functionality Method Spark Python Flink Beam Comment Feature Group Creation from dataframes <code>FeatureGroup.create_feature_group()</code> - - Currently Flink/Beam doesn't support registering feature group metadata. Thus it needs to be pre-registered before you can write real time features computed by Flink/Beam. Training Dataset Creation from dataframes <code>TrainingDataset.save()</code> - - - Functionality was deprecated in version 3.0 Data validation using Great Expectations for streaming dataframes <code>FeatureGroup.validate()</code> <code>FeatureGroup.insert_stream()</code> - - - - <code>insert_stream</code> does not perform any data validation even when a expectation suite is attached. Stream ingestion <code>FeatureGroup.insert_stream()</code> - Python/Pandas/Polars has currently no notion of streaming. Stream ingestion <code>FeatureGroup.insert_stream()</code> - Python/Pandas/Polars has currently no notion of streaming. Reading from Streaming Storage Connectors <code>KafkaConnector.read_stream()</code> - - - Python/Pandas/Polars has currently no notion of streaming. For Flink/Beam only write operations are supported Reading training data from external storage other than S3 <code>FeatureView.get_training_data()</code> - - - Reading training data that was written to external storage using a Storage Connector other than S3 can currently not be read using HSFS APIs, instead you will have to use the storage's native client. Reading External Feature Groups into Dataframe <code>ExternalFeatureGroup.read()</code> - - - Reading an External Feature Group directly into a Pandas/Polars Dataframe is not supported, however, you can use the Query API to create Feature Views/Training Data containing External Feature Groups. Read Queries containing External Feature Groups into Dataframe <code>Query.read()</code> - - - Reading a Query containing an External Feature Group directly into a Pandas/Polars Dataframe is not supported, however, you can use the Query to create Feature Views/Training Data and write the data to a Storage Connector, from where you can read up the data into a Pandas/Polars Dataframe."},{"location":"user_guides/fs/compute_engines/#python","title":"Python","text":""},{"location":"user_guides/fs/compute_engines/#inside-hopsworks","title":"Inside Hopsworks","text":"<p>If you are using Spark or Python within Hopsworks, there is no further configuration required. Head over to the Getting Started Guide.</p>"},{"location":"user_guides/fs/compute_engines/#outside-hopsworks","title":"Outside Hopsworks","text":"<p>Connecting to the Feature Store from any Python environment, such as your local environment or Google Colab, requires setting up an API Key and installing the HSFS Python client library. The Python integration guide explains step by step how to connect to the Feature Store from any Python environment.</p>"},{"location":"user_guides/fs/compute_engines/#spark","title":"Spark","text":""},{"location":"user_guides/fs/compute_engines/#inside-hopsworks_1","title":"Inside Hopsworks","text":"<p>If you are using Spark or Python within Hopsworks, there is no further configuration required. Head over to the Getting Started Guide.</p>"},{"location":"user_guides/fs/compute_engines/#outside-hopsworks_1","title":"Outside Hopsworks","text":"<p>Connecting to the Feature Store from an external Spark cluster, such as Cloudera or Databricks, requires configuring it with the Hopsworks client jars, configuration and certificates. The Spark integration guide explains step by step how to connect to the Feature Store from an external Spark cluster.</p>"},{"location":"user_guides/fs/compute_engines/#flink","title":"Flink","text":""},{"location":"user_guides/fs/compute_engines/#inside-hopsworks_2","title":"Inside Hopsworks","text":"<p>If you are using Flink within Hopsworks, there is no further configuration required. For more details head over to the Getting Started Guide.</p>"},{"location":"user_guides/fs/compute_engines/#outside-hopsworks_2","title":"Outside Hopsworks","text":"<p>Connecting to the Feature Store from an external Flink cluster, such as GCP DataProc or AWS EMR, requires configuring the Hopsworks certificates. The Flink integration guide explains step by step how to connect to the Feature Store from an external Flink cluster.</p>"},{"location":"user_guides/fs/compute_engines/#beam","title":"Beam","text":""},{"location":"user_guides/fs/compute_engines/#inside-hopsworks_3","title":"Inside Hopsworks","text":"<p>Beam is only supported as an external client.</p>"},{"location":"user_guides/fs/compute_engines/#outside-hopsworks_3","title":"Outside Hopsworks","text":"<p>Connecting to the Feature Store from Beam DataFlowRunner, requires configuring the Hopsworks certificates. The Beam integration guide explains step by step how to connect to the Feature Store from Beam Dataflow Runner.</p> <p>Warning</p> <p>Apache Beam integration with Hopsworks feature store was only tested using Dataflow Runner.</p> <p>For more details head over to the Getting Started Guide.</p>"},{"location":"user_guides/fs/transformation_functions/","title":"Transformation Functions","text":"<p>In AI systems, transformation functions transform data to create features, the inputs to machine learning models (in both training and inference). The\u00a0taxonomy of data transformations\u00a0introduces three types of data transformation prevalent in all AI systems. Hopsworks offers simple Python APIs to define custom transformation functions. These can be used along with\u00a0feature groups and feature views to create on-demand transformations and model-dependent transformations, producing modular AI pipelines that are skew-free.</p>"},{"location":"user_guides/fs/transformation_functions/#custom-transformation-function-creation","title":"Custom Transformation Function Creation","text":"<p>User-defined transformation functions can be created in Hopsworks using the <code>@udf</code> decorator. These functions can be either implemented as pure Python UDFs or Pandas UDFs (User-Defined Functions).</p> <p>Hopsworks offers three execution modes to control the execution of transformation functions during training dataset creation, batch inference, and online inference. By default, Hopsworks executes transformation functions as Python UDFs for feature vector retrieval in online inference pipelines and as Pandas UDFs for both batch data retrieval in batch inference pipelines and training dataset creation in training pipelines. Python UDFs are optimized for smaller data volumes, while Pandas UDFs provide better performance on larger datasets. This execution mode provides the optimal balance based on the data size across training dataset generations, batch inference, and online inference. Additionally, Hopsworks allows you to explicitly set the execution mode for a transformation function to <code>python</code> or <code>pandas</code>, forcing the transformation function to always run as either a Python or Pandas UDF as specified.</p> <p>A Pandas UDF in Hopsworks accepts one or more Pandas Series as input and can return either one or more Series or a Pandas DataFrame. When integrated with PySpark applications, Hopsworks automatically executes Pandas UDFs using PySpark\u2019s <code>pandas_udf</code>, enabling the transformation functions to efficiently scale for large datasets.</p> <p>Java/Scala support</p> <p>Hopsworks supports transformations functions in Python (Pandas UDFs, Python UDFs). Transformations functions can also be executed in Python-based DataFrame frameworks (PySpark, Pandas). There is currently no support for transformation functions in SQL or Java-based feature pipelines.</p> <p>Transformation functions created in Hopsworks can be directly attached to feature views or feature groups or stored in the feature store for later retrieval. These functions can be part of a library installed in Hopsworks or be defined in a Jupyter notebook running a Python kernel or added when starting a Jupyter notebook or Hopsworks job.</p> <p>PySpark Kernels</p> <p>Definition transformation function within a Jupyter notebook is only supported in Python Kernel. In a PySpark Kernel transformation function have to defined as modules or added when starting a Jupyter notebook.</p> <p>The <code>@udf</code> decorator in Hopsworks creates a metadata class called <code>HopsworksUdf</code>. This class manages the necessary operations to execute the transformation function. </p> <p>The decorator accepts three parameters:</p> <ul> <li> <p><code>return_type</code> (required): Specifies the data type(s) of the features returned by the transformation function. It can be a single Python type if the function returns one transformed feature, or a list of Python types if it returns multiple transformed features. The supported Python types that be used with the <code>return_type</code> argument are provided in the table below:</p> Supported Python Types str int float bool datetime.datetime datetime.date datetime.time </li> <li> <p><code>drop</code> (optional): Identifies input arguments to exclude from the output after transformations are applied. By default, all inputs are retained in the output. Further details on this argument can be found below.</p> </li> <li> <p><code>mode</code> (optional): Determines the execution mode of the transformation function. The argument accepts three values: <code>default</code>, <code>python</code>, or <code>pandas</code>. By default, the <code>mode</code> is set to <code>default</code>.  Further details on this argument can be found below.</p> </li> </ul> <p>Hopsworks supports four types of transformation functions across all execution modes:</p> <ol> <li>One-to-one: Transforms one feature into one transformed feature.</li> <li>One-to-many: Transforms one feature into multiple transformed features.</li> <li>Many-to-one: Transforms multiple features into one transformed feature.</li> <li>Many-to-many: Transforms multiple features into multiple transformed features.</li> </ol>"},{"location":"user_guides/fs/transformation_functions/#one-to-one-transformations","title":"One-to-one transformations","text":"<p>To create a one-to-one transformation function, the Hopsworks <code>@udf</code> decorator must be provided with the <code>return_type</code> as a single Python type. The transformation function should take one argument as input and return a Pandas Series.</p> Python <p>Creation of a one-to-one transformation function in Hopsworks.</p> <pre><code>from hopsworks import udf\n\n@udf(return_type=int)\ndef add_one(feature):\n    return feature + 1\n</code></pre>"},{"location":"user_guides/fs/transformation_functions/#many-to-one-transformations","title":"Many-to-one transformations","text":"<p>The creation of many-to-one transformation functions is similar to that of a one-to-one transformation function, the only difference being that the transformation function accepts multiple features as input.</p> Python <p>Creation of a many-to-one transformation function in Hopsworks.</p> <pre><code>from hopsworks import udf\n\n@udf(return_type=int)\ndef add_features(feature1, feature2, feature3):\n    return feature + feature2 + feature3\n</code></pre>"},{"location":"user_guides/fs/transformation_functions/#one-to-many-transformations","title":"One-to-many transformations","text":"<p>To create a one-to-many transformation function, the Hopsworks\u00a0<code>@udf</code>\u00a0decorator must be provided with the\u00a0<code>return_type</code>\u00a0as a list of Python types, and the transformation function should take one argument as input and return multiple features as a Pandas DataFrame. The return types provided to the decorator must match the types of each column in the returned Pandas DataFrame.</p> Python <p>Creation of a one-to-many transformation function in Hopsworks.</p> <pre><code>from hopsworks import udf\nimport pandas as pd\n\n@udf(return_type=[int, int])\ndef add_one_and_two(feature1):\n    return feature1 + 1, feature1 + 2\n</code></pre>"},{"location":"user_guides/fs/transformation_functions/#many-to-many-transformations","title":"Many-to-many transformations","text":"<p>The creation of a many-to-many transformation function is similar to that of a one-to-many transformation function, the only difference being that the transformation function accepts multiple features as input.</p> Python <p>Creation of a many-to-many transformation function in Hopsworks.</p> <pre><code>from hopsworks import udf\nimport pandas as pd\n\n@udf(return_type=[int, int, int])\ndef add_one_multiple(feature1, feature2, feature3):\n    return feature1 + 1, feature2 + 1, feature3 + 1\n</code></pre>"},{"location":"user_guides/fs/transformation_functions/#specifying-execution-modes","title":"Specifying execution modes","text":"<p>The <code>mode</code> parameter of the <code>@udf</code> decorator can be used to specify the execution mode of the transformation function. It accepts three possible values <code>default</code>, <code>python</code> and <code>pandas</code>.  Each mode is explained in more detail below:</p>"},{"location":"user_guides/fs/transformation_functions/#default","title":"Default","text":"<p>This execution mode assumes that the transformation function can be executed as either a Pandas UDF or a Python UDF. It serves as the default mode used when the <code>mode</code> parameter is not specified. In this mode, the transformation function is executed as a Pandas UDF during training and in the batch inference pipeline, while it operates as a Python UDF during online inference.</p> Python <p>Creating a many to many transformations function using the default execution mode</p> <pre><code>from hopsworks import udf\nimport pandas as pd\n\n# \"default\" mode is used if the parameter `mode` is not explicitly set.\n@udf(return_type=[int, int, int])\ndef add_one_multiple(feature1, feature2, feature3):\n    return feature1 + 1, feature2 + 1, feature3 + 1\n\n@udf(return_type=[int, int, int], mode=\"default\")\ndef add_two_multiple(feature1, feature2, feature3):\n    return feature1 + 2, feature2 + 2, feature3 + 2\n</code></pre>"},{"location":"user_guides/fs/transformation_functions/#python","title":"Python","text":"<p>The transformation function can be configured to always execute as a Python UDF by setting the <code>mode</code> parameter of the <code>@udf</code> decorator to <code>python</code>.</p> Python <p>Creating a many to many transformation function as a Python UDF</p> <pre><code>from hopsworks import udf\nimport pandas as pd\n\n@udf(return_type=[int, int, int], mode = \"python\")\ndef add_one_multiple(feature1, feature2, feature3):\n    return feature1 + 1, feature2 + 1, feature3 + 1\n</code></pre>"},{"location":"user_guides/fs/transformation_functions/#pandas","title":"Pandas","text":"<p>The transformation function can be configured to always execute as a Pandas UDF by setting the <code>mode</code> parameter of the <code>@udf</code> decorator to <code>pandas</code>.</p> Python <p>Creating a many to many transformations function as a Pandas UDF</p> <pre><code>from hopsworks import udf\nimport pandas as pd\n\n# A Pandas UDF returning a Pandas DataFrame\n@udf(return_type=[int, int, int], mode = \"pandas\")\ndef add_one_multiple(feature1, feature2, feature3):\n    return pd.DataFrame({\"add_one_feature1\":feature1 + 1, \"add_one_feature2\":feature2 + 1, \"add_one_feature3\":feature3 + 1})\n\n# A Pandas UDF returning multiple Pandas Series\n@udf(return_type=[int, int, int], mode=\"pandas\")\ndef add_two_multiple(feature1, feature2, feature3):\n    return feature1 + 2, feature2 + 2, feature3 + 2\n</code></pre>"},{"location":"user_guides/fs/transformation_functions/#dropping-input-features","title":"Dropping input features","text":"<p>The\u00a0<code>drop</code>\u00a0parameter of the\u00a0<code>@udf</code>\u00a0decorator is used to drop specific columns in the input DataFrame after transformation.  If any argument of the transformation function is passed to the <code>drop</code> parameter, then the column mapped to the argument is dropped after the transformation functions are applied. In the example below, the columns mapped to the arguments\u00a0<code>feature1</code>\u00a0and\u00a0<code>feature3</code>\u00a0are dropped after the application of all transformation functions.</p> Python <p>Specify arguments to drop after transformation</p> <pre><code>from hopsworks import udf\nimport pandas as pd\n\n@udf(return_type=[int, int, int], drop=[\"feature1\", \"feature3\"])\ndef add_one_multiple(feature1, feature2, feature3):\n    return feature1 + 1, feature2 + 1, feature3 + 1\n</code></pre>"},{"location":"user_guides/fs/transformation_functions/#specifying-output-features-names-for-transformation-functions","title":"Specifying output features names for transformation functions","text":"<p>The <code>alias</code> function of a transformation function allows the specification of names of transformed features generated by the transformation function. Each name must be uniques and should be at-most 63 characters long. If no name is provided via the <code>alias</code> function, Hopsworks generates default output feature names when on-demand or model-dependent transformation functions are created.</p> Python <p>Specifying output column names for transformation functions.</p> <pre><code>from hopsworks import udf\nimport pandas as pd\n\n@udf(return_type=[int, int, int], drop=[\"feature1\", \"feature3\"])\ndef add_one_multiple(feature1, feature2, feature3):\n    return feature1 + 1, feature2 + 1, feature3 + 1\n\n# Specifying output feature names of the transformation function.\nadd_one_multiple.alias(\"transformed_feature1\", \"transformed_feature2\", \"transformed_feature3\")\n</code></pre>"},{"location":"user_guides/fs/transformation_functions/#training-dataset-statistics","title":"Training dataset statistics","text":"<p>A keyword argument <code>statistics</code> can be defined in the transformation function if it requires training dataset statistics for any of its arguments. The <code>statistics</code> argument must be assigned an instance of the class\u00a0<code>TransformationStatistics</code>\u00a0as the default value. The\u00a0<code>TransformationStatistics</code>\u00a0instance must be initialized using the names of the arguments requiring statistics.</p> <p>Transformation Statistics</p> <p>The statistics provided to the transformation function is the statistics computed using the train set. Training dataset statistics are not available for on-demand transformations.</p> <p>The\u00a0<code>TransformationStatistics</code>\u00a0instance contains separate objects with the same name as the arguments used to initialize it. These objects encapsulate statistics related to the argument as instances of the\u00a0class <code>FeatureTransformationStatistics</code>. Upon instantiation, instances of\u00a0<code>FeatureTransformationStatistics</code>\u00a0contain\u00a0<code>None</code>\u00a0values and are updated with the required statistics after the creation of a training dataset.</p> Python <p>Creation of a transformation function in Hopsworks that uses training dataset statistics</p> <pre><code>from hopsworks import udf\nfrom hopsworks.transformation_statistics import TransformationStatistics\n\nstats = TransformationStatistics(\"argument1\", \"argument2\", \"argument3\") \n\n@udf(int)\ndef add_features(argument1, argument2, argument3, statistics=stats):\n    return argument + argument2 + argument3 + statistics.argument1.mean + statistics.argument2.mean + statistics.argument3.mean\n</code></pre>"},{"location":"user_guides/fs/transformation_functions/#passing-context-variables-to-transformation-function","title":"Passing context variables to transformation function","text":"<p>The <code>context</code> keyword argument can be defined in a transformation function to access shared context variables. These variables contain common data used across transformation functions. By including the context argument, you can pass the necessary data as a dictionary into the into the <code>context</code> argument of the transformation function during training dataset creation or feature vector retrieval or batch data retrieval.</p> Python <p>Creation of a transformation function in Hopsworks that accepts context variables</p> <pre><code>from hopsworks import udf\n\n@udf(int)\ndef add_features(argument1, context):\n    return argument + context[\"value_to_add\"]\n</code></pre>"},{"location":"user_guides/fs/transformation_functions/#saving-to-the-feature-store","title":"Saving to the Feature Store","text":"<p>To save a transformation function to the feature store, use the function\u00a0<code>create_transformation_function</code>.\u00a0It creates a\u00a0<code>TransformationFunction</code>\u00a0object which can then be saved by calling the\u00a0save\u00a0function. The save function will throw an error if another transformation function with the same name and version is already saved in the feature store.</p> Python <p>Register transformation function <code>add_one</code> in the Hopsworks feature store</p> <pre><code>plus_one_meta = fs.create_transformation_function(\n            transformation_function=add_one,\n            version=1)\nplus_one_meta.save()\n</code></pre>"},{"location":"user_guides/fs/transformation_functions/#retrieval-from-the-feature-store","title":"Retrieval from the Feature Store","text":"<p>To retrieve all transformation functions from the feature store, use the function\u00a0<code>get_transformation_functions</code>,\u00a0which returns the list of <code>TransformationFunction</code>\u00a0objects. </p> <p>A specific transformation function can be retrieved using its <code>name</code> and <code>version</code> with the function <code>get_transformation_function</code>. If only the <code>name</code> is provided, then the version will default to 1.</p> Python <p>Retrieving transformation functions from the feature store</p> <pre><code># get all transformation functions\nfs.get_transformation_functions()\n\n# get transformation function by name. This will default to version 1\nplus_one_fn = fs.get_transformation_function(name=\"plus_one\")\n\n# get transformation function by name and version.\nplus_one_fn = fs.get_transformation_function(name=\"plus_one\", version=2)\n</code></pre>"},{"location":"user_guides/fs/transformation_functions/#using-transformation-functions","title":"Using transformation functions","text":"<p>Transformation functions can be used by attaching it to a feature view to create model-dependent transformations or attached to feature groups to  create on-demand transformations </p>"},{"location":"user_guides/fs/vector_similarity_search/","title":"Introduction","text":"<p>Vector similarity search (also called similarity search) is a technique enabling the retrieval of similar items based on their vector embeddings or representations. Its applications range across various domains, from recommendation systems to image similarity and beyond. In Hopsworks, vector similarity search is enabled by extending an online feature group with approximate nearest neighbor search capabilities through a vector database, such as Opensearch. This guide provides a detailed walkthrough on how to leverage Hopsworks for vector similarity search.</p>"},{"location":"user_guides/fs/vector_similarity_search/#extending-feature-groups-with-similarity-search","title":"Extending Feature Groups with Similarity Search","text":"<p>In Hopsworks, each vector embedding in a feature group is stored in an index within the backing vector database. By default, vector embeddings are stored in the default index for the project (created for every project in Hopsworks), but you have the option to create a new index for a feature group if needed. Creating a separate index per feature group is particularly useful for large volumes of data, ensuring that when a feature group is deleted, its associated index is also removed. For feature groups that use the default project index, the index will only be removed when the project is deleted - not when the feature group is deleted. The index will store all the vector embeddings defined in that feature group, if you have more than one vector embedding in the feature group.</p> <p>In the following example, we explicitly define an index for the feature group:</p> <pre><code>from hsfs import embedding\n\n# Specify optionally the index in the vector database\nemb = embedding.EmbeddingIndex(index_name=\"news_fg\")\n</code></pre> <p>Then, add one or more embedding features to the index. Name and dimension of the embedding features are required for identifying which features should be indexed for k-nearest neighbor (KNN) search. In this example, we get the dimension of the embedding by taking the length of the value of the <code>embedding_heading</code> column in the first row of the dataframe <code>df</code>. Optionally, you can specify the similarity function among <code>l2_norm</code>, <code>cosine</code>, and <code>dot_product</code>. Refer to add_embedding for the full list of arguments. <pre><code># Add embedding feature to the index\nemb.add_embedding(\"embedding_heading\", len(df[\"embedding_heading\"][0]))\n</code></pre></p> <p>Next, you create a feature group with the <code>embedding_index</code> and ingest data to the feature group. When the <code>embedding_index</code> is provided, the vector database is used as online feature store. That is, all the features in the feature group are stored exclusively in the vector database. The advantage of storing all features in the vector database is that it enables similarity search, and push-down filtering for all feature values.</p> <pre><code># Create a feature group with the embedding index\nnews_fg = fs.get_or_create_feature_group(\n    name=f\"news_fg\",\n    embedding_index=emb, # Provide the embedding index created\n    primary_key=[\"news_id\"],\n    version=version,\n    online_enabled=True\n)\n\n# Write a DataFrame to the feature group, including the offline store and the ANN index (in the Vector Database)\nnews_fg.insert(df)\n</code></pre>"},{"location":"user_guides/fs/vector_similarity_search/#similarity-search-for-feature-groups-using-vector-embeddings","title":"Similarity Search for Feature Groups using Vector Embeddings","text":"<p>You provide a vector embedding as a parameter to the search query using <code>find_neighbors</code>, and it returns the rows in the online feature group that have vector embedding values most similar to the provided vector embedding.</p> <p>It is also possible to filter rows by specifying a filter on any of the features in the feature group. The filter is pushed down to the vector database to improve query performance.</p> <p>In the first code snippet below, <code>find_neighbor</code>s returns 3 rows in <code>news_fg</code> that have the closest <code>news_description</code> values to the provided <code>news_description</code>. In the second code snippet below, we only return news articles with a <code>newstype</code> of <code>sports</code>. <pre><code># Search neighbor embedding with k=3\nnews_fg.find_neighbors(model.encode(news_description), k=3)\n\n# Filter and search\nnews_fg.find_neighbors(model.encode(news_description), k=3, filter=news_fg.newstype == \"sports\")\n</code></pre></p> <p>To analyze feature values at specific points in time, you can utilize time travel functionality: <pre><code># Time travel and read from the offline feature store\nnews_fg.as_of(time_in_past).read()\n</code></pre></p>"},{"location":"user_guides/fs/vector_similarity_search/#querying-similar-embeddings-with-additional-features","title":"Querying Similar Embeddings with Additional features","text":"<p>You can also use similarity search for vector embedding features in feature views. In the code snippet below, we create a feature view by selecting features from the earlier <code>news_fg</code> and a new feature group <code>view_fg</code>. If you include a feature group with vector embedding features in a feature view, whether or not the vector embedding features are selected, you can call <code>find_neighbors</code> on the feature view, and it will return rows containing all the feature values in the feature view. In the example below, a list of <code>heading</code> and <code>view_cnt</code> will be returned for the news articles which are closet to provided <code>news_description</code>.</p> <pre><code>view_fg = fs.get_or_create_feature_group(\n    name=\"view_fg\",\n    primary_key=[\"news_id\"],\n    version=version,\n    online_enabled=True\n)\n\nfv = fs.get_or_create_feature_view(\n    \"news_view\", version=version,\n    query=news_fg.select([\"heading\"]).join(view_fg.select([\"view_cnt\"]))\n)\n\nfv.find_neighbors(model.encode(news_description), k=5)\n</code></pre> <p>Note that you can use similarity search from the feature view only if the feature group which you are querying with <code>find_neighbors</code> has all the primary keys of the other feature groups. In the example above, you are querying against the feature group <code>news_fg</code> which has the vector embedding features, and it has the feature \"news_id\" which is the primary key of the feature group <code>view_fg</code>. But if <code>page_fg</code> is used as illustrated below, <code>find_neighbors</code> will fail to return any features because primary key <code>page_id</code> does not exist in <code>news_fg</code>.</p> <p> Cases when find_neighbors not works </p> <p>It is also possible to get back feature vector by providing the primary keys, but it is not recommended as explained in the next section. The client fetches feature vector from the vector store and the online store for <code>news_fg</code> and <code>view_fg</code> respectively. <pre><code>fv.get_feature_vector({\"news_id\": 1})\n</code></pre></p>"},{"location":"user_guides/fs/vector_similarity_search/#performance-considerations-for-feature-groups-with-embeddings","title":"Performance considerations for Feature Groups with Embeddings","text":""},{"location":"user_guides/fs/vector_similarity_search/#choose-features-for-vector-store","title":"Choose Features for Vector Store","text":"<p>While it is possible to update feature value in vector store, updating feature value in online store is more efficient. If you have features which are frequently being updated and do not require for filtering, consider storing them separately in a different feature group. As shown in the previous example, <code>view_cnt</code> is updated frequently and stored separately. You can then get all the required features by using feature view.</p>"},{"location":"user_guides/fs/vector_similarity_search/#choose-the-appropriate-online-feature-stores","title":"Choose the Appropriate Online Feature Stores","text":"<p>There are 2 types of online feature stores in Hopsworks: online store (RonDB) and vector store (Opensearch). Online store is designed for retrieving feature vectors efficiently with low latency. Vector store is designed for finding similar embedding efficiently. If similarity search is not required, using online store is recommended for low latency retrieval of feature values including embedding.</p>"},{"location":"user_guides/fs/vector_similarity_search/#use-new-index-per-feature-group","title":"Use New Index per Feature Group","text":"<p>Create a new index per feature group to optimize retrieval performance.</p>"},{"location":"user_guides/fs/vector_similarity_search/#next-step","title":"Next step","text":"<p>Explore the news search example, demonstrating how to use Hopsworks for implementing a news search application using natural language in the application. Additionally, you can see the application of querying similar embeddings with additional features in this news rank example.</p>"},{"location":"user_guides/fs/feature_group/","title":"Feature Group User Guides","text":"<p>This section serves to provide guides and examples for the common usage of abstractions and functionality of the Feature Group through the Hopsworks UI and APIs.</p> <ul> <li>Create a Feature Group</li> <li>Create an external Feature Group</li> <li>Deprecating Feature Group</li> <li>Data Types and Schema management</li> <li>Statistics</li> <li>Data Validation</li> <li>Feature Monitoring</li> </ul>"},{"location":"user_guides/fs/feature_group/create/","title":"How to create a Feature Group","text":""},{"location":"user_guides/fs/feature_group/create/#introduction","title":"Introduction","text":"<p>In this guide you will learn how to create and register a feature group with Hopsworks. This guide covers creating a feature group using the HSFS APIs as well as the user interface.</p>"},{"location":"user_guides/fs/feature_group/create/#prerequisites","title":"Prerequisites","text":"<p>Before you begin this guide we suggest you read the Feature Group concept page to understand what a feature group is and how it fits in the ML pipeline.</p>"},{"location":"user_guides/fs/feature_group/create/#create-using-the-hsfs-apis","title":"Create using the HSFS APIs","text":"<p>To create a feature group using the HSFS APIs, you need to provide a Pandas, Polars or Spark DataFrame. The DataFrame will contain all the features you want to register within the feature group, as well as the primary key, event time and partition key.</p>"},{"location":"user_guides/fs/feature_group/create/#create-a-feature-group","title":"Create a Feature Group","text":"<p>The first step to create a feature group is to create the API metadata object representing a feature group. Using the HSFS API you can execute:</p>"},{"location":"user_guides/fs/feature_group/create/#batch-write-api","title":"Batch Write API","text":"PySpark <pre><code>fg = feature_store.create_feature_group(name=\"weather\",\n    version=1,\n    description=\"Weather Features\",\n    online_enabled=True,\n    primary_key=['location_id'],\n    partition_key=['day'],\n    event_time='event_time',\n    time_travel_format='DELTA',\n)\n</code></pre> <p>The full method documentation is available here. If you need to create a feature group with vector similarity search supported, refer to this guide. <code>name</code> is the only mandatory parameter of the <code>create_feature_group</code> and represents the name of the feature group.</p> <p>In the example above we created the first version of a feature group named weather, we provide a description to make it searchable to the other project members, as well as making the feature group available online.</p> <p>Additionally we specify which columns of the DataFrame will be used as primary key, partition key and event time. Composite primary key and multi level partitioning is also supported.</p> <p>The version number is optional, if you don't specify the version number the APIs will create a new version by default with a version number equals to the highest existing version number plus one.</p> <p>The last parameter used in the examples above is <code>stream</code>. The <code>stream</code> parameter controls whether to enable the streaming write APIs to the online and offline feature store. When using the APIs in a Python environment this behavior is the default and it requires the time travel format to be set to 'HUDI'.</p>"},{"location":"user_guides/fs/feature_group/create/#primary-key","title":"Primary key","text":"<p>A primary key is required when using the default table format (Hudi) to store offline feature data. When inserting data in a feature group on the offline feature store, the DataFrame you are writing is checked against the existing data in the feature group. If a row with the same primary key is found in the feature group, the row will be updated. If the primary key is not found, the row is appended to the feature group. When writing data on the online feature store, existing rows with the same primary key will be overwritten by new rows with the same primary key.</p>"},{"location":"user_guides/fs/feature_group/create/#event-time","title":"Event time","text":"<p>The event time column represents the time at which the event was generated. For example, with transaction data, the event time is the time at which a given transaction happened. In the context of feature pipelines, the event time is often also the end timestamp of the interval of events included in the feature computation. For example, computing the feature \"number of purchases by customer last week\", the event time should be the last day of this \"last week\" window.</p> <p>The event time is added to the primary key when writing to the offline feature store. This will make sure that the offline feature store has the entire history of feature values over time. As an example, if a user has made multiple purchases on a website, each of the purchases for a given user (identified by a user_id) will be saved in the feature group, with each purchase having a different event time (the combination of user_id and event_time makes up the primary key for the offline feature store).</p> <p>The event time is not part of the primary key when writing to the online feature store. This will ensure that the online feature store has the most recent version of the feature vector for each primary key.</p> <p>Event time data type restriction</p> <p>The supported data types for the event time column are: <code>timestamp</code>, <code>date</code> and <code>bigint</code>.</p>"},{"location":"user_guides/fs/feature_group/create/#partition-key","title":"Partition key","text":"<p>It is best practice to add a partition key. When you specify a partition key, the data in the feature group will be stored under multiple directories based on the value of the partition column(s). All the rows with a given value as partition key will be stored in the same directory.</p> <p>Choosing the correct partition key has significant impact on the query performance as the execution engine (Spark) will be able to skip listing and reading files belonging to partitions which are not included in the query. As an example, if you have partitioned your feature group by day and you are creating a training dataset that includes only the last year of data, Spark will read only 365 partitions and not the entire history of data. On the other hand, if the partition key is too fine grained (e.g. timestamp at millisecond resolution) - a large number of small partitions will be generated. This will slow down query execution as Spark will need to list and read a large amount of small directories/files.</p> <p>If you do not provide a partition key, all the feature data will be stored as files in a single directory. The system has a limit of 10240 direct children (files or other subdirectories) per directory. This means that, as you add new data to a non-partitioned feature group, new files will be created and you might reach the limit. If you do reach the limit, your feature engineering pipeline will fail with the following error:</p> <pre><code>MaxDirectoryItemsExceededException - The directory item limit is exceeded: limit=10240 items=10240\n</code></pre> <p>By using partitioning the system will write the feature data in different subdirectories, thus allowing you to write 10240 files per partition.</p>"},{"location":"user_guides/fs/feature_group/create/#table-format","title":"Table format","text":"<p>When you create a feature group, you can specify the table format you want to use to store the data in your feature group by setting the <code>time_travel_format</code> parameter. The currently support values are \"HUDI\", \"DELTA\", \"NONE\" (which defaults to Parquet).</p>"},{"location":"user_guides/fs/feature_group/create/#storage-connector","title":"Storage connector","text":"<p>During the creation of a feature group, it is possible to define the <code>storage_connector</code> parameter, this allows for management of offline data in the desired table format outside the Hopsworks cluster. Currently, only S3 connectors and \"DELTA\" <code>time_travel_format</code> format is supported.</p>"},{"location":"user_guides/fs/feature_group/create/#online-table-configuration","title":"Online Table Configuration","text":"<p>When defining online-enabled feature groups it is also possible to configure the online table. You can specify table options by providing comments. Additionally, it is also possible to define whether online data is stored in memory or on disk using table space.</p> <p>The code example shows the creation of an online-enabled feature group that stores online data on disk using <code>ts_1</code> table space and sets several table properties in the comment section.</p> <pre><code>fg = fs.create_feature_group(\n    name='air_quality',\n    description='Air Quality characteristics of each day',\n    version=1,\n    primary_key=['city','date'],\n    online_enabled=True,\n    online_config={'table_space': 'ts_1', 'online_comments': ['NDB_TABLE=READ_BACKUP=1', 'NDB_TABLE=PARTITION_BALANCE=FOR_RP_BY_LDM_X_2']}\n)\n</code></pre> <p>Note</p> <p>The table space needs to be provisioned at system level before it can be used. You can do so by adding the following parameters to the values.yaml file used for your deployment with the Helm Charts:</p> <pre><code>rondb:\n  resources:\n    requests:\n      storage:\n        diskColumnGiB: 2\n</code></pre>"},{"location":"user_guides/fs/feature_group/create/#streaming-write-api","title":"Streaming Write API","text":"<p>As explained above, the stream parameter controls whether to enable the streaming write APIs to the online and offline feature store. For Python environments, only the stream API is supported (stream=True).</p> PythonPySpark <pre><code>fg = feature_store.create_feature_group(name=\"weather\",\n    version=1,\n    description=\"Weather Features\",\n    online_enabled=True,\n    primary_key=['location_id'],\n    partition_key=['day'],\n    event_time='event_time'\n    time_travel_format='HUDI',\n)\n</code></pre> <pre><code>fg = feature_store.create_feature_group(name=\"weather\",\n    version=1,\n    description=\"Weather Features\",\n    online_enabled=True,\n    primary_key=['location_id'],\n    partition_key=['day'],\n    event_time='event_time',\n    time_travel_format='HUDI',\n    stream=True\n)\n</code></pre> <p>When using the streaming API, the data will be written directly to the online storage (if <code>online_enabled=True</code>). However, you can control when the sync to the offline storage is going to happen. You can do it synchronously after every call to <code>fg.insert()</code>, which is the default. Often, you defer writes to a later point in order to batch together multiple writes to the offline storage (useful to reduce the overhead of many small writes):</p> <pre><code># run multiple inserts without starting the offline materialization job\njob, _ = fg.insert(df1, write_options={\"start_offline_materialization\": False})\njob, _ = fg.insert(df2, write_options={\"start_offline_materialization\": False})\njob, _ = fg.insert(df3, write_options={\"start_offline_materialization\": False})\n\n# start the materialization job for all three inserts\n# note the job object is always the same, you don't need to call it three times\njob.run()\n</code></pre> <p>It is also possible to define the topics used for data ingestion, this can be done by setting the <code>topic_name</code> parameter with your preferred value. By default, feature groups in hopsworks will share a project-wide topic.</p>"},{"location":"user_guides/fs/feature_group/create/#best-practices-for-writing","title":"Best Practices for Writing","text":"<p>When designing a feature group, it is worth taking a look at how this feature group will be queried in the future, in order to optimize it for those query patterns. At the same time, Spark and Hudi tend to overpartition writes, creating too many small parquet files, which is inefficient and slows down writes. But they also slow down queries, because file listings take more time and reading many small files is slower than fewer larger files. The best practices described in this section hold both for the Streaming API and the Batch API.</p> <p>Four main considerations influence the write and the query performance:</p> <ol> <li>Partitioning on a feature group level</li> <li>Parquet file size within a feature group partition</li> <li>Backfilling of feature group partitions</li> <li>The choice of topic for data ingestion</li> </ol>"},{"location":"user_guides/fs/feature_group/create/#partitioning-on-a-feature-group-level","title":"Partitioning on a feature group level","text":"<p>Partitioning on the feature group level allows Hopsworks and the table format (Hudi or Delta) to push down filters to the filesystem when reading from feature groups. In practice that means, less directories need to be listed and less files need to be read, speeding up queries.</p> <p>For example, most commonly, filtering is done on the event time column of a feature group when generating training data or batches of data: <pre><code>query = fg.select_all()\n\n# create a simple feature view\nfv = fs.create_feature_view(\n    name='transactions_view',\n    query=query\n)\n\n# set up dates\nstart_time = \"2022-01-01\"\nend_time = \"2022-06-30\"\n\n# create a training dataset\nversion, job = feature_view.create_training_data(\n    start_time=start_time,\n    end_time=end_time,\n    description='Description of a dataset',\n)\n</code></pre></p> <p>Assuming the feature group was partitioned by a daily event time column, for example, the features are updated with a daily batch job, the feature store will only have to list and read the files in the directories of those six months that are being queried.</p> <p>Too granular event time columns</p> <p>An event time column which is too granular, such as a timestamp, shouldn't be used as partition key. For example, a streaming pipeline generating features where the event time includes seconds, and therefore almost all event timestamps are unique can lead to many partition directories and small files, each of which contains only a few number of rows, which are inefficient to query even with pushed down filters.</p> <p>A good practice are partition keys with at most daily granularity, if they are based on time. Additionally, one can look at the size of a partition directory, which should be in the 100s of MB.</p> <p>Additionally, if you are commonly training models for different categories of your data, you can add another level of partitioning for this. That is, if the query contains an additional filter: <pre><code>query = fg.select_all().filter(fg.country_code == \"US\")\n</code></pre></p> <p>The feature group can be created with the following partition key in order to push down filters also for the <code>country_code</code> category: <pre><code>fg = feature_store.create_feature_group(...\n    partition_key=['day', 'country_code'],\n    event_time='day',\n)\n</code></pre></p>"},{"location":"user_guides/fs/feature_group/create/#parquet-file-size-within-a-feature-group-partition","title":"Parquet file size within a feature group partition","text":"<p>Once you have decided on the feature group level partitioning and you start inserting data to the feature group, there are multiple ways in order to influence how the table format (Hudi or Delta) will split the data between parquet files within the feature group partitions. The two things that influence the number of parquet files per partition are</p> <ol> <li>The number of feature group partitions written in a single insert</li> <li>The shuffle parallelism used by the table format</li> </ol> <p>For example, the inserted dataframe (unique combination of partition key values) will be parallelized according to the following Hudi settings:</p> <p>Default Hudi partitioning</p> <pre><code>write_options = {\n    'hoodie.bulkinsert.shuffle.parallelism': 5,\n    'hoodie.insert.shuffle.parallelism': 5,\n    'hoodie.upsert.shuffle.parallelism': 5\n}\n</code></pre> <p>That means, using Spark, Hudi shuffles the data into five in-memory partitions, which each fill map to a task and finally a parquet file (see figure below). If the inserted Dataframe contains only a single feature group partition, this feature group partition will be written with five parquet files. If the inserted Dataframe contains multiple feature group partitions, the parquet files will be split among those partition, potentially more parquet files will be added.</p> <p> </p> Mapping in-memory partitions to tasks, workers, executors and feature group partition files for a feature group insert <p>Setting shuffle parallelism</p> <p>In practice that means the shuffle parallelism should be set equal to the number of feature group partitions in the inserted dataframe. This will create one parquet file per feature group partition, which in many cases is optimal.</p> <p>Theoretically, this rule holds up to a partition size of 2GB, which is the limit of Spark. However, one should bump this up accordingly already for smaller inputs. We recommend having shuffle parallelism <code>hoodie.[insert|upsert|bulkinsert].shuffle.parallelism</code> such that its at least input_data_size/500MB.</p> <p>You can change the write options on every insert, depending also on the size of the data you are writing: <pre><code>write_options = {\n    'hoodie.bulkinsert.shuffle.parallelism': 5,\n    'hoodie.insert.shuffle.parallelism': 5,\n    'hoodie.upsert.shuffle.parallelism': 5\n}\nfg.insert(df, write_options=write_options)\n</code></pre></p>"},{"location":"user_guides/fs/feature_group/create/#backfilling-of-feature-group-partitions","title":"Backfilling of feature group partitions","text":"<p>Hudi scales well with the number of partitions to write, when performing backfilling of old feature partitions, meaning moving backwards in time with the event-time, it makes sense to batch those feature group partitions together into a single <code>fg.insert()</code> call. As shown in the figure above, the number of utilised executors you choose for the insert depends highly on the number of partitions and shuffle parallelism you are writing. So by writing multiple feature group partitions in a single insert, you can scale up your Spark application and fully utilise the workers. In that case you can increase the Hudi shuffle parallelism accordingly.</p> <p>Concurrent feature group inserts</p> <p>Hopsworks 3.1 and earlier, currently does not support concurrent inserts to feature groups. This means that if your feature pipeline writes to one feature group partition at a time, you cannot run it multiple times in parallel for backfilling.</p> <p>The recommended approach is to unionise the dataframes and insert them with a single <code>fg.insert()</code> instead. For clients that write with the Stream API, it is enough to defer starting the backfill job until after multiple inserts, as described above.</p>"},{"location":"user_guides/fs/feature_group/create/#the-choice-of-topic-for-data-ingestion","title":"The choice of topic for data ingestion","text":"<p>When creating a feature group that uses streaming write APIs for data ingestion it is possible to define the Kafka topics that should be utilized. The default approach of using a project-wide topic functions great for use cases involving little to no overlap when producing data. However, concurrently inserting into multiple feature groups could cause read amplification for the offline materialization job (e.g., Hudi Delta Streamer). Therefore, it is advised to utilize separate topics when ingestions overlap or there is a large frequently running insertion into a specific feature group.</p>"},{"location":"user_guides/fs/feature_group/create/#register-the-metadata-and-save-the-feature-data","title":"Register the metadata and save the feature data","text":"<p>The snippet above only created the metadata object on the Python interpreter running the code. To register the feature group metadata and to save the feature data with Hopsworks, you should invoke the <code>insert</code> method:</p> <pre><code>fg.insert(df)\n</code></pre> <p>The save method takes in input a Pandas, Polars or Spark DataFrame. HSFS will use the DataFrame columns and types to determine the name and types of features, primary key, partition key and event time.</p> <p>The DataFrame must contain the columns specified as primary keys, partition key and event time in the <code>create_feature_group</code> call.</p> <p>If a feature group is online enabled, the <code>insert</code> method will store the feature data to both the online and offline storage.</p>"},{"location":"user_guides/fs/feature_group/create/#api-reference","title":"API Reference","text":"<p>FeatureGroup</p>"},{"location":"user_guides/fs/feature_group/create/#create-using-the-ui","title":"Create using the UI","text":"<p>You can also create a new feature group through the UI. For this, navigate to the <code>Feature Groups</code> section and press the <code>Create</code> button at the top-right corner.</p> <p> </p> <p>Subsequently, you will be able to define its properties (such as name, mode, features, and more). Refer to the documentation above for an explanation of the parameters available, they are the same as when you create a feature group using the SDK. Finally, complete the creation by clicking <code>Create New Feature Group</code> at the bottom of the page.</p> <p> </p>"},{"location":"user_guides/fs/feature_group/create_external/","title":"How to create an External Feature Group","text":""},{"location":"user_guides/fs/feature_group/create_external/#introduction","title":"Introduction","text":"<p>In this guide you will learn how to create and register an external feature group with Hopsworks. This guide covers creating an external feature group using the HSFS APIs as well as the user interface.</p>"},{"location":"user_guides/fs/feature_group/create_external/#prerequisites","title":"Prerequisites","text":"<p>Before you begin this guide we suggest you read the External Feature Group concept page to understand what a feature group is and how it fits in the ML pipeline.</p>"},{"location":"user_guides/fs/feature_group/create_external/#create-using-the-hsfs-apis","title":"Create using the HSFS APIs","text":""},{"location":"user_guides/fs/feature_group/create_external/#retrieve-the-storage-connector","title":"Retrieve the storage connector","text":"<p>To create an external feature group using the HSFS APIs you need to provide an existing storage connector.</p> Python <pre><code>connector = feature_store.get_storage_connector(\"connector_name\")\n</code></pre>"},{"location":"user_guides/fs/feature_group/create_external/#create-an-external-feature-group","title":"Create an External Feature Group","text":"<p>The first step is to instantiate the metadata through the <code>create_external_feature_group</code> method. Once you have defined the metadata, you can persist the metadata and create the feature group in Hopsworks by calling <code>fg.save()</code>.</p>"},{"location":"user_guides/fs/feature_group/create_external/#sql-based-external-feature-group","title":"SQL based external feature group","text":"Python <pre><code>query = \"\"\"\n    SELECT TO_NUMERIC(ss_store_sk) AS ss_store_sk\n        , AVG(ss_net_profit) AS avg_ss_net_profit\n        , SUM(ss_net_profit) AS total_ss_net_profit\n        , AVG(ss_list_price) AS avg_ss_list_price\n        , AVG(ss_coupon_amt) AS avg_ss_coupon_amt\n        , sale_date\n        , ss_store_sk\n    FROM STORE_SALES\n    GROUP BY ss_store_sk, sales_date\n\"\"\"\n\nfg = feature_store.create_external_feature_group(name=\"sales\",\n    version=1,\n    description=\"Physical shop sales features\",\n    query=query,\n    storage_connector=connector,\n    primary_key=['ss_store_sk'],\n    event_time='sale_date'\n)\n\nfg.save()\n</code></pre>"},{"location":"user_guides/fs/feature_group/create_external/#data-lake-based-external-feature-group","title":"Data Lake based external feature group","text":"Python <pre><code>fg = feature_store.create_external_feature_group(name=\"sales\",\n    version=1,\n    description=\"Physical shop sales features\",\n    data_format=\"parquet\",\n    storage_connector=connector,\n    primary_key=['ss_store_sk'],\n    event_time='sale_date'\n)\n\nfg.save()\n</code></pre> <p>The full method documentation is available here. <code>name</code> is a mandatory parameter of the <code>create_external_feature_group</code> and represents the name of the feature group.</p> <p>The version number is optional, if you don't specify the version number the APIs will create a new version by default with a version number equals to the highest existing version number plus one.</p> <p>If the storage connector is defined for a data warehouse (e.g. JDBC, Snowflake, Redshift) you need to provide a SQL statement that will be executed to compute the features. If the storage connector is defined for a data lake, the location of the data as well as the format need to be provided.</p> <p>Additionally we specify which columns of the DataFrame will be used as primary key, and event time. Composite primary keys are also supported.</p>"},{"location":"user_guides/fs/feature_group/create_external/#register-the-metadata","title":"Register the metadata","text":"<p>In the snippet above it's important that the created metadata object gets registered in Hopsworks. To do so, you should invoke the <code>save</code> method:</p> Python <pre><code>fg.save()\n</code></pre>"},{"location":"user_guides/fs/feature_group/create_external/#enable-online-storage","title":"Enable online storage","text":"<p>You can enable online storage for external feature groups, however, the sync from the external storage to Hopsworks online storage is not automatic and needs to be setup manually. For an external feature group to be available online, during the creation of the feature group, the <code>online_enabled</code> option needs to be set to <code>True</code>.</p> Python <pre><code>external_fg = fs.create_external_feature_group(\n            name=\"sales\",\n            version=1,\n            description=\"Physical shop sales features\",\n            query=query,\n            storage_connector=connector,\n            primary_key=['ss_store_sk'],\n            event_time='sale_date',\n            online_enabled=True)\nexternal_fg.save()\n\n# read from external storage and filter data to sync to online\ndf = external_fg.read().filter(external_fg.customer_status == \"active\")\n\n# insert to online storage\nexternal_fg.insert(df)\n</code></pre> <p>The <code>insert()</code> method takes a DataFrame as parameter and writes it only to the online feature store. Users can select which subset of the feature group data they want to make available on the online feature store by using the query APIs.</p>"},{"location":"user_guides/fs/feature_group/create_external/#limitations","title":"Limitations","text":"<p>Hopsworks Feature Store does not support time-travel queries on external feature groups.</p> <p>Additionally, support for <code>.read()</code> and <code>.show()</code> methods when using by the Python engine is limited to external feature groups defined on BigQuery and Snowflake and only when using the Feature Query Service. Nevertheless, external feature groups defined top of any storage connector can be used to create a training dataset from a Python environment invoking one of the following methods: create_training_data, create_train_test_split or the create_train_validation_test_split</p>"},{"location":"user_guides/fs/feature_group/create_external/#api-reference","title":"API Reference","text":"<p>External FeatureGroup</p>"},{"location":"user_guides/fs/feature_group/create_external/#create-using-the-ui","title":"Create using the UI","text":"<p>You can also create a new feature group through the UI. For this, navigate to the <code>Feature Groups</code> section and press the <code>Create</code> button at the top-right corner.</p> <p> </p> <p>Subsequently, you will be able to define its properties (such as name, mode, features, and more). Refer to the documentation above for an explanation of the parameters available, they are the same as when you create a feature group using the SDK. Finally, complete the creation by clicking <code>Create New Feature Group</code> at the bottom of the page.</p> <p> </p>"},{"location":"user_guides/fs/feature_group/create_spine/","title":"How to create Spine Group","text":""},{"location":"user_guides/fs/feature_group/create_spine/#introduction","title":"Introduction","text":"<p>In this guide you will learn how to create and register a Spine Group with Hopsworks.</p>"},{"location":"user_guides/fs/feature_group/create_spine/#prerequisites","title":"Prerequisites","text":"<p>Before you begin this guide we suggest you read the Spine Group concept page to understand what a Spine Group is and how it fits in the ML pipeline.</p>"},{"location":"user_guides/fs/feature_group/create_spine/#create-using-the-hsfs-apis","title":"Create using the HSFS APIs","text":""},{"location":"user_guides/fs/feature_group/create_spine/#create-a-spine-group","title":"Create a Spine Group","text":"<p>Instead of using a feature group to save the label, you can also use a spine to use a Dataframe containing the labels on the fly. A spine is essentially a metadata object similar to a Feature Group, which tells the feature store the relevant event time column and primary key columns to perform point-in-time correct joins.</p> <p>Additionally, apart from primary key and event time information, a Spark dataframe is required in order to infer the schema of the group from.</p> Python <pre><code>trans_spine = fs.get_or_create_spine_group(\n    name=\"spine_transactions\",\n    version=1,\n    description=\"Transaction data\",\n    primary_key=['cc_num'],\n    event_time='datetime',\n    dataframe=trans_df\n)\n</code></pre> <p>Once created, note that you can inspect the dataframe in the Spine Group:</p> Python <pre><code>trans_spine.dataframe.show()\n</code></pre> <p>And you can always also replace the dataframe contained within the Spine Group. You just need to make sure it has the same schema.</p> Python <pre><code>trans_spine.dataframe = new_df\n</code></pre>"},{"location":"user_guides/fs/feature_group/create_spine/#limitations","title":"Limitations","text":"<p>Python support</p> <p>Currently the HSFS library does not support usage of Spine Groups for training data creation or batch data retrieval in the Python engine. However, it is supported to create Spine Groups from the Python engine.</p>"},{"location":"user_guides/fs/feature_group/create_spine/#api-reference","title":"API Reference","text":"<p>SpineGroup</p>"},{"location":"user_guides/fs/feature_group/data_types/","title":"How to manage schema and feature data types","text":""},{"location":"user_guides/fs/feature_group/data_types/#introduction","title":"Introduction","text":"<p>In this guide, you will learn how to manage the feature group schema and control the data type of the features in a feature group.</p>"},{"location":"user_guides/fs/feature_group/data_types/#prerequisites","title":"Prerequisites","text":"<p>Before you begin this guide we suggest you read the Feature Group concept page to understand what a feature group is and how it fits in the ML pipeline. We also suggest you familiarize yourself with the APIs to create a feature group.</p>"},{"location":"user_guides/fs/feature_group/data_types/#feature-group-schema","title":"Feature group schema","text":"<p>When a feature is stored in both the online and offline feature stores, it will be stored in a data type native to each store.</p> <ul> <li>Offline data type: The data type of the feature when stored on the offline feature store. The offline feature store is based on Apache Hudi and Hive Metastore, as such,   Hive Data Types can be leveraged.</li> <li>Online data type: The data type of the feature when stored on the online feature store. The online storage is based on RonDB and hence,   MySQL Data Types can be leveraged.</li> </ul> <p>The offline data type is always required, even if the feature group is stored only online. On the other hand, if the feature group is not online_enabled, its features will not have an online data type.</p> <p>The offline and online types for each feature are automatically inferred from the Spark or Pandas types of the input DataFrame as outlined in the following two sections. The default mapping, however, can be overwritten by using an explicit schema definition.</p>"},{"location":"user_guides/fs/feature_group/data_types/#offline-data-types","title":"Offline data types","text":"<p>When registering a Spark DataFrame in a PySpark environment (S), or a Pandas DataFrame, or a Polars DataFrame in a Python-only environment (P) the following default mapping to offline feature types applies:</p> Spark Type (S) Pandas Type (P) Polars Type (P) Offline Feature Type Remarks BooleanType bool, object(bool) Boolean BOOLEAN ByteType int8, Int8 Int8 TINYINT or INT INT when time_travel_type=\"HUDI\" ShortType uint8, int16, Int16 UInt8, Int16 SMALLINT or INT INT when time_travel_type=\"HUDI\" IntegerType uint16, int32, Int32 UInt16, Int32 INT LongType int, uint32, int64, Int64 UInt32, Int64 BIGINT FloatType float, float16, float32 Float32 FLOAT DoubleType float64 Float64 DOUBLE DecimalType decimal.decimal Decimal DECIMAL(PREC, SCALE) Not supported in PO env. when time_travel_type=\"HUDI\" TimestampType datetime64[ns], datetime64[ns, tz] Datetime TIMESTAMP s. Timestamps and Timezones DateType object (datetime.date) Date DATE StringType object (str), object(np.unicode) String, Utf8 STRING ArrayType object (list), object (np.ndarray) List ARRAY&lt;TYPE&gt; StructType object (dict) Struct STRUCT&lt;NAME: TYPE, ...&gt; BinaryType object (binary) Binary BINARY MapType - - MAP&lt;String,TYPE&gt; Only when time_travel_type!=\"HUDI\"; Only string keys permitted <p>When registering a Pandas DataFrame in a PySpark environment (S) the Pandas DataFrame is first converted to a Spark DataFrame, using Spark's default conversion. It results in a less fine-grained mapping between Python and Spark types:</p> Pandas Type (S) Spark Type Remarks bool BooleanType int8, uint8, int16, uint16, int32, int, uint32, int64 LongType float, float16, float32, float64 DoubleType object (decimal.decimal) DecimalType datetime64[ns], datetime64[ns, tz] TimestampType s. Timestamps and Timezones object (datetime.date) DateType object (str), object(np.unicode) StringType object (list), object (np.ndarray) - Not supported object (dict) StructType object (binary) BinaryType"},{"location":"user_guides/fs/feature_group/data_types/#online-data-types","title":"Online data types","text":"<p>The online data type is determined based on the offline type according to the following mapping, regardless of which environment the data originated from. Only a subset of the data types can be used as primary key, as indicated in the table as well:</p> Offline Feature Type Online Feature Type Primary Key Remarks BOOLEAN TINYINT x TINYINT TINYINT x SMALLINT SMALLINT x INT INT x Also supports: TINYINT, SMALLINT BIGINT BIGINT x FLOAT FLOAT DOUBLE DOUBLE DECIMAL(PREC, SCALE) DECIMAL(PREC, SCALE) e.g. DECIMAL(38, 18) TIMESTAMP TIMESTAMP s. Timestamps and Timezones DATE DATE x STRING VARCHAR(100) x Also supports: TEXT ARRAY&lt;TYPE&gt; VARBINARY(100) x Also supports: BLOB STRUCT&lt;NAME: TYPE, ...&gt; VARBINARY(100) x Also supports: BLOB BINARY VARBINARY(100) x Also supports: BLOB MAP&lt;String,TYPE&gt; VARBINARY(100) x Also supports: BLOB <p>More on how Hopsworks handles string types,  complex data types and the online restrictions for primary keys and row size in the following sections.</p>"},{"location":"user_guides/fs/feature_group/data_types/#string-online-data-types","title":"String online data types","text":"<p>String types are stored as VARCHAR(100) by default. This type is fixed-size, meaning it can only hold as many characters as specified in the argument (e.g. VARCHAR(100) can hold up to 100 unicode characters). The size should thus be within the maximum string length of the input data. Furthermore, the VARCHAR size has to be in line with the online restrictions for row size.</p> <p>If the string size exceeds 100 characters, a larger type (e.g. VARCHAR(500)) can be specified via an explicit schema definition. If the string size is unknown or if it exceeds the maximum row size, then the TEXT type can be used instead.</p> <p>String data that exceeds the specified VARCHAR size will lead to an error when data gets written to the online feature store. When in doubt, use the TEXT type instead, but note that it comes with a potential performance overhead.</p>"},{"location":"user_guides/fs/feature_group/data_types/#complex-online-data-types","title":"Complex online data types","text":"<p>Hopsworks allows users to store complex types (e.g. ARRAY) in the online feature store. Hopsworks serializes the complex features transparently and stores them as VARBINARY in the online feature store. The serialization happens when calling the save(), insert() or insert_stream() methods. The deserialization will be executed when calling the get_serving_vector() method to retrieve data from the online feature store. If users query directly the online feature store, for instance using the <code>fs.sql(\"SELECT ...\", online=True)</code> statement, it will return a binary blob. <p>On the feature store UI, the online feature type for complex features will be reported as VARBINARY.</p> <p>If the binary size exceeds 100 bytes, a larger type (e.g. VARBINARY(500)) can be specified via an explicit schema definition. If the binary size is unknown of if it exceeds the maximum row size, then the BLOB type can be used instead.</p> <p>Binary data that exceeds the specified VARBINARY size will lead to an error when data gets written to the online feature store. When in doubt, use the BLOB type instead, but note that it comes with a potential performance overhead.</p>"},{"location":"user_guides/fs/feature_group/data_types/#online-restrictions-for-primary-key-data-types","title":"Online restrictions for primary key data types","text":"<p>When a feature is being used as a primary key, certain types are not allowed. Examples of such types are FLOAT, DOUBLE, TEXT and BLOB. Additionally, the size of the sum of the primary key online data types storage requirements should not exceed 4KB.</p>"},{"location":"user_guides/fs/feature_group/data_types/#online-restrictions-for-row-size","title":"Online restrictions for row size","text":"<p>The online feature store supports up to 500 columns and all column types combined should not exceed 30000 Bytes. The byte size of each column is determined by its data type and calculated as follows:</p> Online Data Type Byte Size TINYINT 1 SMALLINT 2 INT 4 BIGINT 8 FLOAT 4 DOUBLE 8 DECIMAL(PREC, SCALE) 16 TIMESTAMP 8 DATE 8 VARCHAR(LENGTH) LENGTH * 4 VARCHAR(LENGTH) charset latin1; LENGTH * 1 TEXT 256 VARBINARY(LENGTH) LENGTH / 1.4 BLOB 256 other 8"},{"location":"user_guides/fs/feature_group/data_types/#pre-insert-schema-validation-for-online-feature-groups","title":"Pre-insert schema validation for online feature groups","text":"<p>For online enabled feature groups, the dataframe to be ingested needs to adhere to the online schema definitions. The input dataframe is validated for schema checks accordingly. The validation is enabled by default and can be disabled by setting below key word argument when calling <code>insert()</code></p> Python <pre><code>feature_group.insert(df, validation_options={'online_schema_validation':False})\n</code></pre> <p>The most important validation checks or error messages are mentioned below along with possible corrective actions. </p> <ol> <li> <p>Primary key contains null values </p> <ul> <li>Rule Primary key column should not contain any null values.</li> <li> <p>Example correction Drop the rows containing null primary keys. Alternatively, find the null values and assign them an unique value as per preferred strategy for data imputation.</p> Pandas <pre><code># Drop rows: assuming 'id' is the primary key column\ndf = df.dropna(subset=['id'])\n# For composite keys\ndf = df.dropna(subset=['id1', 'id2'])\n\n# Data imputation: replace null values with incrementing last interger id\n# existing max id \nmax_id = df['id'].max()\n# counter to generate new id\nnext_id = max_id + 1\n# for each null id, assign the next id incrementally\nfor idx in df[df['id'].isna()].index:\n    df.loc[idx, 'id'] = next_id\n    next_id += 1\n</code></pre> </li> </ul> </li> <li> <p>Primary key column missing</p> <ul> <li>Rule The dataframe to be inserted must contain all the columns defined as primary key(s) in the feature group.</li> <li> <p>Example correction Add all the primary key columns in the dataframe.</p> Pandas <pre><code># increamenting primary key upto the length of dataframe\ndf['id'] = range(1, len(df) + 1)\n</code></pre> </li> </ul> </li> <li> <p>String length exceeded</p> <ul> <li>Rule The character length of a string should be within the maximum length capacity in the online schema type of a feature. If the feature group is not created and explicit feature schema was not provided, the limit will be auto-increased to the maximum length found in a string column in the dataframe. </li> <li> <p>Example correction</p> <ul> <li>Trim the string values to fit within maximum limit set during feature group creation.</li> </ul> Pandas <pre><code>max_length = 100\ndf['text_column'] = df['text_column'].str.slice(0, max_length)\n</code></pre> <ul> <li>Another option is to simply create new version of the feature group and insert the dataframe.</li> </ul> <p>Note</p> <p>The total row size limit should be less than 30kb as per row size restrictions. In such cases it is possible to define the feature as TEXT or BLOB. Below is an example of explicitly defining the string column as TEXT as online type.</p> Pandas <pre><code>import pandas as pd\n# example dummy dataframe with the string column\ndf = pd.DataFrame(columns=['id', 'string_col'])\nfrom hsfs.feature import Feature\nfeatures = [\nFeature(name=\"id\",type=\"bigint\",online_type=\"bigint\"),\nFeature(name=\"string_col\",type=\"string\",online_type=\"text\")\n]\n\nfg = fs.get_or_create_feature_group(name=\"fg_manual_text_schema\",\n                            version=1,\n                            features=features,\n                            online_enabled=True,\n                            primary_key=['id'])\nfg.insert(df)\n</code></pre> </li> </ul> </li> </ol>"},{"location":"user_guides/fs/feature_group/data_types/#timestamps-and-timezones","title":"Timestamps and Timezones","text":"<p>All timestamp features are stored in Hopsworks in UTC time. Also, all timestamp-based functions (such as point-in-time joins) use UTC time. This ensures consistency of timestamp features across different client timezones and simplifies working with timestamp-based functions in general. When ingesting timestamp features, the Feature Store Write API will automatically handle the conversion to UTC, if necessary. The following table summarizes how different timestamp types are handled:</p> Data Frame (Data Type) Environment Handling Pandas DataFrame (datetime64[ns]) Python-only and PySpark interpreted as UTC, independent of the client's timezone Pandas DataFrame (datetime64[ns, tz]) Python-only and PySpark timezone-sensitive conversion from 'tz' to UTC Spark (TimestampType) PySpark and Spark interpreted as UTC, independent of the client's timezone <p>Timestamp features retrieved from the Feature Store, e.g. using the Feature Store Read API, use a timezone-unaware format:</p> Data Frame (Data Type) Environment Timezone Pandas DataFrame (datetime64[ns]) Python-only timezone-unaware (UTC) Spark (TimestampType) PySpark and Spark timezone-unaware (UTC) <p>Note that our PySpark/Spark client automatically sets the Spark SQL session's timezone to UTC. This ensures that Spark SQL will correctly interpret all timestamps as UTC. The setting will only apply to the client's session, and you don't have to worry about setting/unsetting the configuration yourself.</p>"},{"location":"user_guides/fs/feature_group/data_types/#explicit-schema-definition","title":"Explicit schema definition","text":"<p>When creating a feature group it is possible for the user to control both the offline and online data type of each column. If users explicitly define the schema for the feature group, Hopsworks is going to use that schema to create the feature group, without performing any type mapping. You can explicitly define the feature group schema as follows:</p> Python <pre><code>from hsfs.feature import Feature\n\nfeatures = [\n    Feature(name=\"id\",type=\"int\",online_type=\"int\"),\n    Feature(name=\"name\",type=\"string\",online_type=\"varchar(20)\")\n]\n\nfg = fs.create_feature_group(name=\"fg_manual_schema\",\n                             features=features,\n                             online_enabled=True)\nfg.save(features)\n</code></pre>"},{"location":"user_guides/fs/feature_group/data_types/#append-features-to-existing-feature-groups","title":"Append features to existing feature groups","text":"<p>Hopsworks supports appending additional features to an existing feature group. Adding additional features to an existing feature group is not considered a breaking change.</p> Python <pre><code>from hsfs.feature import Feature\n\nfeatures = [\n    Feature(name=\"id\",type=\"int\",online_type=\"int\"),\n    Feature(name=\"name\",type=\"string\",online_type=\"varchar(20)\")\n]\n\nfg = fs.get_feature_group(name=\"example\", version=1)\nfg.append_features(features)\n</code></pre> <p>When adding additional features to a feature group, you can provide a default values for existing entries in the feature group. You can also backfill the new features for existing entries by running an <code>insert()</code> operation and update all existing combinations of primary key - event time.</p>"},{"location":"user_guides/fs/feature_group/data_validation/","title":"Data Validation","text":"Validation on Insertion with Hopsworks and Great Expectations."},{"location":"user_guides/fs/feature_group/data_validation/#introduction","title":"Introduction","text":"<p>Clean, high quality feature data is of paramount importance to being able to train and serve high quality models. Hopsworks offers integration with Great Expectations to enable a smooth data validation workflow. This guide is designed to help you integrate a data validation step when inserting new DataFrames into a Feature Group. Note that validation is performed inline as part of your feature pipeline (on the client machine) - it is not executed by Hopsworks after writing features.</p>"},{"location":"user_guides/fs/feature_group/data_validation/#ui","title":"UI","text":""},{"location":"user_guides/fs/feature_group/data_validation/#create-a-feature-group-pre-requisite","title":"Create a Feature Group (Pre-requisite)","text":"<p>In the UI, you must create a Feature Group first before attaching an Expectation Suite. You can find out more information about creating a Feature Group here. You can attach at most one expectation suite to a Feature Group. Data validation is an optional step and is not required to write to a Feature Group.</p>"},{"location":"user_guides/fs/feature_group/data_validation/#step-1-find-and-edit-feature-group","title":"Step 1: Find and Edit Feature Group","text":"<p>Click on the Feature Group section in the navigation menu. Find your Feature Group in the list and click on its name to access the Feature Group page. Select <code>edit</code> in the top right corner or scroll to the Expectations section and click on <code>Edit Expectation Suite</code>.</p>"},{"location":"user_guides/fs/feature_group/data_validation/#step-2-edit-general-expectation-suite-settings","title":"Step 2: Edit General Expectation Suite Settings","text":"<p>Scroll to the Expectation Suite section. Click add Expectation Suite and edit its metadata:</p> <ul> <li>Choose a name for your expectation suite.</li> <li>Checkbox enabled. This controls whether the Expectation Suite will be used to validate a Dataframe automatically upon insertion into a Feature Group. Note that validation is executed by the client. Disabling validation allows you to skip the validation step without deleting the Expectation Suite.</li> <li>'ALWAYS' vs. 'STRICT' mode. This option controls what happens after validation. Hopsworks defaults to 'ALWAYS', where data is written to the Feature Group regardless of the validation result. This means that even if expectations are failing or throw an exception, Hopsworks will attempt to insert the data into the Feature Group. In 'STRICT' mode, Hopsworks will only write data to the Feature Group if each individual expectation has been successful.</li> </ul>"},{"location":"user_guides/fs/feature_group/data_validation/#step-3-add-new-expectations","title":"Step 3: Add new expectations","text":"<p>By clicking on <code>Add expectation</code> one can choose an expectation type from a searchable dropdown menu. Currently, only the built-in expectations from the Great Expectations framework are supported. For user-defined expectations, please use the Rest API or python client.</p> <p>All default kwargs associated to the selected expectation type are populated as a json below the dropdown menu. Edit the arguments in the json to configure the Expectation. In particular, arguments such as <code>column</code>, <code>columnA</code>, <code>columnB</code>, <code>column_set</code> and <code>column_list</code> require valid feature name(s). Click the tick button to save the expectation configuration and append it to the Expectation Suite locally.</p> <p>Info</p> <p>Click the <code>Save feature group</code> button to persist your changes!</p> <p>You can use the button <code>Clear Expectation Suite</code> to clean up before saving changes if you changed your mind. If the Expectation Suite is already registered, it will instead show a button to delete the Expectation Suite.</p>"},{"location":"user_guides/fs/feature_group/data_validation/#step-4-save-new-data-to-a-feature-group","title":"Step 4: Save new data to a Feature Group","text":"<p>Use the python client to write a DataFrame to the Feature Group. Note that if an expectation suite is enabled for a Feature Group, calling the <code>insert</code> method will run validation and default to uploading the corresponding validation report to Hopsworks. The report is uploaded even if validation fails and 'STRICT' mode is selected.</p>"},{"location":"user_guides/fs/feature_group/data_validation/#step-5-check-validation-results-summary","title":"Step 5: Check Validation Results Summary","text":"<p>Hopsworks shows a visual summary of validation reports. To check it out, go to your Feature Group overview and scroll to the expectation section. Click on the <code>Validation Results</code> tab and check that all went according to plan. Each row corresponds to an expectation in the suite. Features can have several corresponding expectations and the same type of expectation can be applied to different features.</p> <p>You can navigate to older reports using the dropdown menu. Should you need more than the information displayed in the UI for e.g., debugging, the full report can be downloaded by clicking on the corresponding button.</p>"},{"location":"user_guides/fs/feature_group/data_validation/#step-6-check-validation-history","title":"Step 6: Check Validation History","text":"<p>The <code>Validation Reports</code> tab in the Expectations section displays a brief history of recent validations. Each row corresponds to a validation report, with some summary information about the success of the validation step. You can download the full report by clicking the download icon button that appears at the end of the row.</p>"},{"location":"user_guides/fs/feature_group/data_validation/#code","title":"Code","text":"<p>Hopsworks python client interfaces with the Great Expectations library to enable you to add data validation to your feature engineering pipeline. In this section, we show you how in a single line you enable automatic validation on each insertion of new data into your Feature Group. Whether you have an existing Feature Group you want to add validation to or Follow the guide or get your hands dirty by running our tutorial data validation notebook in google colab.</p> <p>First checkout the pre-requisite and Hopsworks setup to follow the guide below. Create a project, install the hopsworks client and connect via the generated API key. You are ready to load your data in a DataFrame. The second step is a short introduction to the relevant Great Expectations API to build data validation suited to your data. Third and final step shows how to attach your Expectation Suite to the Feature Group to benefit from automatic validation on insertion capabilities.</p>"},{"location":"user_guides/fs/feature_group/data_validation/#step-1-pre-requisite","title":"Step 1: Pre-requisite","text":"<p>In order to define and validate an expectation when writing to a Feature Group, you will need:</p> <ul> <li>A Hopsworks project. If you don't have a project yet you can go to app.hopsworks.ai, signup with your email and create your first project.</li> <li>An API key, you can get one by going to \"Account Settings\" on app.hopsworks.ai.</li> <li>The Hopsworks Python library installed in your client. See the installation guide.</li> </ul>"},{"location":"user_guides/fs/feature_group/data_validation/#connect-your-notebook-to-hopsworks","title":"Connect your notebook to Hopsworks","text":"<p>Connect the client running your notebooks to Hopsworks.</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nfs = project.get_feature_store()\n</code></pre> <p>You will be prompt to paste your API key to connect the notebook to your project. The <code>fs</code> Feature Store entity is now ready to be used to insert or read data from Hopsworks.</p>"},{"location":"user_guides/fs/feature_group/data_validation/#import-your-data","title":"Import your data","text":"<p>Load your data in a DataFrame using the usual pandas API.</p> <pre><code>import pandas as pd\n\ndf = pd.read_csv(\"https://repo.hops.works/master/hopsworks-tutorials/data/card_fraud_data/transactions.csv\", parse_dates=[\"datetime\"])\n\ndf.head(3)\n</code></pre>"},{"location":"user_guides/fs/feature_group/data_validation/#step-2-great-expectation-introduction","title":"Step 2: Great Expectation Introduction","text":"<p>To validate the data, we will use the Great Expectations library. Below is a short introduction on how to build an Expectation Suite to validate your data. Everything is done using the Great Expectations API so you can re-use any prior knowledge you may have of the library.</p>"},{"location":"user_guides/fs/feature_group/data_validation/#create-an-expectation-suite","title":"Create an Expectation Suite","text":"<p>Create (or import an existing) expectation suite using the Great Expectations library. This suite will hold all the validation tests we want to perform on our data before inserting them into Hopsworks.</p> <pre><code>import great_expectations as ge\n\nexpectation_suite = ge.core.ExpectationSuite(\n    expectation_suite_name=\"validate_on_insert_suite\"\n)\n</code></pre>"},{"location":"user_guides/fs/feature_group/data_validation/#add-expectations-in-the-source-code","title":"Add Expectations in the Source Code","text":"<p>Add some expectation to your suite. Each expectation configuration corresponds to a validation test to be run against your data.</p> <pre><code>expectation_suite.add_expectation(\n    ge.core.ExpectationConfiguration(\n        expectation_type=\"expect_column_min_to_be_between\",\n        kwargs={\n            \"column\": \"foo_id\",\n            \"min_value\": 0,\n            \"max_value\": 1\n        }\n    )\n)\n\nexpectation_suite.add_expectation(\n    ge.core.ExpectationConfiguration(\n        expectation_type=\"expect_column_value_lengths_to_be_between\",\n        kwargs={\n            \"column\": \"bar_name\",\n            \"min_value\": 3,\n            \"max_value\": 10\n        }\n    )\n)\n</code></pre>"},{"location":"user_guides/fs/feature_group/data_validation/#using-great-expectations-profiler","title":"Using Great Expectations Profiler","text":"<p>Building Expectation Suite by hand can be a major time commitment when you have dozens of Features. Great Expectations offers <code>Profiler</code> classes to inspect a sample of your data and infers a suitable Expectation Suite that you will be able to register with Hopsworks.</p> <pre><code>ge_profiler = ge.profile.BasicSuiteBuilderProfiler()\nexpectation_suite_profiler, _ = ge_profiler.profile(ge.from_pandas(df))\n</code></pre> <p>Once you have built an Expectation Suite you are satisfied with, it is time to create your first validation enabled Feature Group.</p>"},{"location":"user_guides/fs/feature_group/data_validation/#step-3-attach-an-expectation-suite-to-your-feature-group-to-enable-automatic-validation-on-insertion","title":"Step 3: Attach an Expectation Suite to your Feature Group to enable Automatic Validation on Insertion.","text":"<p>Writing data in Hopsworks is done using Feature Groups. Once a Feature Group is registered in the Feature Store, you can use it to insert your pandas DataFrames. For more information see create Feature Group. To benefit from automatic validation on insertion, attach your newly created Expectation Suite when creating the Feature Group:</p> <pre><code>fg = fs.create_feature_group(\n  \"fg_with_data_validation\",\n  version=1,\n  description=\"Validated data\",\n  primary_key=['foo_id'],\n  online_enabled=False\n  expectation_suite=expectation_suite\n)\n</code></pre> <p>or, if the Feature Group already exist, you can simply run:</p> <pre><code>fg.save_expectation_suite(expectation_suite)\n</code></pre> <p>That is all there is to it. Hopsworks will now automatically use your suite to validate the DataFrames you want to write to the Feature Group. Try it out!</p> <pre><code>job, validation_report = fg.insert(df.head(5))\n</code></pre> <p>As you can see, Hopsworks runs the validation in the client before attempting to insert the data. By default, Hopsworks will try to insert the data even if validation fails to prevent data loss. However it can be configured for production setup to be more restrictive, checkout the data validation advanced guide.</p> <p>Info</p> <p>Note that once the Expectation Suite is attached to the Feature Group, any subsequent attempt to insert to this Feature Group will apply the Data Validation step even from a different client or in a scheduled job.</p>"},{"location":"user_guides/fs/feature_group/data_validation/#step-4-data-quality-monitoring","title":"Step 4: Data Quality Monitoring","text":"<p>Upon running validation, Great Expectations generates a report to help you assess the quality of your data. Nothing to do here, Hopsworks client automatically uploads the validation report to the backend when ingesting new data. It enables you to monitor the quality of the inserted data in the Feature Group over time.</p> <p>You can checkout a summary of the reports in the UI on your Feature Group page. As you can see, your Feature Group conveniently gather all in one place: your data, the Expectation Suite and the reports generated each time you inserted data!</p> <p>Hopsworks client API allows you to retrieve validation reports for further analysis.</p> <pre><code># load multiple reports\nvalidation_reports = fg.get_validation_reports()\n\n# convenience method for rapid development\nge_latest_report = fg.get_latest_validation_report()\n</code></pre> <p>Similarly you can retrieve the historic of validation results for a particular expectation, e.g to plot a time-series of a given expectation observed value over time.</p> <pre><code>validation_history = fg.get_validation_history(\n    expectationId=1\n)\n</code></pre> <p>You can find the expectationIds in the UI or using <code>fg.get_expectation_suite</code> and looking it up in the expectation's meta field.</p> <p>Info</p> <p>If Validation Reports or Results are too long, they can be truncated to fit in the database. A full version of the reports can be downloaded from the UI.</p>"},{"location":"user_guides/fs/feature_group/data_validation/#conclusion","title":"Conclusion","text":"<p>The integration between Hopsworks and Great Expectations makes it simple to add a data validation step to your feature engineering pipeline. Build your Expectation Suite and attach it to your Feature Group with a single line of code. No need to add any code to your pipeline or job scripts, calling <code>fg.insert</code> will now automatically validate the data before inserting them in the Feature Group. The validation reports are stored along your data in Hopsworks allowing us to provide basic monitoring capabilities to quickly spot a data quality issue in the UI.</p>"},{"location":"user_guides/fs/feature_group/data_validation/#going-further","title":"Going Further","text":"<p>If you wish to find out more about how to use the data validation API or best practices for development or production pipelines in Hopsworks, checkout the advanced guide and best practices guide.</p>"},{"location":"user_guides/fs/feature_group/data_validation_advanced/","title":"Advanced Data Validation Options and Best Practices","text":"<p>The introduction to the data validation guide can be found here. The notebook example to get started with Data Validation in Hopsworks can be found here.</p>"},{"location":"user_guides/fs/feature_group/data_validation_advanced/#data-validation-configuration-options-in-hopsworks","title":"Data Validation Configuration Options in Hopsworks","text":""},{"location":"user_guides/fs/feature_group/data_validation_advanced/#validation-ingestion-policy","title":"Validation Ingestion Policy","text":"<p>Depending on your use case you can setup data validation as a monitoring or gatekeeping tool when trying to insert new data in your Feature Group. Switch behaviour by using the <code>validation_ingestion_policy</code> kwarg:</p> <ul> <li><code>\"ALWAYS\"</code> is the default option and will attempt to insert the data regardless of the validation result. Hassle free, it is ideal to monitor data ingestion in a development setup.</li> <li><code>\"STRICT\"</code> is the best option for production ready projects. This will prevent insertion of DataFrames which do not pass all data quality requirements. Ideal to avoid \"garbage-in, garbage-out\" scenarios, at the price of a potential loss of data. Check out the best practice section for more on that.</li> </ul>"},{"location":"user_guides/fs/feature_group/data_validation_advanced/#in-the-ui","title":"In the UI","text":"<p>Go to the Feature Group edit page, in the Expectation section you can choose between the options above.</p>"},{"location":"user_guides/fs/feature_group/data_validation_advanced/#in-the-python-client","title":"In the python client","text":"<pre><code>fg.expectation_suite.validation_ingestion_policy = \"ALWAYS\" # \"STRICT\"\n</code></pre> <p>If your suite is registered with Hopsworks, it will persist the change to the server.</p>"},{"location":"user_guides/fs/feature_group/data_validation_advanced/#disable-data-validation","title":"Disable Data Validation","text":"<p>Should you wish to do so, you can disable data validation on a punctual basis or until further notice.</p>"},{"location":"user_guides/fs/feature_group/data_validation_advanced/#in-the-ui_1","title":"In the UI","text":"<p>You can do it in the UI in the Expectation section of the Feature Group edit page. Simply tick or untick the enabled checkbox. This will be used as the default option but can be overriden via the API.</p>"},{"location":"user_guides/fs/feature_group/data_validation_advanced/#in-the-python-client_1","title":"In the python client","text":"<p>To disable data validation until further notice in the API, you can update the <code>run_validation</code> field of the expectation suite. If your suite is registered with Hopsworks, this will persist the change to the server.</p> <pre><code>fg.expectation_suite.run_validation = False\n</code></pre> <p>If you wish to override the default behaviour of the suite when inserting data in the Feature Group, you can do so via the <code>validate_options</code> kwarg. The example below will enable validation for this insertion only.</p> <pre><code>fg.insert(df_to_validate, validation_options={\"run_validation\" : True})\n</code></pre> <p>We recommend to avoid using this option in scheduled job as it silently changes the expected behaviour that is displayed in the UI and prevents changes to the default behaviour to change the behaviour of the job.</p>"},{"location":"user_guides/fs/feature_group/data_validation_advanced/#edit-expectations","title":"Edit Expectations","text":"<p>The one constant in life is change. If you need to add, remove or edit an expectation you can do it both in the UI or via the python client. Note that changing the expectation type or its corresponding feature will throw an error in order to preserve a meaningful validation history.</p>"},{"location":"user_guides/fs/feature_group/data_validation_advanced/#in-hopworks-ui","title":"In Hopworks UI","text":"<p>Go to the Feature Group edit page, in the expectation section. You can click on the expectation you want to edit and edit the json configuration. Check out Great Expectations documentation if you need more information on a particular expectation.</p>"},{"location":"user_guides/fs/feature_group/data_validation_advanced/#in-hopsworks-python-client","title":"In Hopsworks Python Client","text":"<p>There are several way to edit an Expectation in the python client. You can use Great Expectations API or directly go through Hopsworks. In the latter case, if you want to edit or remove an expectation, you will need the Hopsworks expectation ID. It can be found in the UI or in the meta field of an expectation. Note that you must have inserted data in the FG and attached the expectation suite to enable the Expectation API.</p> <p>Get an expectation with a given id:</p> <pre><code>my_expectation = fg.expectation_suite.get_expectation(\n    expectation_id = my_expectation_id\n)\n</code></pre> <p>Add a new expectation:</p> <pre><code>new_expectation = ge.core.ExpectationConfiguration(\n    expectation_type=\"expect_column_values_not_to_be_null\",\n    kwargs={\n        \"mostly\": 1\n    }\n)\n\nfg.expectation_suite.add_expectation(new_expectation)\n</code></pre> <p>Edit expectation kwargs of an existing expectation :</p> <pre><code>existing_expectation = fg.expectation_suite.get_expectation(\n    expectation_id=existing_expectation_id\n)\n\nexisting_expectation.kwargs[\"mostly\"] = 0.95\n\nfg.expectation_suite.replace_expectation(existing_expectation)\n</code></pre> <p>Remove an expectation:</p> <pre><code>fg.expectation_suite.remove_expectation(\n    expectation_id=id_of_expectation_to_delete\n)\n</code></pre> <p>If you want to deal only with the Great Expectation API:</p> <pre><code>my_suite = fg.get_expectation_suite()\n\nmy_suite.add_expectation(new_expectation)\n\nfg.save_expectation_suite(my_suite)\n</code></pre>"},{"location":"user_guides/fs/feature_group/data_validation_advanced/#save-validation-reports","title":"Save Validation Reports","text":"<p>When running validation using Great Expectations, a validation report is generated containing all validation results for the different expectations. Each result provides information about whether the provided DataFrame conforms to the corresponding expectation. These reports can be stored in Hopsworks to save a validation history for the data written to a particular Feature Group.</p> <p>The boilerplate of uploading report on insertion is taken care of by hopsworks, however for custom pipelines we provide an alternative method in the python client. The UI does not currently support upload of a validation report.</p>"},{"location":"user_guides/fs/feature_group/data_validation_advanced/#in-hopsworks-python-client_1","title":"In Hopsworks Python Client","text":"<pre><code>fg.save_validation_report(ge_report)\n</code></pre>"},{"location":"user_guides/fs/feature_group/data_validation_advanced/#monitor-and-fetch-validation-reports","title":"Monitor and Fetch Validation Reports","text":"<p>A summary of uploaded reports will then be available via an API call or in the Hopsworks UI enabling easy monitoring. For in-depth analysis, it is possible to download the complete report from the UI.</p>"},{"location":"user_guides/fs/feature_group/data_validation_advanced/#in-hopsworks-ui","title":"In Hopsworks UI","text":"<p>Open the Feature Group overview page and go to the Expectations section. One tab allows you to check the report history with general information, while the other tab allows you to explore a summary of the result for individual expectations.</p>"},{"location":"user_guides/fs/feature_group/data_validation_advanced/#in-hopsworks-python-client_2","title":"In Hopsworks Python Client","text":"<pre><code># convenience method for rapid development\nge_latest_report = fg.get_latest_validation_report()\n# fetching the latest summary prints a link to the UI\n# where you can download full report if summary is insufficient\n\n# or load multiple reports\nvalidation_history = fg.get_validation_reports()\n</code></pre>"},{"location":"user_guides/fs/feature_group/data_validation_advanced/#validate-your-data-manually","title":"Validate your data manually","text":"<p>While Hopsworks provides automatic validation on insertion logic, we recognise that some use cases may require a more fine-grained control over the validation process. Therefore, Feature Group objects offers a convenience wrapper around Great Expectations to manually trigger validation using the registered Expectation Suite.</p>"},{"location":"user_guides/fs/feature_group/data_validation_advanced/#in-the-ui_2","title":"In the UI","text":"<p>You can validate data already ingested in the Feature Group by going to the Feature Group overview page. In the top right corner is a button to trigger a validation. The button will launch a job which will read the Feature Group data, run validation and persist the associated report.</p>"},{"location":"user_guides/fs/feature_group/data_validation_advanced/#in-the-python-client_2","title":"In the python client","text":"<pre><code>ge_report = fg.validate(df, ingestion_result=\"EXPERIMENT\")\n\n# set the save_report parameter to False to skip uploading the report to Hopsworks\n# ge_report = fg.validate(df, save_report=False)\n</code></pre> <p>If you want to apply validation to the data already in the Feature Group you can call the <code>.validate</code> without providing data. It will read the data in the Feature Group.</p> <pre><code>report = fg.validate()\n</code></pre> <p>As validation objects returned by Hopsworks are native Great Expectation objects you can run validation using the usual Great Expectations syntax:</p> <pre><code>ge_df = ge.from_pandas(df, expectation_suite=fg.get_expectation_suite())\nge_report = ge_df.validate()\n</code></pre> <p>Note that you should always use an expectation suite that has been saved to Hopsworks if you intend to upload the associated validation report.</p>"},{"location":"user_guides/fs/feature_group/data_validation_best_practices/","title":"Best practices","text":"<p>Below is a set of recommendations and code snippets to help our users follow best practices when it comes to integrating a data validation step in your feature engineering pipelines. Rather than being prescriptive, we want to showcase how the API and configuration options can help adapt validation to your use-case.</p>"},{"location":"user_guides/fs/feature_group/data_validation_best_practices/#development","title":"Development","text":"<p>Data validation is generally considered to be a production-only feature and as such is often only setup once a project has reached the end of the development phase. At Hopsworks, we think there is a lot of value in setting up validation during early development. That's why we made it quick to get started and ensured that by default data validation is never an obstacle to inserting data.</p>"},{"location":"user_guides/fs/feature_group/data_validation_best_practices/#validate-early","title":"Validate Early","text":"<p>As often with data validation, the best piece of advice is to set it up early in your development process. Use this phase to build a history you can then use when it becomes time to set quality requirements for a project in production. We made a code snippet to help you get started quickly:</p> <pre><code># Load sample data. Replace it with your own!\nmy_data_df = pd.read_csv(\"https://repo.hops.works/master/hopsworks-tutorials/data/card_fraud_data/credit_cards.csv\")\n\n# Use Great Expectation profiler (ignore deprecation warning)\nexpectation_suite_profiled, validation_report = ge.from_pandas(my_data_df).profile(profiler=ge.profile.BasicSuiteBuilderProfiler)\n\n# Create a Feature Group on hopsworks with an expectation suite attached. Don't forget to change the primary key!\nmy_validated_data_fg = fs.get_or_create_feature_group(\n    name=\"my_validated_data_fg\",\n    version=1,\n    description=\"My data\",\n    primary_key=['cc_num'],\n    expectation_suite=expectation_suite_profiled)\n</code></pre> <p>Any data you insert in the Feature Group from now will be validated and a report will be uploaded to Hopsworks.</p> <pre><code># Insert and validate your data\ninsert_job, validation_report = my_validated_data_fg.insert(my_data_df)\n</code></pre> <p>Great Expectations profiler can inspect your data to build a standard Expectation Suite. You can attach this Expectation Suite directly when creating your Feature Group to make sure every piece of data finding its way in Hopsworks gets validated. Hopsworks will default to its <code>\"ALWAYS\"</code> ingestion policy, meaning data are ingested whether validation succeeds or not. This way data validation is not a barrier, just a monitoring tool.</p>"},{"location":"user_guides/fs/feature_group/data_validation_best_practices/#identify-unreliable-features","title":"Identify Unreliable Features","text":"<p>Once you setup data validation, every insertion will upload a validation report to Hopsworks. Identifying Features which often have null values or wild statistical variations can help detecting unreliable Features that need refinements or should be avoided. Here are a few expectations you might find useful:</p> <ul> <li><code>expect_column_values_to_not_be_null</code></li> <li><code>expect_column_(min/max/mean/stdev)_to_be_between</code></li> <li><code>expect_column_values_to_be_unique</code></li> </ul>"},{"location":"user_guides/fs/feature_group/data_validation_best_practices/#get-the-stakeholders-involved","title":"Get the stakeholders involved","text":"<p>Hopsworks UI helps involve every project stakeholder by enabling both setting and monitoring of data quality requirements. No coding skills needed! You can monitor data quality requirements by checking out the validation reports and results on the Feature Group page.</p> <p>If you need to set or edit the existing requirements, you can go on the Feature Group edit page. The Expectation suite section allows you to edit individual expectations and set success parameters that match ever changing business requirements.</p>"},{"location":"user_guides/fs/feature_group/data_validation_best_practices/#production","title":"Production","text":"<p>Models in production require high-quality data to make accurate predictions for your customers. Hopsworks can use your Expectation Suite as a gatekeeper to make it simple to prevent low-quality data to make its way into production. Below are some simple tips and snippets to make the most of your data validation when your project is ready to enter its production phase.</p>"},{"location":"user_guides/fs/feature_group/data_validation_best_practices/#be-strict-in-production","title":"Be Strict in Production","text":"<p>Whether you use an existing or create a new (recommended) Feature Group for production, we recommend you set the validation ingestion policy of your Expectation Suite to <code>\"STRICT\"</code>.</p> <pre><code>fg_prod.save_expectation_suite(\n    my_suite,\n    validation_ingestion_policy=\"STRICT\")\n</code></pre> <p>In this setup, Hopsworks will abort inserting a DataFrame that does not successfully fulfill all expectations in the attached Expectation Suite. This ensures data quality standards are upheld for every insertion and provide downstream users with strong guarantees.</p>"},{"location":"user_guides/fs/feature_group/data_validation_best_practices/#avoid-data-loss-on-materialization-jobs","title":"Avoid Data Loss on materialization jobs","text":"<p>Aborting insertions of DataFrames which do not satisfy the data quality standards can lead to data loss in your materialization job. To avoid such loss we recommend creating a duplicate Feature Group with the same Expectation Suite in <code>\"ALWAYS\"</code> mode which will hold the rejected data.</p> <pre><code>job, report = fg_prod.insert(df)\n\nif report[\"success\"] is False:\n    job, report = fg_rejected.insert(df)\n</code></pre>"},{"location":"user_guides/fs/feature_group/data_validation_best_practices/#take-advantage-of-the-validation-history","title":"Take Advantage of the Validation History","text":"<p>You can easily retrieve the validation history of a specific expectation to export it to your favourite visualisation tool. You can filter on time and on whether insertion was successful or not.</p> <pre><code>validation_history = fg.get_validation_history(\n    expectation_id=my_id,\n    filters=[\"REJECTED\", \"UNKNOWN\"],\n    ge_type=False\n)\n\ntimeseries = pd.DataFrame(\n    {\n        \"observed_value\": [res.result[\"observed_value\"] for res in validation_histoy]],\n        \"validation_time\": [res.validation_time for res in validation_history]\n    }\n)\n\n# export to your preferred Dashboard\n</code></pre>"},{"location":"user_guides/fs/feature_group/data_validation_best_practices/#setup-alerts","title":"Setup Alerts","text":"<p>While checking your feature engineering pipeline executed properly in the morning can be good enough in the development phase, it won't make the cut for demanding production use-cases. In Hopsworks, you can setup alerts if ingestion fails or succeeds.</p> <p>First you will need to configure your preferred communication endpoint: slack, email or pagerduty. Check out this page for more information on how to set it up. A typical use-case would be to add an alert on ingestion success to a Feature Group you created to hold data that failed validation. Here is a quick walkthrough:</p> <ol> <li>Go the Feature Group page in the UI</li> <li>Scroll down and click on the <code>Add an alert</code> button.</li> <li>Choose the trigger, receiver and severity and click save.</li> </ol>"},{"location":"user_guides/fs/feature_group/data_validation_best_practices/#conclusion","title":"Conclusion","text":"<p>Hopsworks extends Great Expectations by automatically running the validation, persisting the reports along your data and allowing you to monitor data quality in its UI. How you decide to make use of these tools depends on your application and requirements. Whether in development or in production, real-time or batch, we think there is configuration that will work for your team. Check out our quick hands-on tutorial to start applying what you learned so far.</p>"},{"location":"user_guides/fs/feature_group/deprecation/","title":"How to deprecate a Feature Group","text":""},{"location":"user_guides/fs/feature_group/deprecation/#introduction","title":"Introduction","text":"<p>To discourage the usage of specific feature groups it is possible to deprecate them. When a feature group is deprecated, user will be warned when they try to use it or use a feature view that depends on it.</p> <p>In this guide you will learn how to deprecate a feature group within Hopsworks, showing examples in HSFS APIs as well as the user interface.</p>"},{"location":"user_guides/fs/feature_group/deprecation/#prerequisites","title":"Prerequisites","text":"<p>Before you begin this guide it is expected that there is an existing feature group in your project. You can familiarize yourself with the creation of a feature group in the user guide.</p>"},{"location":"user_guides/fs/feature_group/deprecation/#deprecate-using-the-hsfs-apis","title":"Deprecate using the HSFS APIs","text":""},{"location":"user_guides/fs/feature_group/deprecation/#retrieve-the-feature-group","title":"Retrieve the feature group","text":"<p>To deprecate a feature group using the HSFS APIs you need to provide a Feature Group.</p> Python <pre><code>fg = fs.get_feature_group(name=\"feature_group_name\", version=feature_group_version)\n</code></pre>"},{"location":"user_guides/fs/feature_group/deprecation/#deprecate-feature-group","title":"Deprecate Feature Group","text":"<p>Feature group deprecation occurs by calling the <code>update_deprecated</code> method on the feature group.</p> Python <pre><code>fg.update_deprecated()\n</code></pre> <p>Users can also un-deprecate the feature group if need be, by setting the <code>deprecate</code> parameter to False.</p> Python <pre><code>fg.update_deprecated(deprecate=False)\n</code></pre>"},{"location":"user_guides/fs/feature_group/deprecation/#deprecate-using-the-ui","title":"Deprecate using the UI","text":"<p>You can deprecate/de-deprecate feature groups through the UI. For this, navigate to the <code>Feature Groups</code> section and select a feature group.</p> <p> </p> <p>Subsequently, make sure that the necessary feature group version is picked.</p> <p> </p> <p>Finally, click on the button with three vertical dots in the right corner and select <code>Deprecate</code>.</p> <p> </p> <p>The Feature group can be de-deprecated by selecting the <code>Undeprecate</code> option on a deprecated feature group.</p>"},{"location":"user_guides/fs/feature_group/feature_monitoring/","title":"Feature Monitoring for Feature Groups","text":"<p>Feature Monitoring complements the Hopsworks data validation capabilities for Feature Groups by allowing you to monitor your data once they have been ingested into the Feature Store. Hopsworks feature monitoring is centered around two functionalities: scheduled statistics and statistics comparison.</p> <p>Before continuing with this guide, see the Feature monitoring guide to learn more about how feature monitoring works, and get familiar with the different use cases of feature monitoring for Feature Groups described in the Use cases sections of the Scheduled statistics guide and Statistics comparison guide.</p> <p>Limited UI support</p> <p>Currently, feature monitoring can only be configured using the Hopsworks Python library. However, you can enable/disable a feature monitoring configuration or trigger the statistics comparison manually from the UI, as shown in the Advanced guide.</p>"},{"location":"user_guides/fs/feature_group/feature_monitoring/#code","title":"Code","text":"<p>In this section, we show you how to setup feature monitoring in a Feature Group using the Hopsworks Python library. Alternatively, you can get started quickly by running our tutorial for feature monitoring.</p> <p>First, checkout the pre-requisite and Hopsworks setup to follow the guide below. Create a project, install the Hopsworks Python library in your environment, connect via the generated API key. The second step is to start a new configuration for feature monitoring. </p> <p>After that, you can optionally define a detection window of data to compute statistics on, or use the default detection window (i.e., whole feature data). If you want to setup scheduled statistics alone, you can jump to the last step to save your configuration. Otherwise, the third and fourth steps are also optional and show you how to setup the comparison of statistics on a schedule by defining a reference window and specifying the statistics metric to monitor.</p>"},{"location":"user_guides/fs/feature_group/feature_monitoring/#step-1-pre-requisite","title":"Step 1: Pre-requisite","text":"<p>In order to setup feature monitoring for a Feature Group, you will need:</p> <ul> <li>A Hopsworks project. If you don't have a project yet you can go to app.hopsworks.ai, signup with your email and create your first project.</li> <li>An API key, you can get one by going to \"Account Settings\" on app.hopsworks.ai.</li> <li>The Hopsworks Python library installed in your client. See the installation guide.</li> <li>A Feature Group</li> </ul>"},{"location":"user_guides/fs/feature_group/feature_monitoring/#connect-your-notebook-to-hopsworks","title":"Connect your notebook to Hopsworks","text":"<p>Connect the client running your notebooks to Hopsworks.</p> Python <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nfs = project.get_feature_store()\n</code></pre> <p>You will be prompted to paste your API key to connect the notebook to your project. The <code>fs</code> Feature Store entity is now ready to be used to insert or read data from Hopsworks.</p>"},{"location":"user_guides/fs/feature_group/feature_monitoring/#get-or-create-a-feature-group","title":"Get or create a Feature Group","text":"<p>Feature monitoring can be enabled on already created Feature Groups. We suggest you read the Feature Group concept page to understand what a feature group is and how it fits in the ML pipeline. We also suggest you familiarize with the APIs to create a feature group.</p> <p>The following is a code example for getting or creating a Feature Group with name <code>trans_fg</code> for transaction data.</p> Python <pre><code># Retrieve an existing feature group\ntrans_fg = fs.get_feature_group(\"trans_fg\", version=1)\n\n# Or, create a new feature group with transactions\ntrans_fg = fs.get_or_create_feature_group(\n    name=\"trans_fg\",\n    version=1,\n    description=\"Transaction data\",\n    primary_key=[\"cc_num\"],\n    event_time=\"datetime\",\n)\ntrans_fg.insert(transactions_df)\n</code></pre>"},{"location":"user_guides/fs/feature_group/feature_monitoring/#step-2-initialize-configuration","title":"Step 2: Initialize configuration","text":""},{"location":"user_guides/fs/feature_group/feature_monitoring/#scheduled-statistics","title":"Scheduled statistics","text":"<p>You can setup statistics monitoring on a single feature or multiple features of your Feature Group.</p> Python <pre><code># compute statistics for all the features\nfg_monitoring_config = trans_fg.create_statistics_monitoring(\n    name=\"trans_fg_all_features_monitoring\",\n    description=\"Compute statistics on all data of all features of the Feature Group on a daily basis\",\n)\n\n# or for a single feature\nfg_monitoring_config = trans_fg.create_statistics_monitoring(\n    name=\"trans_fg_amount_monitoring\",\n    description=\"Compute statistics on all data of a single feature of the Feature Group on a daily basis\",\n    feature_name=\"amount\",\n)\n</code></pre>"},{"location":"user_guides/fs/feature_group/feature_monitoring/#statistics-comparison","title":"Statistics comparison","text":"<p>When enabling the comparison of statistics in a feature monitoring configuration, you need to specify a single feature of your Feature Group. You can create multiple feature monitoring configurations for the same Feature Group, but each of them should point to a single feature in the Feature Group.</p> Python <pre><code>fg_monitoring_config = trans_fg.create_feature_monitoring(\n    name=\"trans_fg_amount_monitoring\",\n    feature_name=\"amount\",\n    description=\"Compute descriptive statistics on the amount Feature of the Feature Group on a daily basis\",\n)\n</code></pre>"},{"location":"user_guides/fs/feature_group/feature_monitoring/#custom-schedule-or-percentage-of-window-data","title":"Custom schedule or percentage of window data","text":"<p>By default, the computation of statistics is scheduled to run endlessly, every day at 12PM. You can modify the default schedule by adjusting the <code>cron_expression</code>, <code>start_date_time</code> and <code>end_date_time</code> parameters.</p> Python <pre><code>fg_monitoring_config = trans_fg.create_statistics_monitoring(\n    name=\"trans_fg_all_features_monitoring\",\n    description=\"Compute statistics on all data of all features of the Feature Group on a weekly basis\",\n    cron_expression=\"0 0 12 ? * MON *\",  # weekly \n    row_percentage=0.8,                  # use 80% of the data\n)\n\n# or\nfg_monitoring_config = trans_fg.create_feature_monitoring(\n    name=\"trans_fg_amount_monitoring\",\n    feature_name=\"amount\",\n    description=\"Compute descriptive statistics on the amount Feature of the Feature Group on a weekly basis\",\n    cron_expression=\"0 0 12 ? * MON *\",  # weekly \n    row_percentage=0.8,                  # use 80% of the data\n)\n</code></pre>"},{"location":"user_guides/fs/feature_group/feature_monitoring/#step-3-optional-define-a-detection-window","title":"Step 3: (Optional) Define a detection window","text":"<p>By default, the detection window is an expanding window covering the whole Feature Group data. You can define a different detection window using the <code>window_length</code> and <code>time_offset</code> parameters provided in the <code>with_detection_window</code> method. Additionally, you can specify the percentage of feature data on which statistics will be computed using the <code>row_percentage</code> parameter.</p> Python <pre><code>fm_monitoring_config.with_detection_window(\n    window_length=\"1w\",  # data ingested during one week\n    time_offset=\"1w\",    # starting from last week\n    row_percentage=0.8,  # use 80% of the data\n)\n</code></pre>"},{"location":"user_guides/fs/feature_group/feature_monitoring/#step-4-optional-define-a-reference-window","title":"Step 4: (Optional) Define a reference window","text":"<p>When setting up feature monitoring for a Feature Group, reference windows can be either a regular window or a specific value (i.e., window of size 1).</p> Python <pre><code># compare statistics against a reference window\nfm_monitoring_config.with_reference_window(\n    window_length=\"1w\",  # data ingested during one week\n    time_offset=\"2w\",    # starting from two weeks ago\n    row_percentage=0.8,  # use 80% of the data\n)\n\n# or a specific value\nfm_monitoring_config.with_reference_value(\n    value=100,\n)\n</code></pre>"},{"location":"user_guides/fs/feature_group/feature_monitoring/#step-5-optional-define-the-statistics-comparison-criteria","title":"Step 5: (Optional) Define the statistics comparison criteria","text":"<p>In order to compare detection and reference statistics, you need to provide the criteria for such comparison. First, you select the metric to consider in the comparison using the <code>metric</code> parameter. Then, you can define a relative or absolute threshold using the <code>threshold</code> and <code>relative</code> parameters.</p> Python <pre><code>fm_monitoring_config.compare_on(\n    metric=\"mean\", \n    threshold=0.2,  # a relative change over 20% is considered anomalous\n    relative=True,  # relative or absolute change\n    strict=False,   # strict or relaxed comparison\n)\n</code></pre> <p>Difference values and thresholds</p> <p>For more information about the computation of difference values and the comparison against threshold bounds see the Comparison criteria section in the Statistics comparison guide.</p>"},{"location":"user_guides/fs/feature_group/feature_monitoring/#step-6-save-configuration","title":"Step 6: Save configuration","text":"<p>Finally, you can save your feature monitoring configuration by calling the <code>save</code> method. Once the configuration is saved, the schedule for the statistics computation and comparison will be activated automatically.</p> Python <pre><code>fm_monitoring_config.save()\n</code></pre> <p>Next steps</p> <p>See the Advanced guide to learn how to delete, disable or trigger feature monitoring manually.</p>"},{"location":"user_guides/fs/feature_group/notification/","title":"Change Data Capture for feature groups","text":""},{"location":"user_guides/fs/feature_group/notification/#introduction","title":"Introduction","text":"<p>Changes to online-enabled feature groups can be captured by listening to events on specified topics. This optimizes the user experience by allowing users to proactively make predictions as soon as there is an update on the features.</p> <p>In this guide you will learn how to enable Change Data Capture (CDC) for online feature groups within Hopsworks, showing examples in HSFS APIs as well as the user interface.</p>"},{"location":"user_guides/fs/feature_group/notification/#prerequisites","title":"Prerequisites","text":"<p>Before you begin this guide we suggest you read the Feature Group concept page to understand what a feature group is and how it fits in the ML pipeline. Subsequently create a Kafka topic, this topic will be used for storing Change Data Capture events.</p>"},{"location":"user_guides/fs/feature_group/notification/#using-hsfs-apis","title":"Using HSFS APIs","text":""},{"location":"user_guides/fs/feature_group/notification/#create-a-feature-group-with-change-data-capture","title":"Create a Feature Group with Change Data Capture","text":"<p>To enable Change Data Capture for an online-enabled feature group using the HSFS APIs you need to create a feature group and set the <code>notification_topic_name</code> properties value to the previously created topic.</p> Python <pre><code>fg = fs.create_feature_group(\n  name=\"feature_group_name\",\n  version=feature_group_version,\n  primary_key=feature_group_primary_keys,\n  online_enabled=True,\n  notification_topic_name=\"notification_topic_name\")\n</code></pre>"},{"location":"user_guides/fs/feature_group/notification/#update-feature-group-change-data-capture-topic","title":"Update Feature Group Change Data Capture topic","text":"<p>The notification topic name can be changed after the creation of the feature group. By setting the <code>notification_topic_name</code> value to <code>None</code> or empty string notification will be disabled. With the default configuration, it can take up to 30 minutes for these changes to take place since the onlinefs service internally caches feature groups.</p> Python <pre><code>fg.update_notification_topic_name(\n  notification_topic_name=\"new_notification_topic_name\")\n</code></pre>"},{"location":"user_guides/fs/feature_group/notification/#using-ui","title":"Using UI","text":""},{"location":"user_guides/fs/feature_group/notification/#create-a-feature-group-with-change-data-capture_1","title":"Create a Feature Group with Change Data Capture","text":"<p>During the creation of the feature group enable online feature serving. When enabled you will be able to set the <code>CDC topic name</code> property.</p> <p> </p>"},{"location":"user_guides/fs/feature_group/notification/#update-feature-group-with-change-data-capture-topic","title":"Update Feature Group with Change Data Capture topic","text":"<p>The notification topic name can be changed after creation by editing the feature group. By setting the <code>CDC topic name</code> value to empty the notifications will be disabled. With the default configuration, it can take up to 30 minutes for these changes to take place since the onlinefs service internally caches feature groups.</p> <p> </p>"},{"location":"user_guides/fs/feature_group/notification/#example-of-change-data-capture-event","title":"Example of Change Data Capture event","text":"<p>Once properly set up the online feature store service will produce events to the provided topic when data ingestion is completed for records.</p> <p>Here is an example output:</p> <pre><code>{\n  \"projectName\":\"project_name\",  # name of the project the feature group belongs to\n  \"projectId\":119,  # id of the project the feature group belongs to\n  \"featureStoreId\":67,  # feature store where changes took place\n  \"featureGroupId\":14,  # id of the feature group\n  \"featureGroupName\":\"fg_name\",  # name of the feature group\n  \"featureGroupVersion\":1,  # version of the feature group\n  \"entry\":{ # values of the affected feature group entry\n    \"id\":\"15\",\n    \"text\":\"test\"\n  },\n  \"featureViews\":[  # list of feature views affected\n    {\n      \"projectName\":\"project_name\", # name of the project the feature view belongs to\n      \"id\":9,  # id of the feature view\n      \"name\":\"test\",  # name of the feature view\n      \"version\":1,  # version of the feature view\n      \"featurestoreId\":67  # feature store where feature view resides\n    }\n  ]\n}\n</code></pre> <p>The list of <code>featureViews</code> in the event could be outdated for up to 10 minutes, due to internal logging in onlinefs service.</p>"},{"location":"user_guides/fs/feature_group/on_demand_transformations/","title":"On-Demand Transformation Functions","text":"<p>On-demand transformations produce on-demand features, which usually require parameters accessible during inference for their calculation. Hopsworks facilitates the creation of on-demand transformations without introducing online-offline skew, ensuring consistency while allowing their dynamic computation during online inference.</p>"},{"location":"user_guides/fs/feature_group/on_demand_transformations/#on-demand-transformation-function-creation","title":"On Demand Transformation Function Creation","text":"<p>An on-demand transformation function may be created by associating a transformation function with a feature group. Each on-demand transformation function can generate one or multiple on-demand features. If the on-demand transformation function returns a single feature, it is automatically assigned the same name as the transformation function. However, if it returns multiple features, they are by default named using the format <code>functionName_outputColumnNumber</code>. For instance, in the example below, the on-demand transformation function <code>transaction_age</code> produces an on-demand feature named <code>transaction_age</code> and the on-demand transformation function <code>stripped_strings</code> produces the on-demand features names <code>stripped_strings_0</code> and <code>stripped_strings_1</code>. Alternatively, the name of the resulting on-demand feature can be explicitly defined using the <code>alias</code> function.</p> <p>On-demand transformation</p> <p>All on-demand transformation functions attached to a feature group must have unique names and, in contrast to model-dependent transformations, they do not have access to training dataset statistics.</p> <p>Each on-demand transformation function can map specific features to its arguments by explicitly providing their names as arguments to the transformation function. If no feature names are provided, the transformation function will default to using features that match the name of the transformation function's argument.</p> Python <p>Creating on-demand transformation functions.</p> <pre><code># Define transformation function\n@hopsworks.udf(return_type=int, drop=[\"current_date\"])\ndef transaction_age(transaction_date, current_date):\n    return (current_date - transaction_date).dt.days\n\n@hopsworks.udf(return_type=[str, str], drop=[\"current_date\"])\ndef stripped_strings(country, city):\n    return county.strip(), city.strip()\n\n# Attach transformation function to feature group to create on-demand transformation function.\nfg = feature_store.create_feature_group(name=\"fg_transactions\",\n            version=1,\n            description=\"Transaction Features\",\n            online_enabled=True,\n            primary_key=['id'],\n            event_time='event_time'\n            transformation_functions=[transaction_age, stripped_strings]\n            )\n</code></pre>"},{"location":"user_guides/fs/feature_group/on_demand_transformations/#specifying-input-features","title":"Specifying input features","text":"<p>The features to be used by the on-demand transformation function can be specified by providing the feature names as input to the transformation functions. </p> Python <p>Creating on-demand transformations by specifying features to be passed to transformation function.</p> <pre><code>fg = feature_store.create_feature_group(name=\"fg_transactions\",\n            version=1,\n            description=\"Transaction Features\",\n            online_enabled=True,\n            primary_key=['id'],\n            event_time='event_time'\n            transformation_functions=[age_transaction('transaction_time', 'current_time')]\n            )\n</code></pre>"},{"location":"user_guides/fs/feature_group/on_demand_transformations/#usage","title":"Usage","text":"<p>On-demand transformation functions attached to a feature group are automatically executed in the feature pipeline when you\u00a0insert data\u00a0into a feature group and by the Python client while retrieving feature vectors for online inference using feature views that contain on-demand features. </p> <p>The on-demand features computed by on-demand transformation functions are positioned after all other features in a feature group and are ordered alphabetically by their names.</p>"},{"location":"user_guides/fs/feature_group/on_demand_transformations/#inserting-data","title":"Inserting data","text":"<p>All on-demand transformation functions attached to a feature group are executed whenever new data is inserted. This process computes on-demand features from historical data. The DataFrame used for insertion must include all features required for executing all on-demand transformation functions in the feature group.</p> <p>Inserting on-demand features as historical features saves time and computational resources by removing the need to compute all on-demand features while generating training or batch data.</p>"},{"location":"user_guides/fs/feature_group/on_demand_transformations/#accessing-on-demand-features-in-feature-views","title":"Accessing on-demand features in feature views","text":"<p>A feature view can include on-demand features from feature groups by selecting them in the query used to create the feature view. These on-demand features are equivalent to regular features, and model-dependent transformations can be applied to them if required.</p> Python <p>Creating feature view with on-demand features</p> <pre><code># Selecting on-demand features in query\nquery = fg.select([\"id\", \"feature1\", \"feature2\", \"on_demand_feature3\", \"on_demand_feature4\"])\n\n# Creating a feature view using a query that contains on-demand transformations and model-dependent transformations\nfeature_view = fs.create_feature_view(\n        name='transactions_view',\n        query=query,\n        transformation_functions=[\n            min_max_scaler(\"feature1\"),\n            min_max_scaler(\"on_demand_feature3\"),\n        ]\n    )\n</code></pre>"},{"location":"user_guides/fs/feature_group/on_demand_transformations/#computing-on-demand-features","title":"Computing on-demand features","text":"<p>On-demand features in the feature view are computed in real-time during online inference using the same on-demand transformation functions used to create them. Hopsworks, by default, automatically computes all on-demand features when retrieving feature view input features (feature vectors) with the functions <code>get_feature_vector</code> and <code>get_feature_vectors</code>. Additionally, on-demand features can be computed using the <code>compute_on_demand_features</code> function or by manually executing the same on-demand transformation function.</p> <p>The values for the input parameters required to compute on-demand features can be provided using the <code>request_parameters</code> argument. If values are not provided through the <code>request_parameters</code> argument, the transformation function will verify if the feature vector contains the necessary input parameters and will use those values instead. However, if the required input parameters are also not present in the feature vector, an error will be thrown.</p> <p>Note</p> <p>By default the functions <code>get_feature_vector</code> and <code>get_feature_vectors</code> will apply model-dependent transformation present in the feature view after computing on-demand features.</p>"},{"location":"user_guides/fs/feature_group/on_demand_transformations/#retrieving-a-feature-vector","title":"Retrieving a feature vector","text":"<p>The <code>get_feature_vector</code> function retrieves a single feature vector based on the feature view's serving key(s). The on-demand features in the feature vector can be computed using real-time data by passing a dictionary that associates the name of each input parameter needed for the on-demand transformation function with its respective new value to the <code>request_parameter</code> argument.</p> Python <p>Computing on-demand features while retrieving a feature vector</p> <pre><code>feature_vector = feature_view.get_feature_vector(\n    entry={\"id\": 1},\n    request_parameter={\n        \"transaction_time\": datetime(2022, 12, 28, 23, 55, 59),\n        \"current_time\": datetime.now(),\n    },\n)\n</code></pre>"},{"location":"user_guides/fs/feature_group/on_demand_transformations/#retrieving-feature-vectors","title":"Retrieving feature vectors","text":"<p>The <code>get_feature_vectors</code> function retrieves multiple feature vectors using a list of feature view serving keys. The <code>request_parameter</code> in this case, can be a list of dictionaries that specifies the input parameters for the computation of on-demand features for each serving key or can be a dictionary if the on-demand transformations require the same parameters for all serving keys.</p> Python <p>Computing on-demand features while retrieving a feature vectors</p> <pre><code># Specify unique request parameters for each serving key.\nfeature_vector = feature_view.get_feature_vectors(\n    entry=[{\"id\": 1}, {\"id\": 2}],\n    request_parameter=[\n        {\n            \"transaction_time\": datetime(2022, 12, 28, 23, 55, 59),\n            \"current_time\": datetime.now(),\n        },\n        {\n            \"transaction_time\": datetime(2022, 11, 20, 12, 50, 00),\n            \"current_time\": datetime.now(),\n        },\n    ],\n)\n\n# Specify common request parameters for all serving key.\nfeature_vector = feature_view.get_feature_vectors(\n    entry=[{\"id\": 1}, {\"id\": 2}],\n    request_parameter={\n        \"transaction_time\": datetime(2022, 12, 28, 23, 55, 59),\n        \"current_time\": datetime.now(),\n    },\n)\n</code></pre>"},{"location":"user_guides/fs/feature_group/on_demand_transformations/#retrieving-feature-vector-without-on-demand-features","title":"Retrieving feature vector without on-demand features","text":"<p>The <code>get_feature_vector</code> and <code>get_feature_vectors</code> methods can return untransformed feature vectors without on-demand features by disabling model-dependent transformations and excluding on-demand features. To achieve this, set the  parameters <code>transform</code> and <code>on_demand_features</code> to <code>False</code>.</p> Python <p>Returning untransformed feature vectors</p> <pre><code>untransformed_feature_vector = feature_view.get_feature_vector(\n    entry={\"id\": 1}, transform=False, on_demand_features=False\n)\nuntransformed_feature_vectors = feature_view.get_feature_vectors(\n    entry=[{\"id\": 1}, {\"id\": 2}], transform=False, on_demand_features=False\n)\n</code></pre>"},{"location":"user_guides/fs/feature_group/on_demand_transformations/#compute-all-on-demand-features","title":"Compute all on-demand features","text":"<p>The <code>compute_on_demand_features</code> function computes all on-demand features attached to a feature view and adds them to the feature vectors provided as input to the function. This function does not apply model-dependent transformations to any of the features. The <code>transform</code> function can be used to apply model-dependent transformations to the returned values if required.</p> <p>The <code>request_parameter</code> in this case, can be a list of dictionaries that specifies the input parameters for the computation of on-demand features for each feature vector given as input to the function or can be a dictionary if the on-demand transformations require the same parameters for all input feature vectors.</p> Python <p>Computing all on-demand features and manually applying model dependent transformations.</p> <pre><code># Specify request parameters for each serving key.\nuntransformed_feature_vector = feature_view.get_feature_vector(\n    entry={\"id\": 1}, transform=False, on_demand_features=False\n)\n\n# re-compute and add on-demand features to the feature vector\nfeature_vector_with_on_demand_features = fv.compute_on_demand_features(\n    untransformed_feature_vector,\n    request_parameter={\n        \"transaction_time\": datetime(2022, 12, 28, 23, 55, 59),\n        \"current_time\": datetime.now(),\n    },\n)\n\n# Applying model dependent transformations\nencoded_feature_vector = fv.transform(feature_vector_with_on_demand_features)\n\n# Specify request parameters for each serving key.\nuntransformed_feature_vectors = feature_view.get_feature_vectors(\n    entry=[{\"id\": 1}, {\"id\": 2}], transform=False, on_demand_features=False\n)\n\n# re-compute and add on-demand features to the feature vectors - Specify unique request parameter for each feature vector\nfeature_vectors_with_on_demand_features = fv.compute_on_demand_features(\n    untransformed_feature_vectors,\n    request_parameter=[\n        {\n            \"transaction_time\": datetime(2022, 12, 28, 23, 55, 59),\n            \"current_time\": datetime.now(),\n        },\n        {\n            \"transaction_time\": datetime(2022, 11, 20, 12, 50, 00),\n            \"current_time\": datetime.now(),\n        },\n    ],\n)\n\n# re-compute and add on-demand feature to the feature vectors - Specify common request parameter for all feature vectors\nfeature_vectors_with_on_demand_features = fv.compute_on_demand_features(\n    untransformed_feature_vectors,\n    request_parameter={\n        \"transaction_time\": datetime(2022, 12, 28, 23, 55, 59),\n        \"current_time\": datetime.now(),\n    },\n)\n\n# Applying model dependent transformations\nencoded_feature_vector = fv.transform(feature_vectors_with_on_demand_features)\n</code></pre>"},{"location":"user_guides/fs/feature_group/on_demand_transformations/#compute-one-on-demand-feature","title":"Compute one on-demand feature","text":"<p>On-demand transformation functions can also be accessed and executed as normal functions by using the dictionary <code>on_demand_transformations</code> that maps the on-demand features to their corresponding on-demand transformation function.</p> Python <p>Executing each on-demand transformation function</p> <pre><code># Specify request parameters for each serving key.\nfeature_vector = feature_view.get_feature_vector(\n    entry={\"id\": 1}, transform=False, on_demand_features=False, return_type=\"pandas\"\n)\n\n# Applying model dependent transformations\nfeature_vector[\"on_demand_feature1\"] = fv.on_demand_transformations[\n    \"on_demand_feature1\"\n](feature_vector[\"transaction_time\"], datetime.now())\n</code></pre>"},{"location":"user_guides/fs/feature_group/statistics/","title":"How to compute statistics on feature data","text":""},{"location":"user_guides/fs/feature_group/statistics/#introduction","title":"Introduction","text":"<p>In this guide you will learn how to configure, compute and visualize statistics for the features registered with Hopsworks. </p> <p>Hopsworks groups statistics in four categories:</p> <ul> <li> <p>Descriptive: These are the basic statistics Hopsworks computes. They include an approximate count of the distinctive values and the completeness (i.e. the percentage of non null values). For numerical features Hopsworks also computes the minimum, maximum, mean, standard deviation and the sum of each feature. Enabled by default.</p> </li> <li> <p>Histograms: Hopsworks computes the distribution of the values of a feature. Exact histograms are computed as long as the number of distinct values is less than 20. If a feature has a numerical data type (e.g. integer, float, double, ...) and has more than 20 unique values, then the values are bucketed in 20 buckets and the histogram represents the distribution of values in those buckets. By default histograms are disabled.</p> </li> <li> <p>Correlation: If enabled, Hopsworks computes the Pearson correlation between features of numerical data type within a feature group. By default correlation is disabled.</p> </li> <li> <p>Exact Statistics: Exact statistics are an enhancement of the descriptive statistics that provide an exact count of distinctive values, entropy, uniqueness and distinctiveness of the value of a feature. These statistics are more expensive to compute as they take into consideration all the values and they don't use approximations. By default they are disabled. </p> </li> </ul> <p>When statistics are enabled, they are computed every time new data is written into the offline storage of a feature group. Statistics are then displayed on the Hopsworks UI and users can track how data has changed over time. </p>"},{"location":"user_guides/fs/feature_group/statistics/#prerequisites","title":"Prerequisites","text":"<p>Before you begin this guide we suggest you read the Feature Group concept page to understand what a feature group is and how it fits in the ML pipeline.  We also suggest you familiarize with the APIs to create a feature group.</p>"},{"location":"user_guides/fs/feature_group/statistics/#enable-statistics-when-creating-a-feature-group","title":"Enable statistics when creating a feature group","text":"<p>As mentioned above, by default only descriptive statistics are enabled when creating a feature group. To enable histograms, correlations or exact statistics the <code>statistics_config</code> configuration parameter can be provided in the create statement.</p> <p>The <code>statistics_config</code> parameter takes a dictionary with the keys: <code>enabled</code>, <code>correlations</code>, <code>histograms</code> and <code>exact_uniqueness</code> and, as values, a boolean to describe whether or not to compute the specific class of statistics.</p> <p>Additionally it is possible to restrict the statistics computation to only a subset of columns. This is configurable by adding a <code>columns</code> key to the <code>statistics_config</code> parameter. The key should contain the list of columns for which to compute statistics.  By default the value is empty list <code>[]</code> and the statistics are computed for all columns in the feature group.</p> Python <pre><code>fg = feature_store.create_feature_group(name=\"weather\",\n    version=1,\n    description=\"Weather Features\",\n    online_enabled=True,\n    primary_key=['location_id'],\n    partition_key=['day'],\n    event_time='event_time',\n    statistics_config={\n        \"enabled\": True,\n        \"histograms\": True,\n        \"correlations\": True,\n        \"exact_uniqueness\": False,\n        \"columns\": []\n    }\n)\n</code></pre>"},{"location":"user_guides/fs/feature_group/statistics/#enable-statistics-after-creating-a-feature-group","title":"Enable statistics after creating a feature group","text":"<p>It is possible users to change the statistics configuration after a feature group was created. Either to add or remove a class of statistics, or to change the set of features for which to compute statistics.</p> Python <pre><code>fg.statistics_config = {\n        \"enabled\": True,\n        \"histograms\": False,\n        \"correlations\": False,\n        \"exact_uniqueness\": False \n        \"columns\": ['location_id', 'min_temp', 'max_temp']\n    }\n\nfg.update_statistics_config()\n</code></pre>"},{"location":"user_guides/fs/feature_group/statistics/#explicitly-compute-statistics","title":"Explicitly compute statistics","text":"<p>As mentioned above, the statistics are computed every time new data is written into the offline storage of a feature group. By invoking the <code>compute_statistics</code> method, users can trigger explicitly the statistics computation for the data available in a feature group.</p> <p>This is useful when a feature group is receiving frequent updates. Users can schedule periodic statistics computation that take into consideration several data commits.</p> <p>By default, the <code>compute_statistics</code> method computes statistics on the most recent version of the data available in a feature group. Users can provide a specific time using the <code>wallclock_time</code> parameter, to compute the statistics for a previous version of the data.</p> <p>Hopsworks can compute statistics of external feature groups. As external feature groups are read only from an Hopsworks perspective, statistics computation can be triggered using the <code>compute_statistics</code> method.</p> Python <pre><code>fg.compute_statistics(wallclock_time='20220611 20:00')\n</code></pre>"},{"location":"user_guides/fs/feature_group/statistics/#inspect-statistics","title":"Inspect statistics","text":"<p>You can also create a new feature group through the UI.</p>"},{"location":"user_guides/fs/feature_monitoring/","title":"Feature Monitoring","text":""},{"location":"user_guides/fs/feature_monitoring/#introduction","title":"Introduction","text":"<p>Feature Monitoring complements the Hopsworks data validation capabilities by allowing you to monitor your data once they have been ingested into the Feature Store. Hopsworks feature monitoring user interface is centered around two functionalities:</p> <ul> <li> <p>Scheduled Statistics: The user defines a detection window over its data for which Hopsworks will compute the statistics on a regular basis. The results are stored in Hopsworks and enable the user to visualise the temporal evolution of statistical metrics on its data. This can be enabled for a whole Feature Group or Feature View, or for a particular Feature. For more details, see the Scheduled statistics guide.</p> </li> <li> <p>Statistics Comparison: Enabled only for individual features, this variant allows the user to schedule the statistics computation on both a detection and a reference window. By providing information about how to compare those statistics, you can setup alerts to quickly detect critical change in the data. For more details, see the Statistics comparison guide.</p> </li> </ul> <p>Important</p> <p>To enable feature monitoring in Hopsworks, you need to set the <code>enable_feature_monitoring</code> configuration option to <code>true</code>. This can also be achieved in the cluster definition by setting the following attribute:</p> <pre><code>hopsworks:\n  enable_feature_monitoring: \"true\"\n</code></pre>"},{"location":"user_guides/fs/feature_monitoring/#statistics-computation-on-windows-of-feature-data","title":"Statistics computation on windows of feature data","text":"<p>Statistics are computed on feature data defined by windows. There are different types of windows depending on how they evolve over time. A window can have either a fixed length (e.g., static window) or variable length (e.g., expanding window). Moreover, windows can stick to a specific point in time (e.g., static window) or move over time (e.g., sliding or rolling window).</p> <p></p> <p>Specific values</p> <p>A specific value can be seen as a window of length 1 where the start and end of the window have the same value.</p> <p>These types of windows apply to both detection and reference windows. Different types of windows allows for different use cases depending on whether you enable feature monitoring on your Feature Groups or Feature Views. </p> <p>See more details about detection and reference windows in the Detection windows and Reference windows guides.</p>"},{"location":"user_guides/fs/feature_monitoring/#visualize-statistics-on-a-time-series","title":"Visualize statistics on a time series","text":"<p>Hopsworks provides an interactive graph to make the exploration of statistics and statistics comparison results more efficient and help you find unexpected trends or anomalous values faster. See the Interactive graph guide for more information.</p> <p></p>"},{"location":"user_guides/fs/feature_monitoring/#alerting","title":"Alerting","text":"<p>Moreover, feature monitoring integrates with the Hopsworks built-in system for alerts, enabling you to setup alerts that will notify you as soon as shift is detected in your feature values. You can setup alerts for feature monitoring at a Feature Group, Feature View, and project level.</p> <p>Select the correct trigger</p> <p>When configuring alerts for feature monitoring, make sure you select the <code>feature monitoring-shift detected</code> or <code>feature monitoring-shift undetected</code> trigger.</p> <p></p>"},{"location":"user_guides/fs/feature_monitoring/feature_monitoring_advanced/","title":"Advanced guide","text":"<p>An introduction to Feature Monitoring can be found in the guides for Feature Groups and Feature Views. In addition, you can get started quickly by running our tutorial for feature monitoring.</p>"},{"location":"user_guides/fs/feature_monitoring/feature_monitoring_advanced/#retrieve-feature-monitoring-configurations","title":"Retrieve feature monitoring configurations","text":""},{"location":"user_guides/fs/feature_monitoring/feature_monitoring_advanced/#from-ui","title":"From UI","text":"<p>An overview of all feature monitoring configurations is listed in the Feature Monitoring section in the Feature Group and Feature View overview page. </p>"},{"location":"user_guides/fs/feature_monitoring/feature_monitoring_advanced/#from-python-client","title":"From Python client","text":"<p>You can retrieve one or more feature monitoring configurations from the Feature Group and Feature View Python objects and filter them by name, configuration id or feature name. </p> Python <pre><code># retrieve all configurations\nfm_configs = trans_fg.get_feature_monitoring_configs()  # from a feature group\nfm_configs = trans_fv.get_feature_monitoring_configs()  # or a feature view\n\n# retrieve a configuration by name\nfm_config = trans_fg.get_feature_monitoring_configs(\n    name=\"trans_fg_all_features_monitoring\",\n)\n\n# or by config id\nfm_config = trans_fg.get_feature_monitoring_configs(\n    config_id=1,\n)\n\n# or for a specific feature\nfm_configs = trans_fg.get_feature_monitoring_configs(\n    feature_name=\"amount\",\n)\n</code></pre>"},{"location":"user_guides/fs/feature_monitoring/feature_monitoring_advanced/#disable-feature-monitoring","title":"Disable feature monitoring","text":"<p>You can enable or disable feature monitoring while keeping the historical statistics and comparison results.</p>"},{"location":"user_guides/fs/feature_monitoring/feature_monitoring_advanced/#from-ui_1","title":"From UI","text":"<p>In the overview page for feature monitoring, you can enable or disable a specific configuration by clicking on the Disable button.</p> <p></p>"},{"location":"user_guides/fs/feature_monitoring/feature_monitoring_advanced/#from-python-client_1","title":"From Python client","text":"<p>You can easily enable or disable a specific feature monitoring configuration using the Python object.</p> Python <pre><code># disable a specific feature monitoring configuration\nfm_config.disable()\n\n# disable a specific feature monitoring configuration\nfm_config.enable()\n</code></pre>"},{"location":"user_guides/fs/feature_monitoring/feature_monitoring_advanced/#run-the-statistics-comparison-manually","title":"Run the statistics comparison manually","text":"<p>You can trigger the feature monitoring job on demand, to compute and compare statistics on the detection and reference windows according to the feature monitoring configuration.</p>"},{"location":"user_guides/fs/feature_monitoring/feature_monitoring_advanced/#from-ui_2","title":"From UI","text":"<p>In the overview page for feature monitoring, you can trigger the computation and comparison of statistics for a specific configuration by clicking on the Run once button.</p> <p>Note</p> <p>Triggering the feature monitoring job manually does not affect the underlying schedule.</p> <p></p>"},{"location":"user_guides/fs/feature_monitoring/feature_monitoring_advanced/#from-python-client_2","title":"From Python client","text":"<p>To trigger the feature monitoring job once from the Python API, use the feature monitoring Python object as shown in the example below.</p> Python <pre><code># run the feature monitoring job once\nfm_config.run_job()\n</code></pre>"},{"location":"user_guides/fs/feature_monitoring/feature_monitoring_advanced/#get-feature-monitoring-results","title":"Get feature monitoring results","text":""},{"location":"user_guides/fs/feature_monitoring/feature_monitoring_advanced/#from-ui_3","title":"From UI","text":"<p>The easiest way to explore the statistics and comparison results is using the Hopsworks interactive graph for Feature Monitoring. See more information on the Interactive graph guide.</p> <p></p>"},{"location":"user_guides/fs/feature_monitoring/feature_monitoring_advanced/#from-python-client_3","title":"From Python client","text":"<p>Alternatively, you can retrieve all the statistics and comparison results using the feature monitoring configuration Python object as shown in the example below.</p> Python <pre><code># retrieve all feature monitoring results from a specific config\nfm_results = fm_config.get_history()\n\n# or filter results by date\nfm_results = fm_config.get_history(\n    start_time=\"2023-01-01\",\n    end_time=\"2023-01-31\",\n)\n</code></pre>"},{"location":"user_guides/fs/feature_monitoring/feature_monitoring_advanced/#delete-a-feature-monitoring-configuration","title":"Delete a feature monitoring configuration","text":"<p>Deleting a feature monitoring configuration also deletes the historical statistics and comparison results attached to this configuration.</p>"},{"location":"user_guides/fs/feature_monitoring/feature_monitoring_advanced/#from-python-client_4","title":"From Python client","text":"<p>You can delete feature monitoring configurations using the Python API only, as shown in the example below.</p> Python <pre><code>fm_config.delete()\n</code></pre>"},{"location":"user_guides/fs/feature_monitoring/interactive_graph/","title":"Interactive Graph","text":"<p>Hopsworks provides an interactive graph to help you explore the statistics computed on your feature data more efficiently and help you identify anomalies faster.</p>"},{"location":"user_guides/fs/feature_monitoring/interactive_graph/#select-a-feature-monitoring-configuration","title":"Select a feature monitoring configuration","text":"<p>First, you need to select a feature monitoring configuration to visualize. You can achieve that by clicking on the dropdown menu under Feature Selection on the controls menu.</p> <p></p>"},{"location":"user_guides/fs/feature_monitoring/interactive_graph/#select-a-statistics-metric-to-visualize","title":"Select a statistics metric to visualize","text":"<p>When you select a feature monitoring configuration, the mean values computed over time are visualized by default on the time series graph. You can choose a different statistics metric on the dropdown menu under Statistics Selection on the controls menu.</p> <p></p>"},{"location":"user_guides/fs/feature_monitoring/interactive_graph/#visualize-multiple-configurations-simultaneously","title":"Visualize multiple configurations simultaneously","text":"<p>Multiple feature monitoring configurations can be visualized at the same time on the graph. You can add a feature monitoring configuration by clicking on the + button on the controls menu.</p> <p>Note</p> <p>The same statistics metric will be visualized for every feature monitoring configuration selected.</p> <p></p>"},{"location":"user_guides/fs/feature_monitoring/interactive_graph/#show-reference-statistics","title":"Show reference statistics","text":"<p>In feature monitoring configurations with reference windows, you can also visualize the reference values by enabling the Reference values checkbox on the controls menu. Reference values can be either statistics computed over time or a specific value shown as an horizontal line. </p> <p>Note</p> <p>The same statistics metric will be visualized for both detection and reference values.</p> <p></p> <p>Info</p> <p>More details about reference windows can be found in Reference windows.</p>"},{"location":"user_guides/fs/feature_monitoring/interactive_graph/#show-threshold-bounds","title":"Show threshold bounds","text":"<p>In addition to reference windows, you can define thresholds to automate the identification of data points as anomalous values. A threshold can be absolute, or relative to the statistics values under comparison. You can visualize the threshold bounds by enabling the Threshold bounds checkbox on the controls menu.</p> <p></p> <p>Info</p> <p>More details about statistics comparison options can be found in Comparison criteria.</p>"},{"location":"user_guides/fs/feature_monitoring/interactive_graph/#highlight-shifted-data-points","title":"Highlight shifted data points","text":"<p>If a reference window and threshold are provided, data points that fall out of the threshold bounds are considered anomalous values. You can highlight these data points by enabling the Shift detected checkbox on the controls menu. </p> <p></p>"},{"location":"user_guides/fs/feature_monitoring/interactive_graph/#visualize-the-computed-differences-between-statistics","title":"Visualize the computed differences between statistics","text":"<p>Alternatively, you can change the time series to show the differences computed between detection and reference statistics rather than the statistics values themselves. You can achieve that by enabling the Difference checkbox on the controls menu.</p> <p></p>"},{"location":"user_guides/fs/feature_monitoring/interactive_graph/#list-of-configurations","title":"List of configurations","text":"<p>Following the time series graph, you can find an overview of all feature monitoring configurations defined for the corresponding Feature Group or Feature View. This overview includes a summary of the detection and reference windows, statistics comparison criteria and job schedule.</p> <p>In addition, you can trigger the statistics comparison manually, or disable the schedule of the feature monitoring job by clicking on Run once or Disable buttons, respectively.</p> <p>Note</p> <p>Triggering the statistics comparison manually does not affect the schedule of the feature monitoring.</p> <p></p>"},{"location":"user_guides/fs/feature_monitoring/scheduled_statistics/","title":"Scheduled Statistics","text":"<p>Hopsworks scheduled statistics allows you to monitor your feature data once they have been ingested into the Feature Store. You can define a detection window over your data for which Hopsworks will compute the statistics on a regular basis. Statistics can be computed on all or a subset of feature values, and on one or more features simultaneously.</p> <p>Hopsworks stores the computed statistics and enable you to visualise the temporal evolution of statistical metrics on your data. </p> <p></p> <p>Interactive graph</p> <p>See the Interactive graph guide to learn how to explore statistics more efficiently.</p>"},{"location":"user_guides/fs/feature_monitoring/scheduled_statistics/#use-cases","title":"Use cases","text":"<p>Scheduled statistics monitoring is a powerful tool that allows you to monitor your data over time and detect anomalies in your feature data at a glance by visualizing the evolution of the statistics properties of your data in a time series. It can be enabled in both Feature Groups and Feature Views, but for different purposes.</p> <p>For Feature Groups, scheduled statistics enables you to analyze how your Feature Group data evolve over time, and leverage your intuition to identify trends or detect noisy values in the inserted feature data.</p> <p>For Feature Views, scheduled statistics enables you to analyze the statistical properties of potentially new training dataset versions without having to actually create new training datasets and, thus, helping you decide when your training data show sufficient significant changes to create a new version.</p>"},{"location":"user_guides/fs/feature_monitoring/scheduled_statistics/#detection-windows","title":"Detection windows","text":"<p>Statistics are computed in a scheduled basis on a pre-defined detection window of feature data. Detection windows can be defined on the whole feature data or a subset of feature data depending on the <code>time_offset</code> and <code>window_length</code> parameters of the <code>with_detection_window</code> method. </p> <p></p> <p>In a previous section we described different types of windows available. Taking a Feature Group as an example, the figure above describes how these windows are applied to Feature Group data, resulting in three different applications:</p> <ul> <li>A expanding window covering the whole Feature Group data from its creation until the time when statistics are computing. It can be seen as an snapshot of the latest version of your feature data.</li> <li>A rolling window covering a variable subset of feature data (e.g., feature data written last week). It helps you analyze the properties of newly inserted feature data.</li> </ul> <p>See more details on how to define a detection window for your Feature Groups and Feature Views in the Feature Monitoring Guides for Feature Groups and Feature Views.</p> <p>Next steps</p> <p>You can also define a reference window to be used as a baseline to compare against the detection window. See more details in the Statistics comparison guide.</p>"},{"location":"user_guides/fs/feature_monitoring/statistics_comparison/","title":"Statistics Comparison","text":"<p>Hopsworks feature monitoring allows you to monitor your feature data once they have been ingested into the Feature Store. You can define detection and reference windows over your data for which Hopsworks will compute the statistics on a regular basis, compare them, and optionally trigger alerts when significant differences are detected. Statistics can be computed on all or a subset of feature values, and on one or more features simultaneously. Also, you can specify the criteria under which statistics will be compared and set thresholds used to classify feature values as anomalous.</p> <p>Hopsworks stores both detection and reference statistics and enable you to visualise the temporal evolution of statistical metrics. </p> <p></p> <p>Interactive graph</p> <p>See the Interactive graph guide to learn how to explore statistics and comparison results more efficiently.</p>"},{"location":"user_guides/fs/feature_monitoring/statistics_comparison/#use-cases","title":"Use cases","text":"<p>Feature monitoring is a powerful tool that allows you to monitor your data over time and quickly detect anomalies in your feature data by comparing statistics computed on different windows of your feature data, notifying you about anomalies, and/or visualizing the evolution of these statistics and comparison results in a time series. It can be enabled in both Feature Groups and Feature Views, but for different purposes.</p> <p>For Feature Groups, feature monitoring helps you rapidly identify unexpected trends or anomalous values in your Feature Group data, facilitating the debugging of possible root causes such as newly introduced changes in your feature pipelines.</p> <p>For Feature Views, feature monitoring helps you quickly detect when newly inserted Feature Group data differs statistically from your existing training datasets, and decide whether to retrain your ML models using a new training dataset version or analyze possible issues in your feature pipelines or inference pipelines.</p>"},{"location":"user_guides/fs/feature_monitoring/statistics_comparison/#reference-windows","title":"Reference windows","text":"<p>To compare statistics computed on a detection window against a baseline, you need to define a reference window of feature data. Reference windows can be defined in different ways depending on whether you are configuring feature monitoring on a Feature Group or Feature View.</p> <p></p> <p>In a previous section we described different types of windows available. Taking a Feature View as an example, the figure above describes how these windows are applied to Feature Group data read by a Feature View query and Training data, resulting in the following applications:</p> <ul> <li>A expanding window covering the whole Feature Group data from its creation until the time when statistics are computing. It can be seen as an snapshot of the latest version of your feature data. This reference window is useful when you want to compare the statistics of newly inserted feature data against all the Feature Group data.</li> <li>A rolling window covering a variable subset of feature data (e.g., feature data written last week). It helps you compare the properties of feature data inserted at different cadences (e.g., feature data inserted last month and two months ago). </li> <li>A static window representing a snapshot of Feature Group data read using the Feature View query at a specific point in time (i.e., Training Dataset). It helps you compare newly inserted feature data into your Feature Groups against a Training Dataset version.</li> <li>A specific value. It helps you target the analysis of feature data to a specific feature and statistics metric.</li> </ul> <p>See more details on how to define a reference window for your Feature Groups and Training Datasets in the Feature Monitoring guides for Feature Groups and Feature Views.</p>"},{"location":"user_guides/fs/feature_monitoring/statistics_comparison/#comparison-criteria","title":"Comparison criteria","text":"<p>After defining the detection and reference windows, you can specify the criteria under which computed statistics will be compared.</p> Statistics metric <p>Although all descriptive statistics are computed on the pre-defined windows of feature data, the comparison of statistics is performed only on one of the statistics metrics. In other words, difference values are only computed for a single statistics metric. You can select the targeted statistics metric using the <code>metric</code> parameter when calling the <code>compare_on</code> method.</p> Threshold bounds <p>Threshold bounds are used to classify feature values as anomalous, by comparing them against the difference values computed on a specific statistics metric. You can defined a threshold value using the <code>threshold</code> parameter when calling the <code>compare_on</code> method.</p> Relative or absolute <p>Difference values represent the amount of change in the detection statistics with regards to the reference values. They can be computed in absolute or relative terms, as specified in the <code>relative</code> boolean parameter when calling the <code>compare_on</code> method.</p> <ul> <li>Absolute: detection value - reference value</li> <li>Relative: (detection value - reference value) / reference value</li> </ul> Strict or relaxed <p>Threshold bounds set the limits under which the amount of change between detection and reference values is normal. These bounds can be strict (<code>&lt;</code> or <code>&gt;</code>) or relaxed (<code>&lt;=</code> and <code>=&gt;</code>), as defined in the <code>strict</code> parameter when calling the <code>compare_on</code> method.</p> <p>Hopsworks stores the results of each statistics comparison and enables you to visualise them together with the detection and reference values in a time series graph.</p> <p></p> <p>Next steps</p> <p>You can setup alerts that will notify you whenever anomalies are detected on your feature data. See more details in the alerting section of the feature monitoring guide.</p>"},{"location":"user_guides/fs/feature_view/","title":"Feature View User Guides","text":"<p>This section serves to provide guides and examples for the common usage of abstractions and functionality of the Feature View through the Hopsworks UI and APIs.</p> <ul> <li>Create a Feature View</li> <li>Create a Training Data with different splits</li> <li>Batch Data</li> <li>Feature Vectors</li> <li>Feature Server</li> <li>Query</li> <li>Helper columns</li> <li>Model-Dependent Transformation Functions</li> <li>Spines</li> <li>Feature Monitoring</li> </ul>"},{"location":"user_guides/fs/feature_view/batch-data/","title":"Batch data (analytical ML systems)","text":""},{"location":"user_guides/fs/feature_view/batch-data/#creation","title":"Creation","text":"<p>It is very common that ML models are deployed in a \"batch\" setting where ML pipelines score incoming new data at a regular interval, for example, daily or weekly. Feature views support batch prediction by returning batch data as a DataFrame over a time range, by <code>start_time</code> and <code>end_time</code>. The resultant DataFrame (or batch-scoring DataFrame) can then be fed to models to make predictions.</p> PythonJava <pre><code># get batch data\ndf = feature_view.get_batch_data(\n    start_time = \"20220620\",\n    end_time = \"20220627\"\n) # return a dataframe\n</code></pre> <pre><code>Dataset&lt;Row&gt; ds = featureView.getBatchData(\"20220620\", \"20220627\")\n</code></pre>"},{"location":"user_guides/fs/feature_view/batch-data/#retrieve-batch-data-with-primary-keys-and-event-time","title":"Retrieve batch data with primary keys and event time","text":"<p>For certain use cases, e.g. time series models, the input data needs to be sorted according to the primary key(s) and event time combination. Or one might want to merge predictions back with the original input data for postmortem analysis. Primary key(s) and event time are not usually included in the feature view query as they are not features used for training. To retrieve the primary key(s) and/or event time when retrieving batch data for inference, you need to set the parameters <code>primary_key=True</code> and/or <code>event_time=True</code>.</p> Python <pre><code># get batch data\ndf = feature_view.get_batch_data(\nstart_time = \"20220620\",\nend_time = \"20220627\",\nprimary_key=True,\nevent_time=True\n) # return a dataframe with primary keys and event time\n</code></pre> <p>Note</p> <p>All primary and event time columns of all the feature groups included in the feature view will be returned. If they have the same names across feature groups and the join prefix was not provided then reading operation will fail with ambiguous column exception. Make sure to define the join prefix if primary key and event time columns have the same names across feature groups.</p> <p>For Python-clients, handling small or moderately-sized data, we recommend enabling the ArrowFlight Server with DuckDB, which will provide significant speedups over Spark/Hive for reading batch data. If the service is enabled, and you want to read this particular batch data with Hive instead, you can set the read_options to <code>{\"use_hive\": True}</code>. <pre><code># get batch data with Hive\ndf = feature_view.get_batch_data(\n    start_time = \"20220620\",\n    end_time = \"20220627\",\n    read_options={\"use_hive: True})\n)\n</code></pre></p>"},{"location":"user_guides/fs/feature_view/batch-data/#creation-with-transformation","title":"Creation with transformation","text":"<p>If you have specified transformation functions when creating a feature view, you will get back transformed batch data as well. If your transformation functions require statistics of training dataset, you must also provide the training data version. <code>init_batch_scoring</code> will then fetch the statistics and initialize the functions with required statistics. Then you can follow the above examples and create the batch data. Please note that transformed batch data can only be returned in the python client but not in the java client.</p> <pre><code>feature_view.init_batch_scoring(training_dataset_version=1)\n</code></pre> <p>It is important to note that in addition to the filters defined in feature view, extra filters will be applied if they are defined in the given training dataset version.</p>"},{"location":"user_guides/fs/feature_view/batch-data/#retrieving-untransformed-batch-data","title":"Retrieving untransformed batch data","text":"<p>By default, the <code>get_batch_data</code> function returns batch data with model-dependent transformations applied. However, you can retrieve untransformed batch data\u2014while still including on-demand features\u2014by setting the <code>transform</code> parameter to <code>False</code>.</p> Python <p>Returning untransformed batch data</p> <pre><code># Fetching untransformed batch data.\nuntransformed_batch_data = feature_view.get_batch_data(transform=False)\n</code></pre>"},{"location":"user_guides/fs/feature_view/batch-data/#passing-context-variables-to-transformation-functions","title":"Passing Context Variables to Transformation Functions","text":"<p>After defining a transformation function using a context variable, you can pass the necessary context variables through the <code>transformation_context</code> parameter when fetching batch data.</p> Python <p>Passing context variables while fetching batch data.</p> <pre><code># Passing context variable to IN-MEMORY Training Dataset.\nbatch_data = feature_view.get_batch_data(transformation_context={\"context_parameter\":10})\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature-server/","title":"Feature Store REST API Server","text":"<p>This API server allows users to retrieve single/batch feature vectors from a feature view.</p>"},{"location":"user_guides/fs/feature_view/feature-server/#how-to-use","title":"How to use","text":"<p>From Hopsworks 3.3, you can connect to the Feature Vector Server via any REST client which supports POST requests. Set the X-API-HEADER to your Hopsworks API Key and send the request with a JSON body, single or batch. By default, the server listens on the <code>0.0.0.0:4406</code> and the api version is set to <code>0.1.0</code>. Please refer to <code>/srv/hops/mysql-cluster/rdrs_config.json</code> config file located on machines running the REST Server for additional configuration parameters.</p> <p>In Hopsworks 3.7, we introduced a python client for the Online Store REST API Server. The python client is available in the <code>hsfs</code> module and can be installed using <code>pip install hsfs</code>. This client can be used instead of the Online Store SQL client in the <code>FeatureView.get_feature_vector(s)</code> methods. Check the corresponding documentation for these methods.</p>"},{"location":"user_guides/fs/feature_view/feature-server/#single-feature-vector","title":"Single feature vector","text":""},{"location":"user_guides/fs/feature_view/feature-server/#request","title":"Request","text":"<p><code>POST /{api-version}/feature_store</code></p> <p>Body</p> <pre><code>{\n        \"featureStoreName\": \"fsdb002\",\n        \"featureViewName\": \"sample_2\",\n        \"featureViewVersion\": 1,\n        \"passedFeatures\": {},\n        \"entries\": {\n                \"id1\": 36\n        },\n        \"metadataOptions\": {\n                \"featureName\": true,\n                \"featureType\": true\n        },\n        \"options\": {\n                \"validatePassedFeatures\": true,\n                \"includeDetailedStatus\": true\n        }\n}\n</code></pre> <p>Parameters</p> parameter type note featureStoreName string featureViewName string featureViewVersion number(int) entries objects Map of serving key of feature view as key and value of serving key as value. Serving key are a set of the primary key of feature groups which are included in the feature view query. If feature groups are joint with prefix, the primary key needs to be attached with prefix. passedFeatures objects Optional. Map of feature name as key and feature value as value. This overwrites feature values in the response. metadataOptions objects Optional. Map of metadataoption as key and boolean as value. Default metadata option is false. Metadata is returned on request. Metadata options available: 1. featureName 2. featureType options objects Optional. Map of option as key and boolean as value. Default option is false. Options available: 1. validatePassedFeatures 2. includeDetailedStatus"},{"location":"user_guides/fs/feature_view/feature-server/#response","title":"Response","text":"<pre><code>{\n        \"features\": [\n                36,\n                \"2022-01-24\",\n                \"int24\",\n                \"str14\"\n        ],\n        \"metadata\": [\n                {\n                        \"featureName\": \"id1\",\n                        \"featureType\": \"bigint\"\n                },\n                {\n                        \"featureName\": \"ts\",\n                        \"featureType\": \"date\"\n                },\n                {\n                        \"featureName\": \"data1\",\n                        \"featureType\": \"string\"\n                },\n                {\n                        \"featureName\": \"data2\",\n                        \"featureType\": \"string\"\n                }\n        ],\n        \"status\": \"COMPLETE\",\n        \"detailedStatus\": [\n                {\n                        \"featureGroupId\": 1,\n                        \"httpStatus\": 200,\n                },\n                {\n                        \"featureGroupId\": 2,\n                        \"httpStatus\": 200,\n                },\n        ]\n}\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature-server/#error-handling","title":"Error handling","text":"Code reason response 200 400 Requested metadata does not exist 400 Error in pk or passed feature value 401 Access denied Access unshared feature store failed 500 Failed to read feature store metadata <p>Response with pk/pass feature error</p> <pre><code>{\n        \"code\": 12,\n        \"message\": \"Wrong primay-key column. Column: ts\",\n        \"reason\": \"Incorrect primary key.\"\n}\n</code></pre> <p>Response with metadata error</p> <pre><code>{\n        \"code\": 2,\n        \"message\": \"\",\n        \"reason\": \"Feature store does not exist.\"\n}\n</code></pre> <p>Pk value no match</p> <pre><code>{\n        \"features\": [\n                9876543,\n                null,\n                null,\n                null\n        ],\n        \"metadata\": null,\n        \"status\": \"MISSING\"\n}\n</code></pre> <p>Detailed Status</p> <p>If <code>includeDetailedStatus</code> option is set to true, detailed status is returned in the response. Detailed status is a list of feature group id and http status code, corresponding to each read operations perform internally by RonDB. Meaning is as follows:</p> <ul> <li><code>featureGroupId</code>: Id of the feature group, used to identify which table the operation correspond from.</li> <li><code>httpStatus</code>: Http status code of the operation.          * 200 means success         * 400 means bad request, likely pk name is wrong or pk is incomplete. In particular, if pk for this table/feature group is not provided in the request, this http status is returned.         * 404 means no row corresponding to PK         * 500 means internal error.</li> </ul> <p>Both <code>404</code> and <code>400</code> set the status to <code>MISSING</code> in the response. Examples below corresponds respectively to missing row and bad request.</p> <p>Missing Row: The pk name,value was correctly passed but the corresponding row was not found in the feature group. <pre><code>{\n        \"features\": [\n                36,\n                \"2022-01-24\",\n                null,\n                null\n        ],\n        \"status\": \"MISSING\",\n        \"detailedStatus\": [\n                {\n                        \"featureGroupId\": 1,\n                        \"httpStatus\": 200,\n                },\n                {\n                        \"featureGroupId\": 2,\n                        \"httpStatus\": 404,\n                },\n        ]\n}\n</code></pre></p> <p>Bad Request e.g pk name,value pair for FG2 not provided or the corresponding column names was incorrect. <pre><code>{\n        \"features\": [\n                36,\n                \"2022-01-24\",\n                null,\n                null\n        ],\n        \"status\": \"MISSING\",\n        \"detailedStatus\": [\n                {\n                        \"featureGroupId\": 1,\n                        \"httpStatus\": 200,\n                },\n                {\n                        \"featureGroupId\": 2,\n                        \"httpStatus\": 400,\n                },\n        ]\n}\n</code></pre></p>"},{"location":"user_guides/fs/feature_view/feature-server/#batch-feature-vectors","title":"Batch feature vectors","text":""},{"location":"user_guides/fs/feature_view/feature-server/#request_1","title":"Request","text":"<p><code>POST /{api-version}/batch_feature_store</code></p> <p>Body</p> <pre><code>{\n        \"featureStoreName\": \"fsdb002\",\n        \"featureViewName\": \"sample_2\",\n        \"featureViewVersion\": 1,\n        \"passedFeatures\": [],\n        \"entries\": [\n                {\n                        \"id1\": 16\n                },\n                {\n                        \"id1\": 36\n                },\n                {\n                        \"id1\": 71\n                },\n                {\n                        \"id1\": 48\n                },\n                {\n                        \"id1\": 29\n                }\n        ],\n        \"requestId\": null,\n        \"metadataOptions\": {\n                \"featureName\": true,\n                \"featureType\": true\n        },\n        \"options\": {\n                \"validatePassedFeatures\": true,\n                \"includeDetailedStatus\": true\n        }\n}\n</code></pre> <p>Parameters</p> parameter type note featureStoreName string featureViewName string featureViewVersion number(int) entries <code>array&lt;objects&gt;</code> Each items is a map of serving key as key and value of serving key as value. Serving key of feature view. passedFeatures <code>array&lt;objects&gt;</code> Optional. Each items is a map of feature name as key and feature value as value. This overwrites feature values in the response. If provided, its size and order has to be equal to the size of entries. Item can be null. metadataOptions objects Optional. Map of metadataoption as key and boolean as value. Default metadata option is false. Metadata is returned on request. Metadata options available: 1. featureName 2. featureType options objects Optional. Map of option as key and boolean as value. Default option is false. Options available: 1. validatePassedFeatures 2. includeDetailedStatus"},{"location":"user_guides/fs/feature_view/feature-server/#response_1","title":"Response","text":"<pre><code>{\n        {\n        \"features\": [\n                [\n                        16,\n                        \"2022-01-27\",\n                        \"int31\",\n                        \"str24\"\n                ],\n                [\n                        36,\n                        \"2022-01-24\",\n                        \"int24\",\n                        \"str14\"\n                ],\n                [\n                        71,\n                        null,\n                        null,\n                        null\n                ],\n                [\n                        48,\n                        \"2022-01-26\",\n                        \"int92\",\n                        \"str31\"\n                ],\n                [\n                        29,\n                        \"2022-01-03\",\n                        \"int53\",\n                        \"str91\"\n                ]\n        ],\n        \"metadata\": [\n                {\n                        \"featureName\": \"id1\",\n                        \"featureType\": \"bigint\"\n                },\n                {\n                        \"featureName\": \"ts\",\n                        \"featureType\": \"date\"\n                },\n                {\n                        \"featureName\": \"data1\",\n                        \"featureType\": \"string\"\n                },\n                {\n                        \"featureName\": \"data2\",\n                        \"featureType\": \"string\"\n                }\n        ],\n        \"status\": [\n                \"COMPLETE\",\n                \"COMPLETE\",\n                \"MISSING\",\n                \"COMPLETE\",\n                \"COMPLETE\"\n        ],\n        \"detailedStatus\": [\n                [{\n                        \"featureGroupId\": 1,\n                        \"httpStatus\": 200,\n                }],\n                [{\n                        \"featureGroupId\": 1,\n                        \"httpStatus\": 200,\n                }],\n                [{\n                        \"featureGroupId\": 1,\n                        \"httpStatus\": 404,\n                }],\n                [{\n                        \"featureGroupId\": 1,\n                        \"httpStatus\": 200,\n                }],\n                [{\n                        \"featureGroupId\": 1,\n                        \"httpStatus\": 200,\n                }]\n        ]\n}\n</code></pre> <p>note: Order of the returned features are the same as the order of entries in the request.</p>"},{"location":"user_guides/fs/feature_view/feature-server/#error-handling_1","title":"Error handling","text":"Code reason response 200 400 Requested metadata does not exist 404 Missing row corresponding to pk value 401 Access denied Access unshared feature store failed 500 Failed to read feature store metadata <p>Response with partial failure</p> <pre><code>{\n        \"features\": [\n                [\n                        81,\n                        \"id81\",\n                        \"2022-01-29 00:00:00\",\n                        6\n                ],\n                null,\n                [\n                        51,\n                        null,\n                        null,\n                        null,\n                ]\n        ],\n        \"metadata\": null,\n        \"status\": [\n                \"COMPLETE\",\n                \"ERROR\",\n                \"MISSING\"\n        ],\n        \"detailedStatus\": [\n                [{\n                        \"featureGroupId\": 1,\n                        \"httpStatus\": 200,\n                }],\n                [{\n                        \"featureGroupId\": 1,\n                        \"httpStatus\": 400,\n                }],\n                [{\n                        \"featureGroupId\": 1,\n                        \"httpStatus\": 404,\n                }]\n        ]\n}\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature-server/#access-control-to-feature-store","title":"Access control to feature store","text":"<p>Currently, the REST API server only supports Hopsworks API Keys for authentication and authorization. Add the API key to the HTTP requests using the <code>X-API-KEY</code> header.</p>"},{"location":"user_guides/fs/feature_view/feature-vectors/","title":"Feature Vectors","text":"<p>The Hopsworks Platform integrates real-time capabilities with its Online Store. Based on RonDB, your feature vectors are served at scale at in-memory latency (~1-10ms). Checkout the benchmarks results here and the code here. The same Feature View which was used to create training datasets can be used to retrieve feature vectors for real-time predictions. This allows you to serve the same features to your model in training and serving, ensuring consistency and reducing boilerplate. Whether you are either inside the Hopsworks platform, a model serving platform, or in an external environment, such as your application server.</p> <p>Below is a practical guide on how to use the Online Store Python and Java Client. The aim is to get you started quickly by providing code snippets which illustrate various use cases and functionalities of the clients. If you need to get more familiar with the concept of feature vectors, you can read this short introduction first.</p>"},{"location":"user_guides/fs/feature_view/feature-vectors/#retrieval","title":"Retrieval","text":"<p>You can get back feature vectors from either python or java client by providing the primary key value(s) for the feature view. Note that filters defined in feature view and training data will not be applied when feature vectors are returned. If you need to retrieve a complete value of feature vectors without missing values, the required <code>entry</code> are feature_view.primary_keys. Alternative, you can provide the primary key of the feature groups as the key of the entry. It is also possible to provide a subset of the entry, which will be discussed below.</p> PythonJava <pre><code># get a single vector\nfeature_view.get_feature_vector(\n    entry = {\"pk1\": 1, \"pk2\": 2}\n)\n\n# get multiple vectors\nfeature_view.get_feature_vectors(\n    entry = [\n        {\"pk1\": 1, \"pk2\": 2},\n        {\"pk1\": 3, \"pk2\": 4},\n        {\"pk1\": 5, \"pk2\": 6}\n    ]\n)\n</code></pre> <pre><code>// get a single vector\nMap&lt;String, Object&gt; entry1 = Maps.newHashMap();\nentry1.put(\"pk1\", 1);\nentry1.put(\"pk2\", 2);\nfeatureView.getFeatureVector(entry1);\n\n// get multiple vectors\nMap&lt;String, Object&gt; entry2 = Maps.newHashMap();\nentry2.put(\"pk1\", 3);\nentry2.put(\"pk2\", 4);\nfeatureView.getFeatureVectors(Lists.newArrayList(entry1, entry2);\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature-vectors/#required-entry","title":"Required entry","text":"<p>Starting from python client v3.4, you can specify different values for the primary key of the same name which exists in multiple feature groups but are not joint by the same name. The table below summarises the value of <code>primary_keys</code> in different settings. Considering that you are joining 2 feature groups, namely, <code>left_fg</code> and <code>right_fg</code>, the feature groups have different primary keys, and features (<code>feature_*</code>) in each setting. Also, the 2 feature groups are joint on different join conditions and prefix as <code>left_fg.join(right_fg, &lt;join conditions&gt;, prefix=&lt;prefix&gt;)</code>.</p> <p>For java client, and python client before v3.4, the <code>primary_keys</code> are the set of primary key of all the feature groups in the query. Python client is backward compatible. It means that the <code>primary_keys</code> used before v3.4 can be applied to python client of later versions as well.</p> Setting primary key of <code>left_fg</code> primary key of <code>right_fg</code> join conditions prefix primary_keys note 1 id id <code>on=[\"id\"]</code> id Same feature name is used in the join. 2 id1 id2 <code>left_on=[\"id1\"], right_on=[\"id2\"]</code> id1 Different feature names are used in the join. 3 id1, id2 id1 <code>on=[\"id1\"]</code> id1, id2 <code>id2</code> is not part of the join conditions 4 id, user_id id <code>left_on=[\"user_id\"], right_on=[\"id\"]</code> id, user_id Value of <code>user_id</code> is used for retrieving features from <code>right_fg</code> 5 id1 id1, id2 <code>on=[\"id1\"]</code> id1, id2 <code>id2</code> is not part of the join conditions 6 id id, user_id <code>left_on=[\"id\"], right_on=[\"user_id\"]</code> \u201cright_\u201c id, \u201cright_id\u201c Value of \u201cright_id\u201c and \"id\" are used for retrieving features from <code>right_fg</code> 7 id id, user_id <code>left_on=[\"id\"], right_on=[\"user_id\"]</code> id, \u201cfgId_&lt;rightFgId&gt;_&lt;joinIndex&gt;_id\u201d Value of \u201cfgId_&lt;rightFgId&gt;_&lt;joinIndex&gt;_id\u201c and \"id\" are used for retrieving features from <code>right_fg</code>. See note below. 8 id id <code>left_on=[\"id\"], right_on=[\"feature_1\"]</code> \u201cright_\u201c id, \u201cright_id\u201c No primary key from <code>right_fg</code> is used in the join. Value of <code>right_id</code> is used for retrieving features from <code>right_fg</code> 9 id id <code>left_on=[\"id\"], right_on=[\"feature_1\"]</code> id1, \u201cfgId_&lt;rightFgId&gt;_&lt;joinIndex&gt;_id\u201d No primary key from <code>right_fg</code> is used in the join. Value of \"fgId_&lt;rightFgId&gt;_&lt;joinIndex&gt;_id\" is used for retrieving features from \"right_fg`. See note below. 10 id id <code>left_on=[\"feature_1\"], right_on=[\"id\"]</code> \u201cright_\u201c id, \u201cright_id\u201c No primary key from <code>left_fg</code> is used in the join. Value of <code>right_id</code> is used for retrieving features from <code>right_fg</code> 11 id id <code>left_on=[\"feature_1\"], right_on=[\"id\"]</code> id1, \u201cfgId_&lt;rightFgId&gt;_&lt;joinIndex&gt;_id\u201d No primary key from <code>left_fg</code> is used in the join. Value of \u201cfgId_&lt;rightFgId&gt;_&lt;joinIndex&gt;_id\u201d is used for retrieving features from <code>right_fg</code>. See note below. 12 user, year user, year <code>left_on=[\"user\"], right_on=[\"user\"]</code> \u201cright_\u201c user, year, \u201cright_year\u201c Value of \"user\" and \"right_year\" are used for retrieving features from <code>right_fg</code>. <code>right_fg</code> can be the same as feature group as <code>left_fg</code>. 13 user, year user, year <code>left_on=[\"user\"], right_on=[\"user\"]</code> user, year, \u201cfgId_&lt;rightFgId&gt;_&lt;joinIndex&gt;_year\u201d Value of \"user\" and \"fgId_&lt;rightFgId&gt;_&lt;joinIndex&gt;_year\" are used for retrieving features from <code>right_fg</code>. <code>right_fg</code> can be the same as feature group as <code>left_fg</code>. See note below. <p>Note:</p> <p>\"&lt;rightFgId&gt;\" can be found by <code>right_fg.id</code>. \"&lt;joinIndex&gt;\" is the order or the feature group in the join. In the example, it is 1 because <code>right_fg</code> is in the first join in the query <code>left_fg.join(right_fg, &lt;join conditions&gt;)</code>.</p>"},{"location":"user_guides/fs/feature_view/feature-vectors/#missing-primary-key-entries","title":"Missing Primary Key Entries","text":"<p>It can happen that some of the primary key entries are not available in some or all of the feature groups used by a feature view.</p> <p>Take the above example assuming the feature view consists of two joined feature groups, first one with primary key column <code>pk1</code>, the second feature group with primary key column <code>pk2</code>.</p> PythonJava <pre><code># get a single vector\nfeature_view.get_feature_vector(\n    entry = {\"pk1\": 1, \"pk2\": 2}\n)\n</code></pre> <pre><code>// get a single vector\nMap&lt;String, Object&gt; entry1 = Maps.newHashMap();\nentry1.put(\"pk1\", 1);\nentry1.put(\"pk2\", 2);\nfeatureView.getFeatureVector(entry1);\n</code></pre> <p>This call will raise an exception if <code>pk1 = 1</code> OR <code>pk2 = 2</code> can't be found but also if <code>pk1 = 1</code> AND <code>pk2 = 2</code> can't be found, meaning, it will not return a partial or empty feature vector.</p> <p>When retrieving a batch of vectors, the behaviour is slightly different.</p> PythonJava <pre><code># get multiple vectors\nfeature_view.get_feature_vectors(\n    entry = [\n        {\"pk1\": 1, \"pk2\": 2},\n        {\"pk1\": 3, \"pk2\": 4},\n        {\"pk1\": 5, \"pk2\": 6}\n    ]\n)\n</code></pre> <pre><code>// get multiple vectors\nMap&lt;String, Object&gt; entry2 = Maps.newHashMap();\nentry2.put(\"pk1\", 3);\nentry2.put(\"pk2\", 4);\nMap&lt;String, Object&gt; entry3 = Maps.newHashMap();\nentry3.put(\"pk1\", 5);\nentry3.put(\"pk2\", 6);\nfeatureView.getFeatureVectors(Lists.newArrayList(entry1, entry2, entry3);\n</code></pre> <p>This call will raise an exception if for example for the third entry <code>pk1 = 5</code> OR <code>pk2 = 6</code> can't be found, however, it will simply not return a vector for this entry if <code>pk1 = 5</code> AND <code>pk2 = 6</code> can't be found. That means, <code>get_feature_vectors</code> will never return partial feature vector, but will omit empty feature vectors.</p> <p>If you are aware of missing features, you can use the passed features or Partial feature retrieval functionality, described down below.</p>"},{"location":"user_guides/fs/feature_view/feature-vectors/#partial-feature-retrieval","title":"Partial feature retrieval","text":"<p>If your model can handle missing value or if you want to impute the missing value, you can get back feature vectors with partial values using python client starting from version 3.4 (Note that this does not apply to java client.). In the example below, let's say you join 2 feature groups by <code>fg1.join(fg2, left_on=[\"pk1\"], right_on=[\"pk2\"])</code>, required keys of the <code>entry</code> are <code>pk1</code> and <code>pk2</code>. If <code>pk2</code> is not provided, this returns feature values from the first feature group and null values from the second feature group when using the option <code>allow_missing=True</code>, otherwise it raises exception.</p> Python <pre><code># get a single vector with\nfeature_view.get_feature_vector(\n    entry = {\"pk1\": 1},\n    allow_missing=True\n)\n\n# get multiple vectors\nfeature_view.get_feature_vectors(\n    entry = [\n        {\"pk1\": 1},\n        {\"pk1\": 3},\n    ],\n    allow_missing=True\n)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature-vectors/#retrieval-with-transformation","title":"Retrieval with transformation","text":"<p>If you have specified transformation functions when creating a feature view, you receive transformed feature vectors. If your transformation functions require statistics of training dataset, you must also provide the training data version. <code>init_serving</code> will then fetch the statistics and initialize the functions with the required statistics. Then you can follow the above examples and retrieve the feature vectors. Please note that transformed feature vectors can only be returned in the python client but not in the java client.</p> Python <pre><code>feature_view.init_serving(training_dataset_version=1)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature-vectors/#passed-features","title":"Passed features","text":"<p>If some of the features values are only known at prediction time and cannot be computed and cached in the online feature store, you can provide those values as <code>passed_features</code> option. The <code>get_feature_vector</code> method is going to use the passed values to construct the final feature vector to submit to the model.</p> <p>You can use the <code>passed_features</code> parameter to overwrite individual features being retrieved from the online feature store. The feature view will apply the necessary transformations to the passed features as it does for the feature data retrieved from the online feature store.</p> <p>Please note that passed features is only available in the python client but not in the java client.</p> Python <pre><code># get a single vector\nfeature_view.get_feature_vector(\n    entry = {\"pk1\": 1, \"pk2\": 2},\n    passed_features = {\"feature_a\": \"value_a\"}\n)\n\n# get multiple vectors\nfeature_view.get_feature_vectors(\n    entry = [\n        {\"pk1\": 1, \"pk2\": 2},\n        {\"pk1\": 3, \"pk2\": 4},\n        {\"pk1\": 5, \"pk2\": 6}\n    ],\n    passed_features = [\n        {\"feature_a\": \"value_a1\"},\n        {\"feature_a\": \"value_a2\"},\n        {\"feature_a\": \"value_a3\"},\n    ]\n)\n</code></pre> <p>You can also use the parameter to provide values for all the features which are part of a specific feature group and used in the feature view. In this second case, you do not have to provide the primary key value for that feature group as no data needs to be retrieved from the online feature store.</p> Python <pre><code># get a single vector, replace values from an entire feature group\n# note how in this example you don't have to provide the value of\n# pk2, but you need to provide the features coming from that feature group\n# in this case feature_b and feature_c\n\nfeature_view.get_feature_vector(\n    entry = { \"pk1\": 1 },\n    passed_features = {\n        \"feature_a\": \"value_a\",\n        \"feature_b\": \"value_b\",\n        \"feature_c\": \"value_c\"\n    }\n)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature-vectors/#retrieving-untransformed-feature-vectors","title":"Retrieving untransformed feature vectors","text":"<p>By default, the <code>get_feature_vector</code> and <code>get_feature_vectors</code> functions return transformed feature vectors, which has model-dependent transformations applied and includes on-demand features.</p> <p>However, you can retrieve the untransformed feature vectors without applying model-dependent transformations while still including on-demand features by setting the <code>transform</code> parameter to False.</p> Python <p>Returning untransformed feature vectors</p> <pre><code># Fetching untransformed feature vector.\nuntransformed_feature_vector = feature_view.get_feature_vector(\n    entry={\"id\": 1}, transform=False\n)\n\n# Fetching untransformed feature vectors.\nuntransformed_feature_vectors = feature_view.get_feature_vectors(\n    entry=[{\"id\": 1}, {\"id\": 2}], transform=False\n)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature-vectors/#retrieving-feature-vector-without-on-demand-features","title":"Retrieving feature vector without on-demand features","text":"<p>The <code>get_feature_vector</code> and <code>get_feature_vectors</code> methods can also return untransformed feature vectors without on-demand features by disabling model-dependent transformations and excluding on-demand features. To achieve this, set the  parameters <code>transform</code> and <code>on_demand_features</code> to <code>False</code>.</p> Python <p>Returning untransformed feature vectors</p> <pre><code>untransformed_feature_vector = feature_view.get_feature_vector(\n    entry={\"id\": 1}, transform=False, on_demand_features=False\n)\nuntransformed_feature_vectors = feature_view.get_feature_vectors(\n    entry=[{\"id\": 1}, {\"id\": 2}], transform=False, on_demand_features=False\n)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature-vectors/#passing-context-variables-to-transformation-functions","title":"Passing Context Variables to Transformation Functions","text":"<p>After defining a transformation function using a context variable, you can pass the required context variables using the <code>transformation_context</code> parameter when fetching the feature vectors.</p> Python <p>Passing context variables while fetching batch data.</p> <pre><code># Passing context variable to IN-MEMORY Training Dataset.\nbatch_data = feature_view.get_feature_vectors(\n    entry = [{ \"pk1\": 1 }],\n    transformation_context={\"context_parameter\":10}\n)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature-vectors/#choose-the-right-client","title":"Choose the right Client","text":"<p>The Online Store can be accessed via the Python or Java client allowing you to use your language of choice to connect to the Online Store. Additionally, the Python client provides two different implementations to fetch data: SQL or REST. The SQL client is the default implementation. It requires a direct SQL connection to your RonDB cluster and uses python asyncio to offer high performance even when your Feature View rows involve querying multiple different tables. The REST client is an alternative implementation connecting to RonDB Feature Vector Server. Perfect if you want to avoid exposing ports of your database cluster directly to clients. This implementation is available as of Hopsworks 3.7.</p> <p>Initialise the client by calling the <code>init_serving</code> method on the Feature View object before starting to fetch feature vectors. This will initialise the chosen client, test the connection, and initialise the transformation functions registered with the Feature View. Note to use the REST client in the Hopsworks Cluster python environment you will need to provide an API key explicitly as JWT authentication is not yet supported. More configuration options can be found in the API documentation.</p> Python <p><pre><code># initialize the SQL client to fetch feature vectors from the Online Store\nmy_feature_view.init_serving()\n\n# or use the REST client\nmy_feature_view.init_serving(\n    init_rest_client=True,\n    config_rest_client={\n        \"api_key\": \"your_api_key\",\n    }\n)\n</code></pre> Once the client is initialised, you can start fetching feature vector(s) via the Feature View methods: <code>get_feature_vector(s)</code>. You can initialise both clients for a given Feature View and switch between them by using the force flags in the get_feature_vector(s) methods.</p> Python <pre><code># initialize both clients and set the default to REST\nmy_feature_view.init_serving(\n    init_rest_client=True,\n    init_sql_client=True,\n    config_rest_client={\n        \"api_key\": \"your_api_key\",\n    },\n    default_client=\"rest\"\n)\n\n# this will fetch a feature vector via REST\ntry:\n    my_feature_view.get_feature_vector(\n        entry = {\"pk1\": 1, \"pk2\": 2},\n    )\nexcept TimeoutException:\n    # if the REST client times out, the SQL client will be used\n    my_feature_view.get_feature_vector(\n        entry = {\"pk1\": 1, \"pk2\": 2},\n        force_sql=True\n    )\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature-vectors/#feature-server","title":"Feature Server","text":"<p>In addition to Python/Java clients, from Hopsworks 3.3, a new feature server implemented in Go is introduced. With this new API, single or batch feature vectors can be retrieved in any programming language. Note that you can connect to the Feature Vector Server via any REST client. However registered transformation function will not be applied to values in the JSON response and values stored in Feature Groups which contain embeddings will be missing.</p>"},{"location":"user_guides/fs/feature_view/feature_logging/","title":"User Guide: Feature and Prediction Logging with a Feature View","text":"<p>Feature logging is essential for debugging, monitoring, and auditing the data your models use. This guide explains how to log features and predictions, and retrieve and manage these logs with feature view in Hopsworks.</p>"},{"location":"user_guides/fs/feature_view/feature_logging/#logging-features-and-predictions","title":"Logging Features and Predictions","text":"<p>After you have trained a model, you can log the features it uses and the predictions with the feature view used to create the training data for the model. You can log either transformed or/and untransformed features values.</p>"},{"location":"user_guides/fs/feature_view/feature_logging/#enabling-feature-logging","title":"Enabling Feature Logging","text":"<p>To enable logging, set <code>logging_enabled=True</code> when creating the feature view. Two feature groups will be created for storing transformed and untransformed features, but they are not visible in the UI. The logged features will be written to the offline feature store every hour by scheduled materialization jobs which are created automatically.</p> <pre><code>feature_view = fs.create_feature_view(\"name\", query, logging_enabled=True)\n</code></pre> <p>Alternatively, you can enable logging on an existing feature view by calling <code>feature_view.enable_logging()</code>. Also, calling <code>feature_view.log()</code> will implicitly enable logging if it has not already been enabled.</p>"},{"location":"user_guides/fs/feature_view/feature_logging/#logging-features-and-predictions_1","title":"Logging Features and Predictions","text":"<p>You can log features and predictions by calling <code>feature_view.log</code>. The logged features are written periodically to the offline store. If you need it to be available immediately, call <code>feature_view.materialize_log</code>.</p> <p>You can log either transformed or/and untransformed features. To get untransformed features, you can specify <code>transform=False</code> in <code>feature_view.get_batch_data</code> or <code>feature_view.get_feature_vector(s)</code>. Inference helper columns are returned along with the untransformed features. If you have On-Demand features as well, call <code>feature_view.compute_on_demand_features</code> to get the on demand features before calling <code>feature_view.log</code>.To get the transformed features, you can call <code>feature_view.transform</code> and pass the untransformed feature with the on-demand feature.</p> <p>Predictions can be optionally provided as one or more columns in the DataFrame containing the features or separately in the <code>predictions</code> argument. There must be the same number of prediction columns as there are labels in the feature view. It is required to provide predictions in the <code>predictions</code> argument if you provide the features as <code>list</code> instead of pandas <code>dataframe</code>. The training dataset version will also be logged if you have called either <code>feature_view.init_serving(...)</code> or <code>feature_view.init_batch_scoring(...)</code> or if the provided model has a training dataset version.</p> <p>The wallclock time of calling <code>feature_view.log</code> is automatically logged, enabling filtering by logging time when retrieving logs.</p>"},{"location":"user_guides/fs/feature_view/feature_logging/#example-1-log-features-only","title":"Example 1: Log Features Only","text":"<p>You have a DataFrame of features you want to log.</p> <pre><code>import pandas as pd\n\nfeatures = pd.DataFrame({\n    \"feature1\": [1.1, 2.2, 3.3],\n    \"feature2\": [4.4, 5.5, 6.6]\n})\n\n# Log features\nfeature_view.log(features)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature_logging/#example-2-log-features-predictions-and-model","title":"Example 2: Log Features, Predictions, and Model","text":"<p>You can also log predictions, and optionally the training dataset and the model used for prediction.</p> <pre><code>predictions = pd.DataFrame({\n    \"prediction\": [0, 1, 0]\n})\n\n# Log features and predictions\nfeature_view.log(features, \n                 predictions=predictions, \n                 training_dataset_version=1, \n                 model=Model(1, \"model\", version=1)\n)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature_logging/#example-3-log-both-transformed-and-untransformed-features","title":"Example 3: Log Both Transformed and Untransformed Features","text":"<p>Batch Features <pre><code>untransformed_df = fv.get_batch_data(transformed=False)\n# then apply the transformations after:\ntransformed_df = fv.transform(untransformed_df)\n# Log untransformed features\nfeature_view.log(untransformed_df)\n# Log transformed features\nfeature_view.log(transformed_features=transformed_df)\n</code></pre></p> <p>Real-time Features <pre><code>untransformed_vector = fv.get_feature_vector({\"id\": 1}, transform=False)\n# then apply the transformations after:\ntransformed_vector = fv.transform(untransformed_vector)\n# Log untransformed features\nfeature_view.log(untransformed_vector)\n# Log transformed features\nfeature_view.log(transformed_features=transformed_vector)\n</code></pre></p>"},{"location":"user_guides/fs/feature_view/feature_logging/#retrieving-the-log-timeline","title":"Retrieving the Log Timeline","text":"<p>To audit and review the feature/prediction logs, you might want to retrieve the timeline of log entries. This helps understand when data was logged and monitor the logs.</p>"},{"location":"user_guides/fs/feature_view/feature_logging/#retrieve-log-timeline","title":"Retrieve Log Timeline","text":"<p>A log timeline is the hudi commit timeline of the logging feature group.</p> <pre><code># Retrieve the latest 10 log entries\nlog_timeline = feature_view.get_log_timeline(limit=10)\nprint(log_timeline)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature_logging/#reading-log-entries","title":"Reading Log Entries","text":"<p>You may need to read specific log entries for analysis, such as entries within a particular time range or for a specific model version and training dataset version.</p>"},{"location":"user_guides/fs/feature_view/feature_logging/#read-all-log-entries","title":"Read all Log Entries","text":"<p>Read all log entries for comprehensive analysis. The output will return all values of the same primary keys instead of just the latest value.</p> <pre><code># Read all log entries\nlog_entries = feature_view.read_log()\nprint(log_entries)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature_logging/#read-log-entries-within-a-time-range","title":"Read Log Entries within a Time Range","text":"<p>Focus on logs within a specific time range. You can specify <code>start_time</code> and <code>end_time</code> for filtering, but the time columns will not be returned in the DataFrame. You can provide the <code>start/end_time</code> as <code>datetime</code>, <code>date</code>, <code>int</code>, or <code>str</code> type. Accepted date format are: <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code></p> <pre><code># Read log entries from January 2022\nlog_entries = feature_view.read_log(start_time=\"2022-01-01\", end_time=\"2022-01-31\")\nprint(log_entries)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature_logging/#read-log-entries-by-training-dataset-version","title":"Read Log Entries by Training Dataset Version","text":"<p>Analyze logs from a particular version of the training dataset. The training dataset version column will be returned in the DataFrame.</p> <pre><code># Read log entries of training dataset version 1\nlog_entries = feature_view.read_log(training_dataset_version=1)\nprint(log_entries)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature_logging/#read-log-entries-by-model-in-hopsworks","title":"Read Log Entries by Model in Hopsworks","text":"<p>Analyze logs from a particular name and version of the HSML model. The HSML model column will be returned in the DataFrame.</p> <pre><code># Read log entries of a specific HSML model\nlog_entries = feature_view.read_log(model=Model(1, \"model\", version=1))\nprint(log_entries)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature_logging/#read-log-entries-using-a-custom-filter","title":"Read Log Entries using a Custom Filter","text":"<p>Provide filters which work similarly to the filter method in the <code>Query</code> class. The filter should be part of the query in the feature view.</p> <pre><code># Read log entries where feature1 is greater than 0\nlog_entries = feature_view.read_log(filter=fg.feature1 &gt; 0)\nprint(log_entries)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature_logging/#pausing-and-resuming-logging","title":"Pausing and Resuming Logging","text":"<p>During maintenance or updates, you might need to pause logging to save computation resources.</p>"},{"location":"user_guides/fs/feature_view/feature_logging/#pause-logging","title":"Pause Logging","text":"<p>Pause the schedule of the materialization job for writing logs to the offline store.</p> <pre><code># Pause logging\nfeature_view.pause_logging()\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature_logging/#resume-logging","title":"Resume Logging","text":"<p>Resume the schedule of the materialization job for writing logs to the offline store.</p> <pre><code># Resume logging\nfeature_view.resume_logging()\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature_logging/#materializing-logs","title":"Materializing Logs","text":"<p>Besides the scheduled materialization job, you can materialize logs from Kafka to the offline store on demand. This does not pause the scheduled job. By default, it materializes both transformed and untransformed logs, optionally specifying whether to materialize transformed (transformed=True) or untransformed (transformed=False) logs.</p>"},{"location":"user_guides/fs/feature_view/feature_logging/#materialize-logs","title":"Materialize Logs","text":"<p>Materialize logs and optionally wait for the process to complete.</p> <pre><code># Materialize logs and wait for completion\nmaterialization_result = feature_view.materialize_log(wait=True)\n# Materialize only transformed log entries\nfeature_view.materialize_log(wait=True, transformed=True)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature_logging/#deleting-logs","title":"Deleting Logs","text":"<p>When log data is no longer needed, you might want to delete it to free up space and maintain data hygiene. This operation deletes the feature groups and recreates new ones. Scheduled materialization job and log timeline are reset as well.</p>"},{"location":"user_guides/fs/feature_view/feature_logging/#delete-logs","title":"Delete Logs","text":"<p>Remove all log entries (both transformed and untransformed logs), optionally specifying whether to delete transformed (transformed=True) or untransformed (transformed=False) logs. </p> <pre><code># Delete all log entries\nfeature_view.delete_log()\n\n# Delete only transformed log entries\nfeature_view.delete_log(transformed=True)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature_logging/#summary","title":"Summary","text":"<p>Feature logging is a crucial part of maintaining and monitoring your machine learning workflows. By following these examples, you can effectively log, retrieve, and delete logs, as well as manage the lifecycle of log materialization jobs, adding observability for your AI system and making it auditable.</p>"},{"location":"user_guides/fs/feature_view/feature_monitoring/","title":"Feature Monitoring for Feature Views","text":"<p>Feature Monitoring complements the Hopsworks data validation capabilities for Feature Group data by allowing you to monitor your data once they have been ingested into the Feature Store. Hopsworks feature monitoring is centered around two functionalities: scheduled statistics and statistics comparison.</p> <p>Before continuing with this guide, see the Feature monitoring guide to learn more about how feature monitoring works, and get familiar with the different use cases of feature monitoring for Feature Views described in the Use cases sections of the Scheduled statistics guide and Statistics comparison guide.</p> <p>Limited UI support</p> <p>Currently, feature monitoring can only be configured using the Hopsworks Python library. However, you can enable/disable a feature monitoring configuration or trigger the statistics comparison manually from the UI, as shown in the Advanced guide.</p>"},{"location":"user_guides/fs/feature_view/feature_monitoring/#code","title":"Code","text":"<p>In this section, we show you how to setup feature monitoring in a Feature View using the Hopsworks Python library. Alternatively, you can get started quickly by running our tutorial for feature monitoring.</p> <p>First, checkout the pre-requisite and Hopsworks setup to follow the guide below. Create a project, install the Hopsworks Python library in your environment and connect via the generated API key. The second step is to start a new configuration for feature monitoring. </p> <p>After that, you can optionally define a detection window of data to compute statistics on, or use the default detection window (i.e., whole feature data). If you want to setup scheduled statistics alone, you can jump to the last step to save your configuration. Otherwise, the third and fourth steps are also optional and show you how to setup the comparison of statistics on a schedule by defining a reference window and specifying the statistics metric to be compared.</p>"},{"location":"user_guides/fs/feature_view/feature_monitoring/#step-1-pre-requisite","title":"Step 1: Pre-requisite","text":"<p>In order to setup feature monitoring for a Feature View, you will need:</p> <ul> <li>A Hopsworks project. If you don't have a project yet you can go to app.hopsworks.ai, signup with your email and create your first project.</li> <li>An API key, you can get one by going to \"Account Settings\" on app.hopsworks.ai.</li> <li>The Hopsworks Python library installed in your client. See the installation guide.</li> <li>A Feature View</li> <li>A Training Dataset</li> </ul>"},{"location":"user_guides/fs/feature_view/feature_monitoring/#connect-your-notebook-to-hopsworks","title":"Connect your notebook to Hopsworks","text":"<p>Connect the client running your notebooks to Hopsworks.</p> Python <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nfs = project.get_feature_store()\n</code></pre> <p>You will be prompted to paste your API key to connect the notebook to your project. The <code>fs</code> Feature Store entity is now ready to be used to insert or read data from Hopsworks.</p>"},{"location":"user_guides/fs/feature_view/feature_monitoring/#get-or-create-a-feature-view","title":"Get or create a Feature View","text":"<p>Feature monitoring can be enabled on already created Feature Views. We suggest you read the Feature View concept page to understand what a feature view is. We also suggest you familiarize with the APIs to create a feature view and how to create them using the query abstraction.</p> <p>The following is a code example for getting or creating a Feature View with name <code>trans_fv</code> for transaction data.</p> Python <pre><code># Retrieve an existing feature view\ntrans_fv = fs.get_feature_view(\"trans_fv\", version=1)\n\n# Or, create a new feature view\nquery = trans_fg.select([\"fraud_label\", \"amount\", \"cc_num\"])\ntrans_fv = fs.create_feature_view(\n    name=\"trans_fv\",\n    version=1,\n    query=query,\n    labels=[\"fraud_label\"],\n)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature_monitoring/#get-or-create-a-training-dataset","title":"Get or create a Training Dataset","text":"<p>The following is a code example for creating a training dataset with two splits using a previously created feature view.</p> Python <pre><code># Create a training dataset with train and test splits\n_, _ = trans_fv.create_train_validation_test_split(\n    description = 'transactions fraud batch training dataset',\n    data_format = 'csv',\n    validation_size = 0.2,\n    test_size = 0.1,\n)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature_monitoring/#step-2-initialize-configuration","title":"Step 2: Initialize configuration","text":""},{"location":"user_guides/fs/feature_view/feature_monitoring/#scheduled-statistics","title":"Scheduled statistics","text":"<p>You can setup statistics monitoring on a single feature or multiple features of your Feature Group data, included in your Feature View query.</p> Python <pre><code># compute statistics for all the features\nfg_monitoring_config = trans_fv.create_statistics_monitoring(\n    name=\"trans_fv_all_features_monitoring\",\n    description=\"Compute statistics on all data of all features of the Feature Group data on a daily basis\",\n)\n\n# or for a single feature\nfg_monitoring_config = trans_fv.create_statistics_monitoring(\n    name=\"trans_fv_amount_monitoring\",\n    description=\"Compute statistics on all data of a single feature of the Feature Group data on a daily basis\",\n    feature_name=\"amount\",\n)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature_monitoring/#statistics-comparison","title":"Statistics comparison","text":"<p>When enabling the comparison of statistics in a feature monitoring configuration, you need to specify a single feature of your Feature Group data, included in your Feature View query. You can create multiple feature monitoring configurations on the same Feature View, but each of them should point to a single feature in the Feature View query.</p> Python <pre><code>fg_monitoring_config = trans_fv.create_feature_monitoring(\n    name=\"trans_fv_amount_monitoring\",\n    feature_name=\"amount\",\n    description=\"Compute descriptive statistics on the amount Feature of the Feature Group data on a daily basis\",\n)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature_monitoring/#custom-schedule-or-percentage-of-window-data","title":"Custom schedule or percentage of window data","text":"<p>By default, the computation of statistics is scheduled to run endlessly, every day at 12PM. You can modify the default schedule by adjusting the <code>cron_expression</code>, <code>start_date_time</code> and <code>end_date_time</code> parameters.</p> Python <pre><code>fg_monitoring_config = trans_fv.create_statistics_monitoring(\n    name=\"trans_fv_all_features_monitoring\",\n    description=\"Compute statistics on all data of all features of the Feature Group data on a weekly basis\",\n    cron_expression=\"0 0 12 ? * MON *\",  # weekly \n    row_percentage=0.8,                  # use 80% of the data\n)\n\n# or\nfg_monitoring_config = trans_fv.create_feature_monitoring(\n    name=\"trans_fv_amount_monitoring\",\n    feature_name=\"amount\",\n    description=\"Compute descriptive statistics on the amount Feature of the Feature Group data on a weekly basis\",\n    cron_expression=\"0 0 12 ? * MON *\",  # weekly \n    row_percentage=0.8,                  # use 80% of the data\n)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature_monitoring/#step-3-optional-define-a-detection-window","title":"Step 3: (Optional) Define a detection window","text":"<p>By default, the detection window is an expanding window covering the whole Feature Group data. You can define a different detection window using the <code>window_length</code> and <code>time_offset</code> parameters provided in the <code>with_detection_window</code> method. Additionally, you can specify the percentage of feature data on which statistics will be computed using the <code>row_percentage</code> parameter.</p> Python <pre><code>fm_monitoring_config.with_detection_window(\n    window_length=\"1w\",  # data ingested during one week\n    time_offset=\"1w\",    # starting from last week\n    row_percentage=0.8,  # use 80% of the data\n)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature_monitoring/#step-4-optional-define-a-reference-window","title":"Step 4: (Optional) Define a reference window","text":"<p>When setting up feature monitoring for a Feature View, reference windows can be either a regular window, a specific value (i.e., window of size 1) or a training dataset.</p> Python <pre><code># compare statistics against a reference window\nfm_monitoring_config.with_reference_window(\n    window_length=\"1w\",  # data ingested during one week\n    time_offset=\"2w\",    # starting from two weeks ago\n    row_percentage=0.8,  # use 80% of the data\n)\n\n# or a specific value\nfm_monitoring_config.with_reference_value(\n    value=100,\n)\n\n# or a training dataset\nfm_monitoring_config.with_reference_training_dataset(\n    training_dataset_version=1, # use the training dataset used to train your production model\n)\n</code></pre>"},{"location":"user_guides/fs/feature_view/feature_monitoring/#step-5-optional-define-the-statistics-comparison-criteria","title":"Step 5: (Optional) Define the statistics comparison criteria","text":"<p>In order to compare detection and reference statistics, you need to provide the criteria for such comparison. First, you select the metric to consider in the comparison using the <code>metric</code> parameter. Then, you can define a relative or absolute threshold using the <code>threshold</code> and <code>relative</code> parameters.</p> Python <pre><code>fm_monitoring_config.compare_on(\n    metric=\"mean\", \n    threshold=0.2,  # a relative change over 20% is considered anomalous\n    relative=True,  # relative or absolute change\n    strict=False,   # strict or relaxed comparison\n)\n</code></pre> <p>Difference values and thresholds</p> <p>For more information about the computation of difference values and the comparison against threshold bounds see the Comparison criteria section in the Statistics comparison guide.</p>"},{"location":"user_guides/fs/feature_view/feature_monitoring/#step-6-save-configuration","title":"Step 6: Save configuration","text":"<p>Finally, you can save your feature monitoring configuration by calling the <code>save</code> method. Once the configuration is saved, the schedule for the statistics computation and comparison will be activated automatically.</p> Python <pre><code>fm_monitoring_config.save()\n</code></pre> <p>Next steps</p> <p>See the Advanced guide to learn how to delete, disable or trigger feature monitoring manually.</p>"},{"location":"user_guides/fs/feature_view/helper-columns/","title":"Helper columns","text":"<p>Hopsworks Feature Store provides a functionality to define two types of helper columns <code>inference_helper_columns</code> and <code>training_helper_columns</code> for feature views.</p> <p>Note</p> <p>Both inference and training helper column name(s) must be part of the <code>Query</code> object. If helper column name(s) belong to feature group that is part of a <code>Join</code> with <code>prefix</code> defined, then this prefix needs to prepended to the original column name when defining helper column list.</p>"},{"location":"user_guides/fs/feature_view/helper-columns/#inference-helper-columns","title":"Inference Helper columns","text":"<p><code>inference_helper_columns</code> are a list of feature names that are not used for training the model itself but are used for extra information during online or batch inference. For example, computing an on-demand feature such as <code>days_valid</code> (days left that a credit card is valid at the time of the transaction)  in a credit card fraud detection system. The feature <code>days_valid</code> will be computed using the credit card expiry date that needs to be fetched from the feature store and compared to the transaction  date that the transaction is performed on (<code>days_valid</code> = <code>expiry_date</code> - <code>current_date</code>). In this use case <code>expiry_date</code> is an inference helper column. It is not used for training but is necessary  for computing the on-demand feature<code>days_valid</code> feature.</p> Python <p>Define inference columns for feature views.</p> <pre><code># define query object \nquery = label_fg.select(\"fraud_label\")\\\n                .join(trans_fg.select([\"amount\", \"days_valid\", \"expiry_date\", \"category\"])) \n\n# define feature view with helper columns\nfeature_view = fs.get_or_create_feature_view(\n    name='fv_with_helper_col',\n    version=1,\n    query=query,\n    labels=[\"fraud_label\"],\n    transformation_functions=transformation_functions,\n    inference_helper_columns=[\"expiry_date\"],\n)\n</code></pre>"},{"location":"user_guides/fs/feature_view/helper-columns/#retrieval","title":"Retrieval","text":"<p>When retrieving data for model inference, helper columns will be omitted. However, they can be optionally fetched with inference or training data.</p>"},{"location":"user_guides/fs/feature_view/helper-columns/#batch-inference","title":"Batch inference","text":"Python <p>Fetch inference helper column values and compute on-demand features during batch inference.</p> <pre><code># import feature functions\nfrom feature_functions import time_delta\n\n# Fetch feature view object  \nfeature_view = fs.get_feature_view(\n    name='fv_with_helper_col',\n    version=1,\n)\n\n# Fetch feature data for batch inference with helper columns\ndf = feature_view.get_batch_data(start_time=start_time, end_time=end_time, inference_helpers=True, event_time=True)\n\n# compute location delta\ndf['days_valid'] = df.apply(lambda row: time_delta(row['expiry_date'], row['transaction_date']), axis=1)\n\n# prepare datatame for prediction\ndf = df[[f.name for f in feature_view.features if not (f.label or f.inference_helper_column or f.training_helper_column)]]\n</code></pre>"},{"location":"user_guides/fs/feature_view/helper-columns/#online-inference","title":"Online inference","text":"Python <p>Fetch inference helper column values and compute on-demand features during online inference.</p> <pre><code>from feature_functions import time_delta\n\n# Fetch feature view object  \nfeature_view = fs.get_feature_view(\n    name='fv_with_helper_col',\n    version=1,\n)\n\n# Fetch feature data for batch inference without helper columns\ndf_without_inference_helpers = feature_view.get_batch_data()\n\n# Fetch feature data for batch inference with helper columns\ndf_with_inference_helpers = feature_view.get_batch_data(inference_helpers=True)\n\n# here cc_num, longitute and lattitude are provided as parameters to the application\ncc_num = ...\ntransaction_date = ...\n\n# get previous transaction location of this credit card\ninference_helper = feature_view.get_inference_helper({\"cc_num\": cc_num}, return_type=\"dict\")\n\n# compute location delta \ndays_valid = time_delta(transaction_date, inference_helper['expiry_date'])\n\n# Now get assembled feature vector for prediction\nfeature_vector = feature_view.get_feature_vector({\"cc_num\": cc_num}, \n                                                  passed_features={\"days_valid\": days_valid}\n                                                 )\n</code></pre>"},{"location":"user_guides/fs/feature_view/helper-columns/#training-helper-columns","title":"Training Helper columns","text":"<p><code>training_helper_columns</code> are a list of feature names that are not the part of the model schema itself but are used during training for the extra information.  For example one might want to use feature like <code>category</code> of the purchased product to assign different weights.</p> Python <p>Define training helper columns for feature views.</p> <pre><code># define query object \nquery = label_fg.select(\"fraud_label\")\\\n                .join(trans_fg.select([\"amount\", \"days_valid\", \"expiry_date\", \"category\"])) \n\n# define feature view with helper columns\nfeature_view = fs.get_or_create_feature_view(\n    name='fv_with_helper_col',\n    version=1,\n    query=query,\n    labels=[\"fraud_label\"],\n    transformation_functions=transformation_functions,\n    training_helper_columns=[\"category\"]\n)\n</code></pre>"},{"location":"user_guides/fs/feature_view/helper-columns/#retrieval_1","title":"Retrieval","text":"<p>When retrieving training data helper columns will be omitted. However, they can be optionally fetched.</p> Python <p>Fetch training data with or without inference helper column values.</p> <pre><code># import feature functions\nfrom feature_functions import location_delta, time_delta\n\n# Fetch feature view object  \nfeature_view = fs.get_feature_view(\n    name='fv_with_helper_col',\n    version=1,\n)\n\n# Create and training data with training helper columns\nTEST_SIZE = 0.2\nX_train, X_test, y_train, y_test = feature_view.train_test_split(\n    description='transactions fraud training dataset',\n    test_size=TEST_SIZE,\n     training_helper_columns=True\n)\n\n# Get existing training data with training helper columns\nX_train, X_test, y_train, y_test = feature_view.get_train_test_split(\n     training_dataset_version=1,\n     training_helper_columns=True\n)\n</code></pre> <p>Note</p> <p>To use helper columns with materialized training dataset it needs to be created with <code>training_helper_columns=True</code>.  </p>"},{"location":"user_guides/fs/feature_view/model-dependent-transformations/","title":"Model Dependent Transformation Functions","text":"<p>Model-dependent transformations transform feature data for a specific model. Feature encoding is one example of such a transformations. Feature encoding is parameterized by statistics from the training dataset, and, as such, many model-dependent transformations require the training dataset statistics as a parameter. Hopsworks enhances the robustness of AI pipelines by preventing training-inference skew by ensuring that the same model-dependent transformations and statistical parameters are used during both training dataset generation and online inference.</p> <p>Additionally, Hopsworks offers built-in model-dependent transformation functions, such as <code>min_max_scaler</code>, <code>standard_scaler</code>, <code>robust_scaler</code>, <code>label_encoder</code>, and <code>one_hot_encoder</code>, which can be easily imported and declaratively applied to features in a feature view.</p>"},{"location":"user_guides/fs/feature_view/model-dependent-transformations/#model-dependent-transformation-function-creation","title":"Model Dependent Transformation Function Creation","text":"<p>Hopsworks allows you to create a model-dependent transformation function by attaching a\u00a0transformation function\u00a0to a feature view. The attached transformation function can be a simple function that takes one feature as input and outputs the transformed feature data. For example, in the case of min-max scaling a numerical feature, you will have a number as input parameter to the transformation function and a number as output. However, in the case of one-hot encoding a categorical variable, you will have a string as input and an array of 1s and 0s and output. You can also have transformation functions that take multiple features as input and produce one or more values as output. That is, transformation functions can be one-to-one, one-to-many, many-to-one, or many-to-many.</p> <p>Each model-dependent transformation function can map specific features to its arguments by explicitly providing their names as arguments to the transformation function. If no feature names are provided, the transformation function will default to using features from the feature view that match the name of the transformation function's argument.</p> <p>Hopsworks by default generates default names of transformed features output by a model-dependent transformation function. The generated names follows a naming convention structured as\u00a0<code>functionName_features_outputColumnNumber</code> if the transformation function outputs multiple columns and <code>functionName_features</code> if the transformation function outputs one column. For instance, for the function named\u00a0<code>add_one_multiple</code> that outputs multiple columns\u00a0in the example given below, produces output columns that would be labeled as\u00a0 <code>add_one_multiple_feature1_feature2_feature3_0</code>,\u00a0 <code>add_one_multiple_feature1_feature2_feature3_1</code>  and  \u00a0<code>add_one_multiple_feature1_feature2_feature3_2</code>. The function named\u00a0<code>add_two</code> that outputs a single column in the example given below, produces a single output column names as <code>add_two_feature</code>. Additionally, Hopsworks also allows users to specify custom names for transformed feature using the <code>alias</code> function.</p> Python <p>Creating model-dependent transformation functions</p> <pre><code># Defining a many to many transformation function.\n@udf(return_type=[int, int, int], drop=[\"feature1\", \"feature3\"])\ndef add_one_multiple(feature1, feature2, feature3):\n    return pd.DataFrame({\"add_one_feature1\":feature1 + 1, \"add_one_feature2\":feature2 + 1, \"add_one_feature3\":feature3 + 1})\n\n# Defining a one to one transformation function.\n@udf(return_type=int)\ndef add_two(feature):\n    return feature + 2\n\n# Creating model-dependent transformations by attaching transformation functions to feature views.\nfeature_view = fs.create_feature_view(\n    name='transactions_view',\n    query=query,\n    labels=[\"fraud_label\"],\n    transformation_functions=[\n        add_two,\n        add_one_multiple\n    ]\n)\n</code></pre>"},{"location":"user_guides/fs/feature_view/model-dependent-transformations/#specifying-input-features","title":"Specifying input features","text":"<p>The features to be used by a model-dependent transformation function can be specified by providing the feature names (from the feature view / feature group) as input to the transformation functions. </p> Python <p>Specifying input features to be passed to a model-dependent transformation function</p> <pre><code>feature_view = fs.create_feature_view(\n    name='transactions_view',\n    query=query,\n    labels=[\"fraud_label\"],\n    transformation_functions=[\n        add_two(\"feature_1\"),\n        add_two(\"feature_2\"),\n        add_one_multiple(\"feature_5\", \"feature_6\", \"feature_7\")\n    ]\n)\n</code></pre>"},{"location":"user_guides/fs/feature_view/model-dependent-transformations/#using-built-in-transformations","title":"Using built-in transformations","text":"<p>Built-in transformation functions are attached in the same way. The only difference is that they can either be retrieved from the Hopsworks or imported from the <code>hopsworks</code> module.</p> Python <p>Creating model-dependent transformation using built-in transformation functions retrieved from Hopsworks</p> <pre><code>min_max_scaler = fs.get_transformation_function(name=\"min_max_scaler\")\nstandard_scaler = fs.get_transformation_function(name=\"standard_scaler\")\nrobust_scaler = fs.get_transformation_function(name=\"robust_scaler\")\nlabel_encoder = fs.get_transformation_function(name=\"label_encoder\")\n\nfeature_view = fs.create_feature_view(\n    name='transactions_view',\n    query=query,\n    labels=[\"fraud_label\"],\n    transformation_functions = [\n        label_encoder(\"category\"),\n        robust_scaler(\"amount\"),\n        min_max_scaler(\"loc_delta\"),\n        standard_scaler(\"age_at_transaction\")\n    ]\n)\n</code></pre> <p>To attach built-in transformation functions from the <code>hopsworks</code> module they can be directly imported into the code from <code>hopsworks.builtin_transformations</code>.</p> Python <p>Creating model-dependent transformation using built-in transformation functions imported from hopsworks</p> <pre><code>from hopsworks.hsfs.builtin_transformations import min_max_scaler, label_encoder, robust_scaler, standard_scaler\n\nfeature_view = fs.create_feature_view(\n    name='transactions_view',\n    query=query,\n    labels=[\"fraud_label\"],\n    transformation_functions = [\n        label_encoder(\"category\"),\n        robust_scaler(\"amount\"),\n        min_max_scaler(\"loc_delta\"),\n        standard_scaler(\"age_at_transaction\")\n    ]\n)\n</code></pre>"},{"location":"user_guides/fs/feature_view/model-dependent-transformations/#using-model-dependent-transformations","title":"Using Model Dependent Transformations","text":"<p>Model-dependent transformations attached to a feature view are automatically applied when you create training data, read training data, read batch inference data, or get feature vectors. The generated data includes untransformed features, on-demand features, if any, and the transformed features. The transformed features are organized by their output column names in alphabetical order and are positioned after the untransformed and on-demand features. </p> <p>Model-dependent transformation functions can also be manually applied to a feature vector using the <code>transform</code> function. </p> Python <p>Manually applying model-dependent transformations during online inference</p> <pre><code># Initialize the feature view with the correct training dataset version used for model-dependent transformations\nfv.init_serving(training_dataset_version)\n\n# Get untransformed feature Vector\nfeature_vector = fv.get_feature_vector(entry={\"index\":10}, transform=False, return_type=\"pandas\")\n\n# Apply Model Dependent transformations\nencoded_feature_vector = fv.transform(feature_vector)\n</code></pre>"},{"location":"user_guides/fs/feature_view/model-dependent-transformations/#retrieving-untransformed-feature-vector-and-batch-inference-data","title":"Retrieving untransformed feature vector and batch inference data","text":"<p>The <code>get_feature_vector</code>, <code>get_feature_vectors</code>, and <code>get_batch_data</code> methods can return untransformed feature vectors and batch data without applying model-dependent transformations while still including on-demand features. To achieve this, set the <code>transform</code> parameter to False.</p> Python <p>Returning untransformed feature vectors and batch data.</p> <pre><code># Fetching untransformed feature vector.\nuntransformed_feature_vector = feature_view.get_feature_vector(\n    entry={\"id\": 1}, transform=False\n)\n\n# Fetching untransformed feature vectors.\nuntransformed_feature_vectors = feature_view.get_feature_vectors(\n    entry=[{\"id\": 1}, {\"id\": 2}], transform=False\n)\n\n# Fetching untransformed batch data.\nuntransformed_batch_data = feature_view.get_batch_data(\n    transform=False\n)\n</code></pre>"},{"location":"user_guides/fs/feature_view/overview/","title":"Feature View","text":"<p>A feature view is a set of features that come from one or more feature groups. It is a logical view over the feature groups, as the feature data is only stored in feature groups. Feature views are used to read feature data for both training and serving (online and batch). You can create training datasets, create batch data and get feature vectors.</p> <p>If you want to understand more about the concept of feature view, you can refer to here.</p>"},{"location":"user_guides/fs/feature_view/overview/#feature-view-creation","title":"Feature View Creation","text":"<p>Query and transformation function are the building blocks of a feature view. You can define your set of features by building a <code>query</code>. You can also define which columns in your feature view are the <code>labels</code>, which is useful for supervised machine learning tasks. Furthermore, in python client, each feature can be attached to its own transformation function. This way, when a feature is read (for training or scoring), the transformation is executed on-demand - just before the feature data is returned. For example, when a client reads a numerical feature, the feature value could be normalized by a StandardScalar transformation function before it is returned to the client.</p> PythonJava <pre><code># create a simple feature view\nfeature_view = fs.create_feature_view(\n    name='transactions_view',\n    query=query\n)\n\n# create a feature view with transformation and label\nfeature_view = fs.create_feature_view(\n    name='transactions_view',\n    query=query,\n    labels=[\"fraud_label\"],\n    transformation_functions={\n        \"amount\": fs.get_transformation_function(name=\"standard_scaler\", version=1)\n    }\n)\n</code></pre> <pre><code>// create a simple feature view\nFeatureView featureView = featureStore.createFeatureView()\n                                        .name(\"transactions_view)\n                                        .query(query)\n                                        .build();\n\n// create a feature view with label\nFeatureView featureView = featureStore.createFeatureView()\n                                        .name(\"transactions_view)\n                                        .query(query)\n                                        .labels(Lists.newArrayList(\"fraud_label\")\n                                        .build();\n</code></pre> <p>You can refer to query and transformation function for creating <code>query</code> and <code>transformation_function</code>. To see a full example of how to create a feature view, you can read this notebook.</p>"},{"location":"user_guides/fs/feature_view/overview/#retrieval","title":"Retrieval","text":"<p>Once you have created a feature view, you can retrieve it by its name and version.</p> PythonJava <pre><code>feature_view = fs.get_feature_view(name=\"transactions_view\", version=1)\n</code></pre> <pre><code>FeatureView featureView = featureStore.getFeatureView(\"transactions_view\", 1)\n</code></pre>"},{"location":"user_guides/fs/feature_view/overview/#deletion","title":"Deletion","text":"<p>If there are some feature view instances which you do not use anymore, you can delete a feature view. It is important to mention that all training datasets (include all materialised hopsfs training data) will be deleted along with the feature view.</p> PythonJava <pre><code>feature_view.delete()\n</code></pre> <pre><code>featureView.delete()\n</code></pre>"},{"location":"user_guides/fs/feature_view/overview/#tags","title":"Tags","text":"<p>Feature views also support tags. You can attach, get, and remove tags. You can refer to here if you want to learn more about how tags work.</p> PythonJava <pre><code># attach\nfeature_view.add_tag(name=\"tag_schema\", value={\"key\", \"value\"}\n\n# get\nfeature_view.get_tag(name=\"tag_schema\")\n\n#remove\nfeature_view.delete_tag(name=\"tag_schema\")\n</code></pre> <pre><code>// attach\nMap&lt;String, String&gt; tag = Maps.newHashMap();\ntag.put(\"key\", \"value\");\nfeatureView.addTag(\"tag_schema\", tag)\n\n// get\nfeatureView.getTag(\"tag_schema\")\n\n// remove\nfeatureView.deleteTag(\"tag_schema\")\n</code></pre>"},{"location":"user_guides/fs/feature_view/overview/#next","title":"Next","text":"<p>Once you have created a feature view, you can now create training data</p>"},{"location":"user_guides/fs/feature_view/query/","title":"Query vs DataFrame","text":"<p>HSFS provides a DataFrame API to ingest data into the Hopsworks Feature Store. You can also retrieve feature data in a DataFrame, that can either be used directly to train models or materialized to file(s) for later use to train models.</p> <p>The idea of the Feature Store is to have pre-computed features available for both training and serving models. The key functionality required to generate training datasets from reusable features are: feature selection, joins, filters, and point in time queries. The Query object enables you to select features from different feature groups to join together to be used in a feature view.</p> <p>The joining functionality is heavily inspired by the APIs used by Pandas to merge DataFrames. The APIs allow you to specify which features to select from which feature group, how to join them and which features to use in join conditions.</p> PythonScala <pre><code>fs = ...\ncredit_card_transactions_fg = fs.get_feature_group(name=\"credit_card_transactions\", version=1)\naccount_details_fg = fs.get_feature_group(name=\"account_details\", version=1)\nmerchant_details_fg = fs.get_feature_group(name=\"merchant_details\", version=1)\n\n# create a query\nselected_features = credit_card_transactions_fg.select_all() \\\n    .join(account_details_fg.select_all(), on=[\"cc_num\"]) \\\n    .join(merchant_details_fg.select_all())\n\n# save the query to feature view\nfeature_view = fs.create_feature_view(\n    version=1, \n    name='credit_card_fraud',\n    labels=[\"is_fraud\"],\n    query=selected_features\n)\n\n# retrieve the query back from the feature view\nfeature_view = fs.get_feature_view(\u201ccredit_card_fraud\u201d, version=1)\nquery = feature_view.query\n</code></pre> <pre><code>val fs = ...\nval creditCardTransactionsFg = fs.getFeatureGroup(\"credit_card_transactions\", 1)\nval accountDetailsFg = fs.getFeatureGroup(name=\"account_details\", version=1)\nval merchantDetailsFg = fs.getFeatureGroup(\"merchant_details\", 1)\n\n// create a query\nval selectedFeatures = (creditCardTransactionsFg.selectAll()\n    .join(accountDetailsFg.selectAll(), on=Seq(\"cc_num\"))\n    .join(merchantDetailsFg.selectAll()))\n\nval featureView = featureStore.createFeatureView()\n    .name(\"credit_card_fraud\")\n    .query(selectedFeatures)\n    .build();\n\n// retrieve the query back from the feature view\nval featureView = fs.getFeatureView(\u201ccredit_card_fraud\u201d, 1)\nval query = featureView.getQuery()\n</code></pre> <p>If a data scientist wants to modify a new feature that is not available in the feature store, she can write code to compute the new feature (using existing features or external data) and ingest the new feature values into the feature store. If the new feature is based solely on existing feature values in the Feature Store, we call it a derived feature. The same HSFS APIs can be used to compute derived features as well as features using external data sources.</p>"},{"location":"user_guides/fs/feature_view/query/#the-query-abstraction","title":"The Query Abstraction","text":"<p>Most operations performed on <code>FeatureGroup</code> metadata objects will return a <code>Query</code> with the applied operation.</p>"},{"location":"user_guides/fs/feature_view/query/#examples","title":"Examples","text":"<p>Selecting features from a feature group is a lazy operation, returning a query with the selected features only:</p> PythonScala <pre><code>credit_card_transactions_fg = fs.get_feature_group(\"credit_card_transactions\")\n\n# Returns Query\nselected_features = credit_card_transactions_fg.select([\"amount\", \"latitude\", \"longitude\"])\n</code></pre> <pre><code>val creditCardTransactionsFg = fs.getFeatureGroup(\"credit_card_transactions\")\n\n# Returns Query\nval selectedFeatures = creditCardTransactionsFg.select(Seq(\"amount\", \"latitude\", \"longitude\"))\n</code></pre>"},{"location":"user_guides/fs/feature_view/query/#join","title":"Join","text":"<p>Similarly, joins return query objects. The simplest join in one where we join all of the features together from two different feature groups without specifying a join key - <code>HSFS</code> will infer the join key as a common primary key between the two feature groups. By default, Hopsworks will use the maximal matching subset of the primary keys of the two feature groups as joining key(s), if not specified otherwise.</p> PythonScala <pre><code># Returns Query\nselected_features = credit_card_transactions_fg.join(account_details_fg)\n</code></pre> <pre><code>// Returns Query\nval selectedFeatures = creditCardTransactionsFg.join(accountDetailsFg)\n</code></pre> <p>More complex joins are possible by selecting subsets of features from the joined feature groups and by specifying a join key and type. Possible join types are \"inner\", \"left\" or \"right\". By default<code>join_type</code> is `\"left\". Furthermore, it is possible to specify different  features for the join key of the left and right feature group. The join key lists should contain the names of the features to join on.</p> PythonScala <pre><code>selected_features = credit_card_transactions_fg.select_all() \\\n    .join(account_details_fg.select_all(), on=[\"cc_num\"]) \\\n    .join(merchant_details_fg.select_all(), left_on=[\"merchant_id\"], right_on=[\"id\"], join_type=\"inner\")\n</code></pre> <pre><code>val selectedFeatures = (creditCardTransactionsFg.selectAll()\n    .join(accountDetailsFg.selectAll(), Seq(\"cc_num\"))\n    .join(merchantDetailsFg.selectAll(), Seq(\"merchant_id\"), Seq(\"id\"), \"inner\"))\n</code></pre> <p>Warning</p> <p>If there is feature name clash in the query then prefixes will be automatically generated and applied. Generated prefix is feature group alias in the query (e.g. fg1, fg2). Prefix is applied to the right feature group of the query.</p>"},{"location":"user_guides/fs/feature_view/query/#data-modeling-in-hopsworks","title":"Data modeling in Hopsworks","text":"<p>Since v4.0 Hopsworks Feature selection API supports both Star and Snowflake Schema data models.</p>"},{"location":"user_guides/fs/feature_view/query/#star-schema-data-model","title":"Star schema data model","text":"<p>When choosing Star Schema data model all tables are children of the parent (the left most) feature group, which has all  foreign keys for its child feature groups.</p> <p> Star schema data model </p> Python <pre><code>   selected_features = credit_card_transactions.select_all()\n    .join(aggregated_cc_transactions.select_all())\n    .join(account_details.select_all())\n    .join(merchant_details.select_all())\n    .join(cc_issuer_details.select_all())\n</code></pre> <p>In online inference, when you want to retrieve features in your online model, you have to provide all foreign key values,  known as the serving_keys, from the parent feature group to retrieve your precomputed feature values using the feature view.</p> Python <pre><code>  feature vector = feature_view.get_feature_vector({\n    \u2018cc_num\u2019: \u201c1234 5555 3333 8888\u201d,\n    \u2018issuer_id\u2019: 20440455,\n    \u2018merchant_id\u2019: 44208484,\n    \u2018account_id\u2019: 84403331\n    })\n</code></pre>"},{"location":"user_guides/fs/feature_view/query/#snowflake-schema","title":"Snowflake schema","text":"<p>Hopsworks also provides the possibility to define a feature view that consists of a nested tree of children (to up to a depth of 20)  from the root (left most) feature group. This is called  Snowflake Schema data model where you need to build nested tables (subtrees) using joins, and then join the subtrees to their parents iteratively until you reach the root node (the leftmost feature group in the feature selection):</p> <p> Snowflake schema data model </p> Python <pre><code>    nested_selection = aggregated_cc_transactions.select_all()\n    .join(account_details.select_all())\n    .join(cc_issuer_details.select_all())\n\n    selected_features = credit_card_transactions.select_all()\n            .join(nested_selection)\n    .join(merchant_details.select_all())\n</code></pre> <p>Now, you have the benefit that in online inference you only need to pass two serving key values (the foreign keys of the leftmost feature group) to retrieve the precomputed features:</p> Python <pre><code>    feature vector = feature_view.get_feature_vector({\n      \u2018cc_num\u2019: \u201c1234 5555 3333 8888\u201d, \n      \u2018merchant_id\u2019: 44208484,\n    })\n</code></pre>"},{"location":"user_guides/fs/feature_view/query/#filter","title":"Filter","text":"<p>In the same way as joins, applying filters to feature groups creates a query with the applied filter.</p> <p>Filters are constructed with Python Operators <code>==</code>, <code>&gt;=</code>, <code>&lt;=</code>, <code>!=</code>, <code>&gt;</code>, <code>&lt;</code> and additionally with the methods <code>isin</code> and <code>like</code>. Bitwise Operators <code>&amp;</code> and <code>|</code> are used to construct conjunctions. For the Scala part of the API, equivalent methods are available in the <code>Feature</code> and <code>Filter</code> classes.</p> PythonScala <pre><code>filtered_credit_card_transactions = credit_card_transactions_fg.filter(credit_card_transactions_fg.category == \"Grocery\")\n</code></pre> <pre><code>val filteredCreditCardTransactions = creditCardTransactionsFg.filter(creditCardTransactionsFg.getFeature(\"category\").eq(\"Grocery\"))\n</code></pre> <p>Filters are fully compatible with joins:</p> PythonScala <pre><code>selected_features = credit_card_transactions_fg.select_all() \\\n    .join(account_details_fg.select_all(), on=[\"cc_num\"]) \\\n    .join(merchant_details_fg.select_all(), left_on=[\"merchant_id\"], right_on=[\"id\"]) \\\n    .filter((credit_card_transactions_fg.category == \"Grocery\") | (credit_card_transactions_fg.category == \"Restaurant/Cafeteria\"))\n</code></pre> <pre><code>val selectedFeatures = (creditCardTransactionsFg.selectAll()\n    .join(accountDetailsFg.selectAll(), Seq(\"cc_num\"))\n    .join(merchantDetailsFg.selectAll(), Seq(\"merchant_id\"), Seq(\"id\"), \"left\")\n    .filter(creditCardTransactionsFg.getFeature(\"category\").eq(\"Grocery\").or(creditCardTransactionsFg.getFeature(\"category\").eq(\"Restaurant/Cafeteria\"))))\n</code></pre> <p>The filters can be applied at any point of the query:</p> PythonScala <pre><code>selected_features = credit_card_transactions_fg.select_all() \\\n    .join(accountDetails_fg.select_all().filter(accountDetails_fg.avg_temp &gt;= 22), on=[\"cc_num\"]) \\\n    .join(merchant_details_fg.select_all(), left_on=[\"merchant_id\"], right_on=[\"id\"]) \\\n    .filter(credit_card_transactions_fg.category == \"Grocery\")\n</code></pre> <pre><code>val selectedFeatures = (creditCardTransactionsFg.selectAll()\n    .join(accountDetailsFg.selectAll().filter(accountDetailsFg.getFeature(\"avg_temp\").ge(22)), Seq(\"cc_num\"))\n    .join(merchantDetailsFg.selectAll(), Seq(\"merchant_id\"), Seq(\"id\"), \"left\")\n    .filter(creditCardTransactionsFg.getFeature(\"category\").eq(\"Grocery\")))\n</code></pre>"},{"location":"user_guides/fs/feature_view/query/#joins-andor-filters-on-feature-view-query","title":"Joins and/or Filters on feature view query","text":"<p>The query retrieved from a feature view can be extended with new joins and/or new filters. However, this operation will not update the metadata and persist the updated query of the feature view itself. This query can then be used to create a new feature view.</p> PythonScala <pre><code>fs = ...\nmerchant_details_fg = fs.get_feature_group(name=\"merchant_details\", version=1)\ncredit_card_transactions_fg = fs.get_feature_group(name=\"credit_card_transactions\", version=1)\nfeature_view = fs.get_feature_view(\u201ccredit_card_fraud\u201d, version=1)\nfeature_view.query \\\n    .join(merchant_details_fg.select_all()) \\\n    .filter((credit_card_transactions_fg.category == \"Cash Withdrawal\")\n</code></pre> <pre><code>val fs = ...\nval merchantDetailsFg = fs.getFeatureGroup(\"merchant_details\", 1)\nval creditCardTransactionsFg = fs.getFeatureGroup(\"credit_card_transactions\", 1)\nval featureView = fs.getFeatureView(\u201ccredit_card_fraud\u201d, 1)\nfeatureView.getQuery()\n    .join(merchantDetailsFg.selectAll())\n    .filter(creditCardTransactionsFg.getFeature(\"category\").eq(\"Cash Withdrawal\"))\n</code></pre> <p>Warning</p> <p>Every join/filter operation applied to an existing feature view query instance will update its state and accumulate. To successfully apply new join/filter logic it is recommended to refresh the query instance by re-fetching the feature view:</p> PythonScala <pre><code>fs = ...\n\nmerchant_details_fg = fs.get_feature_group(name=\"merchant_details\", version=1)\naccount_details_fg = fs.get_feature_group(name=\"account_details\", version=1)\ncredit_card_transactions_fg = fs.get_feature_group(name=\"credit_card_transactions\", version=1)\n\n# fetch new feature view and its query instance\nfeature_view = fs.get_feature_view(\u201ccredit_card_fraud\u201d, version=1)\n\n# apply join/filter logic based on purchase type\nfeature_view.query.join(merchant_details_fg.select_all()) \\\n    .filter((credit_card_transactions_fg.category == \"Cash Withdrawal\")\n\n# to apply new logic independent of purchase type from above \n# re-fetch new feature view and its query instance\nfeature_view = fs.get_feature_view(\u201ccredit_card_fraud\u201d, version=1)\n\n# apply new join/filter logic based on account details\nfeature_view.query.join(merchant_details_fg.select_all()) \\\n    .filter(account_details_fg.gender == \"F\")\n</code></pre> <pre><code>fs = ...\nmerchantDetailsFg = fs.getFeatureGroup(\"merchant_details\", 1)\naccountDetailsFg = fs.getFeatureGroup(\"account_details\", 1)\ncreditCardTransactionsFg = fs.getFeatureGroup(\"credit_card_transactions\", 1)\n\n// fetch new feature view and its query instance\nval featureView = fs.getFeatureView(\u201ccredit_card_fraud\u201d, version=1)\n\n// apply join/filter logic based on purchase type\nfeatureView.getQuery.join(merchantDetailsFg.selectAll())\n    .filter(creditCardTransactionsFg.getFeature(\"category\").eq(\"Cash Withdrawal\"))\n\n// to apply new logic independent of purchase type from above \n// re-fetch new feature view and its query instance\nval featureView = fs.getFeatureView(\u201ccredit_card_fraud\u201d, 1)\n\n// apply new join/filter logic based on account details\nfeatureView.getQuery.join(merchantDetailsFg.selectAll())\n    .filter(accountDetailsFg.getFeature(\"gender\").eq(\"F\"))\n</code></pre>"},{"location":"user_guides/fs/feature_view/spine-query/","title":"Using Spines","text":"<p>In this section we will illustrate how to use a Spine Group instead of a regular Feature Group for performing point-in-time joins when reading batch data for inference or when creating training datasets.</p>"},{"location":"user_guides/fs/feature_view/spine-query/#prerequisites","title":"Prerequisites","text":"<ol> <li>Make sure you have read the concept section about spines in feature and inference pipelines.</li> <li>Make sure you have gone through the Spine Group creation guide.</li> <li>Make sure you understand the concept of feature views and how to create them using the query abstraction</li> </ol>"},{"location":"user_guides/fs/feature_view/spine-query/#feature-view-with-a-spine-group","title":"Feature View with a Spine Group","text":""},{"location":"user_guides/fs/feature_view/spine-query/#step-1-query-definition","title":"Step 1: Query Definition","text":"<p>The first step before creating a Feature View, is to construct the query by selecting the label and features which are needed:</p> <pre><code># Select features for training data.\nds_query = trans_fg.select([\"fraud_label\"])\\\n    .join(window_aggs_fg.select_except([\"cc_num\"]), on=\"cc_num\")\n\nds_query.show(5)\n</code></pre> <p>Similarly you can construct the query using a previously created spine equivalent.</p> <p>However, there are two thing to note:</p> <ol> <li>If you want to use the query for a feature view to be used for online serving, you can only select the \"label\" or target feature from the spine.</li> <li>Spine groups can only be used on the left side of the join. Think of the left side of the join as the base set of entities that should be included in you batch of data or training dataset, which we enrich with the relevant and point-in-time correct feature values.</li> </ol> <pre><code>trans_spine = fs.get_or_create_spine_group(\n    name=\"spine_transactions\",\n    version=1,\n    description=\"Transaction data\",\n    primary_key=['cc_num'],\n    event_time='datetime',\n    dataframe=trans_df\n)\n\n# Select features for training data.\nds_query_spine = trans_spine.select([\"fraud_label\"])\\\n    .join(window_aggs_fg.select_except([\"cc_num\"]), on=\"cc_num\")\n</code></pre> <p>Calling the <code>show()</code> or <code>read()</code> method of this query object will use the spine dataframe included in the Spine Group object to perform the join.</p> <pre><code>ds_query_spine.show(10)\n</code></pre>"},{"location":"user_guides/fs/feature_view/spine-query/#step-2-feature-view-creation","title":"Step 2: Feature View Creation","text":"<p>With the above defined query, we can continue to create the Feature View in the same way we would do it also without a spine:</p> <pre><code>feature_view_spine = fs.get_or_create_feature_view(\n    name='transactions_view_spine',\n    query=ds_query_spine,\n    version=1,\n    labels=[\"fraud_label\"],\n)\n</code></pre>"},{"location":"user_guides/fs/feature_view/spine-query/#step-3-training-dataset-creation","title":"Step 3: Training Dataset Creation","text":"<p>With the regular feature view, the labels are fetched from the feature store, but with the feature view created with a spine, you need to provide the dataframe. Here you have the chance to pass a different set of entities to generate the training dataset.</p> <pre><code>X_train, X_test, y_train, y_test = feature_view_spine.train_test_split(0.2, spine=new_entities_df)\n\nX_train.show()\n</code></pre>"},{"location":"user_guides/fs/feature_view/spine-query/#step-4-retrieving-new-batches-inference-data","title":"Step 4: Retrieving New Batches Inference Data","text":"<p>You can now use the offline and online API of the feature stores to read features for inference. Similarly to training dataset creation, every time you read up a new batch of data, you can pass a different spine dataframe.</p> <pre><code>feature_view_spine.get_batch_data(spine=scroing_spine_df).show()\n</code></pre>"},{"location":"user_guides/fs/feature_view/spine-query/#step-5-online-feature-lookup","title":"Step 5: Online Feature Lookup","text":"<p>For the online lookup, the label is not required, therefore it was important to only select label from the left spine group, so that we don't need to provide a spine for online serving:</p> <pre><code># Note: no spine needs to be passed\nfeature_view.get_feature_vector({\"cc_num\": 4473593503484549})\n</code></pre>"},{"location":"user_guides/fs/feature_view/spine-query/#replacing-a-regular-feature-group-with-a-spine-at-serving-time","title":"Replacing a Regular Feature Group with a Spine at Serving Time","text":"<p>In the case where you create a feature view with a regular feature group, but you would like to retrieve batch inference data using IDs (primary key values), you can use a spine to replace the left feature group. To do this, you can pass the Spine Group instead of a dataframe.</p> <pre><code># Note: here feature_view was created with regular feature groups only\n# and trans_spine is of type SpineGroup instead of a dataframe\nfeature_view.get_batch_data(spine=trans_spine).show()\n</code></pre>"},{"location":"user_guides/fs/feature_view/training-data/","title":"Training data","text":"<p>Training data can be created from the feature view and used by different ML libraries for training different models.</p> <p>You can read training data concepts for more details. To see a full example of how to create training data, you can read this notebook.</p> <p>For Python-clients, handling small or moderately-sized data, we recommend enabling the ArrowFlight Server with DuckDB service, which will provide significant speedups over Spark/Hive for reading and creating in-memory training datasets.</p>"},{"location":"user_guides/fs/feature_view/training-data/#creation","title":"Creation","text":"<p>It can be created as in-memory DataFrames or materialised as <code>tfrecords</code>, <code>parquet</code>, <code>csv</code>, or <code>tsv</code> files to HopsFS or in all other locations, for example, S3, GCS. If you materialise a training dataset, a <code>PySparkJob</code> will be launched. By default, <code>create_training_data</code> waits for the job to finish. However, you can run the job asynchronously by passing <code>write_options={\"wait_for_job\": False}</code>. You can monitor the job status in the jobs overview UI. </p> <pre><code># create a training dataset as dataframe\nfeature_df, label_df = feature_view.training_data(\n    description = 'transactions fraud batch training dataset',\n)\n\n# materialise a training dataset\nversion, job = feature_view.create_training_data(\n    description = 'transactions fraud batch training dataset',\n    data_format = 'csv',\n    write_options = {\"wait_for_job\": False}\n) # By default, it is materialised to HopsFS\nprint(job.id) # get the job's id and view the job status in the UI\n</code></pre>"},{"location":"user_guides/fs/feature_view/training-data/#extra-filters","title":"Extra filters","text":"<p>Sometimes data scientists need to train different models using subsets of a dataset. For example, there can be different models for different countries, seasons, and different groups. One way is to create different feature views for training different models. Another way is to add extra filters on top of the feature view when creating training data.</p> <p>In the transaction fraud example, there are different transaction categories, for example: \"Health/Beauty\", \"Restaurant/Cafeteria\", \"Holliday/Travel\" etc. Examples below show how to create training data for different transaction categories. <pre><code># Create a training dataset for Health/Beauty\ndf_health = feature_view.training_data(\n    description = 'transactions fraud batch training dataset for Health/Beauty',\n    extra_filter = trans_fg.category == \"Health/Beauty\"\n)\n# Create a training dataset for Restaurant/Cafeteria and Holliday/Travel\ndf_restaurant_travel = feature_view.training_data(\n    description = 'transactions fraud batch training dataset for Restaurant/Cafeteria and Holliday/Travel',\n    extra_filter = trans_fg.category == \"Restaurant/Cafeteria\" and trans_fg.category == \"Holliday/Travel\"\n)\n</code></pre></p>"},{"location":"user_guides/fs/feature_view/training-data/#trainvalidationtest-splits","title":"Train/Validation/Test Splits","text":"<p>In most cases, ML practitioners want to slice a dataset into multiple splits, most commonly train-test splits or train-validation-test splits, so that they can train and test their models. Feature view provides a sklearn-like API for this purpose, so it is very easy to create a training dataset with different splits.</p> <p>Create a training dataset (as in-memory DataFrames) or materialise a training dataset with train and test splits. <pre><code># create a training dataset \nX_train, X_test, y_train, y_test = feature_view.train_test_split(test_size=0.2)\n\n# materialise a training dataset\nversion, job = feature_view.create_train_test_split(\n    test_size = 0.2,\n    description = 'transactions fraud batch training dataset',\n    data_format = 'csv'\n)\n</code></pre></p> <p>Create a training dataset (as in-memory DataFrames) or materialise a training dataset with train, validation, and test splits. <pre><code># create a training dataset as DataFrame\nX_train, X_val, X_test, y_train, y_val, y_test = feature_view.train_validation_test_split(validation_size=0.3, test_size=0.2)\n\n# materialise a training dataset\nversion, job = feature_view.create_train_validation_test_split(\n    validation_size = 0.3, \n    test_size = 0.2\n    description = 'transactions fraud batch training dataset',\n    data_format = 'csv'\n)\n</code></pre></p> <p>If the ArrowFlight Server with DuckDB service is enabled, and you want to create a particular in-memory training dataset with Hive instead, you can set <code>read_options={\"use_hive\": True}</code>. <pre><code># create a training dataset as DataFrame with Hive\nX_train, X_test, y_train, y_test = feature_view.train_test_split(test_size=0.2, read_options={\"use_hive: True})\n</code></pre></p>"},{"location":"user_guides/fs/feature_view/training-data/#read-training-data","title":"Read Training Data","text":"<p>Once you have created a training dataset, all its metadata are saved in Hopsworks. This enables you to reproduce exactly the same dataset at a later point in time. This holds for training data as both DataFrames or files. That is, you can delete the training data files (for example, to reduce storage costs), but still reproduce the training data files later on if you need to. <pre><code># get a training dataset\nfeature_df, label_df = feature_view.get_training_data(training_dataset_version=1)\n\n# get a training dataset with train and test splits\nX_train, X_test, y_train, y_test = feature_view.get_train_test_split(training_dataset_version=1)\n\n# get a training dataset with train, validation and test splits\nX_train, X_val, X_test, y_train, y_val, y_test = feature_view.get_train_validation_test_split(training_dataset_version=1)\n</code></pre></p>"},{"location":"user_guides/fs/feature_view/training-data/#passing-context-variables-to-transformation-functions","title":"Passing Context Variables to Transformation Functions","text":"<p>Once you have defined a transformation function using a context variable, you can pass the required context variables using the <code>transformation_context</code> parameter when generating IN-MEMORY training data or materializing a training dataset.</p> <p>Note</p> <p>Passing context variables for materializing a training dataset is only supported in the PySpark Kernel.</p> Python <p>Passing context variables while creating training data.</p> <pre><code># Passing context variable to IN-MEMORY Training Dataset.\nX_train, X_test, y_train, y_test = feature_view.get_train_test_split(training_dataset_version=1, \n                                                                 primary_key=True,\n                                                                 event_time=True,\n                                                                 transformation_context={\"context_parameter\":10})\n\n# Passing context variable to Materialized Training Dataset.\nversion, job = feature_view.get_train_test_split(training_dataset_version=1, \n                                                                 primary_key=True,\n                                                                 event_time=True,\n                                                                 transformation_context={\"context_parameter\":10})\n</code></pre>"},{"location":"user_guides/fs/feature_view/training-data/#read-training-data-with-primary-keys-and-event-time","title":"Read training data with primary key(s) and event time","text":"<p>For certain use cases, e.g. time series models, the input data needs to be sorted according to the primary key(s) and event time combination.  Primary key(s) and event time are not usually included in the feature view query as they are not features used for training. To retrieve the primary key(s) and/or event time when retrieving training data, you need to set the parameters <code>primary_key=True</code> and/or <code>event_time=True</code>.</p> <pre><code># get a training dataset\nX_train, X_test, y_train, y_test = feature_view.get_train_test_split(training_dataset_version=1, \n                                                                     primary_key=True,\n                                                                     event_time=True)\n</code></pre> <p>Note</p> <p>All primary and event time columns of all the feature groups included in the feature view will be returned. If they have the same names across feature groups and the join prefix was not provided then reading operation will fail with ambiguous column exception. Make sure to define the join prefix if primary key and event time columns have the same names across feature groups. </p> <p>To use primary key(s) and event time column with materialized training datasets it needs to be created with <code>primary_key=True</code> and/or <code>with_event_time=True</code>.  </p>"},{"location":"user_guides/fs/feature_view/training-data/#deletion","title":"Deletion","text":"<p>To clean up unused training data, you can delete all training data or for a particular version. Note that all metadata of training data and materialised files stored in HopsFS will be deleted and cannot be recreated anymore. <pre><code># delete a training data version\nfeature_view.delete_training_dataset(training_dataset_version=1)\n\n# delete all training datasets\nfeature_view.delete_all_training_datasets()\n</code></pre> It is also possible to keep the metadata and delete only the materialised files. Then you can recreate the deleted files by just specifying a version, and you get back the exact same dataset again. This is useful when you are running out of storage. <pre><code># delete files of a training data version\nfeature_view.purge_training_data(training_dataset_version=1)\n\n# delete files of all training datasets\nfeature_view.purge_all_training_data()\n</code></pre> To recreate a training dataset: <pre><code>feature_view.recreate_training_dataset(training_dataset_version =1)\n</code></pre></p>"},{"location":"user_guides/fs/feature_view/training-data/#tags","title":"Tags","text":"<p>Similar to feature view, You can attach, get, and remove tags. You can refer to here if you want to learn more about how tags work. <pre><code># attach\nfeature_view.add_training_dataset_tag(\n    training_dataset_version=1, \n    name=\"tag_schema\", \n    value={\"key\", \"value\"}\n)\n\n# get\nfeature_view.get_training_dataset_tag(training_dataset_version=1, name=\"tag_schema\")\n\n#remove\nfeature_view.delete_training_dataset_tag(training_dataset_version=1, name=\"tag_schema\")\n</code></pre></p>"},{"location":"user_guides/fs/feature_view/training-data/#next","title":"Next","text":"<p>Once you have created a training dataset and trained your model, you can deploy your model in a \"batch\"  or \"online\" setting. Next, you can learn how to create batch data and get feature vectors.</p>"},{"location":"user_guides/fs/provenance/provenance/","title":"Provenance","text":""},{"location":"user_guides/fs/provenance/provenance/#introduction","title":"Introduction","text":"<p>Hopsworks allows users to track provenance (lineage) between:</p> <ul> <li>storage connectors</li> <li>feature groups</li> <li>feature views</li> <li>training datasets</li> <li>models</li> </ul> <p>In the provenance pages we will call a provenance artifact or shortly artifact, any of the five entities above.</p> <p>With the following provenance graph:</p> <pre><code>storage connector -&gt; feature group -&gt; feature group -&gt; feature view -&gt; training dataset -&gt; model\n</code></pre> <p>we will call the parent, the artifact to the left, and the child, the artifact to the right. So a feature view has a number of feature groups as parents and can have a number of training datasets as children.</p> <p>Tracking provenance allows users to determine where and if an artifact is being used. You can track, for example, if feature groups are being used to create additional (derived) feature groups or feature views, or if their data is eventually used to train models.</p> <p>You can interact with the provenance graph using the UI or the APIs.</p>"},{"location":"user_guides/fs/provenance/provenance/#step-1-storage-connector-lineage","title":"Step 1: Storage connector lineage","text":"<p>The relationship between storage connectors and feature groups is captured automatically when you create an external feature group. You can inspect the relationship between storage connectors and feature groups using the APIs.</p> Python <pre><code># Retrieve the storage connector\nsnowflake_sc = fs.get_storage_connector(\"snowflake_sc\")\n\n# Create the user profiles feature group\nuser_profiles_fg = fs.create_external_feature_group(\n    name=\"user_profiles\",\n    version=1,\n    storage_connector=snowflake_sc,\n    query=\"SELECT * FROM USER_PROFILES\"\n)\nuser_profiles_fg.save()\n</code></pre>"},{"location":"user_guides/fs/provenance/provenance/#using-the-apis","title":"Using the APIs","text":"<p>Starting from a feature group metadata object, you can traverse upstream the provenance graph to retrieve the metadata objects of the storage connectors that are part of the feature group. To do so, you can use the get_storage_connector_provenance method.</p> PythonPython <pre><code># Returns all storage connectors linked to the provided feature group\nlineage = user_profiles_fg.get_storage_connector_provenance()\n\n# List all accessible parent storage connectors\nlineage.accessible\n\n# List all deleted parent storage connectors\nlineage.deleted\n\n# List all the inaccessible parent storage connectors\nlineage.inaccessible\n</code></pre> <pre><code># Returns an accessible storage connector linked to the feature group (if it exists)\nuser_profiles_fg.get_storage_connector()\n</code></pre> <p>To traverse the provenance graph in the opposite direction (i.e. from the storage connector to the feature group), you can use the get_feature_groups_provenance method. When navigating the provenance graph downstream, the <code>deleted</code> feature groups are not tracked by provenance, as such, the <code>deleted</code> property will always return an empty list.</p> PythonPython <pre><code># Returns all feature groups linked to the provided storage connector\nlineage = snowflake_sc.get_feature_groups_provenance()\n\n# List all accessible downstream feature groups\nlineage.accessible\n\n# List all the inaccessible downstream feature groups\nlineage.inaccessible\n</code></pre> <pre><code># Returns all accessible feature groups linked to the storage connector (if any exists)\nsnowflake_sc.get_feature_groups()\n</code></pre>"},{"location":"user_guides/fs/provenance/provenance/#step-2-feature-group-lineage","title":"Step 2: Feature group lineage","text":""},{"location":"user_guides/fs/provenance/provenance/#assign-parents-to-a-feature-group","title":"Assign parents to a feature group","text":"<p>When creating a feature group, it is possible to specify a list of feature groups used to create the derived features. For example, you could have an external feature group defined over a Snowflake or Redshift table, which you use to compute the features and save them in a feature group. You can mark the external feature group as parent of the feature group you are creating by using the <code>parents</code> parameter in the get_or_create_feature_group or create_feature_group methods:</p> Python <pre><code># Retrieve the feature group\nprofiles_fg = fs.get_external_feature_group(\"user_profiles\", version=1)\n\n# Do feature engineering\nage_df = transaction_df.merge(profiles_fg.read(), on=\"cc_num\", how=\"left\")\ntransaction_df[\"age_at_transaction\"] = (age_df[\"datetime\"] - age_df[\"birthdate\"]) / np.timedelta64(1, \"Y\")\n\n# Create the transaction feature group\ntransaction_fg = fs.get_or_create_feature_group(\n    name=\"transaction_fraud_batch\",\n    version=1,\n    description=\"Transaction features\",\n    primary_key=[\"cc_num\"],\n    event_time=\"datetime\",\n    parents=[profiles_fg]\n)\ntransaction_fg.insert(transaction_df)\n</code></pre> <p>Another example use case for derived feature group is if you have a feature group containing features with daily resolution and you are using the content of that feature group to populate a second feature group with monthly resolution:</p> Python <pre><code># Retrieve the feature group\ndaily_transaction_fg = fs.get_feature_group(\"daily_transaction\", version=1)\ndaily_transaction_df = daily_transaction_fg.read()\n\n# Do feature engineering\ncc_group = daily_transaction_df[[\"cc_num\", \"amount\", \"datetime\"]] \\\n                .groupby(\"cc_num\") \\\n                .rolling(\"1M\", on=\"datetime\")\nmonthly_transaction_df  = pd.DataFrame(cc_group.mean())\n\n# Create the transaction feature group\nmonthly_transaction_fg = fs.get_or_create_feature_group(\n    name=\"monthly_transaction_fraud_batch\",\n    version=1,\n    description=\"Transaction features - monthly aggregates\",\n    primary_key=[\"cc_num\"],\n    event_time=\"datetime\",\n    parents=[daily_transaction_fg]\n)\nmonthly_transaction_fg.insert(monthly_transaction_df)\n</code></pre>"},{"location":"user_guides/fs/provenance/provenance/#list-feature-group-parents","title":"List feature group parents","text":"<p>You can query the provenance graph of a feature group using the UI and the APIs. From the APIs you can list the parent feature groups by calling the method get_parent_feature_groups</p> Python <pre><code>lineage = transaction_fg.get_parent_feature_groups()\n\n# List all accessible parent feature groups\nlineage.accessible\n\n# List all deleted parent feature groups\nlineage.deleted\n\n# List all the inaccessible parent feature groups\nlineage.inaccessible\n</code></pre> <p>A parent is marked as <code>deleted</code> (and added to the deleted list) if the parent feature group was deleted. <code>inaccessible</code> if you no longer have access to the parent feature group (e.g. the parent feature group belongs to a project you no longer have access to).</p> <p>To traverse the provenance graph in the opposite direction (i.e. from the parent feature group to the child), you can use the get_generate_feature_groups method. When navigating the provenance graph downstream, the <code>deleted</code> feature groups are not tracked by provenance, as such, the <code>deleted</code> property will always return an empty list.</p> Python <pre><code>lineage = transaction_fg.get_generated_feature_groups()\n\n# List all accessible child feature groups\nlineage.accessible\n\n# List all the inaccessible child feature groups\nlineage.inaccessible\n</code></pre> <p>You can also visualize the relationship between the parent and child feature groups in the UI. In each feature group overview page you can find a provenance section with the graph of parent storage connectors/feature groups and child feature groups/feature views.</p> <p> Provenance graph of derived feature groups </p>"},{"location":"user_guides/fs/provenance/provenance/#step-3-feature-view-lineage","title":"Step 3: Feature view lineage","text":"<p>The relationship between feature groups and feature views is captured automatically when you create a feature view. You can inspect the relationship between feature groups and feature views using the APIs or the UI.</p>"},{"location":"user_guides/fs/provenance/provenance/#using-the-apis_1","title":"Using the APIs","text":"<p>Starting from a feature view metadata object, you can traverse upstream the provenance graph to retrieve the metadata objects of the feature groups that are part of the feature view. To do so, you can use the get_parent_feature_groups method.</p> Python <pre><code>lineage = fraud_fv.get_parent_feature_groups()\n\n# List all accessible parent feature groups\nlineage.accessible\n\n# List all deleted parent feature groups\nlineage.deleted\n\n# List all the inaccessible parent feature groups\nlineage.inaccessible\n</code></pre> <p>You can also traverse the provenance graph in the opposite direction. Starting from a feature group you can navigate downstream and list all the feature views the feature group is used in. As for the derived feature group example above, when navigating the provenance graph downstream <code>deleted</code> feature views are not tracked. As such, the <code>deleted</code> property will always be empty.</p> Python <pre><code>lineage = transaction_fg.get_generated_feature_views()\n\n# List all accessible downstream feature views\nlineage.accessible\n\n# List all the inaccessible downstream feature views\nlineage.inaccessible\n</code></pre> <p>Users can call the get_models_provenance method which will return a Link object.</p> <p>You can also retrive directly the accessible models, without the need to extract them from the provenance links object:</p> Python <pre><code>#List all accessible models\nmodels = fraud_fv.get_models()\n\n#List accessible models trained from a specific training dataset version\nmodels = fraud_fv.get_models(training_dataset_version: 1)\n</code></pre> <p>Also we added a utility method to retrieve from the user's accessible models, the last trained one. Last is determined based on timestamp when it was saved into the model registry.</p> Python <pre><code>#Retrieve newest model from all user's accessible models based on this feature view\nmodel = fraud_fv.get_newest_model()\n#Retrieve newest model from all user's accessible models based on this training dataset version\nmodel = fraud_fv.get_newest_model(training_dataset_version: 1)\n</code></pre>"},{"location":"user_guides/fs/provenance/provenance/#using-the-ui","title":"Using the UI","text":"<p>In the feature view overview UI you can explore the provenance graph of the feature view:</p> <p> Feature view provenance graph </p>"},{"location":"user_guides/fs/provenance/provenance/#provenance-links","title":"Provenance Links","text":"<p>All the <code>_provenance</code> methods return a <code>Link</code> dictionary object that contains <code>accessible</code>, <code>inaccesible</code>, <code>deleted</code> lists.</p> <ul> <li><code>accessible</code> - contains any artifact from the result, that the user has access to.</li> <li><code>inaccessible</code> - contains any artifacts that might have been shared at some point in the past, but where this sharing was retracted. Since the relation between artifacts is still maintained in the provenance, the user will only have access to limited metadata and the artifacts will be included in this <code>inaccessible</code> list.</li> <li><code>deleted</code> - contains artifacts that are deleted with children stil present in the system. There is minimum amount of metadata for the deleted allowing for some limited human readable identification.</li> </ul>"},{"location":"user_guides/fs/sharing/sharing/","title":"Sharing","text":""},{"location":"user_guides/fs/sharing/sharing/#introduction","title":"Introduction","text":"<p>Hopsworks allows artifacts (e.g. feature groups, feature views) to be shared between projects. There are two main use cases for sharing features between projects:</p> <ul> <li> <p>If you have multiple teams working on the same Hopsworks deployment. Each team works within its own set of projects.    If team A wants to leverage features built by team B, they can do so by sharing the feature groups from a team A project to a team B project.</p> </li> <li> <p>By creating different projects for the different stages of the development lifecycle (e.g. a dev project, a testing project, and a production project),    you can make sure that changes on the development project don't impact the features in the production project. At the same time, you might want to    leverage production features to develop new models or additional features. In this case, you can share the production feature store with the    development feature store in <code>read-only</code> mode.</p> </li> </ul>"},{"location":"user_guides/fs/sharing/sharing/#step-1-open-the-project-of-the-feature-store-that-you-would-like-to-share-on-hopsworks","title":"Step 1: Open the project of the feature store that you would like to share on Hopsworks.","text":"<p>In the <code>Project Settings</code> navigate to the <code>Shared with other projects</code> section.</p> <p> </p>"},{"location":"user_guides/fs/sharing/sharing/#step-2-share","title":"Step 2: Share","text":"<p>Click <code>Share feature store</code> to bring up the dialog for sharing.</p> <p>In the <code>Project</code> section choose project you wish to share the feature store with.</p> <p> </p> <p>Feature stores can be shared exclusively using <code>read-only</code> permission. This means that a member is not capable of enacting any changes on the shared project.</p>"},{"location":"user_guides/fs/sharing/sharing/#step-3-accept-the-invitation","title":"Step 3: Accept the Invitation","text":"<p>In the project where the feature store was shared (step 2) go to <code>Project Settings</code> and navigate to the <code>Shared from other projects</code> section. Click <code>accept</code>.</p> <p> </p> <p>After accepting the share, the shared feature store is listed under the <code>Shared from other projects</code> section.</p> <p> </p>"},{"location":"user_guides/fs/sharing/sharing/#use-features-from-a-shared-feature-store","title":"Use features from a shared feature store","text":""},{"location":"user_guides/fs/sharing/sharing/#step-1-get-feature-store-handles","title":"Step 1: Get feature store handles","text":"<p>To access features from a shared feature store you need to first retrieve the handle for the shared feature store.  To retrieve the handle use the get_feature_store() method and provide the name of the shared feature store</p> <pre><code>import hopsworks \n\nproject = hopsworks.login()\n\nproject_feature_store = project.get_feature_store()\nshared_feature_store = project.get_feature_store(name=\"name_of_shared_feature_store\")\n</code></pre>"},{"location":"user_guides/fs/sharing/sharing/#step-2-fetch-feature-groups","title":"Step 2: Fetch feature groups","text":"<pre><code># fetch feature group object from shared feature store\nshared_fg = shared_feature_store.get_feature_group(\n    name=\"shared_fg_name\",\n    version=\"1\")\n\n# fetch feature group object from project feature store\nfg_a = project_feature_store.get_or_create_feature_group(\n    name=\"feature_group_name\",\n    version=1)\n</code></pre>"},{"location":"user_guides/fs/sharing/sharing/#step-3-join-feature-groups","title":"Step 3: Join feature groups","text":"<pre><code># join above feature groups\nquery = shared_fg.select_all().join(fg_a.select_all())\n</code></pre>"},{"location":"user_guides/fs/storage_connector/","title":"Storage Connector Guides","text":"<p>You can define storage connectors in Hopsworks for batch and streaming data sources. Storage connectors securely store the authentication information about how to connect to an external data store. They can be used from programs within Hopsworks or externally.</p> <p>There are three main use cases for Storage Connectors:</p> <ul> <li>Simply use it to read data from the storage into a dataframe.</li> <li>External (on-demand) Feature Groups can be defined with storage connectors as data source. This way, Hopsworks stores only the metadata about the features, but does not keep a copy of the data itself. This is also called the Connector API.</li> <li>Write training data to an external storage system to make it accessible by third parties.</li> <li>Manage feature group that stores offline data in an external storage system.</li> </ul> <p>Storage connectors provide two main mechanisms for authentication: using credentials or an authentication role (IAM Role on AWS or Managed Identity on Azure). Hopsworks supports both a single IAM role (AWS) or Managed Identity (Azure) for the whole Hopsworks cluster or multiple IAM roles (AWS) or Managed Identities (Azure) that can only be assumed by users with a specific role in a specific project.</p> <p>By default, each project is created with three default Storage Connectors: A JDBC connector to the online feature store, a HopsFS connector to the Training Datasets directory of the project and a JDBC connector to the offline feature store.</p> <p> </p> The Storage Connector View in the User Interface"},{"location":"user_guides/fs/storage_connector/#cloud-agnostic","title":"Cloud Agnostic","text":"<p>Cloud agnostic storage systems:</p> <ol> <li>JDBC: Connect to JDBC compatible databases and query them using SQL.</li> <li>Snowflake: Query Snowflake databases and tables using SQL.</li> <li>Kafka: Read data from a Kafka cluster into a Spark Structured Streaming Dataframe.</li> <li>HopsFS: Easily connect and read from directories of Hopsworks' internal File System.</li> </ol>"},{"location":"user_guides/fs/storage_connector/#aws","title":"AWS","text":"<p>For AWS the following storage systems are supported:</p> <ol> <li>S3: Read data from a variety of file based storage in S3 such as parquet or CSV.</li> <li>Redshift: Query Redshift databases and tables using SQL.</li> </ol>"},{"location":"user_guides/fs/storage_connector/#azure","title":"Azure","text":"<p>For AWS the following storage systems are supported:</p> <ol> <li>ADLS: Read data from a variety of file based storage in ADLS such as parquet or CSV.</li> </ol>"},{"location":"user_guides/fs/storage_connector/#gcp","title":"GCP","text":"<p>For GCP the following storage systems are supported:</p> <ol> <li>BigQuery: Query BigQuery databases and tables using SQL.</li> <li>GCS: Read data from a variety of file based storage in Google Cloud Storage such as parquet or CSV.</li> </ol>"},{"location":"user_guides/fs/storage_connector/#next-steps","title":"Next Steps","text":"<p>Move on to the Configuration and Creation Guides to learn how to set up a storage connector.</p>"},{"location":"user_guides/fs/storage_connector/usage/","title":"Storage Connector Usage","text":"<p>Here, we look at how to use a Storage Connector after it has been created. Storage Connectors provide an important first step for integrating with external data sources. The 3 fundamental functionalities where storage connectors are used are:</p> <ol> <li>Reading data into Spark Dataframes</li> <li>Creating external feature groups</li> <li>Writing training data</li> </ol> <p>We will walk through each functionality in the sections below.</p>"},{"location":"user_guides/fs/storage_connector/usage/#retrieving-a-storage-connector","title":"Retrieving a Storage Connector","text":"<p>We retrieve a storage connector simply by its unique name.</p> PySparkScala <pre><code>import hopsworks \n# Connect to the Hopsworks feature store\nproject = hopsworks.login()\nfeature_store = project.get_feature_store()\n# Retrieve storage connector\nconnector = feature_store.get_storage_connector('connector_name')\n</code></pre> <pre><code>import com.logicalclocks.hsfs._\nval connection = HopsworksConnection.builder().build();\nval featureStore = connection.getFeatureStore();\n// get directly via connector sub-type class e.g. for GCS type\nval connector = featureStore.getGcsConnector(\"connector_name\")\n</code></pre>"},{"location":"user_guides/fs/storage_connector/usage/#reading-a-spark-dataframe-from-a-storage-connector","title":"Reading a Spark Dataframe from a Storage Connector","text":"<p>One of the most common usages of a Storage Connector is to read data directly into a Spark Dataframe. It's achieved via the <code>read</code> API of the connector object, which hides all the complexity of authentication and integration with a data storage source. The <code>read</code> API primarily has two parameters for specifying the data source, <code>path</code> and <code>query</code>, depending on the storage connector type. The exact behaviour could change depending on the storage connector type, but broadly they could be classified as below</p>"},{"location":"user_guides/fs/storage_connector/usage/#data-lakeobject-based-connectors","title":"Data lake/object based connectors","text":"<p>For data sources based on object/file storage such as AWS S3, ADLS, GCS, we set the full object path in the <code>path</code> argument and users should pass a Spark data format (parquet, csv, orc, hudi, delta) to the <code>data_format</code> argument.</p> PySparkScala <pre><code># read data into dataframe using path\ndf = connector.read(data_format='data_format', path='fileScheme://bucket/path/')\n</code></pre> <pre><code>// read data into dataframe using path\nval df = connector.read(\"\", \"data_format\", new HashMap(), \"fileScheme://bucket/path/\")\n</code></pre>"},{"location":"user_guides/fs/storage_connector/usage/#prepare-spark-api","title":"Prepare Spark API","text":"<p>Additionally, for reading file based data sources, another way to read the data is using the <code>prepare_spark</code> method. This method can be used if you are reading the data directly through Spark.</p> <p>Firstly, it handles the setup of all Spark configurations or properties necessary for a particular type of connector and prepares the absolute path to read from, along with bucket name and the appropriate file scheme of the data source. A Spark session can handle only one configuration setup at a time, so HSFS cannot set the Spark configurations when retrieving the connector since it would lead to only always initialising the last connector being retrieved. Instead, user can do this setup explicitly with the <code>prepare_spark</code> method and therefore potentially use multiple connectors in one Spark session. <code>prepare_spark</code> handles only one bucket associated with that particular connector, however, it is possible to set up multiple connectors with different types as long as their Spark properties do not interfere with each other. So, for example a S3 connector and a Snowflake connector can be used in the same session, without calling <code>prepare_spark</code> multiple times, as the properties don\u2019t interfere with each other.</p> <p>If the storage connector is used in another API call, <code>prepare_spark</code> gets implicitly invoked, for example, when a user materialises a training dataset using a storage connector or uses the storage connector to set up an External Feature Group. So users do not need to call <code>prepare_spark</code> every time they do an operation with a connector, it is only necessary when reading directly using Spark . Using <code>prepare_spark</code> is also not necessary when using the <code>read</code> API.</p> <p>For example, to read directly from a S3 connector, we use the <code>prepare_spark</code> as follows</p> PySpark <pre><code>connector.prepare_spark()\nspark.read.format(\"json\").load(\"s3a://[bucket]/path\")\n# or\nspark.read.format(\"json\").load(connector.prepare_spark(\"s3a://[bucket]/path\"))\n</code></pre>"},{"location":"user_guides/fs/storage_connector/usage/#data-warehousesql-based-connectors","title":"Data warehouse/SQL based connectors","text":"<p>For data sources accessed via SQL such as data warehouses and JDBC compliant databases, e.g. Redshift, Snowflake, BigQuery, JDBC, users pass the SQL query to read the data to the <code>query</code> argument. In most cases, this will be some form of a <code>SELECT</code> query. Depending on the connector type, users can also just set the table path and read the whole table without explicitly passing any SQL query to the <code>query</code> argument. This is mostly relevant for Google BigQuery.</p> PySparkScala <pre><code># read results from a SQL\ndf = connector.read(query=\"SELECT * FROM TABLE\")\n# or directly read a table if set on connector\ndf = connector.read()\n</code></pre> <pre><code>// read results from a SQL\nval df = connector.read(\"SELECT * FROM TABLE\", \"\" , new HashMap(),\"\")\n</code></pre>"},{"location":"user_guides/fs/storage_connector/usage/#streaming-based-connector","title":"Streaming based connector","text":"<p>For reading data streams, the Kafka Storage Connector supports reading a Kafka topic into Spark Structured Streaming Dataframes instead of a static Dataframe as in other connector types.</p> PySpark <pre><code>df = connector.read_stream(topic='kafka_topic_name')\n</code></pre>"},{"location":"user_guides/fs/storage_connector/usage/#creating-an-external-feature-group","title":"Creating an External Feature Group","text":"<p>Another important aspect of a storage connector is its ability to facilitate creation of external feature groups with the Connector API. External feature groups are basically offline feature groups and essentially stored as tables on external data sources. The <code>Connector API</code> relies on storage connectors behind the scenes to integrate with external datasource. This enables seamless integration with any data source as long as there is a storage connector defined.</p> <p>To create an external feature group, we use the <code>create_external_feature_group</code> API, also known as <code>Connector API</code>, and simply pass the storage connector created before to the <code>storage_connector</code> argument. Depending on the external data source, we should set either the <code>query</code> argument for data warehouse based data sources, or the <code>path</code> and <code>data_format</code> arguments for data lake based sources, similar to reading into dataframes as explained in above section.</p> <p>Example for any data warehouse/SQL based external sources, we set the desired SQL to <code>query</code> argument, and set the <code>storage_connector</code> argument to the storage connector object of desired data source.</p> PySpark <pre><code>fg = feature_store.create_external_feature_group(name=\"sales\",\n    version=1\n    description=\"Physical shop sales features\",\n    query=\"SELECT * FROM TABLE\",\n    storage_connector=connector,\n    primary_key=['ss_store_sk'],\n    event_time='sale_date'\n)\n</code></pre> <p><code>Connector API</code> (external feature groups) only stores the metadata about the features within Hopsworks, while the actual data is still stored externally. This enables users to create feature groups within Hopsworks without the hassle of data migration. For more information on <code>Connector API</code>, read detailed guide about external feature groups.</p>"},{"location":"user_guides/fs/storage_connector/usage/#writing-training-data","title":"Writing Training Data","text":"<p>Storage connectors are also used while writing training data to external sources. While calling the Feature View API <code>create_training_data</code> , we can pass the <code>storage_connector</code> argument which is necessary to materialise the data to external sources, as shown below.</p> PySpark <pre><code># materialise a training dataset\nversion, job = feature_view.create_training_data(\n    description = 'describe training data',\n    data_format = 'spark_data_format', # e.g. data_format = \"parquet\" or data_format = \"csv\"\n    write_options = {\"wait_for_job\": False},\n    storage_connector = connector\n)\n</code></pre> <p>Read more about training data creation here.</p>"},{"location":"user_guides/fs/storage_connector/usage/#next-steps","title":"Next Steps","text":"<p>We have gone through the basic use cases of a storage connector. For more details about the API functionality for any specific connector type, checkout the API section.</p>"},{"location":"user_guides/fs/storage_connector/creation/adls/","title":"How-To set up a ADLS Storage Connector","text":""},{"location":"user_guides/fs/storage_connector/creation/adls/#introduction","title":"Introduction","text":"<p>Azure Data Lake Storage (ADLS) Gen2 is a HDFS-compatible filesystem on Azure for data analytics. The ADLS Gen2 filesystem stores its data in Azure Blob storage, ensuring low-cost storage, high availability, and disaster recovery. In Hopsworks, you can access ADLS Gen2 by defining a Storage Connector and creating and granting permissions to a service principal.</p> <p>In this guide, you will configure a Storage Connector in Hopsworks to save all the authentication information needed in order to set up a connection to your Azure ADLS filesystem. When you're finished, you'll be able to read files using Spark through HSFS APIs. You can also use the connector to write out training data from the Feature Store, in order to make it accessible by third parties.</p> <p>Note</p> <p>Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically.</p>"},{"location":"user_guides/fs/storage_connector/creation/adls/#prerequisites","title":"Prerequisites","text":"<p>Before you begin this guide you'll need to retrieve the following information from your Azure ADLS account:</p> <ul> <li>Data Lake Storage Gen2 Account: Create an Azure Data Lake Storage Gen2 account and initialize a filesystem, enabling the hierarchical namespace. Note that your storage account must belong to an Azure resource group.</li> <li>Azure AD application and service principal: Create an Azure AD application and service principal that can access your ADLS storage account and its resource group.</li> <li>Service Principal Registration: Register the service principal, granting it a role assignment such as Storage Blob Data Contributor, on the Azure Data Lake Storage Gen2 account.</li> </ul> <p>Info</p> <p>When you specify the 'container name' in the ADLS storage connector, you need to have previously created that container - the Hopsworks Feature Store will not create that storage container for you.</p>"},{"location":"user_guides/fs/storage_connector/creation/adls/#creation-in-the-ui","title":"Creation in the UI","text":""},{"location":"user_guides/fs/storage_connector/creation/adls/#step-1-set-up-new-storage-connector","title":"Step 1: Set up new storage connector","text":"<p>Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2).</p> <p> </p> The Storage Connector View in the User Interface"},{"location":"user_guides/fs/storage_connector/creation/adls/#step-2-enter-adls-information","title":"Step 2: Enter ADLS Information","text":"<p>Enter the details for your ADLS connector. Start by giving it a name and an optional description.</p> <p> </p> ADLS Connector Creation Form"},{"location":"user_guides/fs/storage_connector/creation/adls/#step-3-azure-create-an-adls-resource","title":"Step 3: Azure Create an ADLS Resource","text":"<p>When programmatically signing in, you need to pass the tenant ID with your authentication request and the application ID. You also need a certificate or an authentication key (described in the following section). To get those values, use the following steps:</p> <ol> <li>Select Azure Active Directory.</li> <li>From App registrations in Azure AD, select your application.</li> <li> <p>Copy the Directory (tenant) ID and store it in your application code.      You need to copy the Directory (tenant) id and paste it to the Hopsworks ADLS storage connector  \"Directory id\" text field. </p> </li> <li> <p>Copy the Application ID and store it in your application code.      &gt;You need to copy the Application id and paste it to the Hopsworks ADLS storage connector \"Application id\" text field. </p> </li> <li> <p>Create an Application Secret and copy it into the Service Credential field.      You need to copy the Application Secret and paste it to the Hopsworks ADLS storage connector \"Service Credential\" text field. </p> </li> </ol>"},{"location":"user_guides/fs/storage_connector/creation/adls/#common-problems","title":"Common Problems","text":"<p>If you get a permission denied error when writing or reading to/from a ADLS container, it is often because the storage principal (app) does not have the correct permissions. Have you added the \"Storage Blob Data Owner\" or \"Storage Blob Data Contributor\" role to the resource group for your storage account (or the subscription for your storage group, if you apply roles at the subscription level)? Go to your resource group, then in \"Access Control (IAM)\", click the \"Add\" button to add a \"role assignment\".</p> <p>If you get an error \"StatusCode=404 StatusDescription=The specified filesystem does not exist.\", then maybe you have not created the storage account or the storage container.</p>"},{"location":"user_guides/fs/storage_connector/creation/adls/#references","title":"References","text":"<ul> <li>How to create a service principal on Azure</li> </ul>"},{"location":"user_guides/fs/storage_connector/creation/adls/#next-steps","title":"Next Steps","text":"<p>Move on to the usage guide for storage connectors to see how you can use your newly created ADLS connector.</p>"},{"location":"user_guides/fs/storage_connector/creation/bigquery/","title":"How-To set up a BigQuery Storage Connector","text":""},{"location":"user_guides/fs/storage_connector/creation/bigquery/#introduction","title":"Introduction","text":"<p>A BigQuery storage connector provides integration to Google Cloud BigQuery. BigQuery is Google Cloud's managed data warehouse supporting that lets you run analytics and  execute SQL queries over large scale data. Such data warehouses are often the source of raw data for feature  engineering pipelines.</p> <p>In this guide, you will configure a Storage Connector in Hopsworks to connect to your BigQuery project by saving the necessary information. When you're finished, you'll be able to execute queries and read results of BigQuery using Spark through  HSFS APIs.</p> <p>The storage connector uses the Google <code>spark-bigquery-connector</code> behind the scenes. To read more about the spark connector, like the spark options or usage, check Apache Spark SQL connector for Google BigQuery.</p> <p>Note</p> <pre><code>Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically.\n</code></pre>"},{"location":"user_guides/fs/storage_connector/creation/bigquery/#prerequisites","title":"Prerequisites","text":"<p>Before you begin this guide you'll need to retrieve the following information about your GCP account:</p> <ul> <li>BigQuery Project: You need a BigQuery project, dataset and table created and have read access to it. Or, if    you wish to query a public dataset you need its corresponding details.</li> <li>Authentication Method: Authentication to GCP account is handled by uploading the <code>JSON keyfile for service   account</code> to the Hopsworks Project. You will need to create this JSON keyfile from GCP. For more information on   service accounts   and creating keyfile in GCP, read Google Cloud documentation.</li> </ul> <p>Note</p> <pre><code>To read data, the BigQuery service account user needs permission to `create read sesssion` which is available in **BigQuery Admin role**.\n</code></pre>"},{"location":"user_guides/fs/storage_connector/creation/bigquery/#creation-in-the-ui","title":"Creation in the UI","text":""},{"location":"user_guides/fs/storage_connector/creation/bigquery/#step-1-set-up-new-storage-connector","title":"Step 1: Set up new storage connector","text":"<p>Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2).</p> <p> </p> The Storage Connector View in the User Interface"},{"location":"user_guides/fs/storage_connector/creation/bigquery/#step-2-enter-connector-details","title":"Step 2: Enter connector details","text":"<p>Enter the details for your BigQuery connector. Start by giving it a unique name and an optional description.</p> <p> </p> BigQuery Connector Creation Form <ol> <li>Choose <code>Google BigQuery</code> from the connector options.</li> <li>Next, set the name of the parent BigQuery project. This is used for billing by GCP.</li> <li>Authentication: Here you should upload your <code>JSON keyfile for service     account</code> used for authentication. You can choose to either    upload from your local using <code>Upload new file</code> or choose an existing file within project using <code>From Project</code>.</li> <li> <p>Read Options: There are two ways to read via BigQuery, using the BigQuery Table or BigQuery Query option:</p> <ol> <li>BigQuery Table - This option reads directly from BigQuery table reference. Note that it can only be used in <code>read</code> API for reading data from BigQuery. Creating external Feature Groups using this option is not yet supported. In the UI set the below fields, <ol> <li>BigQuery Project: The BigQuery project</li> <li>BigQuery Dataset: The dataset of the table</li> <li>BigQuery Table: The table to read</li> </ol> </li> <li>BigQuery Query - This option executes a SQL query at runtime. It can be used for both reading data and creating external Feature Groups. <ol> <li>Materialization Dataset: Temporary dataset used by BigQuery for writing. It must be set to a dataset where the GCP user has table creation permission. The queried table must be in the same location as the <code>materializationDataset</code> (e.g 'EU' or 'US'). Also, if a table in the <code>SQL statement</code> is from project other than the <code>parentProject</code> then use the fully qualified table name i.e. <code>[project].[dataset].[table]</code>   (Read more details from Google documentation on usage of query for BigQuery spark connector here).</li> </ol> </li> </ol> </li> <li> <p>Spark Options: Optionally, you can set additional spark options using the <code>Key - Value</code> pairs.</p> </li> </ol>"},{"location":"user_guides/fs/storage_connector/creation/bigquery/#next-steps","title":"Next Steps","text":"<p>Move on to the usage guide for storage connectors to see how you can use your newly created BigQuery  connector.</p>"},{"location":"user_guides/fs/storage_connector/creation/gcs/","title":"How-To set up a GCS Storage Connector","text":""},{"location":"user_guides/fs/storage_connector/creation/gcs/#introduction","title":"Introduction","text":"<p>This particular type of storage connector provides integration to Google Cloud Storage (GCS). GCS is  an object storage service offered by Google Cloud. An object could be simply any piece  of immutable data consisting of a file of any format, for example a <code>CSV</code> or <code>PARQUET</code>. These objects are stored in  containers called as <code>buckets</code>.</p> <p>These types of storages are often the source for raw data from which features can be engineered.</p> <p>In this guide, you will configure a Storage Connector in Hopsworks to connect to your GCS bucket by saving the  necessary information. When you're finished, you'll be able to read files from the GCS bucket using Spark through HSFS APIs.</p> <p>The storage connector uses the Google <code>gcs-connector-hadoop</code> behind the scenes. For more information, check out Google Cloud Storage Connector for Spark and Hadoop</p> <p>Note</p> <p>Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically.</p>"},{"location":"user_guides/fs/storage_connector/creation/gcs/#prerequisites","title":"Prerequisites","text":"<p>Before you begin this guide you'll need to retrieve the following information about your GCP account and bucket:</p> <ul> <li>Bucket: You need a GCS bucket created and have read access to it. The bucket is identified by its name.</li> <li>Authentication Method: Authentication to GCP account is handled by uploading the <code>JSON keyfile for service    account</code> to the Hopsworks Project. You will need to create this JSON keyfile from GCP. For more information on    service accounts    and creating keyfile in GCP, read Google Cloud documentation.</li> <li>Server-side Encryption GCS encrypts the data on server side by default. The connector additionally supports the    optional encryption method <code>Customer Supplied Encryption Key</code> by GCP. You can choose the encryption option <code>AES-256</code> and provide AES-256 key and hash, encoded in    standard Base64. The encryption details are stored as Secrets    in the Hopsworks for keeping it secure.   Read more about encryption on Google Documentation.</li> </ul>"},{"location":"user_guides/fs/storage_connector/creation/gcs/#creation-in-the-ui","title":"Creation in the UI","text":""},{"location":"user_guides/fs/storage_connector/creation/gcs/#step-1-set-up-new-storage-connector","title":"Step 1: Set up new storage connector","text":"<p>Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2).</p> <p> </p> The Storage Connector View in the User Interface"},{"location":"user_guides/fs/storage_connector/creation/gcs/#step-2-enter-connector-details","title":"Step 2: Enter connector details","text":"<p>Enter the details for your GCS connector. Start by giving  it a unique name and an optional  description.</p> <p> </p> GCS Connector Creation Form <ol> <li>Choose <code>Google Cloud Storage</code> from the connector options.</li> <li>Next, set the name of the GCS Bucket you wish to connect with.</li> <li>Authentication: Here you should upload your <code>JSON keyfile for service    account</code> used for authentication. You can choose to either    upload from your local using <code>Upload new file</code> or choose an existing file within project using <code>From Project</code>.</li> <li>GCS Server Side Encryption: You can leave this to <code>Default Encryption</code> if you do not wish to provide explicit encrypting keys. Otherwise,  optionally you can set the encryption setting for <code>AES-256</code> and provide the encryption key and hash when selected.</li> </ol>"},{"location":"user_guides/fs/storage_connector/creation/gcs/#next-steps","title":"Next Steps","text":"<p>Move on to the usage guide for storage connectors to see how you can use your newly created GCS  connector.</p>"},{"location":"user_guides/fs/storage_connector/creation/hopsfs/","title":"How-To set up a HopsFS Storage Connector","text":""},{"location":"user_guides/fs/storage_connector/creation/hopsfs/#introduction","title":"Introduction","text":"<p>HopsFS is a HDFS-compatible filesystem on AWS/Azure/on-premises for data analytics. HopsFS stores its data on object storage in the cloud (S3 in AWs and Blob storage on Azure) and on commodity servers on-premises, ensuring low-cost storage, high availability, and disaster recovery. In Hopsworks, you can access HopsFS natively in programs (Spark, TensorFlow, etc) without the need to define a Storage Connector. By default, every Project has a Storage Connector for Training Datasets. When you create training datasets from features in the Feature Store the HopsFS connector is the default Storage Connector. However, if you want to output data to a different dataset, you can define a new Storage Connector for that dataset.</p> <p>In this guide, you will configure a HopsFS Storage Connector in Hopsworks which points at a different directory on the file system than the Training Datasets directory. When you're finished, you'll be able to write training data to different locations in your cluster through HSFS APIs.</p> <p>Note</p> <p>Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically.</p>"},{"location":"user_guides/fs/storage_connector/creation/hopsfs/#prerequisites","title":"Prerequisites","text":"<p>Before you begin this guide you'll need to identify a directory on the filesystem of Hopsworks, to which you want to point the Storage Connector that you are going to create.</p>"},{"location":"user_guides/fs/storage_connector/creation/hopsfs/#creation-in-the-ui","title":"Creation in the UI","text":""},{"location":"user_guides/fs/storage_connector/creation/hopsfs/#step-1-set-up-new-storage-connector","title":"Step 1: Set up new storage connector","text":"<p>Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2).</p> <p> </p> The Storage Connector View in the User Interface"},{"location":"user_guides/fs/storage_connector/creation/hopsfs/#step-2-enter-hopsfs-settings","title":"Step 2: Enter HopsFS Settings","text":"<p>Enter the details for your HopsFS connector. Start by giving it a name and an optional description.</p> <ol> <li>Select \"HopsFS\" as connector protocol.</li> <li>Select the top-level directory to point the connector to.</li> <li>Click \"Setup storage connector\".</li> </ol> <p> </p> HopsFS Connector Creation Form"},{"location":"user_guides/fs/storage_connector/creation/hopsfs/#next-steps","title":"Next Steps","text":"<p>Move on to the usage guide for storage connectors to see how you can use your newly created HopsFS connector.</p>"},{"location":"user_guides/fs/storage_connector/creation/jdbc/","title":"How-To set up a JDBC Storage Connector","text":""},{"location":"user_guides/fs/storage_connector/creation/jdbc/#introduction","title":"Introduction","text":"<p>JDBC is an API provided by many database systems. Using JDBC connections one can query and update data in a database, usually oriented towards relational databases. Examples of databases you can connect to using JDBC are MySQL, Postgres, Oracle, DB2, MongoDB or Microsoft SQLServer.</p> <p>In this guide, you will configure a Storage Connector in Hopsworks to save all the authentication information needed in order to set up a JDBC connection to your database of choice. When you're finished, you'll be able to query the database using Spark through HSFS APIs.</p> <p>Note</p> <p>Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically.</p>"},{"location":"user_guides/fs/storage_connector/creation/jdbc/#prerequisites","title":"Prerequisites","text":"<p>Before you begin this guide you'll need to retrieve the following information from your JDBC compatible database:</p> <ul> <li>JDBC Connection URL: Consult the documentation of your target database to determine the correct JDBC URL and parameters. As an example, for MySQL the URL could be:</li> </ul> <pre><code>jdbc:mysql://10.0.2.15:3306/[databaseName]?useSSL=false&amp;allowPublicKeyRetrieval=true\n</code></pre> <ul> <li>Username and Password: Typically, you will need to add username and password in your JDBC URL or as key/value parameters. So make sure you have retrieved a username and password with the suitable permissions for the database and table you want to query.</li> </ul>"},{"location":"user_guides/fs/storage_connector/creation/jdbc/#creation-in-the-ui","title":"Creation in the UI","text":""},{"location":"user_guides/fs/storage_connector/creation/jdbc/#step-1-set-up-new-storage-connector","title":"Step 1: Set up new storage connector","text":"<p>Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2).</p> <p> </p> The Storage Connector View in the User Interface"},{"location":"user_guides/fs/storage_connector/creation/jdbc/#step-2-enter-jdbc-settings","title":"Step 2: Enter JDBC Settings","text":"<p>Enter the details for your JDBC enabled database.</p> <p> </p> JDBC Connector Creation Form <ol> <li>Select \"JDBC\" as connector protocol.</li> <li>Enter the JDBC connection url. This can for example also contain the username and password.</li> <li> <p>Add additional key/value arguments to be passed to the connection, such as username or password. These might differ by database.</p> <p>Note</p> <p>Driver class name is a mandatory argument even if using the default MySQL driver. Add it by specifying a property with the name <code>driver</code> and class name as value. The driver class name will differ based on the database. For MySQL databases, the class name is <code>com.mysql.cj.jdbc.Driver</code>, as shown in the example image.</p> </li> <li> <p>Click \"Setup storage connector\".</p> </li> </ol> <p>Note</p> <p>To be able to use the connector, you need to upload the driver JAR file to the Jupyter configuration or Job configuration in <code>Addtional Jars</code>.  For MySQL connections the default JDBC driver is already included in Hopsworks so this step can be skipped.</p>"},{"location":"user_guides/fs/storage_connector/creation/jdbc/#next-steps","title":"Next Steps","text":"<p>Move on to the usage guide for storage connectors to see how you can use your newly created JDBC connector.</p>"},{"location":"user_guides/fs/storage_connector/creation/kafka/","title":"How-To set up a Kafka Storage Connector","text":""},{"location":"user_guides/fs/storage_connector/creation/kafka/#introduction","title":"Introduction","text":"<p>Apache Kafka is a distributed event store and stream-processing platform. It's a very popular framework for handling realtime data streams and is often used as a message broker for events coming from production systems until they are being processed and either loaded into a data warehouse or aggregated into features for Machine Learning.</p> <p>In this guide, you will configure a Storage Connector in Hopsworks to save all the authentication information needed in order to set up a connection to your Kafka cluster. When you're finished, you'll be able to read from Kafka topics in your cluster using Spark through HSFS APIs.</p> <p>Note</p> <p>Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically.</p>"},{"location":"user_guides/fs/storage_connector/creation/kafka/#prerequisites","title":"Prerequisites","text":"<p>Before you begin this guide you'll need to retrieve the following information from Kafka cluster, the following options are mandatory:</p> <ul> <li>Kafka Bootstrap servers: It is the url of one of the Kafka brokers which you give to fetch the initial metadata about your Kafka cluster. The metadata consists of the topics, their partitions, the leader brokers for those partitions etc. Depending upon this metadata your producer or consumer produces or consumes the data.</li> <li>Security Protocol: The security protocol you want to use to authenticate with your Kafka cluster. Make sure the chosen protocol is supported by your cluster. For an overview of the available protocols, please see the Confluent Kafka Documentation.</li> <li>Certificates: Depending on the chosen security protocol, you might need TrustStore and KeyStore files along with the corresponding key password. Contact your Kafka administrator, if you don't know how to retrieve these. If you want to setup a storage connector to Hopsworks' internal Kafka cluster, you can download the needed certificates from the integration tab in your project settings.</li> </ul>"},{"location":"user_guides/fs/storage_connector/creation/kafka/#creation-in-the-ui","title":"Creation in the UI","text":""},{"location":"user_guides/fs/storage_connector/creation/kafka/#step-1-set-up-new-storage-connector","title":"Step 1: Set up new storage connector","text":"<p>Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2).</p> <p> </p> The Storage Connector View in the User Interface"},{"location":"user_guides/fs/storage_connector/creation/kafka/#step-2-enter-kafka-settings","title":"Step 2: Enter Kafka Settings","text":"<p>Enter the details for your Kafka connector. Start by giving it a name and an optional description.</p> <ol> <li>Select \"Kafka\" as connector protocol.</li> <li>Add all the bootstrap server addresses and ports that you want the consumers/producers to connect to. The client will make use of all servers irrespective of which servers are specified here for bootstrapping\u2014this list only impacts the initial hosts used to discover the full set of servers.</li> <li> <p>Choose the Security protocol.</p> <p>TSL/SSL</p> <p>By default, Apache Kafka communicates in <code>PLAINTEXT</code>, which means that all data is sent in the clear. To encrypt communication, you should configure all the Confluent Platform components in your deployment to use TLS/SSL encryption.</p> <p>TLS uses private-key/certificate pairs, which are used during the TLS handshake process.</p> <p>Each broker needs its own private-key/certificate pair, and the client uses the certificate to authenticate the broker. Each logical client needs a private-key/certificate pair if client authentication is enabled, and the broker uses the certificate to authenticate the client.</p> <p>These are provided in the form of TrustStore and KeyStore <code>JKS</code> files together with a key password. For more information, refer to the official Apacha Kafka Guide for TSL/SSL authentication.</p> <p>SASL SSL or SASL plaintext</p> <p>Apache Kafka brokers support client authentication using SASL. SASL authentication can be enabled concurrently with TLS/SSL encryption (TLS/SSL client authentication will be disabled).</p> <p>This authentication method often requires extra arguments depending on your setup. Make use of the optional additional key/value arguments (5) to provide these.</p> <p>SASL authentication can be enabled concurrently with TLS/SSL encryption (TLS/SSL client authentication will be disabled). For more information, please refer to the offical Apache Kafka Guide for SASL authentication.</p> </li> <li> <p>The endpoint identification algorithm used by clients to validate server host name. The default value is <code>https</code>. Clients including client connections created by the broker for inter-broker communication verify that the broker host name matches the host name in the broker\u2019s certificate.</p> </li> <li>Optional additional key/value arguments.</li> <li>Click \"Setup storage connector\".</li> </ol> <p> </p> Kafka Connector Creation Form"},{"location":"user_guides/fs/storage_connector/creation/kafka/#next-steps","title":"Next Steps","text":"<p>Move on to the usage guide for storage connectors to see how you can use your newly created Kafka connector.</p>"},{"location":"user_guides/fs/storage_connector/creation/redshift/","title":"How-To set up a Redshift Storage Connector","text":""},{"location":"user_guides/fs/storage_connector/creation/redshift/#introduction","title":"Introduction","text":"<p>Amazon Redshift is a popular managed data warehouse on AWS, used as a data warehouse in many enterprises. </p> <p>Data warehouses are often the source of raw data for feature engineering pipelines and Redshift supports scalable feature computation with SQL. However, Redshift is not viable as an online feature store that serves features to models in production, with its columnar database layout its latency is too high compared to OLTP databases or key-value stores.</p> <p>In this guide, you will configure a Storage Connector in Hopsworks to save all the authentication information needed in order to set up a connection to your AWS Redshift cluster. When you're finished, you'll be able to query the database using Spark through HSFS APIs.</p> <p>Note</p> <p>Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically.</p>"},{"location":"user_guides/fs/storage_connector/creation/redshift/#prerequisites","title":"Prerequisites","text":"<p>Before you begin this guide you'll need to retrieve the following information from your AWS account and Redshift database, the following options are mandatory:</p> <ul> <li>Cluster identifier: The name of the cluster.</li> <li>Database endpoint: The endpoint for the database. Should be in the format of <code>[UUID].eu-west-1.redshift.amazonaws.com</code>.</li> <li>Database name: The name of the database to query.</li> <li>Database port: The port of the cluster. Defaults to 5349.</li> <li>Authentication method: There are three options available for authenticating with the Redshift cluster. The first option is to configure a username and a password.  The second option is to configure an IAM role. With IAM roles, Jobs or notebooks launched on Hopsworks do not need to explicitly authenticate with Redshift, as the HSFS library will transparently use the IAM role to acquire a temporary credential to authenticate the specified user.  Read more about IAM roles in our AWS credentials pass-through guide. Lastly,    option <code>Instance Role</code> will use the default ARN Role configured for the cluster instance.</li> </ul>"},{"location":"user_guides/fs/storage_connector/creation/redshift/#creation-in-the-ui","title":"Creation in the UI","text":""},{"location":"user_guides/fs/storage_connector/creation/redshift/#step-1-set-up-new-storage-connector","title":"Step 1: Set up new storage connector","text":"<p>Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2).</p> <p> </p> The Storage Connector View in the User Interface"},{"location":"user_guides/fs/storage_connector/creation/redshift/#step-2-enter-the-connector-information","title":"Step 2: Enter The Connector Information","text":"<p>Enter the details for your Redshift connector. Start by giving it a name and an optional description.</p> <ol> <li>Select \"Redshift\" as connector protocol.</li> <li>The name of the cluster.</li> <li>The database endpoint. Should be in the format <code>[UUID].eu-west-1.redshift.amazonaws.com</code>. For example, if the endpoint info     displayed in Redshift is <code>cluster-id.uuid.eu-north-1.redshift.amazonaws.com:5439/dev</code> the value to enter     here is just <code>uuid.eu-north-1.redshift.amazonaws.com</code> </li> <li>The database name.</li> <li>The database port.</li> <li>The database username, here you have the possibility to let Hopsworks auto-create the username for you.</li> <li>Database Driver (optional): You can use the default JDBC Redshift Driver <code>com.amazon.redshift.jdbc42.Driver</code>     included in Hopsworks or set a different driver (More on this later).</li> <li>Optionally provide the database group and table for the connector. A database group is the group created     for the user if applicable. More information, at redshift documentation</li> <li>Set the appropriate authentication method. </li> </ol> <p> </p> Redshift Connector Creation Form <p>Session Duration</p> <p>By default, the session duration that the role will be assumed for is 1 hour or 3600 seconds. This means if you want to use the storage connector for example to read or create an external Feature Group from Redshift, the operation cannot take longer than one hour.</p> <p>Your administrator can change the default session duration for AWS storage connectors, by first increasing the max session duration of the IAM Role that you are assuming. And then changing the <code>fs_storage_connector_session_duration</code> configuration property to the appropriate value in seconds.</p>"},{"location":"user_guides/fs/storage_connector/creation/redshift/#step-3-upload-the-redshift-database-driver-optional","title":"Step 3: Upload the Redshift database driver (optional)","text":"<p>The <code>redshift-jdbc42</code> JDBC driver is included by default in the Hopsworks distribution.  If you wish to use a different driver, you need to upload it on Hopsworks and add it as a dependency of Jobs and Jupyter Notebooks that need it. First, you need to download the library. Select the driver version without the AWS SDK.</p>"},{"location":"user_guides/fs/storage_connector/creation/redshift/#add-the-driver-to-jupyter-notebooks-and-spark-jobs","title":"Add the driver to Jupyter Notebooks and Spark jobs","text":"<p>You can now add the driver file to the default job and Jupyter configuration. This way, all jobs and Jupyter instances in the project will have the driver attached so that Spark can access it.</p> <ol> <li>Go into the Project's settings.</li> <li>Select \"Compute configuration\".</li> <li>Select \"Spark\".</li> <li>Under \"Additional Jars\" choose \"Upload new file\" to upload the driver jar file.</li> </ol> <p> </p> Attaching the Redshift Driver to all Jobs and Jupyter Instances of the Project <p>Alternatively, you can choose the \"From Project\" option. You will first have to upload the jar file to the Project using the File Browser. After you have uploaded the jar  file, you can select it using the \"From Project\" option. To upload the jar file to the Project through the File Browser, see the example below:</p> <ol> <li>Open File Browser</li> <li>Navigate to \"Resources\" directory</li> <li>Upload the jar file</li> </ol> <p> </p> Redshift Driver Upload in the File Browser <p>Tip</p> <p>If you face network connectivity issues to your Redshift cluster, a common cause could be the cluster database port  not being accessible from outside the Redshift cluster VPC network. A quick and dirty way to enable connectivity is  to Enable Publicly Accessible. However, in a production setting, you should use VPC peering  or some equivalent mechanism for connecting the clusters.</p>"},{"location":"user_guides/fs/storage_connector/creation/redshift/#next-steps","title":"Next Steps","text":"<p>Move on to the usage guide for storage connectors to see how you can use your newly created Redshift connector.</p>"},{"location":"user_guides/fs/storage_connector/creation/s3/","title":"How-To set up a S3 Storage Connector","text":""},{"location":"user_guides/fs/storage_connector/creation/s3/#introduction","title":"Introduction","text":"<p>Amazon S3 or Amazon Simple Storage Service is a service offered by AWS that provides object storage. That means you can store arbitrary objects associated with a key. These kind of storage systems are often used as Data Lakes with large volumes of unstructured data or file based storage. Popular file formats are <code>CSV</code> or <code>PARQUET</code>.</p> <p>There are so called Data Lake House technologies such as Delta Lake or Apache Hudi, building an additional layer on top of object based storage with files, to provide database semantics like ACID transactions among others. This has the advantage that cheap storage can be turned into a cloud native data warehouse. These kind of storages are often the source for raw data from which features can be engineered.</p> <p>In this guide, you will configure a Storage Connector in Hopsworks to save all the authentication information needed in order to set up a connection to your AWS S3 bucket. When you're finished, you'll be able to read files using Spark through HSFS APIs. You can also use the connector to write out training data from the Feature Store, in order to make it accessible by third parties.</p> <p>Note</p> <p>Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically.</p>"},{"location":"user_guides/fs/storage_connector/creation/s3/#prerequisites","title":"Prerequisites","text":"<p>Before you begin this guide you'll need to retrieve the following information from your AWS S3 account and bucket:</p> <ul> <li>Bucket: You will need a S3 bucket that you have access to. The bucket is identified by its name.</li> <li>Path (Optional): If needed, a path can be defined to ensure that all operations are restricted to a specific location within the bucket.</li> <li>Region (Optional): You will need an S3 region to have complete control over data when managing the feature group that relies on this storage connector. The region is identified by its code.</li> <li>Authentication Method: You can authenticate using Access Key/Secret, or use IAM roles. If you want to use an IAM role it either needs to be attached to the entire Hopsworks cluster or Hopsworks needs to be able to assume the role. See IAM role documentation for more information.</li> <li>Server Side Encryption details: If your bucket has server side encryption (SSE) enabled, make sure you know which algorithm it is using (AES256 or SSE-KMS). If you are using SSE-KMS, you need the resource ARN of the managed key.</li> </ul>"},{"location":"user_guides/fs/storage_connector/creation/s3/#creation-in-the-ui","title":"Creation in the UI","text":""},{"location":"user_guides/fs/storage_connector/creation/s3/#step-1-set-up-new-storage-connector","title":"Step 1: Set up new storage connector","text":"<p>Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2).</p> <p> </p> The Storage Connector View in the User Interface"},{"location":"user_guides/fs/storage_connector/creation/s3/#step-2-enter-bucket-information","title":"Step 2: Enter Bucket Information","text":"<p>Enter the details for your S3 connector. Start by giving it a name and an optional description. And set the name of the S3 Bucket you want to point the connector to. Optionally, specify the region if you wish to have a Hopsworks-managed feature group stored using this connector.</p> <p> </p> S3 Connector Creation Form"},{"location":"user_guides/fs/storage_connector/creation/s3/#step-3-configure-authentication","title":"Step 3: Configure Authentication","text":""},{"location":"user_guides/fs/storage_connector/creation/s3/#instance-role","title":"Instance Role","text":"<p>Choose instance role if you have an EC2 instance profile attached to your Hopsworks cluster nodes with a role which grants you access to the specified bucket.</p>"},{"location":"user_guides/fs/storage_connector/creation/s3/#temporary-credentials","title":"Temporary Credentials","text":"<p>Choose temporary credentials if you are using AWS Role chaining to control the access permission on a project and user role base. Once you have selected Temporary Credentials select the role that give access to the specified bucket. For this role to appear in the list it needs to have been configured by an administrator, see the AWS Role chaining documentation for more details.</p> <p>Session Duration</p> <p>By default, the session duration that the role will be assumed for is 1 hour or 3600 seconds. This means if you want to use the storage connector for example to write training data to S3, the training dataset creation cannot take longer than one hour.</p> <p>Your administrator can change the default session duration for AWS storage connectors, by first increasing the max session duration of the IAM Role that you are assuming. And then changing the <code>fs_storage_connector_session_duration</code> configuration variable to the appropriate value in seconds.</p>"},{"location":"user_guides/fs/storage_connector/creation/s3/#access-keysecret","title":"Access Key/Secret","text":"<p>The most simple authentication method are Access Key/Secret, choose this option to get started quickly, if you are able to retrieve the keys using the IAM user administration.</p>"},{"location":"user_guides/fs/storage_connector/creation/s3/#step-4-configure-server-side-encryption","title":"Step 4: Configure Server Side Encryption","text":"<p>Additionally, you can specify if your Bucket has SSE enabled.</p>"},{"location":"user_guides/fs/storage_connector/creation/s3/#aes256","title":"AES256","text":"<p>For AES256, there is nothing to do but enabling the encryption by toggling the <code>AES256</code> option. This is using S3-Managed Keys, also called SSE-S3. </p>"},{"location":"user_guides/fs/storage_connector/creation/s3/#sse-kms","title":"SSE-KMS","text":"<p>With this option the encryption key is managed by AWS KMS, with some additional benefits and charges for using this service. The difference is that you need to provide the resource ARN of the key. </p> <p>If you have SSE-KMS enabled for your bucket, you can find the key ARN in the \"Properties\" section of the bucket details on AWS.</p>"},{"location":"user_guides/fs/storage_connector/creation/s3/#step-5-add-spark-options-optional","title":"Step 5: Add Spark Options (optional)","text":"<p>Here you can specify any additional spark options that you wish to add to the spark context at runtime. Multiple options can be added as key - value pairs.</p> <p>To connect to a S3 compatiable storage other than AWS S3, you can add the option with key as <code>fs.s3a.endpoint</code> and the endpoint you want to use as value. The storage connector will then be able to read from your specified S3 compatible storage.</p> <p>Spark Configuration</p> <p>When using the storage connector within a Spark application, the credentials are set at application level. This allows users to access multiple buckets with the same storage connector within the same application (assuming the credentials allow it). You can disable this behaviour by setting the option <code>fs.s3a.global-conf</code> to <code>False</code>. If the <code>global-conf</code> option is disabled, the credentials are set on a per-bucket basis and users will be able to use the credentials to access data only from the bucket specified in the storage connector configuration.</p>"},{"location":"user_guides/fs/storage_connector/creation/s3/#next-steps","title":"Next Steps","text":"<p>Move on to the usage guide for storage connectors to see how you can use your newly created S3 connector.</p>"},{"location":"user_guides/fs/storage_connector/creation/snowflake/","title":"How-To set up a Snowflake Storage Connector","text":""},{"location":"user_guides/fs/storage_connector/creation/snowflake/#introduction","title":"Introduction","text":"<p>Snowflake provides a cloud-based data storage and analytics service, used as a data warehouse in many enterprises. </p> <p>Data warehouses are often the source of raw data for feature engineering pipelines and Snowflake supports scalable feature computation with SQL. However, Snowflake is not viable as an online feature store that serves features to models in production, with its columnar database layout its latency is too high compared to OLTP databases or key-value stores.</p> <p>In this guide, you will configure a Storage Connector in Hopsworks to save all the authentication information needed in order to set up a connection to your Snowflake database. When you're finished, you'll be able to query the database using Spark through HSFS APIs.</p> <p>Note</p> <p>Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically.</p>"},{"location":"user_guides/fs/storage_connector/creation/snowflake/#prerequisites","title":"Prerequisites","text":"<p>Before you begin this guide you'll need to retrieve the following information from your Snowflake account and database, the following options are mandatory:</p> <ul> <li>Snowflake Connection URL: Consult the documentation of your target snowflake account to determine the correct connection URL. This is usually some form of your Snowflake account identifier. For example: <pre><code>&lt;account_identifier&gt;.snowflakecomputing.com\n</code></pre> OR: <pre><code>https://&lt;orgname&gt;-&lt;account_name&gt;.snowflakecomputing.com\n</code></pre> The account and organization details can be viewed in the Snowsight UI under Admin &gt; Account or by querying it in  SQL, as explained in Snowflake documentation. Below is an example of how to view the account and organization to get the account identifier from the Snowsight UI.</li> </ul> <p> </p> Viewing Snowflake account identifier <p>Token-based authentication or password based</p> <p>The Snowflake storage connector supports both username and password authentication as well as token-based authentication.</p> <p>Currently token-based authentication is in beta phase. Users are advised to use username/password and/or create a service account for accessing Snowflake from Hopsworks.</p> <ul> <li>Username and Password: Login name for the Snowflake user and password. This is often also referred to as <code>sfUser</code> and <code>sfPassword</code>.</li> <li>Warehouse: The warehouse to use for the session after connecting</li> <li>Database: The database to use for the session after connecting.</li> <li>Schema: The schema to use for the session after connecting.</li> </ul> <p>These are a few additional optional arguments:</p> <ul> <li>Role: The role field can be used to specify which Snowflake security role to assume for the session after the connection is established.</li> <li>Application: The application field can also be specified to have better observability in Snowflake with regards to which application is running which query. The application field can be a simple string like \u201cHopsworks\u201d or, for instance, the project name, to track usage and queries from each Hopsworks project.</li> </ul>"},{"location":"user_guides/fs/storage_connector/creation/snowflake/#creation-in-the-ui","title":"Creation in the UI","text":""},{"location":"user_guides/fs/storage_connector/creation/snowflake/#step-1-set-up-new-storage-connector","title":"Step 1: Set up new storage connector","text":"<p>Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2).</p> <p> </p> The Storage Connector View in the User Interface"},{"location":"user_guides/fs/storage_connector/creation/snowflake/#step-2-enter-snowflake-settings","title":"Step 2: Enter Snowflake Settings","text":"<p>Enter the details for your Snowflake connector. Start by giving it a name and an optional description.</p> <ol> <li>Select \"Snowflake\" as connector protocol.</li> <li>Specify the hostname for your account in the following format <code>&lt;account_identifier&gt;.snowflakecomputing.com</code>  or <code>https://&lt;orgname&gt;-&lt;account_name&gt;.snowflakecomputing.com</code>.</li> <li>Login name for the Snowflake user.</li> <li>Password for the Snowflake user or Token.</li> <li>The database to connect to.</li> <li>The schema to use for the connection to the database.</li> <li>Additional optional arguments. For example, you can point the connector to a specific table in the database only.</li> <li>Optional additional key/value arguments.</li> <li>Click \"Setup storage connector\".</li> </ol> <p> </p> Snowflake Connector Creation Form"},{"location":"user_guides/fs/storage_connector/creation/snowflake/#next-steps","title":"Next Steps","text":"<p>Move on to the usage guide for storage connectors to see how you can use your newly created Snowflake connector.</p>"},{"location":"user_guides/fs/tags/tags/","title":"Tags","text":""},{"location":"user_guides/fs/tags/tags/#introduction","title":"Introduction","text":"<p>Hopsworks feature store enables users to attach tags to artifacts, such as feature groups, feature views or training datasets.</p> <p>A tag is a <code>{key: value}</code> pair which provides additional information about the data managed by Hopsworks. Tags allow you to design custom metadata for your artifacts. For example, you could design a tag schema that encodes governance rules for your feature store, such as classifying data as personally identifiable, defining a data retention period for the data, and defining who signed off on the creation of some feature.</p>"},{"location":"user_guides/fs/tags/tags/#prerequisites","title":"Prerequisites","text":"<p>Tags have a schema. Before you can attach a tag to an artifact and fill in the tag values, you first need to select an existing tag schema or create a new tag schema.</p> <p>Tag schemas can be defined by Hopsworks administrator in the <code>Cluster settings</code> section of the platform. Schemas are defined globally across all projects. When users attach tags to an artifact, the tag will be validated against a specific schema. This allows tags to be consistent no matter the project or the team generating them. </p> <p>Immutable</p> <p>Tag schemas are immutable. Once defined, a tag schema cannot be edited nor deleted. </p>"},{"location":"user_guides/fs/tags/tags/#step-1-define-a-tag-schema","title":"Step 1: Define a tag schema","text":"<p>Tag schemas can be defined using the UI wizard in the <code>Cluster settings</code> &gt; <code>Tag schemas</code> section.  Tag schemas have a name, the name is used to uniquely identify the schema. You can also provide an optional description.</p> <p>You can define a schema by using the UI tool or by providing the schema in JSON format. If you use the UI tool, you should provide the name of the property in the schema, the type of the property, whether or not the property is required and an optional description. </p> <p> UI tag schema definition </p> <p>The UI tool allows you to define simple not-nested schemas. For more advanced use cases, more complex schemas (e.g. nested schemas) might be required to fully express the content of a given artifact. In such cases it is possible to provide the schema directly as JSON string. The JSON should follow the standard https://json-schema.org. An example of complex schema is the following:</p> <pre><code>{\n  \"type\" : \"object\",\n  \"properties\" :\n  {\n    \"first_name\" : { \"type\" : \"string\" },\n    \"last_name\" : { \"type\" : \"string\" },\n    \"age\" : { \"type\" : \"integer\" },\n    \"hobbies\" : {\n        \"type\" : \"array\",\n        \"items\" : { \"type\" : \"string\" }\n    }\n  },\n  \"required\" : [\"first_name\", \"last_name\", \"age\"],\n  \"additionalProperties\": false\n}\n</code></pre> <p>Additionally it is also possible to define a single property as tag. You can achieve this by defining a JSON schema like the following:</p> <pre><code>{ \"type\" : \"string\" }\n</code></pre> <p>Where the type is a valid primitive type: <code>string</code>, <code>boolean</code>, <code>integer</code>, <code>number</code>.</p>"},{"location":"user_guides/fs/tags/tags/#step-2-attach-a-tag-to-an-artifact","title":"Step 2: Attach a tag to an artifact","text":"<p>Once the tag schema has been created, you can attach a tag with that schema to a feature group, feature view or training datasets either using the feature store APIs, or by using the UI.</p>"},{"location":"user_guides/fs/tags/tags/#using-the-api","title":"Using the API","text":"<p>You can attach tags to feature groups and feature views by using the <code>add_tag()</code> method of the feature store APIs:</p> Python <pre><code># Retrieve the feature group\nfg = fs.get_feature_group(\"transactions_4h_aggs_fraud_batch_fg\", version=1)\n\n# Define the tag\ntag = {\n    'business_unit': 'Fraud',\n    'data_owner': 'email@hopsworks.ai',\n    'pii': True\n}\n\n# Attach the tag\nfg.add_tag(\"data_privacy\", tag) \n</code></pre> <p>You can see the list of tags attached to a given artifact by using the <code>get_tags()</code> method:</p> Python <pre><code># Retrieve the feature group\nfg = fs.get_feature_group(\"transactions_4h_aggs_fraud_batch_fg\", version=1)\n\n# Retrieve the tags for this feature group \nfg.get_tags()\n</code></pre> <p>Finally you can remove a tag from a given artifact by calling the <code>delete_tag()</code> method:</p> Python <pre><code># Retrieve the feature group\nfg = fs.get_feature_group(\"transactions_4h_aggs_fraud_batch_fg\", version=1)\n\n# Retrieve the tags for this feature group \nfg.delete_tag(\"data_privacy\")\n</code></pre> <p>The same APIs work for feature views and training dataset alike.</p>"},{"location":"user_guides/fs/tags/tags/#using-the-ui","title":"Using the UI","text":"<p>You can attach tags to feature groups and feature views directly from the UI. You can navigate on the artifact page and click on the <code>Add tags</code> button. From there you can select the tag schema of the tag you want to attach and populate the values as shown in the gif below.</p> <p> Attach tag to a feature group </p>"},{"location":"user_guides/fs/tags/tags/#step-3-search","title":"Step 3: Search","text":"<p>Hopsworks indexes the tags attached to feature groups, feature views and training datasets. The tags will then be searchable using the free text search box located at the top of the UI. </p> <p> Search for tags in the feature store </p>"},{"location":"user_guides/integrations/","title":"Client Integrations","text":"<p>Hopsworks is an open platform aiming to be accessible from a variety of tools. Learn in this section how to connect to Hopsworks from</p> <ul> <li>Python, AWS SageMaker, Google Colab, Kubeflow</li> <li>Databricks</li> <li>AWS EMR</li> <li>Azure HDInsight</li> <li>Azure Machine Learning</li> <li>Apache Spark</li> <li>Apache Flink</li> <li>Apache Beam</li> </ul>"},{"location":"user_guides/integrations/beam/","title":"Apache Beam Dataflow Runner","text":"<p>Connecting to the Feature Store from an Apache Beam Dataflow Runner, requires configuring the Hopsworks certificates. For this in your Beam Java application <code>pom.xml</code> file include following snippet: <pre><code>    &lt;resources&gt;\n      &lt;resource&gt;\n        &lt;directory&gt;java.io.tmpdir&lt;/directory&gt;\n        &lt;includes&gt;\n          &lt;include&gt;**/*.jks&lt;/include&gt;\n        &lt;/includes&gt;\n      &lt;/resource&gt;\n    &lt;/resources&gt;\n</code></pre></p>"},{"location":"user_guides/integrations/beam/#generating-an-api-key","title":"Generating an API Key","text":"<p>For instructions on how to generate an API key follow this user guide. For the Beam integration to work correctly make sure you add the following scopes to your API key:</p> <ol> <li>featurestore</li> <li>project</li> <li>job</li> <li>kafka</li> </ol>"},{"location":"user_guides/integrations/beam/#connecting-to-the-feature-store","title":"Connecting to the Feature Store","text":"<p>You are now ready to connect to the Hopsworks Feature Store from Beam:</p> <pre><code>//Establish connection with Hopsworks.\nHopsworksConnection hopsworksConnection = HopsworksConnection.builder()\n  .host(\"my_instance\")                      // DNS of your Feature Store instance\n  .port(443)                                // Port to reach your Hopsworks instance, defaults to 443\n  .project(\"my_project\")                    // Name of your Hopsworks Feature Store project \n  .apiKeyValue(\"api_key\")                   // The API key to authenticate with the feature store\n  .hostnameVerification(false)              // Disable for self-signed certificates\n  .build();\n\n//get feature store handle\nFeatureStore fs = hopsworksConnection.getFeatureStore();\n</code></pre>"},{"location":"user_guides/integrations/beam/#next-steps","title":"Next Steps","text":"<p>For more information and how to integrate Beam feature pipeline  to the Hopsworks Feature store follow the tutorial.</p>"},{"location":"user_guides/integrations/flink/","title":"Flink Integration","text":"<p>Connecting to the Feature Store from an external Flink cluster, such as AWS EMR and GCP DataProc requires configuring it with the Hopsworks certificates, done automatically when using Hopsworks API. This guide explains how to connect to the Feature Store from an external Flink cluster.</p>"},{"location":"user_guides/integrations/flink/#generating-an-api-key","title":"Generating an API Key","text":"<p>For instructions on how to generate an API key follow this user guide. For the Flink integration to work correctly make sure you add the following scopes to your API key:</p> <ol> <li>featurestore</li> <li>project</li> <li>job</li> <li>kafka</li> </ol>"},{"location":"user_guides/integrations/flink/#connecting-to-the-feature-store","title":"Connecting to the Feature Store","text":"<p>You are now ready to connect to the Hopsworks Feature Store from Flink:</p> <pre><code>//Establish connection with Hopsworks.\nHopsworksConnection hopsworksConnection = HopsworksConnection.builder()\n  .host(\"my_instance\")                      // DNS of your Feature Store instance\n  .port(443)                                // Port to reach your Hopsworks instance, defaults to 443\n  .project(\"my_project\")                    // Name of your Hopsworks Feature Store project \n  .apiKeyValue(\"api_key\")                   // The API key to authenticate with the feature store\n  .hostnameVerification(false)              // Disable for self-signed certificates\n  .build();\n\n//get feature store handle\nFeatureStore fs = hopsworksConnection.getFeatureStore();\n</code></pre>"},{"location":"user_guides/integrations/flink/#next-steps","title":"Next Steps","text":"<p>For more information and how to integrate Flink streaming feature pipeline to the Hopsworks Feature store follow the tutorial.</p>"},{"location":"user_guides/integrations/hdinsight/","title":"Configure HDInsight for the Hopsworks Feature Store","text":"<p>To enable HDInsight to access the Hopsworks Feature Store, you need to set up a Hopsworks API key, add a script action and configurations to your HDInsight cluster.</p> <p>Prerequisites</p> <p>A HDInsight cluster with cluster type Spark is required to connect to the Feature Store. You can either use an existing cluster or create a new one.</p> <p>Network Connectivity</p> <p>To be able to connect to the Feature Store, please ensure that your HDInsight cluster and the Hopsworks Feature Store are either in the same Virtual Network or Virtual Network Peering is set up between the different networks. In addition, ensure that the Network Security Group of your Hopsworks instance is configured to allow incoming traffic from your HDInsight cluster on ports 443, 3306, 8020, 30010, 9083 and 9085 (443,3306,8020,30010,9083,9085). See Network security groups for more information.</p>"},{"location":"user_guides/integrations/hdinsight/#step-1-set-up-a-hopsworks-api-key","title":"Step 1: Set up a Hopsworks API key","text":"<p>For instructions on how to generate an API key follow this user guide. For the HDInsight integration to work correctly make sure you add the following scopes to your API key:</p> <ol> <li>featurestore</li> <li>project</li> <li>job</li> <li>kafka</li> </ol>"},{"location":"user_guides/integrations/hdinsight/#step-2-use-a-script-action-to-install-the-feature-store-connector","title":"Step 2:  Use a script action to install the Feature Store connector","text":"<p>HDInsight requires Hopsworks connectors to be able to communicate with the Hopsworks Feature Store. These connectors can be installed with the script action shown below. Copy the content into a file, name the file <code>hopsworks.sh</code> and replace MY_INSTANCE, MY_PROJECT, MY_VERSION, MY_API_KEY and MY_CONDA_ENV with your values. Copy the <code>hopsworks.sh</code> file into any storage that is readable by your HDInsight clusters and take note of the URI of that file e.g., <code>https://account.blob.core.windows.net/scripts/hopsworks.sh</code>.</p> <p>The script action needs to be applied head and worker nodes and can be applied during cluster creation or to an existing cluster. Ensure to persist the script action so that it is run on newly created nodes. For more information about how to use script actions, see Customize Azure HDInsight clusters by using script actions.</p> <p>Matching Hopsworks version</p> <p>We recommend that the major and minor version of the Python library match the major and minor version of the Hopsworks deployment.</p> <p><p> You find the Hopsworks version inside any of your Project's settings tab on Hopsworks </p></p> <p>Feature Store script action: <pre><code>set -e\n\nHOST=\"MY_INSTANCE.cloud.hopsworks.ai\" # DNS of your Feature Store instance\nPROJECT=\"MY_PROJECT\"                  # Port to reach your Hopsworks instance, defaults to 443\nHOPSWORKS_VERSION=\"MY_VERSION\"        # The major version of Hopsworks library needs to match the major version of Hopsworks\nAPI_KEY=\"MY_API_KEY\"                  # The API key to authenticate with Hopsworks\nCONDA_ENV=\"MY_CONDA_ENV\"              # py35 is the default for HDI 3.6\n\napt-get --assume-yes install python3-dev\napt-get --assume-yes install jq\n\n/usr/bin/anaconda/envs/$CONDA_ENV/bin/pip install hopsworks==$HOPSWORKS_VERSION\n\nPROJECT_ID=$(curl -H \"Authorization: ApiKey ${API_KEY}\" https://$HOST/hopsworks-api/api/project/getProjectInfo/$PROJECT | jq -r .projectId)\n\nmkdir -p /usr/lib/hopsworks\nchown root:hadoop /usr/lib/hopsworks\ncd /usr/lib/hopsworks\n\ncurl -o client.tar.gz -H \"Authorization: ApiKey ${API_KEY}\" https://$HOST/hopsworks-api/api/project/$PROJECT_ID/client\n\ntar -xvf client.tar.gz\ntar -xzf client/apache-hive-*-bin.tar.gz\nmv apache-hive-*-bin apache-hive-bin\nrm client.tar.gz\nrm client/apache-hive-*-bin.tar.gz\n\ncurl -H \"Authorization: ApiKey ${API_KEY}\" https://$HOST/hopsworks-api/api/project/$PROJECT_ID/credentials | jq -r .kStore | base64 -d &gt; keyStore.jks\n\ncurl -H \"Authorization: ApiKey ${API_KEY}\" https://$HOST/hopsworks-api/api/project/$PROJECT_ID/credentials | jq -r .tStore | base64 -d &gt; trustStore.jks\n\necho -n $(curl -H \"Authorization: ApiKey ${API_KEY}\" https://$HOST/hopsworks-api/api/project/$PROJECT_ID/credentials | jq -r .password) &gt; material_passwd\n\nchown -R root:hadoop /usr/lib/hopsworks\n</code></pre></p>"},{"location":"user_guides/integrations/hdinsight/#step-3-configure-hdinsight-for-feature-store-access","title":"Step 3: Configure HDInsight for Feature Store access","text":"<p>The Hadoop and Spark installations of the HDInsight cluster need to be configured in order to access the Feature Store. This can be achieved either by using a bootstrap script when creating clusters or using Ambari on existing clusters. Apply the following configurations to your HDInsight cluster.</p> <p>Using Hive and the Feature Store</p> <p>HDInsight clusters cannot use their local Hive when being configured for the Feature Store as the Feature Store relies on custom Hive binaries and its own Metastore which will overwrite the local one. If you rely on Hive for feature engineering then it is advised to write your data to an external data storage such as ADLS from your main HDInsight cluster and in the Feature Store, create an on-demand Feature Group on the storage container in ADLS.</p> <p>Hadoop hadoop-env.sh: <pre><code>export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/usr/lib/hopsworks/client/*\n</code></pre></p> <p>Hadoop core-site.xml: <pre><code>hops.ipc.server.ssl.enabled=true\nfs.hopsfs.impl=io.hops.hopsfs.client.HopsFileSystem\nclient.rpc.ssl.enabled.protocol=TLSv1.2\nhops.ssl.keystore.name=/usr/lib/hopsworks/keyStore.jks\nhops.rpc.socket.factory.class.default=io.hops.hadoop.shaded.org.apache.hadoop.net.HopsSSLSocketFactory\nhops.ssl.keystores.passwd.name=/usr/lib/hopsworks/material_passwd\nhops.ssl.hostname.verifier=ALLOW_ALL\nhops.ssl.trustore.name=/usr/lib/hopsworks/trustStore.jks\n</code></pre></p> <p>Spark spark-defaults.conf: <pre><code>spark.executor.extraClassPath=/usr/lib/hopsworks/client/*\nspark.driver.extraClassPath=/usr/lib/hopsworks/client/*\nspark.sql.hive.metastore.jars=path\nspark.sql.hive.metastore.jars.path=/usr/lib/hopsworks/apache-hive-bin/lib/*\n</code></pre></p> <p>Spark hive-site.xml: <pre><code>hive.metastore.uris=thrift://MY_HOPSWORKS_INSTANCE_PRIVATE_IP:9083\n</code></pre></p> <p>Info</p> <p>Replace MY_HOPSWORKS_INSTANCE_PRIVATE_IP with the private IP address of you Hopsworks Feature Store.</p>"},{"location":"user_guides/integrations/hdinsight/#step-5-connect-to-the-feature-store","title":"Step 5: Connect to the Feature Store","text":"<p>You are now ready to connect to the Hopsworks Feature Store, for instance using a Jupyter notebook in HDInsight with a PySpark3 kernel:</p> <pre><code>import hopsworks\n\n# Put the API key into Key Vault for any production setup:\n# See, https://azure.microsoft.com/en-us/services/key-vault/\nsecret_value = 'MY_API_KEY'\n\n# Create a connection\nproject = hopsworks.login(\n    host='MY_INSTANCE.cloud.hopsworks.ai', # DNS of your Feature Store instance\n    port=443,                              # Port to reach your Hopsworks instance, defaults to 443\n    project='MY_PROJECT',                  # Name of your Hopsworks project\n    api_key_value=secret_value,            # The API key to authenticate with Hopsworks\n    hostname_verification=True             # Disable for self-signed certificates\n)\n\n# Get the feature store handle for the project's feature store\nfs = project.get_feature_store()\n</code></pre>"},{"location":"user_guides/integrations/hdinsight/#next-steps","title":"Next Steps","text":"<p>For more information on how to use the Hopsworks API check out the other guides or the Login API. </p>"},{"location":"user_guides/integrations/mlstudio_designer/","title":"Azure Machine Learning Designer Integration","text":"<p>Connecting to Hopsworks from the Azure Machine Learning Designer requires setting up a Hopsworks API key for the Designer and installing the Hopsworks Python library on the Designer. This guide explains step by step how to connect to the Feature Store from Azure Machine Learning Designer.</p> <p>Network Connectivity</p> <p>To be able to connect to the Feature Store, please ensure that the Network Security Group of your Hopsworks instance on Azure is configured to allow incoming traffic from your compute target on ports 443, 9083 and 9085 (443,9083,9085). See Network security groups for more information. If your compute target is not in the same VNet as your Hopsworks instance and the Hopsworks instance is not accessible from the internet then you will need to configure Virtual Network Peering.</p>"},{"location":"user_guides/integrations/mlstudio_designer/#generate-an-api-key","title":"Generate an API key","text":"<p>For instructions on how to generate an API key follow this user guide. For the Azure ML Designer integration to work correctly make sure you add the following scopes to your API key:</p> <ol> <li>featurestore</li> <li>project</li> <li>job</li> <li>kafka</li> </ol>"},{"location":"user_guides/integrations/mlstudio_designer/#connect-to-hopsworks","title":"Connect to Hopsworks","text":"<p>To connect to Hopsworks from the Azure Machine Learning Designer, create a new pipeline or open an existing one:</p> <p> Add an Execute Python Script step </p> <p>In the pipeline, add a new <code>Execute Python Script</code> step and replace the Python script from the next step:</p> <p> Add the code to access the Hopsworks </p> <p>Updating the script</p> <p>Replace MY_VERSION, MY_API_KEY, MY_INSTANCE, MY_PROJECT and MY_FEATURE_GROUP with the respective values. The major version set for MY_VERSION needs to match the major version of Hopsworks. Check PyPI for available releases.</p> <p><p> You find the Hopsworks version inside any of your Project's settings tab on Hopsworks </p></p> <pre><code>import os\nimport importlib.util\n\n\npackage_name = 'hopsworks'\nversion = 'MY_VERSION'\nspec = importlib.util.find_spec(package_name)\nif spec is None:\n    import os\n    os.system(f\"pip install %s[python]==%s\" % (package_name, version))\n\n# Put the API key into Key Vault for any production setup:\n# See, https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-secrets-in-runs\n#from azureml.core import Experiment, Run\n#run = Run.get_context()\n#secret_value = run.get_secret(name=\"fs-api-key\")\nsecret_value = 'MY_API_KEY'\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n\n    import hopsworks\n    project = hopsworks.login(\n        host='MY_INSTANCE.cloud.hopsworks.ai', # DNS of your Hopsworks instance\n        port=443,                              # Port to reach your Hopsworks instance, defaults to 443\n        project='MY_PROJECT',                  # Name of your Hopsworks project\n        api_key_value=secret_value,            # The API key to authenticate with Hopsworks\n        hostname_verification=True,            # Disable for self-signed certificates\n        engine='python'                        # Choose python as engine\n    )\n    fs = project.get_feature_store()              # Get the project's default feature store\n\n    return fs.get_feature_group('MY_FEATURE_GROUP', version=1).read(),\n</code></pre> <p>Select a compute target and save the step. The step is now ready to use:</p> <p> Select a compute target </p> <p>As a next step, you have to connect the previously created <code>Execute Python Script</code> step with the next step in the pipeline. For instance, to export the features to a CSV file, create a <code>Export Data</code> step:</p> <p> Add an Export Data step </p> <p>Configure the <code>Export Data</code> step to write to you data store of choice:</p> <p> Configure the Export Data step </p> <p>Connect the to steps by drawing a line between them:</p> <p> Connect the steps </p> <p>Finally, submit the pipeline and wait for it to finish:</p> <p>Performance on the first execution</p> <p>The <code>Execute Python Script</code> step can be slow when being executed for the first time as the Hopsworks library needs to be installed on the compute target. Subsequent executions on the same compute target should use the already installed library.</p> <p> Execute the pipeline </p>"},{"location":"user_guides/integrations/mlstudio_designer/#next-steps","title":"Next Steps","text":"<p>For more information on how to use the Hopsworks API check out the other guides or the Login API. </p>"},{"location":"user_guides/integrations/mlstudio_notebooks/","title":"Azure Machine Learning Notebooks Integration","text":"<p>Connecting to the Hopsworks from Azure Machine Learning Notebooks requires setting up a Hopsworks API key for Azure Machine Learning Notebooks and installing the Hopsworks Python library on the notebook. This guide explains step by step how to connect to the Hopsworks from Azure Machine Learning Notebooks.</p> <p>Network Connectivity</p> <p>To be able to connect to the Feature Store, please ensure that the Network Security Group of your Hopsworks instance on Azure is configured to allow incoming traffic from your compute target on ports 443, 9083 and 9085 (443,9083,9085). See Network security groups for more information. If your compute target is not in the same VNet as your Hopsworks instance and the Hopsworks instance is not accessible from the internet then you will need to configure Virtual Network Peering.</p>"},{"location":"user_guides/integrations/mlstudio_notebooks/#install-hopsworks-python-library","title":"Install Hopsworks Python Library","text":"<p>To be able to interact with Hopsworks from a Python environment you need to install the <code>Hopsworks</code> Python library. The library is available on PyPi and can be installed using <code>pip</code>: </p> <pre><code>pip install hopsworks[python]~=[HOPSWORKS_VERSION]\n</code></pre> <p>Python Profile</p> <p>By default, <code>pip install hopsworks</code> does not install all the necessary dependencies required to use the Hopsworks library from a local Python environment. To ensure that all the dependencies are installed, you should install the library using with the Python profile <code>pip install hopsworks[python]</code>.</p> <p>Matching Hopsworks version</p> <p>We recommend that the major and minor version of the Python library match the major and minor version of the Hopsworks deployment.</p> <p><p> You find the Hopsworks version inside any of your Project's settings tab on Hopsworks </p></p>"},{"location":"user_guides/integrations/mlstudio_notebooks/#generate-an-api-key","title":"Generate an API key","text":"<p>For instructions on how to generate an API key follow this user guide. For the Azure ML Notebooks integration to work correctly make sure you add the following scopes to your API key:</p> <ol> <li>featurestore</li> <li>project</li> <li>job</li> <li>kafka</li> </ol>"},{"location":"user_guides/integrations/mlstudio_notebooks/#connect-from-an-azure-machine-learning-notebook","title":"Connect from an Azure Machine Learning Notebook","text":"<p>To access Hopsworks from Azure Machine Learning, open a Python notebook and proceed with the following steps to install Hopsworks and connect to the Feature Store:</p> <p> Connecting from an Azure Machine Learning Notebook </p>"},{"location":"user_guides/integrations/mlstudio_notebooks/#connect-to-hopsworks","title":"Connect to Hopsworks","text":"<p>You are now ready to connect to Hopsworks Feature Store from the notebook:</p> <pre><code>import hopsworks \n\n# Put the API key into Key Vault for any production setup:\n# See, https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-secrets-in-runs\n#from azureml.core import Experiment, Run\n#run = Run.get_context()\n#secret_value = run.get_secret(name=\"fs-api-key\")\nsecret_value = 'MY_API_KEY'\n\n# Create a connection\nproject = hopsworks.login(\n    host='MY_INSTANCE.cloud.hopsworks.ai', # DNS of your Hopsworks instance\n    port=443,                              # Port to reach your Hopsworks instance, defaults to 443\n    project='MY_PROJECT',                  # Name of your Hopsworks project\n    api_key_value=secret_value,            # The API key to authenticate with Hopsworks\n    hostname_verification=True,            # Disable for self-signed certificates\n    engine='python'                        # Choose Python as engine\n)\n\n# Get the feature store handle for the project's feature store\nfs = project.get_feature_store()\n</code></pre>"},{"location":"user_guides/integrations/mlstudio_notebooks/#next-steps","title":"Next Steps","text":"<p>For more information on how to use the Hopsworks API check out the other guides or the Login API. </p>"},{"location":"user_guides/integrations/python/","title":"Python Environments (Local, AWS SageMaker, Google Colab or Kubeflow)","text":"<p>This guide explains step by step how to connect to Hopsworks from any Python environment such as your local environment, AWS SageMaker, Google Colab or Kubeflow.</p>"},{"location":"user_guides/integrations/python/#install-python-library","title":"Install Python Library","text":"<p>To be able to interact with Hopsworks from a Python environment you need to install the <code>Hopsworks</code> Python library. The library is available on PyPi and can be installed using <code>pip</code>: </p> <pre><code>pip install hopsworks[python]~=[HOPSWORKS_VERSION]\n</code></pre> <p>Python Profile</p> <p>By default, <code>pip install hopsworks</code>, does not install all the necessary dependencies required to use the Hopsworks library from a pure Python environment. To ensure that all the dependencies are installed, you should install the library using with the Python profile <code>pip install hopsworks[python]</code>.</p> <p>Matching Hopsworks version</p> <p>We recommend that the major and minor version of the Python library match the major and minor version of the Hopsworks deployment.</p> <p><p> You find the Hopsworks version inside any of your Project's settings tab on Hopsworks </p></p>"},{"location":"user_guides/integrations/python/#generate-an-api-key","title":"Generate an API key","text":"<p>For instructions on how to generate an API key follow this user guide. For the Python client to work correctly make sure you add the following scopes to your API key:</p> <ol> <li>featurestore</li> <li>project</li> <li>job</li> <li>kafka</li> </ol>"},{"location":"user_guides/integrations/python/#connect-to-the-feature-store","title":"Connect to the Feature Store","text":"<p>You are now ready to connect to Hopsworks from your Python environment:</p> <pre><code>import hopsworks \nproject = hopsworks.login(\n    host='my_instance',                 # DNS of your Hopsworks instance\n    port=443,                           # Port to reach your Hopsworks instance, defaults to 443\n    project='my_project',               # Name of your Hopsworks project\n    api_key_value='apikey',             # The API key to authenticate with Hopsworks\n    engine='python',                    # Use the Python engine\n)\nfs = project.get_feature_store()        # Get the project's default feature store\n</code></pre> <p>Engine</p> <p><code>Hopsworks</code> leverages several engines depending on whether you are running using Apache Spark or Pandas/Polars. The default behaviour of the library is to use the <code>spark</code> engine if you do not specify any <code>engine</code> option in the <code>login</code> method and if the <code>PySpark</code> library is available in the environment.</p> <p>Please refer to the Spark integration guide to configure your PySpark cluster to interact with Hopsworks.</p>"},{"location":"user_guides/integrations/python/#next-steps","title":"Next Steps","text":"<p>For more information on how to use the Hopsworks API check out the other guides or the Login API. </p>"},{"location":"user_guides/integrations/spark/","title":"Spark Integration","text":"<p>Connecting to the Feature Store from an external Spark cluster, such as Cloudera, requires configuring it with the Hopsworks client jars and configuration. This guide explains step by step how to connect to the Feature Store from an external Spark cluster.</p>"},{"location":"user_guides/integrations/spark/#download-the-hopsworks-client-jars","title":"Download the Hopsworks Client Jars","text":"<p>In the Project Settings, select the integration tab and scroll to the Configure Spark Integration section. Click on Download client Jars. This will start the download of the client.tar.gz archive. The archive contains two jar files for HopsFS, the Apache Hudi jar and the Java version of the HSFS library. You should upload these libraries to your Spark cluster and attach them as local resources to your Job. If you are using <code>spark-submit</code>, you should specify the <code>--jar</code> option. For more details see: Spark Dependency Management.</p> <p> The Spark Integration gives access to Jars and configuration for an external Spark cluster </p>"},{"location":"user_guides/integrations/spark/#download-the-certificates","title":"Download the certificates","text":"<p>Download the certificates from the same section as above. Hopsworks uses X.509 certificates for authentication and authorization. If you are interested in the Hopsworks security model, you can read more about it in this blog post. The certificates are composed of three different components: the <code>keyStore.jks</code> containing the private key and the certificate for your project user, the <code>trustStore.jks</code> containing the certificates for the Hopsworks certificates authority, and a password to unlock the private key in the <code>keyStore.jks</code>. The password is displayed in a pop-up when downloading the certificate and should be saved in a file named <code>material_passwd</code>.</p> <p>Warning</p> <p>When you copy-paste the password to the <code>material_passwd</code> file, pay attention to not introduce additional empty spaces or new lines.</p> <p>The three files (<code>keyStore.jks</code>, <code>trustStore.jks</code> and <code>material_passwd</code>) should be attached as resources to your Spark application as well.</p>"},{"location":"user_guides/integrations/spark/#configure-your-spark-cluster","title":"Configure your Spark cluster","text":"<p>Spark version limitation</p> <p>Currently Spark version 3.3.x is suggested to be able to use the full suite of Hopsworks Feature Store capabilities.</p> <p>Add the following configuration to the Spark application:</p> <pre><code>spark.hadoop.fs.hopsfs.impl                         io.hops.hopsfs.client.HopsFileSystem\nspark.hadoop.hops.ipc.server.ssl.enabled            true\nspark.hadoop.hops.ssl.hostname.verifier             ALLOW_ALL\nspark.hadoop.hops.rpc.socket.factory.class.default  io.hops.hadoop.shaded.org.apache.hadoop.net.HopsSSLSocketFactory\nspark.hadoop.client.rpc.ssl.enabled.protocol        TLSv1.2\nspark.hadoop.hops.ssl.keystores.passwd.name         material_passwd\nspark.hadoop.hops.ssl.keystore.name                 keyStore.jks\nspark.hadoop.hops.ssl.trustore.name                 trustStore.jks\nspark.sql.hive.metastore.jars                       path\nspark.sql.hive.metastore.jars.path                  [Path to the Hopsworks Hive Jars]\nspark.hadoop.hive.metastore.uris                    thrift://[metastore_ip]:[metastore_port]\n</code></pre> <p><code>spark.sql.hive.metastore.jars.path</code> should point to the path with the jars from the uncompressed Hive archive you can find in clients.tar.gz.</p>"},{"location":"user_guides/integrations/spark/#pyspark","title":"PySpark","text":"<p>To use PySpark, install the HSFS Python library which can be found on PyPi.</p> <p>Matching Hopsworks version</p> <p>The major version of <code>HSFS</code> needs to match the major version of Hopsworks.</p>"},{"location":"user_guides/integrations/spark/#generating-an-api-key","title":"Generating an API Key","text":"<p>For instructions on how to generate an API key follow this user guide. For the Spark integration to work correctly make sure you add the following scopes to your API key:</p> <ol> <li>featurestore</li> <li>project</li> <li>job</li> <li>kafka</li> </ol>"},{"location":"user_guides/integrations/spark/#connecting-to-the-feature-store","title":"Connecting to the Feature Store","text":"<p>You are now ready to connect to the Hopsworks Feature Store from Spark:</p> <pre><code>import hopsworks \nproject = hopsworks.login(\n    host='my_instance',                 # DNS of your Feature Store instance\n    port=443,                           # Port to reach your Hopsworks instance, defaults to 443\n    project='my_project',               # Name of your Hopsworks Feature Store project\n    api_key_value='api_key',            # The API key to authenticate with the feature store\n    hostname_verification=True          # Disable for self-signed certificates\n)\nfs = project.get_feature_store()           # Get the project's default feature store\n</code></pre> <p>Engine</p> <p><code>Hopsworks</code> leverages several engines depending on whether you are running using Apache Spark or Pandas/Polars. The default behaviour of the library is to use the <code>spark</code> engine if you do not specify any <code>engine</code> option in the <code>login</code> method and if the <code>PySpark</code> library is available in the environment.</p>"},{"location":"user_guides/integrations/spark/#next-steps","title":"Next Steps","text":"<p>For more information about how to connect, see the Login API API reference. Or continue with the Data Source guide to import your own data to the Feature Store.</p>"},{"location":"user_guides/integrations/databricks/api_key/","title":"Hopsworks API key","text":"<p>In order for the Databricks cluster to be able to communicate with Hopsworks, clients running on Databricks need to be able to access a Hopsworks API key.</p>"},{"location":"user_guides/integrations/databricks/api_key/#generate-an-api-key","title":"Generate an API key","text":"<p>For instructions on how to generate an API key follow this user guide. For the Databricks integration to work make sure you add the following scopes to your API key:</p> <ol> <li>featurestore</li> <li>project</li> <li>job</li> <li>kafka</li> </ol>"},{"location":"user_guides/integrations/databricks/api_key/#quickstart-api-key-argument","title":"Quickstart API key Argument","text":"<p>API key as Argument</p> <p>To get started quickly, without saving the Hopsworks API in a secret storage, you can simply supply it as an argument when instantiating a connection:</p> <pre><code>    import hopsworks \n    project = hopsworks.login(\n        host='my_instance',                 # DNS of your Feature Store instance\n        port=443,                           # Port to reach your Hopsworks instance, defaults to 443\n        project='my_project',               # Name of your Hopsworks Feature Store project\n        api_key_value='apikey',             # The API key to authenticate with Hopsworks\n    )\n    fs = project.get_feature_store()           # Get the project's default feature store\n</code></pre>"},{"location":"user_guides/integrations/databricks/api_key/#next-steps","title":"Next Steps","text":"<p>Continue with the configuration guide to finalize the configuration of the Databricks Cluster to communicate with the Hopsworks Feature Store.</p>"},{"location":"user_guides/integrations/databricks/configuration/","title":"Databricks Integration","text":"<p>Users can configure their Databricks clusters to write the results of feature engineering pipelines in the Hopsworks Feature Store using HSFS. Configuring a Databricks cluster can be done from the Hopsworks Feature Store UI. This guide explains each step.</p>"},{"location":"user_guides/integrations/databricks/configuration/#prerequisites","title":"Prerequisites","text":"<p>In order to be able to configure a Databricks cluster to use the Feature Store of your Hopsworks instance, it is necessary to ensure networking is setup correctly between the instances and that the Databricks cluster has access to the Hopsworks API key to perform requests with HSFS from Databricks to Hopsworks.</p>"},{"location":"user_guides/integrations/databricks/configuration/#networking","title":"Networking","text":"<p>If you haven't done so already, follow the networking guides for either AWS or Azure for instructions on how to configure networking properly between Databricks' VPC (or Virtual Network on Azure) and the managed.hopsworks.ai VPC/VNet.</p>"},{"location":"user_guides/integrations/databricks/configuration/#hopsworks-api-key","title":"Hopsworks API key","text":"<p>In order for the Feature Store API to be able to communicate with the user's Hopsworks instance, the client library (HSFS) needs to have access to a previously generated API key from Hopsworks. For ways to setup and store the Hopsworks API key, please refer to the API key guide for Databricks.</p>"},{"location":"user_guides/integrations/databricks/configuration/#databricks-api-key","title":"Databricks API key","text":"<p>Hopsworks uses the Databricks REST APIs to communicate with the Databricks instance and configure clusters on behalf of users. To achieve that, the first step is to register an instance and a valid API key in Hopsworks.</p> <p>Users can get a valid Databricks API key by following the Databricks Documentation</p> <p>Cluster access control</p> <p>If users have enabled Databricks Cluster access control, it is important that the users running the cluster configuration (i.e. the user generating the API key) has <code>Can Manage</code> privileges on the cluster they are trying to configure.</p>"},{"location":"user_guides/integrations/databricks/configuration/#register-a-new-databricks-instance","title":"Register a new Databricks Instance","text":"<p>To register a new Databricks instance, first navigate to <code>Project settings</code>, which can be found on the left-hand side of a Project's landing page, then select the <code>Integrations</code> tab.</p> <p> Hopsworks's Integrations page </p> <p>Registering a Databricks instance requires adding the instance address and the Databricks API key.</p> <p>The instance name corresponds to the address of the Databricks instance and should be in the format <code>[UUID].cloud.databricks.com</code> (or <code>adb-[UUID].19.azuredatabricks.net</code> for Databricks on Azure), essentially the same web address used to reach the Databricks instance from the browser.</p> <p> Register a Databricks Instance along with a Databricks API key </p> <p>The API key will be stored in the Hopsworks secret store for the user and will be available only for that user.  If multiple users need to configure Databricks clusters, each has to generate an API key and register an instance. The Databricks instance registration does not have a project scope, meaning that once registered, the user can configure clusters for all projects they are part of.</p>"},{"location":"user_guides/integrations/databricks/configuration/#databricks-cluster","title":"Databricks Cluster","text":"<p>A cluster needs to exist before users can configure it using the Hopsworks UI. The cluster can be in any state prior to the configuration.</p> <p>Runtime limitation</p> <p>Currently Runtime 12.2 LTS is suggested to be able to use the full suite of Hopsworks Feature Store capabilities.</p>"},{"location":"user_guides/integrations/databricks/configuration/#configure-a-cluster","title":"Configure a cluster","text":"<p>Clusters are configured for a project user, which, in Hopsworks terms, means a user operating within the scope of a project. To configure a cluster, click on the <code>Configure</code> button. By default the cluster will be configured for the user making the request. If the user doesn't have <code>Can Manage</code> privilege on the cluster, they can ask a project <code>Data Owner</code> to configure it for them. Hopsworks <code>Data Owners</code> are allowed to configure clusters for other project users, as long as they have the required Databricks privileges.</p> <p> Configure a Databricks Cluster from Hopsworks </p> <p>During the cluster configuration the following steps will be taken:</p> <ul> <li>Upload an archive to DBFS containing the necessary Jars for HSFS and HopsFS to be able to read and write from the Hopsworks Feature Store</li> <li>Add an initScript to configure the Jars when the cluster is started</li> <li>Install <code>hsfs</code> python library</li> <li>Configure the necessary Spark properties to authenticate and communicate with the Feature Store</li> </ul> <p>HopsFS configuration</p> <p>It is not necessary to configure HopsFS if data is stored outside the Hopsworks file system. To do this define Storage Connectors and link them to Feature Groups and Training Datasets.</p> <p>When a cluster is configured for a specific project user, all the operations with the Hopsworks Feature Store will be executed as that project user. If another user needs to re-use the same cluster, the cluster can be reconfigured by following the same steps above.</p>"},{"location":"user_guides/integrations/databricks/configuration/#connecting-to-the-feature-store","title":"Connecting to the Feature Store","text":"<p>At the end of the configuration, Hopsworks will start the cluster. Once the cluster is running users can establish a connection to the Hopsworks Feature Store from Databricks:</p> <pre><code>import hopsworks \nproject = hopsworks.login(\n    host='my_instance',                 # DNS of your Hopsworks instance\n    port=443,                           # Port to reach your Hopsworks instance, defaults to 443\n    project='my_project',               # Name of your Hopsworks project\n    api_key_value='apikey',             # The API key to authenticate with Hopsworks\n)\nfs = project.get_feature_store()           # Get the project's default feature store\n</code></pre>"},{"location":"user_guides/integrations/databricks/configuration/#next-steps","title":"Next Steps","text":"<p>For more information about how to connect, see the Login API API reference. Or continue with the Data Source guide to import your own data to the Feature Store.</p>"},{"location":"user_guides/integrations/databricks/networking/","title":"Networking","text":"<p>In order for Spark to communicate with the Feature Store from Databricks, networking needs to be set up correctly. This includes deploying the Hopsworks Instance to either the same VPC or enable VPC/VNet peering between the VPC/VNet of the Databricks Cluster and the Hopsworks Cluster.</p>"},{"location":"user_guides/integrations/databricks/networking/#aws","title":"AWS","text":""},{"location":"user_guides/integrations/databricks/networking/#step-1-ensure-network-connectivity","title":"Step 1: Ensure network connectivity","text":"<p>The DataFrame API needs to be able to connect directly to the IP on which the Feature Store is listening. This means that if you deploy the Feature Store on AWS you will either need to deploy the Feature Store in the same VPC as your Databricks cluster or to set up VPC Peering between your Databricks VPC and the Feature Store VPC.</p> <p>Option 1: Deploy the Feature Store in the Databricks VPC</p> <p>When you deploy the Feature Store Hopsworks instance, select the Databricks VPC and Availability Zone as the VPC and Availability Zone of your Feature Store cluster. Identify your Databricks VPC by searching for VPCs containing Databricks in their name in your Databricks AWS region in the AWS Management Console:</p> <p> Identify the Databricks VPC </p> <p>Option 2: Set up VPC peering</p> <p>Follow the guide VPC Peering to set up VPC peering between the Feature Store cluster and Databricks. Get your Feature Store VPC ID and CIDR by searching for the Feature Store VPC in the AWS Management Console:</p> <p>managed.hopsworks.ai</p> <p>On managed.hopsworks.ai, the VPC is shown in the cluster details.</p> <p> Identify the Feature Store VPC </p>"},{"location":"user_guides/integrations/databricks/networking/#step-2-configure-the-security-group","title":"Step 2: Configure the Security Group","text":"<p>The Feature Store Security Group needs to be configured to allow traffic from your Databricks clusters to be able to connect to the Feature Store.</p> <p>managed.hopsworks.ai</p> <p>If you deployed your Hopsworks Feature Store with managed.hopsworks.ai, you only need to enable outside access of the Feature Store, Online Feature Store, and Kafka services.</p> <p>Open your feature store instance under EC2 in the AWS Management Console and ensure that ports 443, 3306, 9083, 9085, 8020, 50010, and 9092 are reachable from the Databricks Security Group:</p> <p> Hopsworks Feature Store Security Group </p> <p>Connectivity from the Databricks Security Group can be allowed by opening the Security Group, adding a port to the Inbound rules and searching for dbe-worker in the source field. Selecting any of the dbe-worker Security Groups will be sufficient:</p> <p> Hopsworks Feature Store Security Group details </p>"},{"location":"user_guides/integrations/databricks/networking/#azure","title":"Azure","text":""},{"location":"user_guides/integrations/databricks/networking/#step-1-set-up-vnet-peering-between-hopsworks-and-databricks","title":"Step 1: Set up VNet peering between Hopsworks and Databricks","text":"<p>VNet peering between the Hopsworks and the Databricks virtual network is required to be able to connect to the Feature Store from Databricks.</p> <p>In the Azure portal, go to Azure Databricks and go to Virtual Network Peering:</p> <p> Azure Databricks </p> <p>Select Add Peering:</p> <p> Add peering </p> <p>Name the peering and select the virtual network used by your Hopsworks cluster. The virtual network is shown in the cluster details on managed.hopsworks.ai (see the next picture). Ensure to press the copy button on the bottom of the page and save the value somewhere. Press Add and create the peering:</p> <p> Configure peering </p> <p>The virtual network used by your cluster is shown under Details:</p> <p> Check the Hopsworks virtual network </p> <p>The peering connection should now be listed as initiated:</p> <p> Peering connection initiated </p> <p>On the Azure portal, go to Virtual networks and search for the virtual network used by your Hopsworks cluster:</p> <p> Virtual networks </p> <p>Open the network and select Peerings:</p> <p> Select peerings </p> <p>Choose to add a peering connection:</p> <p> Add a peering connection </p> <p>Name the peering connection and select I know my resource ID. Paste the string copied when creating the peering from Databricks Azure. If you haven't copied that string, then manually select the virtual network used by Databricks and press OK to create the peering:</p> <p> Configure peering </p> <p>The peering should now be Updating:</p> <p> Cloud account settings </p> <p>Wait for the peering to show up as Connected. There should now be bi-directional network connectivity between the Feature Store and Databricks:</p> <p> Cloud account settings </p>"},{"location":"user_guides/integrations/databricks/networking/#step-2-configure-the-network-security-group","title":"Step 2: Configure the Network Security Group","text":"<p>The virtual network peering will allow full access between the Hopsworks virtual network and the Databricks virtual network by default. However, if you have a different setup, ensure that the Network Security Group of the Feature Store is configured to allow traffic from your Databricks clusters.</p> <p>Ensure that ports 443, 9083, 9085, 8020, 50010, and 9092 are reachable from the Databricks cluster Network Security Group.</p> <p>managed.hopsworks.ai</p> <p>If you deployed your Hopsworks Feature Store instance with managed.hopsworks.ai, it suffices to enable outside access of the Feature Store, Online Feature Store, and Kafka services.</p>"},{"location":"user_guides/integrations/databricks/networking/#next-steps","title":"Next Steps","text":"<p>Continue with the Hopsworks API key guide to setup access to a Hopsworks API key from the Databricks Cluster, in order to be able to use the Hopsworks Feature Store.</p>"},{"location":"user_guides/integrations/emr/emr_configuration/","title":"Configure EMR for the Hopsworks Feature Store","text":"<p>To enable EMR to access the Hopsworks Feature Store, you need to set up a Hopsworks API key, add a bootstrap action and configurations to your EMR cluster.</p> <p>Info</p> <p>Ensure Networking is set up correctly before proceeding with this guide.</p>"},{"location":"user_guides/integrations/emr/emr_configuration/#step-1-set-up-a-hopsworks-api-key","title":"Step 1: Set up a Hopsworks API key","text":"<p>For instructions on how to generate an API key follow this user guide. For the EMR integration to work correctly make sure you add the following scopes to your API key:</p> <ol> <li>featurestore</li> <li>project</li> <li>job</li> <li>kafka</li> </ol>"},{"location":"user_guides/integrations/emr/emr_configuration/#store-the-api-key-in-the-aws-secrets-manager","title":"Store the API key in the AWS Secrets Manager","text":"<p>In the AWS management console ensure that your active region is the region you use for EMR. Go to the AWS Secrets Manager and select Store new secret. Select Other type of secrets and add api-key as the key and paste the API key created in the previous step as the value. Click next.</p> <p> Store a Hopsworks API key in the Secrets Manager </p> <p>As a secret name, enter hopsworks/featurestore. Select next twice and finally store the secret. Then click on the secret in the secrets list and take note of the Secret ARN.</p> <p> Name the secret </p>"},{"location":"user_guides/integrations/emr/emr_configuration/#grant-access-to-the-secret-to-the-emr-ec2-instance-profile","title":"Grant access to the secret to the EMR EC2 instance profile","text":"<p>Identify your EMR EC2 instance profile in the EMR cluster summary:</p> <p> Identify your EMR EC2 instance profile </p> <p>In the AWS Management Console, go to IAM, select Roles and then the EC2 instance profile used by your EMR cluster. Select Add inline policy. Choose Secrets Manager as a service, expand the Read access level and check GetSecretValue. Expand Resources and select Add ARN. Paste the ARN of the secret created in the previous step. Click on Review, give the policy a name and click on Create policy.</p> <p> Configure the access policy for the Secrets Manager </p>"},{"location":"user_guides/integrations/emr/emr_configuration/#step-2-configure-your-emr-cluster","title":"Step 2: Configure your EMR cluster","text":""},{"location":"user_guides/integrations/emr/emr_configuration/#add-the-hopsworks-feature-store-configuration-to-your-emr-cluster","title":"Add the Hopsworks Feature Store configuration to your EMR cluster","text":"<p>In order for EMR to be able to talk to the Feature Store, you need to update the Hadoop and Spark configurations. Copy the configuration below and replace ip-XXX-XX-XX-XXX.XX-XXXX-X.compute.internal with the private DNS name of your Hopsworks master node.</p> <pre><code>[\n  {\n    \"Classification\": \"hadoop-env\",\n    \"Properties\": {\n\n    },\n    \"Configurations\": [\n      {\n        \"Classification\": \"export\",\n        \"Properties\": {\n          \"HADOOP_CLASSPATH\": \"$HADOOP_CLASSPATH:/usr/lib/hopsworks/client/*\"\n        },\n        \"Configurations\": [\n\n        ]\n      }\n    ]\n  },\n  {\n    \"Classification\": \"spark-defaults\",\n    \"Properties\": {\n      \"spark.hadoop.hops.ipc.server.ssl.enabled\": true,\n      \"spark.hadoop.fs.hopsfs.impl\": \"io.hops.hopsfs.client.HopsFileSystem\",\n      \"spark.hadoop.client.rpc.ssl.enabled.protocol\": \"TLSv1.2\",\n      \"spark.hadoop.hops.ssl.hostname.verifier\": \"ALLOW_ALL\",\n      \"spark.hadoop.hops.rpc.socket.factory.class.default\": \"io.hops.hadoop.shaded.org.apache.hadoop.net.HopsSSLSocketFactory\",\n      \"spark.hadoop.hops.ssl.keystores.passwd.name\": \"/usr/lib/hopsworks/material_passwd\",\n      \"spark.hadoop.hops.ssl.keystore.name\": \"/usr/lib/hopsworks/keyStore.jks\",\n      \"spark.hadoop.hops.ssl.trustore.name\": \"/usr/lib/hopsworks/trustStore.jks\",\n      \"spark.serializer\": \"org.apache.spark.serializer.KryoSerializer\",\n      \"spark.executor.extraClassPath\": \"/usr/lib/hopsworks/client/*\",\n      \"spark.driver.extraClassPath\": \"/usr/lib/hopsworks/client/*\",\n      \"spark.sql.hive.metastore.jars\": \"path\",\n      \"spark.sql.hive.metastore.jars.path\": \"/usr/lib/hopsworks/apache-hive-bin/lib/*\",\n      \"spark.hadoop.hive.metastore.uris\": \"thrift://ip-XXX-XX-XX-XXX.XX-XXXX-X.compute.internal:9083\"\n    }\n  },\n]\n</code></pre> <p>When you create your EMR cluster, add the configuration:</p> <p>Note</p> <p>Don't forget to replace ip-XXX-XX-XX-XXX.XX-XXXX-X.compute.internal with the private DNS name of your Hopsworks master node.</p> <p> Configure EMR to access the Feature Store </p>"},{"location":"user_guides/integrations/emr/emr_configuration/#add-the-bootstrap-action-to-your-emr-cluster","title":"Add the Bootstrap Action to your EMR cluster","text":"<p>EMR requires Hopsworks connectors to be able to communicate with the Hopsworks Feature Store. These connectors can be installed with the bootstrap action shown below. Copy the content into a file and name the file <code>hopsworks.sh</code>. Copy that file into any S3 bucket that is readable by your EMR clusters and take note of the S3 URI of that file e.g., <code>s3://my-emr-init/hopsworks.sh</code>.</p> <pre><code>#!/bin/bash\nset -e\n\nif [ \"$#\" -ne 3 ]; then\n    echo \"Usage hopsworks.sh HOPSWORKS_API_KEY_SECRET, HOPSWORKS_HOST, PROJECT_NAME\"\n    exit 1\nfi\n\nSECRET_NAME=$1\nHOST=$2\nPROJECT=$3\n\nAPI_KEY=$(aws secretsmanager get-secret-value --secret-id $SECRET_NAME | jq -r .SecretString | jq -r '.[\"api-key\"]')\n\nPROJECT_ID=$(curl -H \"Authorization: ApiKey ${API_KEY}\" https://$HOST/hopsworks-api/api/project/getProjectInfo/$PROJECT | jq -r .projectId)\n\nsudo yum -y install python3-devel.x86_64 || true\n\nsudo mkdir /usr/lib/hopsworks\nsudo chown hadoop:hadoop /usr/lib/hopsworks\ncd /usr/lib/hopsworks\n\ncurl -o client.tar.gz -H \"Authorization: ApiKey ${API_KEY}\" https://$HOST/hopsworks-api/api/project/$PROJECT_ID/client\n\ntar -xvf client.tar.gz\ntar -xzf client/apache-hive-*-bin.tar.gz || true\nmv apache-hive-*-bin apache-hive-bin\nrm client.tar.gz\nrm client/apache-hive-*-bin.tar.gz\n\ncurl -H \"Authorization: ApiKey ${API_KEY}\" https://$HOST/hopsworks-api/api/project/$PROJECT_ID/credentials | jq -r .kStore | base64 -d &gt; keyStore.jks\n\ncurl -H \"Authorization: ApiKey ${API_KEY}\" https://$HOST/hopsworks-api/api/project/$PROJECT_ID/credentials | jq -r .tStore | base64 -d &gt; trustStore.jks\n\necho -n $(curl -H \"Authorization: ApiKey ${API_KEY}\" https://$HOST/hopsworks-api/api/project/$PROJECT_ID/credentials | jq -r .password) &gt; material_passwd\n\nchmod -R o-rwx /usr/lib/hopsworks\n\nsudo pip3 install --upgrade hopsworks~=X.X.0\n</code></pre> <p>Matching Hopsworks version</p> <p>We recommend that the major and minor version of the Python library match the major and minor version of the Hopsworks deployment.</p> <p><p> You find the Hopsworks version inside any of your Project's settings tab on Hopsworks </p></p> <p>Add the bootstrap actions when configuring your EMR cluster. Provide 3 arguments to the bootstrap action: The name of the API key secret e.g., <code>hopsworks/featurestore</code>, the public DNS name of your Hopsworks cluster, such as <code>ad005770-33b5-11eb-b5a7-bfabd757769f.cloud.hopsworks.ai</code>, and the name of your Hopsworks project, e.g. <code>demo_fs_meb10179</code>.</p> <p> Set the bootstrap action for EMR </p> <p>Your EMR cluster will now be able to access your Hopsworks Feature Store.</p>"},{"location":"user_guides/integrations/emr/emr_configuration/#next-steps","title":"Next Steps","text":"<p>Use the Login API to connect to the Hopsworks Feature Store. For more information about how to use the Feature Store, see the Quickstart Guide.</p>"},{"location":"user_guides/integrations/emr/networking/","title":"Networking","text":"<p>In order for Spark to communicate with the Hopsworks Feature Store from EMR, networking needs to be set up correctly. This includes deploying the Hopsworks Feature Store to either the same VPC or enable VPC peering between the VPC of the EMR cluster and the Hopsworks Feature Store.</p>"},{"location":"user_guides/integrations/emr/networking/#step-1-ensure-network-connectivity","title":"Step 1: Ensure network connectivity","text":"<p>The DataFrame API needs to be able to connect directly to the IP on which the Feature Store is listening. This means that if you deploy the Feature Store on AWS you will either need to deploy the Feature Store in the same VPC as your EMR cluster or to set up VPC Peering between your EMR VPC and the Feature Store VPC.</p> <p>Option 1: Deploy the Feature Store in the EMR VPC</p> <p>When deploying the Hopsworks Feature Store, select the EMR VPC and Availability Zone as the VPC and Availability Zone of your Feature Store. Identify your EMR VPC in the Summary of your EMR cluster:</p> <p> Identify the EMR VPC </p> <p> Identify the EMR VPC </p> <p>Option 2: Set up VPC peering</p> <p>Follow the guide VPC Peering to set up VPC peering between the Feature Store and EMR. Get your Feature Store VPC ID and CIDR by searching for the Feature Store VPC in the AWS Management Console:</p> <p> Identify the Feature Store VPC </p>"},{"location":"user_guides/integrations/emr/networking/#step-2-configure-the-security-group","title":"Step 2: Configure the Security Group","text":"<p>The Feature Store Security Group needs to be configured to allow traffic from your EMR clusters to be able to connect to the Feature Store.</p> <p>Open your feature store instance under EC2 in the AWS Management Console and ensure that ports 443, 3306, 9083, 9085, 8020 and 30010 (443,3306,8020,30010,9083,9085) are reachable from the EMR Security Group:</p> <p> Hopsworks Feature Store Security Group </p> <p>Connectivity from the EMR Security Group can be allowed by opening the Security Group, adding a port to the Inbound rules and setting the EMR master and core security group as source:</p> <p> Hopsworks Feature Store Security Group details </p> <p>You can find your EMR security groups in the EMR cluster summary:</p> <p> EMR Security Groups </p>"},{"location":"user_guides/integrations/emr/networking/#next-steps","title":"Next Steps","text":"<p>Continue with the Configure EMR for the Hopsworks Feature Store, in order to be able to use the Hopsworks Feature Store.</p>"},{"location":"user_guides/migration/30_migration/","title":"3.0 Migration Guide","text":""},{"location":"user_guides/migration/30_migration/#breaking-changes","title":"Breaking Changes","text":""},{"location":"user_guides/migration/30_migration/#feature-view","title":"Feature View","text":"<p>Feature View is a new core abstraction introduced in version 3.0. The Feature View extends and replaces the old Training Dataset. Feature views are now the gateway for users to access feature data from the feature store.</p> Pre-3.03.0 <pre><code>td = fs.create_training_dataset()\n</code></pre> <p>First create the feature view using a Query object: <pre><code>fv = fs.create_feature_view()\n</code></pre></p> <p>You can then create training data from the feature view by writing it as new files with <code>.create_</code> methods: <pre><code>td_df = fv.create_training_data()\nfv.create_train_test_split()\nfv.create_train_validation_test_split()\n</code></pre></p> <p>Or you can directly create the training data in memory by below methods: <pre><code>fv.training_data()\nfv.train_test_split()\nfv.train_validation_test_split()\n</code></pre></p> <p>This means that the process for generating training data changes slightly and training data is grouped by feature views. This has the following advantages:</p> <ol> <li>It will be easier to create new versions of training data for re-training of models in the future.</li> <li>You do not have to keep references to training data in your model serving code, instead you use the Feature View object.</li> <li>The feature view offers the same interface for retrieving batch data for batch scoring, so you don\u2019t have to execute SQL against the feature store explicitly anymore.</li> </ol> <p>A feature view uses the Query abstraction to define the schema of the view, and therefore, the Query is a mandatory argument, this effectively removes the possibility to create a training dataset directly from a Spark Dataframe.</p>"},{"location":"user_guides/migration/30_migration/#required-changes","title":"Required changes","text":"<p>After the upgrade all existing training datasets will be located under a Feature View with the same name. The versions of the training datasets stay untouched. You can use <code>feature_view.get_training_data</code> to get back existing training datasets. Except that training datasets created directly from Spark Dataframe are not migrated to feature view. In this case, you can use the old APIs for retrieving these training datasets, so that your existing training pipelines are still functional. </p> <p>However, if you have any pipelines, that automatically create training datasets, you will have to adjust these to the above workflow. <code>fs.create_training_dataset()</code> has been removed.</p> <p>To learn more about the new Feature View, have a look at the dedicated concept and guide section in the documentation.</p>"},{"location":"user_guides/migration/30_migration/#deequ-based-data-validation-in-favour-of-great-expectations","title":"Deequ-based Data Validation in favour of Great Expectations","text":"<p>Unfortunately, the Deequ data validation library is no longer actively maintained which makes it impossible for us to maintain the functionality within Hopsworks. Therefore, we are dropping the entire support for Deequ as validation engine in Hopsworks 3.0 in favour of Great Expectations (GE).</p> <p>This has the following advantages:</p> <ol> <li>Great Expectations has become the defacto standard for data validation within the community.</li> <li>Hopsworks is fully compatible with GE native objects, that means you can bring your existing expectation suites without the need for rewriting them.</li> <li>GE is both available for Spark and for Pandas Dataframes, whereas Deequ was only supporting Spark.</li> </ol>"},{"location":"user_guides/migration/30_migration/#required-changes_1","title":"Required changes","text":"<p>All APIs regarding data validation have been redesigned to accommodate the functionality of GE. This means that you will have to redesign your previous expectations in the form of GE expectation suites that you can attach to Feature Groups. Please refer to the data validation guide for a full specification of the functionality.</p>"},{"location":"user_guides/migration/30_migration/#limitations","title":"Limitations","text":"<p>GE is a Python library and therefore we can support synchronous data validation only in Python and PySpark kernels and not on Java/Scala Spark kernels. However, you have the possibility to launch a job asynchronously after writing with Java/Scala in order to perform data validation.</p>"},{"location":"user_guides/migration/30_migration/#deprecated-features","title":"Deprecated Features","text":"<p>These changes or new features introduce changes in APIs which might break your pipelines in the future. We try to keep old APIs around until the next major release in order to give you some time to adapt your pipelines, however, this is not always possible, and these methods might be removed in any upcoming release, so we recommend addressing these changes as soon as possible. For this reason, we list some of the changes as breaking change, even though they are still backwards compatible.</p>"},{"location":"user_guides/migration/30_migration/#on-demand-feature-groups-are-now-called-external-feature-groups","title":"On-Demand Feature Groups are now called External Feature Groups","text":"<p>Most data engineers but also many data scientists have a background where they at least partially where exposed to database terminology. Therefore, we decided to rename On-Demand Feature Groups to simply External Feature Groups. We think this makes the abstraction clearer, as practitioners are usually familiar with the concept of External Tables in a database.</p> <p>This lead to a change in HSFS APIs:</p> Pre-3.03.0 <pre><code>fs.create_on_demand_feature_group()\nfs.get_on_demand_feature_group()\nfs.get_on_demand_feature_groups()\n</code></pre> <pre><code>fs.create_external_feature_group()\nfs.get_external_feature_group()\nfs.get_external_feature_groups()\n</code></pre> <p>Note, pre-3.0 methods are marked as deprecated and still available in the library for backwards compatibility.</p>"},{"location":"user_guides/migration/30_migration/#streaming-api-for-writing-becomes-the-python-default","title":"Streaming API for writing becomes the Python Default","text":"<p>Hopsworks provides three write APIs to the Feature Store to accommodate the different use cases:</p> <ol> <li>Batch Write: This was the default mode prior to version 3.0. It involves writing a DataFrame in batch either to the offline feature store, or the online one, or both. This mode is still the default when you are writing Spark DataFrames on Hopsworks.</li> <li>External connectors: This mode allows users to mount external tables existing in DataWarehouses like Snowflake, Redshift and BigQuery as feature groups in Hopsworks. In this case the data is not moved and remains on the external data storage.</li> <li>Stream Write: This mode was introduced in version 2.0 and expanded in version 3.0. This mode has a \"Kappa-style\" architecture, where the DataFrame gets streamed into a Kafka topic and, as explained later in the post, the data is picked up from Kafka by Hopsworks and written into the desired stores. In Hopsworks 3.0 this is the default mode for Python clients.</li> </ol> <p>With 3.0 the stream API becomes the default for Feature Groups created from pure Python environments with Pandas Dataframes.</p> <p>This has the following advantages:</p> <ol> <li> <p>Reduced write amplification: Instead of uploading data to Hopsworks and subsequently starting a Spark job to upsert the data on offline storage and writing it to Kafka for the online storage upsert, the data is directly written to Kafka and from there it\u2019s being upserted directly to offline and/or online.</p> </li> <li> <p>Fresher features: Since new data gets written to Kafka directly without prior upload, the data ends up in the online feature store with subsecond latency, which is a massive improvement given it is written from Python without any Streaming framework.</p> </li> <li> <p>Batching of offline upserts: You can control now yourself how often the Spark application that performs the upsert on the offline feature store is running. Either you run it synchronously with every new Dataframe ingestion, or you batch multiple Dataframes by launching the job less regularly.</p> </li> </ol>"},{"location":"user_guides/migration/30_migration/#required-changes_2","title":"Required changes","text":"<p>Your existing feature groups will not be affected by this change, that means all existing feature groups will continue to use the old upload path for ingestion. However, we strongly recommend creating new versions of your existing feature groups that use ingest to using Python, in order to leverage the above advantages.</p>"},{"location":"user_guides/migration/30_migration/#built-in-transformation-functions-dont-have-to-be-registered-explicitly-for-every-project","title":"Built-in transformation functions don\u2019t have to be registered explicitly for every project","text":"<p>In Hopsworks 2.5 users had to register the built-in transformation functions (min-max scaler, standard scaler, label encoder and robust scaler) explicitly for every project by calling <code>fs.register_builtin_transformation_functions()</code>. This is no longer necessary, as all new projects will have the functions registered by default.</p>"},{"location":"user_guides/migration/30_migration/#hive-installation-extra-deprecated-in-favour-of-python-extra","title":"Hive installation extra deprecated in favour of Python extra","text":"<p>In the past when using HSFS in pure Python environments without PySpark, users had to install the hive extra when installing the PyPi package. This extra got deprecated and users now have to install an extra called python to reflect the environment:</p> Pre-3.03.0 <pre><code>pip install hsfs[hive]\n</code></pre> <pre><code>pip install hsfs[python]\n</code></pre>"},{"location":"user_guides/migration/30_migration/#more-restrictive-feature-types","title":"More restrictive feature types","text":"<p>With Hopsworks 3.0 we made feature types more strict and therefore made ingestion pipelines more robust. Both Spark and Pandas are quite forgiving when it comes to types, which often led to schema incompatibilities when ingesting to a feature group.</p> <p>In this release we narrowed down the allowed Python types, and defined a clear mapping to Spark and Online Feature Store types. Please refer to the feature type guide in the documentation for the exact mapping.</p>"},{"location":"user_guides/migration/30_migration/#required-changes_3","title":"Required Changes","text":"<p>The most common Python/Pandas types are still supported, we recommend you double check your feature groups with the type mapping above.</p>"},{"location":"user_guides/migration/30_migration/#deprecation-of-save-methods-in-favour-of-insert-together-with-get_or_create_","title":"Deprecation of .save methods in favour of .insert together with .get_or_create_","text":"<p>The <code>.save()</code> methods to create feature store entities has been deprecated in favour of <code>.insert()</code>. That means if there is no metadata for an entity in the feature store, a call to <code>.insert()</code> will create it.</p> <p>Together with the new <code>.get_or_create_ APIs</code> this will avoid that users have to change their code between creating entities and deploying the same code into production.</p> Pre-3.03.0 <pre><code>try:\n    fg = fs.get_feature_group(...)\n    fg.insert(df)\nexcept RESTError as e:\n    fg = fs.create_feature_group(...)\n    fg.save(df)\n</code></pre> <pre><code>fg = fs.get_or_create_feature_group(...)\nfg.insert(df)\n</code></pre>"},{"location":"user_guides/migration/30_migration/#hops-python-library-superseded-by-hopsworks-library","title":"hops python library superseded by Hopsworks library","text":"<p>The hops python library is now deprecated and is superseded by the hopsworks python library. <code>hopsworks</code> is essentially a reimplementation of <code>hops</code>, but with an object-oriented API, similar in style with hsfs. For guides on how to use the API follow the Projects guides.</p> <p>Furthermore, the functionality provided by the <code>model</code> and <code>serving</code> module in <code>hops</code>, is now ported to the hsml python library. To create models and serving follow the MLOps guides.</p>"},{"location":"user_guides/migration/30_migration/#new-feature-highlights","title":"New Feature Highlights","text":"<p>This list is meant to serve as a starting point to explore the new features of the Hopsworks 3.0 release, which can significantly improve your workflows.</p>"},{"location":"user_guides/migration/30_migration/#added-new-storage-connectors-gcs-bigquery-and-kafka","title":"Added new Storage Connectors: GCS, BigQuery and Kafka","text":"<p>With the added support for Google Cloud, we added also two new storage connectors: Google Cloud Storage and Google BigQuery. Users can use these connectors to create external feature groups or write out training data.</p> <p>Additionally, to make it easier for users to get started with Spark Streaming applications, we added a Kafka connector, which let\u2019s you easily read a Kafka topic into a Spark Streaming Dataframe.</p>"},{"location":"user_guides/migration/30_migration/#optimized-default-hudi-options","title":"Optimized Default Hudi Options","text":"<p>By default, Hudi tends to over-partition input, and therefore the layout of Feature Groups. The default parallelism is 200, to ensure each Spark partition stays within the 2GB limit for inputs up to 500GB. The new default is the following for all insert/upsert operations:</p> <pre><code>extra_write_options = {\n  'hoodie.bulkinsert.shuffle.parallelism': '5',\n  'hoodie.insert.shuffle.parallelism': '5',\n  'hoodie.upsert.shuffle.parallelism': '5'\n}\n</code></pre> <p>In most of the cases, you will not have to change this default. For very large inputs you should bump this up accordingly, by passing it to the <code>write_options</code> argument of the Feature Group <code>.insert()</code> method:</p> <pre><code>fg.insert(df, write_options=extra_write_options)\n</code></pre> <p>We recommend having shuffle parallelism <code>hoodie.[insert|upsert|bulkinsert].shuffle.parallelism</code> such that its at least input_data_size/500MB.</p>"},{"location":"user_guides/migration/30_migration/#feature-view-passed-features","title":"Feature View passed features","text":"<p>With the introduction of the Feature View abstraction, we added APIs to allow users to overwrite features with so-called passed features when calling <code>fv.get_feature_vector()</code>:</p> <pre><code># get a single vector\nfeature_view.get_feature_vector(\n    entry = {\"pk1\": 1, \"pk2\": 2},\n    passed_features = {\"feature_a\": \"value_a\"}\n)\n# get multiple vectors\nfeature_view.get_feature_vectors(\n    entry = [\n        {\"pk1\": 1, \"pk2\": 2},\n        {\"pk1\": 3, \"pk2\": 4},\n        {\"pk1\": 5, \"pk2\": 6}\n    ],\n    passed_features = [\n        {\"feature_a\": \"value_a\"},\n        {\"feature_a\": \"value_b\"},\n        {\"feature_a\": \"value_c\"},\n    ]\n)\n</code></pre> <p>This is useful, if some of the features values are only known at prediction time and cannot be computed and cached in the online feature store, you can provide those values as <code>passed_features</code> option. The <code>get_feature_vector</code> method is going to use the passed values to construct the final feature vector to submit to the model, it will also apply any transformations to the passed features attached to the respective features.</p>"},{"location":"user_guides/migration/30_migration/#reading-training-data-directly-from-hopsfs-in-python-environments","title":"Reading Training Data directly from HopsFS in Python environments","text":"<p>In Hopsworks 3.0 we added support for reading training data from Hopsworks directly into external Python environments. Previously, users had to write the training data to external storage like S3 in order to access it from external environment.</p>"},{"location":"user_guides/migration/40_migration/","title":"4.0 Migration Guide","text":""},{"location":"user_guides/migration/40_migration/#breaking-changes","title":"Breaking Changes","text":"<p>With the release of Hopsworks 4.0, a number of necessary breaking  changes have been put in place to improve the overall experience of  using the Hopsworks platform. These breaking changes can be categorized  in the following areas:</p> <ul> <li> <p>Python API</p> </li> <li> <p>Multi-Environment Docker Images</p> </li> <li> <p>On-Demand Transformation Functions</p> </li> </ul>"},{"location":"user_guides/migration/40_migration/#python-api","title":"Python API","text":"<p>A number of significant changes have been made in the Python API  Hopsworks 4.0. Previously, in Hopsworks 3.X, there were 3 python  libraries used (\u201chopsworks\u201d, \u201chsfs\u201d &amp; \u201chsml\u201d) to develop feature,  training &amp; inference pipelines, with the 4.0 release there is now  one single \u201chopsworks\u201d python library that can should be used. For  backwards compatibility, it will still be possible to import both  the \u201chsfs\u201d &amp; \u201chsml\u201d libraries but these are now effectively aliases  to the \u201chopsworks\u201d python library and their use going forward should  be considered as deprecated.</p> <p>Another significant change in the Hopsworks Python API is the use of  optional extras to allow a developer to easily import exactly what is  needed as part of their work. The main ones are great-expectations and  polars. It is arguable whether this is a breaking change but it is  important to note depending on how a particular pipeline has been  written which may encounter a problem when executing using Hopsworks  4.0.</p> <p>Finally, there are a number of relatively small breaking changes and  deprecated methods to improve the developer experience, these include:</p> <ul> <li> <p>connection.init() is now considered deprecated</p> </li> <li> <p>When loading arrow_flight_client, an OptionalDependencyNotFoundError can be now thrown providing more detailed information on the error than the previous ModuleNotFoundError in 3.X.</p> </li> <li> <p>DatasetApi's zip and unzip will now return False when a timeout is exceeded instead of previously throwing an Exception</p> </li> </ul>"},{"location":"user_guides/migration/40_migration/#multi-environment-docker-images","title":"Multi-Environment Docker Images","text":"<p>As part of the Hopsworks 4.0 release, an engineering team using  Hopsworks can now customize the docker images that they use for their  feature, training and inference pipelines. By adding this flexibility,  a set of breaking changes are necessary. Instead of having one common  docker image for fti pipelines, with the release of 4.0 a number of  specific docker images are provided to allow an engineering team using  Hopsworks to install exactly what they need to get their feature,  training and inference pipelines up and running. This breaking change  will require existing customers running Hopsworks 3.X to test their  existing pipelines using Hopsworks 4.0 before upgrading their  production environments.</p>"},{"location":"user_guides/migration/40_migration/#on-demand-transformation-functions","title":"On-Demand Transformation Functions","text":"<p>A number of changes have been made to transformation functions in the  last releases of Hopsworks. With 4.0, On-Demand Transformation Functions  are now better supported which has resulted in some breaking changes.  The following is how transformation functions were used in previous versions of Hopsworks and the how transformation functions are used in the 4.0 release.</p> Pre-4.04.0 <pre><code>#################################################\n# Creating transformation funciton Hopsworks 3.8#\n#################################################\n\n# Define custom transformation function\ndef add_one(feature):\n    return feature + 1\n\n# Create transformation function\nadd_one = fs.create_transformation_function(add_one, \n    output_type=int,\n    version=1,\n)\n\n# Save transformation function\nadd_one.save()\n\n# Retrieve transformation function\nscaler = fs.get_transformation_function(\n    name=\"add_one\",\n    version=1,\n)\n\n# Create feature view\nfeature_view = fs.get_or_create_feature_view(\n    name='serving_fv',\n    version=1,\n    query=selected_features,\n    # Apply your custom transformation functions to the feature `feature_1`\n    transformation_functions={\n        \"feature_1\": add_one,\n    },\n    labels=['target'],\n)\n</code></pre> <pre><code>#################################################\n# Creating transformation funciton Hopsworks 4.0#\n#################################################\n\n# Define custom transformation function\n@hopsworks.udf(int)\ndef add_one(feature):\n    return feature + 1\n\n# Create feature view\nfeature_view = fs.get_or_create_feature_view(\n    name='serving_fv',\n    version=1,\n    query=selected_features,\n    # Apply the custom transformation functions defined to the feature `feature_1` \n    transformation_functions=[\n        add_one(\"feature_1\"),\n    ],\n    labels=['target'],\n)\n</code></pre> <p>Note that the number of lines of code required has been significantly  reduced using the \u201c@hopsworks.udf\u201d python decorator.</p>"},{"location":"user_guides/mlops/","title":"Model Registry &amp; Serving Guides","text":"<p>This section serves to provide guides and examples for the common usage of abstractions and functionality of Models and Deployments through the Hopsworks UI and APIs.</p> <ul> <li>Model Registry</li> <li>Model Serving</li> <li>Vector Database</li> </ul>"},{"location":"user_guides/mlops/provenance/provenance/","title":"Provenance","text":""},{"location":"user_guides/mlops/provenance/provenance/#introduction","title":"Introduction","text":"<p>Hopsworks allows users to track provenance (lineage) between:</p> <ul> <li>storage connectors</li> <li>feature groups</li> <li>feature views</li> <li>training datasets</li> <li>models</li> </ul> <p>In the provenance pages we will call a provenance artifact or shortly artifact, any of the five entities above.</p> <p>With the following provenance graph:</p> <pre><code>storage connector -&gt; feature group -&gt; feature group -&gt; feature view -&gt; training dataset -&gt; model\n</code></pre> <p>we will call the parent, the artifact to the left, and the child, the artifact to the right. So a feature view has a number of feature groups as parents and can have a number of training datasets as children.</p> <p>Tracking provenance allows users to determine where and if an artifact is being used. You can track, for example, if feature groups are being used to create additional (derived) feature groups or feature views, or if their data is eventually used to train models.</p> <p>You can interact with the provenance graph using the UI or the APIs.</p>"},{"location":"user_guides/mlops/provenance/provenance/#model-provenance","title":"Model provenance","text":"<p>The relationship between feature views and models is captured in the model constructor. If you do not provide at least the feature view object to the constructor, the provenance will not capture this relation and you will not be able to navigate from model to the feature view it used or from the feature view to this model.</p> <p>You can provide the feature view object and have the training dataset version be inferred.</p> Python <pre><code># this fv object will be provided to the model constructor\nfv = hsfs.get_feature_view(...)\n\n# when calling trainig data related methods on the feature view, the training dataset version is cached in the feature view and is implicitly provided to the model constructor\nX_train, X_test, y_train, y_test = feature_view.train_test_split(...)\n\n# provide the feature_view object in the model constructor\nhsml.model_registry.ModelRegistry.python.create_model(\n    ...\n    feature_view = fv\n    ...)\n</code></pre> <p>You can of course explicitly provide the training dataset version.</p> Python <pre><code># this object will be provided to the model constructor\nfv = hsfs.get_feature_view(...)\n\n# this training dataset version will be provided to the model constructor\nX_train, X_test, y_train, y_test = feature_view.get_train_test_split(training_dataset_version=1)\n\n# provide the feature_view object in the model constructor\nhsml.model_registry.ModelRegistry.python.create_model(\n    ...\n    feature_view = fv,\n    training_dataset_version = 1,\n    ...)\n</code></pre> <p>Once the relation is stored in the provenance graph, you can navigate the graph from model to feature view or training dataset and the other way around.</p> <p>Users can call the [get_feature_view_provenance(https://docs.hopsworks.ai/hopsworks-api/4.3/generated/model_registry/model_api/#get_feature_view_provenance) method or the [get_training_dataset_provenance(https://docs.hopsworks.ai/hopsworks-api/4.3/generated/model_registry/model_api/#get_training_dataset_provenance) method which will each return a Link object.</p> <p>You can also retrieve directly the parent feature view object, without the need to extract them from the provenance links object, using the [get_feature_view(https://docs.hopsworks.ai/hopsworks-api/4.3/generated/model_registry/model_api/#get_feature_view ) method</p> Python <pre><code>feature_view = model.get_feature_view()\n</code></pre> <p>This utility method also has the options to initialize the required components for batch or online retrieval of feature vectors.</p> Python <pre><code>model.get_feature_view(init: bool = True, online: Optional[bool]: None)\n</code></pre> <p>By default, the base init for feature vector retrieval is enabled. In case you have a workflow that requires more particular options, you can disable this base init by setting the <code>init</code> to <code>false</code>. The method detects if it is running within a deployment and will initialize the feature vector retrieval for the serving. If the <code>online</code> argument is provided and <code>true</code> it will initialize for online feature vector retrieval. If the <code>online</code> argument is provided and <code>false</code> it will initialize the feature vector retrieval for batch scoring.</p>"},{"location":"user_guides/mlops/provenance/provenance/#using-the-ui","title":"Using the UI","text":"<p>In the model overview UI you can explore the provenance graph of the model:</p> <p> Provenance graph of derived feature groups </p>"},{"location":"user_guides/mlops/provenance/provenance/#provenance-links","title":"Provenance Links","text":"<p>All the <code>_provenance</code> methods return a <code>Link</code> dictionary object that contains <code>accessible</code>, <code>inaccesible</code>, <code>deleted</code> lists.</p> <ul> <li><code>accessible</code> - contains any artifact from the result, that the user has access to.</li> <li><code>inaccessible</code> - contains any artifacts that might have been shared at some point in the past, but where this sharing was retracted. Since the relation between artifacts is still maintained in the provenance, the user will only have access to limited metadata and the artifacts will be included in this <code>inaccessible</code> list.</li> <li><code>deleted</code> - contains artifacts that are deleted with children stil present in the system. There is minimum amount of metadata for the deleted allowing for some limited human readable identification.</li> </ul>"},{"location":"user_guides/mlops/registry/","title":"Model Registry Guides","text":"<p>Hopsworks Model Registry is a centralized repository, within an organization, to manage machine learning models. A model is the product of training a machine learning algorithm with training data.</p> <p>This section provides guides for creating models and publish them to the Model Registry to make them available for download for batch predictions, or deployed to serve realtime applications.</p>"},{"location":"user_guides/mlops/registry/#exporting-a-model","title":"Exporting a model","text":"<p>Follow these framework-specific guides to export a Model to the Model Registry.</p> <ul> <li> <p>TensorFlow</p> </li> <li> <p>Torch</p> </li> <li> <p>Scikit-learn</p> </li> <li> <p>LLM</p> </li> <li> <p>Other Python frameworks</p> </li> </ul>"},{"location":"user_guides/mlops/registry/#model-schema","title":"Model Schema","text":"<p>A Model schema describes the input and outputs for a model. It provides a functional description of the model which makes it simpler to get started working with it. For example if the model inputs a tensor, the model schema can define the shape and data type of the tensor.</p>"},{"location":"user_guides/mlops/registry/#input-example","title":"Input Example","text":"<p>An Input example provides an instance of a valid model input. Input examples are stored with the model as separate artifacts.</p>"},{"location":"user_guides/mlops/registry/input_example/","title":"How To Attach An Input Example","text":""},{"location":"user_guides/mlops/registry/input_example/#introduction","title":"Introduction","text":"<p>In this guide you will learn how to attach an input example to a model. An input example is simply an instance of a valid model input. Attaching an input example to your model will give other users a better understanding of what data it expects.</p>"},{"location":"user_guides/mlops/registry/input_example/#code","title":"Code","text":""},{"location":"user_guides/mlops/registry/input_example/#step-1-connect-to-hopsworks","title":"Step 1: Connect to Hopsworks","text":"Python <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n</code></pre>"},{"location":"user_guides/mlops/registry/input_example/#step-2-generate-an-input-example","title":"Step 2: Generate an input example","text":"<p>Generate an input example which corresponds to a valid input to your model. Currently we support <code>pandas.DataFrame, pandas.Series, numpy.ndarray, list</code> to be passed as input example.</p> Python <pre><code>import numpy as np\n\ninput_example = np.random.randint(0, high=256, size=784, dtype=np.uint8)\n</code></pre>"},{"location":"user_guides/mlops/registry/input_example/#step-3-set-input_example-parameter","title":"Step 3: Set input_example parameter","text":"<p>Set the <code>input_example</code> parameter in the <code>create_model</code> function and call <code>save()</code> to attaching it to the model and register it in the registry.</p> Python <pre><code>model = mr.tensorflow.create_model(name=\"mnist\",\n                                input_example=input_example)\nmodel.save(\"./model\")\n</code></pre>"},{"location":"user_guides/mlops/registry/model_evaluation_images/","title":"How To Save Model Evaluation Images","text":""},{"location":"user_guides/mlops/registry/model_evaluation_images/#introduction","title":"Introduction","text":"<p>In this guide, you will learn how to attach model evaluation images to a model. Model evaluation images are images that visually describe model performance metrics. For example, confusion matrices, ROC curves, model bias tests, and training loss curves are examples of common model evaluation images. By attaching model evaluation images to your versioned model, other users can better understand the model performance and evaluation metrics.</p>"},{"location":"user_guides/mlops/registry/model_evaluation_images/#code","title":"Code","text":""},{"location":"user_guides/mlops/registry/model_evaluation_images/#step-1-connect-to-hopsworks","title":"Step 1: Connect to Hopsworks","text":"Python <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n</code></pre>"},{"location":"user_guides/mlops/registry/model_evaluation_images/#step-2-generate-model-evaluation-images","title":"Step 2: Generate model evaluation images","text":"<p>Generate an image that visualizes model performance and evaluation metrics</p> Python <pre><code>import seaborn\nfrom sklearn.metrics import confusion_matrix\n\n# Predict the training data using the trained model\ny_pred_train = model.predict(X_train)\n\n# Predict the test data using the trained model\ny_pred_test = model.predict(X_test)\n\n# Calculate and print the confusion matrix for the test predictions\nresults = confusion_matrix(y_test, y_pred_test)\n\n# Create a DataFrame for the confusion matrix results\ndf_confusion_matrix = pd.DataFrame(\n    results, \n    ['True Normal', 'True Fraud'],\n    ['Pred Normal', 'Pred Fraud'],\n)\n\n# Create a heatmap using seaborn with annotations\nheatmap = seaborn.heatmap(df_confusion_matrix, annot=True)\n\n# Get the figure and display it\nfig = heatmap.get_figure()\nfig.show()\n</code></pre>"},{"location":"user_guides/mlops/registry/model_evaluation_images/#step-3-save-the-figure-to-a-file-inside-the-model-directory","title":"Step 3: Save the figure to a file inside the model directory","text":"<p>Save the figure to a file with a common filename extension (for example, .png or .jpeg), and place it in a directory called <code>images</code> - a subdirectory of the model directory that is registered to Hopsworks.</p> Python <pre><code># Specify the directory name for saving the model and related artifacts\nmodel_dir = \"./model\"\n\n# Create a subdirectory of model_dir called 'images' for saving the model evaluation images\nmodel_images_dir = model_dir + \"/images\"\nif not os.path.exists(model_images_dir):\n    os.mkdir(model_images_dir)\n\n# Save the figure to an image file in the images directory\nfig.savefig(model_images_dir + \"/confusion_matrix.png\")\n\n# Register the model\npy_model = mr.python.create_model(name=\"py_model\")\npy_model.save(\"./model\")\n</code></pre>"},{"location":"user_guides/mlops/registry/model_schema/","title":"How To Attach A Model Schema","text":""},{"location":"user_guides/mlops/registry/model_schema/#introduction","title":"Introduction","text":"<p>In this guide you will learn how to attach a model schema to your model. A model schema, describes the type and shape of inputs and outputs (predictions) for your model. Attaching a model schema to your model will give other users a better understanding of what data it expects.</p>"},{"location":"user_guides/mlops/registry/model_schema/#code","title":"Code","text":""},{"location":"user_guides/mlops/registry/model_schema/#step-1-connect-to-hopsworks","title":"Step 1: Connect to Hopsworks","text":"Python <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n</code></pre>"},{"location":"user_guides/mlops/registry/model_schema/#step-2-create-modelschema","title":"Step 2: Create ModelSchema","text":"<p>Create a ModelSchema for your inputs and outputs by passing in an example that your model is trained on and a valid prediction. Currently, we support <code>pandas.DataFrame, pandas.Series, numpy.ndarray, list</code>.</p> Python <pre><code># Import a Schema and ModelSchema definition\nfrom hsml.utils.model_schema import ModelSchema\nfrom hsml.utils.schema import Schema\n\n# Model inputs for MNIST dataset\ninputs = [{'type': 'uint8', 'shape': [28, 28, 1], 'description': 'grayscale representation of 28x28 MNIST images'}]\n\n# Build the input schema\ninput_schema = Schema(inputs)\n\n# Model outputs\noutputs = [{'type': 'float32', 'shape': [10]}]\n\n# Build the output schema\noutput_schema = Schema(outputs)\n\n# Create ModelSchema object\nmodel_schema = ModelSchema(input_schema=input_schema, output_schema=output_schema)\n</code></pre>"},{"location":"user_guides/mlops/registry/model_schema/#step-3-set-model_schema-parameter","title":"Step 3: Set model_schema parameter","text":"<p>Set the <code>model_schema</code> parameter in the <code>create_model</code> function and call <code>save()</code> to attaching it to the model and register it in the registry.</p> Python <pre><code>model = mr.tensorflow.create_model(name=\"mnist\",\n                                model_schema=model_schema)\nmodel.save(\"./model\")\n</code></pre>"},{"location":"user_guides/mlops/registry/frameworks/llm/","title":"How To Export a Large Language Model (LLM)","text":""},{"location":"user_guides/mlops/registry/frameworks/llm/#introduction","title":"Introduction","text":"<p>In this guide you will learn how to export a Large Language Model (LLM) and register it in the Model Registry.</p>"},{"location":"user_guides/mlops/registry/frameworks/llm/#code","title":"Code","text":""},{"location":"user_guides/mlops/registry/frameworks/llm/#step-1-connect-to-hopsworks","title":"Step 1: Connect to Hopsworks","text":"Python <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n</code></pre>"},{"location":"user_guides/mlops/registry/frameworks/llm/#step-2-download-the-llm","title":"Step 2: Download the LLM","text":"<p>Download your base or fine-tuned LLM. LLMs can typically be downloaded using the official frameworks provided by their creators (e.g., HuggingFace, Ollama, ...)</p> Python <pre><code># Download LLM (e.g., using huggingface to download Llama-3.1-8B base model)\nfrom huggingface_hub import snapshot_download\n\nmodel_dir = snapshot_download(\n                \"meta-llama/Llama-3.1-8B\",\n                ignore_patterns=\"original/*\"\n            )\n</code></pre>"},{"location":"user_guides/mlops/registry/frameworks/llm/#step-3-optional-fine-tune-llm","title":"Step 3: (Optional) Fine-tune LLM","text":"<p>If necessary, fine-tune your LLM with an instruction set. A LLM can be fine-tuned fully or using Parameter Efficient Fine Tuning (PEFT) methods such as LoRA or QLoRA.</p> Python <pre><code># Fine-tune LLM using PEFT (LoRA, QLoRA) or other methods\nmodel_dir = ...\n</code></pre>"},{"location":"user_guides/mlops/registry/frameworks/llm/#step-4-register-model-in-registry","title":"Step 4: Register model in registry","text":"<p>Use the <code>ModelRegistry.llm.create_model(..)</code> function to register a model as LLM. Define a name, and attach optional metrics for your model, then invoke the <code>save()</code> function with the parameter being the path to the local directory where the model was exported to.</p> Python <pre><code># Model evaluation metrics\nmetrics = {'f1-score': 0.8, 'perplexity': 31.62, 'bleu-score': 0.73}\n\nllm_model = mr.llm.create_model(\"llm_model\", metrics=metrics)\n\nllm_model.save(model_dir)\n</code></pre>"},{"location":"user_guides/mlops/registry/frameworks/llm/#going-further","title":"Going Further","text":"<p>You can attach an Input Example and a Model Schema to your model to document the shape and type of the data the model was trained on.</p>"},{"location":"user_guides/mlops/registry/frameworks/python/","title":"How To Export a Python Model","text":""},{"location":"user_guides/mlops/registry/frameworks/python/#introduction","title":"Introduction","text":"<p>In this guide you will learn how to export a generic Python model and register it in the Model Registry.</p>"},{"location":"user_guides/mlops/registry/frameworks/python/#code","title":"Code","text":""},{"location":"user_guides/mlops/registry/frameworks/python/#step-1-connect-to-hopsworks","title":"Step 1: Connect to Hopsworks","text":"Python <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n</code></pre>"},{"location":"user_guides/mlops/registry/frameworks/python/#step-2-train","title":"Step 2: Train","text":"<p>Define your XGBoost model and run the training loop.</p> Python <pre><code># Define a model\nmodel = XGBClassifier()\n\n# Train model\nmodel.fit(X_train, y_train)\n</code></pre>"},{"location":"user_guides/mlops/registry/frameworks/python/#step-3-export-to-local-path","title":"Step 3: Export to local path","text":"<p>Export the XGBoost model to a directory on the local filesystem.</p> Python <pre><code>model_file = \"model.json\"\n\nmodel.save_model(model_file)\n</code></pre>"},{"location":"user_guides/mlops/registry/frameworks/python/#step-4-register-model-in-registry","title":"Step 4: Register model in registry","text":"<p>Use the <code>ModelRegistry.python.create_model(..)</code> function to register a model as a Python model. Define a name, and attach optional metrics for your model, then invoke the <code>save()</code> function with the parameter being the path to the local directory where the model was exported to.</p> Python <pre><code># Model evaluation metrics\nmetrics = {'accuracy': 0.92}\n\npy_model = mr.python.create_model(\"py_model\", metrics=metrics)\n\npy_model.save(model_dir)\n</code></pre>"},{"location":"user_guides/mlops/registry/frameworks/python/#going-further","title":"Going Further","text":"<p>You can attach an Input Example and a Model Schema to your model to document the shape and type of the data the model was trained on.</p>"},{"location":"user_guides/mlops/registry/frameworks/skl/","title":"How To Export a Scikit-learn Model","text":""},{"location":"user_guides/mlops/registry/frameworks/skl/#introduction","title":"Introduction","text":"<p>In this guide you will learn how to export a Scikit-learn model and register it in the Model Registry.</p>"},{"location":"user_guides/mlops/registry/frameworks/skl/#code","title":"Code","text":""},{"location":"user_guides/mlops/registry/frameworks/skl/#step-1-connect-to-hopsworks","title":"Step 1: Connect to Hopsworks","text":"Python <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n</code></pre>"},{"location":"user_guides/mlops/registry/frameworks/skl/#step-2-train","title":"Step 2: Train","text":"<p>Define your Scikit-learn model and run the training loop.</p> Python <pre><code># Define a model\niris_knn = KNeighborsClassifier(..)\n\niris_knn.fit(..)\n</code></pre>"},{"location":"user_guides/mlops/registry/frameworks/skl/#step-3-export-to-local-path","title":"Step 3: Export to local path","text":"<p>Export the Scikit-learn model to a directory on the local filesystem.</p> Python <pre><code>model_file = \"skl_knn.pkl\"\n\njoblib.dump(iris_knn, model_file)\n</code></pre>"},{"location":"user_guides/mlops/registry/frameworks/skl/#step-4-register-model-in-registry","title":"Step 4: Register model in registry","text":"<p>Use the <code>ModelRegistry.sklearn.create_model(..)</code> function to register a model as a Scikit-learn model. Define a name, and attach optional metrics for your model, then invoke the <code>save()</code> function with the parameter being the path to the local directory where the model was exported to.</p> Python <pre><code># Model evaluation metrics\nmetrics = {'accuracy': 0.92}\n\nskl_model = mr.sklearn.create_model(\"skl_model\", metrics=metrics)\n\nskl_model.save(model_file)\n</code></pre>"},{"location":"user_guides/mlops/registry/frameworks/skl/#going-further","title":"Going Further","text":"<p>You can attach an Input Example and a Model Schema to your model to document the shape and type of the data the model was trained on.</p>"},{"location":"user_guides/mlops/registry/frameworks/tch/","title":"How To Export a Torch Model","text":""},{"location":"user_guides/mlops/registry/frameworks/tch/#introduction","title":"Introduction","text":"<p>In this guide you will learn how to export a Torch model and register it in the Model Registry.</p>"},{"location":"user_guides/mlops/registry/frameworks/tch/#code","title":"Code","text":""},{"location":"user_guides/mlops/registry/frameworks/tch/#step-1-connect-to-hopsworks","title":"Step 1: Connect to Hopsworks","text":"Python <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n</code></pre>"},{"location":"user_guides/mlops/registry/frameworks/tch/#step-2-train","title":"Step 2: Train","text":"<p>Define your Torch model and run the training loop.</p> Python <pre><code># Define the model architecture\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        ...\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        ...\n        return x\n\n# Instantiate the model\nnet = Net()\n\n# Run the training loop\nfor epoch in range(n):\n    ...\n</code></pre>"},{"location":"user_guides/mlops/registry/frameworks/tch/#step-3-export-to-local-path","title":"Step 3: Export to local path","text":"<p>Export the Torch model to a directory on the local filesystem.</p> Python <pre><code>model_dir = \"./model\"\n\ntorch.save(net.state_dict(), model_dir)\n</code></pre>"},{"location":"user_guides/mlops/registry/frameworks/tch/#step-4-register-model-in-registry","title":"Step 4: Register model in registry","text":"<p>Use the <code>ModelRegistry.torch.create_model(..)</code> function to register a model as a Torch model. Define a name, and attach optional metrics for your model, then invoke the <code>save()</code> function with the parameter being the path to the local directory where the model was exported to.  </p> Python <pre><code># Model evaluation metrics\nmetrics = {'accuracy': 0.92}\n\ntch_model = mr.torch.create_model(\"tch_model\", metrics=metrics)\n\ntch_model.save(model_dir)\n</code></pre>"},{"location":"user_guides/mlops/registry/frameworks/tch/#going-further","title":"Going Further","text":"<p>You can attach an Input Example and a Model Schema to your model to document the shape and type of the data the model was trained on.</p>"},{"location":"user_guides/mlops/registry/frameworks/tf/","title":"How To Export a TensorFlow Model","text":""},{"location":"user_guides/mlops/registry/frameworks/tf/#introduction","title":"Introduction","text":"<p>In this guide you will learn how to export a TensorFlow model and register it in the Model Registry.</p> <p>Save in SavedModel format</p> <p>Make sure the model is saved in the SavedModel format to be able to deploy it on TensorFlow Serving.</p>"},{"location":"user_guides/mlops/registry/frameworks/tf/#code","title":"Code","text":""},{"location":"user_guides/mlops/registry/frameworks/tf/#step-1-connect-to-hopsworks","title":"Step 1: Connect to Hopsworks","text":"Python <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n</code></pre>"},{"location":"user_guides/mlops/registry/frameworks/tf/#step-2-train","title":"Step 2: Train","text":"<p>Define your TensorFlow model and run the training loop.</p> Python <pre><code># Define a model\nmodel = tf.keras.Sequential()\n\n# Add layers\nmodel.add(..)\n\n# Compile the model.\nmodel.compile(..)\n\n# Train the model\nmodel.fit(..)\n</code></pre>"},{"location":"user_guides/mlops/registry/frameworks/tf/#step-3-export-to-local-path","title":"Step 3: Export to local path","text":"<p>Export the TensorFlow model to a directory on the local filesystem.</p> Python <pre><code>model_dir = \"./model\"\n\ntf.saved_model.save(model, model_dir)\n</code></pre>"},{"location":"user_guides/mlops/registry/frameworks/tf/#step-4-register-model-in-registry","title":"Step 4: Register model in registry","text":"<p>Use the <code>ModelRegistry.tensorflow.create_model(..)</code> function to register a model as a TensorFlow model. Define a name, and attach optional metrics for your model, then invoke the <code>save()</code> function with the parameter being the path to the local directory where the model was exported to.  </p> Python <pre><code># Model evaluation metrics\nmetrics = {'accuracy': 0.92}\n\ntf_model = mr.tensorflow.create_model(\"tf_model\", metrics=metrics)\n\ntf_model.save(model_dir)\n</code></pre>"},{"location":"user_guides/mlops/registry/frameworks/tf/#going-further","title":"Going Further","text":"<p>You can attach an Input Example and a Model Schema to your model to document the shape and type of the data the model was trained on.</p>"},{"location":"user_guides/mlops/serving/","title":"Model Serving Guide","text":""},{"location":"user_guides/mlops/serving/#deployment","title":"Deployment","text":"<p>Assuming you have already created a model in the Model Registry, a deployment can now be created to prepare a model artifact for this model and make it accessible for running predictions behind a REST or gRPC endpoint. Follow the Deployment Creation Guide to create a Deployment for your model.</p>"},{"location":"user_guides/mlops/serving/#predictor","title":"Predictor","text":"<p>Predictors are responsible for running a model server that loads a trained model, handles inference requests and returns predictions, see the Predictor Guide.</p>"},{"location":"user_guides/mlops/serving/#transformer","title":"Transformer","text":"<p>Transformers are used to apply transformations on the model inputs before sending them to the predictor for making predictions using the model, see the Transformer Guide.</p>"},{"location":"user_guides/mlops/serving/#resource-allocation","title":"Resource Allocation","text":"<p>Configure the resources to be allocated for predictor and transformer in a model deployment, see the Resource Allocation Guide.</p>"},{"location":"user_guides/mlops/serving/#inference-batcher","title":"Inference Batcher","text":"<p>Configure the predictor to batch inference requests, see the Inference Batcher Guide.</p>"},{"location":"user_guides/mlops/serving/#inference-logger","title":"Inference Logger","text":"<p>Configure the predictor to log inference requests and predictions, see the Inference Logger Guide.</p>"},{"location":"user_guides/mlops/serving/#troubleshooting","title":"Troubleshooting","text":"<p>Inspect the model server logs to troubleshoot your model deployments, see the Troubleshooting Guide.</p>"},{"location":"user_guides/mlops/serving/#external-access","title":"External access","text":"<p>Grant users authenticated by an external Identity Provider access to model deployments, see the External Access Guide.</p>"},{"location":"user_guides/mlops/serving/api-protocol/","title":"How to Select the API protocol for a Deployment","text":""},{"location":"user_guides/mlops/serving/api-protocol/#introduction","title":"Introduction","text":"<p>Hopsworks supports both REST and gRPC as API protocols for sending inference requests to model deployments. While REST API protocol is supported in all types of model deployments, support for gRPC is only available for models served with KServe.</p> <p>Warning</p> <p>At the moment, the gRPC API protocol is only supported for Python model deployments (e.g., scikit-learn, xgboost). Support for Tensorflow model deployments is coming soon.</p>"},{"location":"user_guides/mlops/serving/api-protocol/#gui","title":"GUI","text":""},{"location":"user_guides/mlops/serving/api-protocol/#step-1-create-a-new-deployment","title":"Step 1: Create a new deployment","text":"<p>If you have at least one model already trained and saved in the Model Registry, navigate to the deployments page by clicking on the <code>Deployments</code> tab on the navigation menu on the left.</p> <p> Deployments navigation tab </p> <p>Once in the deployments page, you can create a new deployment by either clicking on <code>New deployment</code> (if there are no existing deployments) or on <code>Create new deployment</code> it the top-right corner. Both options will open the deployment creation form.</p>"},{"location":"user_guides/mlops/serving/api-protocol/#step-2-go-to-advanced-options","title":"Step 2: Go to advanced options","text":"<p>A simplified creation form will appear including the most common deployment fields from all available configurations. Resource allocation is part of the advanced options of a deployment. To navigate to the advanced creation form, click on <code>Advanced options</code>.</p> <p> Advanced options. Go to advanced deployment creation form </p>"},{"location":"user_guides/mlops/serving/api-protocol/#step-3-select-the-api-protocol","title":"Step 3: Select the API protocol","text":"<p>Enabling gRPC as the API protocol for a model deployment requires KServe as the serving platform for the deployment. Make sure that KServe is enabled by activating the corresponding checkbox.</p> <p> Enable KServe in the advanced deployment form </p> <p>Then, you can select the API protocol to be enabled in your model deployment.</p> <p> Select gRPC API protocol </p> <p>Only one API protocol can be enabled in a model deployment (they cannot support both gRPC and REST)</p> <p>Currently, KServe model deployments are limited to one API protocol at a time. Therefore, only one of REST or gRPC API protocols can be enabled at the same time on the same model deployment. You cannot change the API protocol of existing deployments.</p> <p>Once you are done with the changes, click on <code>Create new deployment</code> at the bottom of the page to create the deployment for your model.</p>"},{"location":"user_guides/mlops/serving/api-protocol/#code","title":"Code","text":""},{"location":"user_guides/mlops/serving/api-protocol/#step-1-connect-to-hopsworks","title":"Step 1: Connect to Hopsworks","text":"Python <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n</code></pre>"},{"location":"user_guides/mlops/serving/api-protocol/#step-2-create-a-deployment-with-a-specific-api-protocol","title":"Step 2: Create a deployment with a specific API protocol","text":"Python <pre><code>my_model = mr.get_model(\"my_model\", version=1)\n\nmy_predictor = ms.create_predictor(my_model,\n                                  api_protocol=\"GRPC\"  # defaults to \"REST\"\n                                  )\nmy_predictor.deploy()\n\n# or\n\nmy_deployment = ms.create_deployment(my_predictor)\nmy_deployment.save()\n</code></pre>"},{"location":"user_guides/mlops/serving/api-protocol/#api-reference","title":"API Reference","text":"<p>API Protocol</p>"},{"location":"user_guides/mlops/serving/deployment-state/","title":"How To Inspect A Deployment State","text":""},{"location":"user_guides/mlops/serving/deployment-state/#introduction","title":"Introduction","text":"<p>In this guide, you will learn how to inspect the state of a deployment.</p> <p>A state can be seen as a snapshot of the current inner workings of a deployment. The following is the state transition diagram for deployments.</p> <p> State transitions of deployments </p> <p>States are composed of a status and a condition. While a status represents a high-level view of the state, conditions contain more detailed information closely related to infrastructure terms.</p>"},{"location":"user_guides/mlops/serving/deployment-state/#gui","title":"GUI","text":""},{"location":"user_guides/mlops/serving/deployment-state/#step-1-inspect-deployment-status","title":"Step 1: Inspect deployment status","text":"<p>If you have at least one deployment already created, navigate to the deployments page by clicking on the <code>Deployments</code> tab on the navigation menu on the left.</p> <p> Deployments navigation tab </p> <p>Once in the deployments page, find the deployment you want to inspect. Next to the actions buttons, you can find an indicator showing the current status of the deployment. This indicator changes its color based on the status.</p> <p>To inspect the condition of the deployment, click on the name of the deployment to open the deployment overview page.</p>"},{"location":"user_guides/mlops/serving/deployment-state/#step-2-inspect-condition","title":"Step 2: Inspect condition","text":"<p>Once in the deployment overview page, you can find the aforementioned status indicator at the top of page. Below it, a one-line message is shown with a more detailed description of the deployment status. This message is built using the current condition of the deployment.</p> <p> Deployments status condition </p>"},{"location":"user_guides/mlops/serving/deployment-state/#step-3-check-no-of-running-instances","title":"Step 3: Check n\u00ba of running instances","text":"<p>Additionally, you can find the n\u00ba of instances currently running by scrolling down to the <code>resource allocation</code> section.</p> <p> Resource allocation for a deployment </p> <p>Scale-to-zero capabilities</p> <p>If scale-to-zero capabilities are enabled, you can see how the n\u00ba of instances of a running deployment goes to zero and the status changes to <code>idle</code>. To enable scale-to-zero in a deployment, see Resource Allocation Guide</p>"},{"location":"user_guides/mlops/serving/deployment-state/#code","title":"Code","text":""},{"location":"user_guides/mlops/serving/deployment-state/#step-1-connect-to-hopsworks","title":"Step 1: Connect to Hopsworks","text":"Python <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n</code></pre>"},{"location":"user_guides/mlops/serving/deployment-state/#step-2-retrieve-an-existing-deployment","title":"Step 2: Retrieve an existing deployment","text":"Python <pre><code>deployment = ms.get_deployment(\"mydeployment\")\n</code></pre>"},{"location":"user_guides/mlops/serving/deployment-state/#step-3-inspect-deployment-state","title":"Step 3: Inspect deployment state","text":"Python <pre><code>state = deployment.get_state()\n\nstate.describe()\n</code></pre>"},{"location":"user_guides/mlops/serving/deployment-state/#step-4-check-no-of-running-instances","title":"Step 4: Check n\u00ba of running instances","text":"Python <pre><code># n\u00ba of predictor instances\ndeployment.resources.describe()\n\n# n\u00ba of transformer instances\ndeployment.transformer.resources.describe()\n</code></pre>"},{"location":"user_guides/mlops/serving/deployment-state/#api-reference","title":"API Reference","text":"<p>Deployment</p> <p>PredictorState</p>"},{"location":"user_guides/mlops/serving/deployment-state/#deployment-status","title":"Deployment status","text":"<p>The status of a deployment is a high-level description of its current state.</p> Show deployment status Status Description CREATED Deployment has never been started STARTING Deployment is starting RUNNING Deployment is ready and running. Predictions are served without additional latencies. IDLE Deployment is ready, but idle. Higher latencies (i.e., cold-start) are expected in the first incoming inference requests FAILED Deployment is in a failed state, which can be due to multiple reasons. More details can be found in the condition UPDATING Deployment is applying updates to the running instances STOPPING Deployment is stopping STOPPED Deployment has been stopped"},{"location":"user_guides/mlops/serving/deployment-state/#deployment-conditions","title":"Deployment conditions","text":"<p>A condition contains more specific information about the status of the deployment. They are mainly useful to track the progress of starting or stopping deployments.</p> <p>Status conditions contain three pieces of information: type, status and reason. While the type describes the purpose of the condition, the status represents its progress. Additionally, a reason field is provided with a more descriptive message of the status.</p> Show deployment conditions Type Status Description STOPPED <code>Unknown</code> Deployment is stopping. <code>True</code> Deployment is stopped. Therefore, no instances are running and no resources are allocated. SCHEDULED <code>Unknown</code> Deployment is being scheduled <code>False</code> Deployment failed to be scheduled. This is commonly due to insufficient resources to satisfy the deployment requirements <code>True</code> Deployment has been scheduled successfully. At this point, resources have been already allocated for the deployment. INITIALIZED <code>Unknown</code> Deployment is initializing. This step involves initialization tasks such as pulling docker images or mounting data volumes <code>False</code> Deployment failed to initialized <code>True</code> Deployment has been initialized successfully. At this point, the docker images have been pulled and data volumes mounted STARTED <code>Unknown</code> Deployment is starting. In this step, the model server is started and predictor / transformer scripts (if provided) are executed <code>False</code> Deployment failed to start. This can be due to errors in the predictor / transformer script, missing dependencies or model server incompatibilities. <code>True</code> Deployment has been started successfully. At this point, the model server has been started and the predictor / transformer scripts (if provided) executed. READY <code>Unknown</code> Connectivity is being set up. <code>False</code> Connectivity failed to be set up, mainly due to networking issues. <code>True</code> Connectivity has been set up and the deployment is ready <p>The following are two diagrams with the state transitions of conditions in starting and stopping deployments, respectively.</p> <p> Condition transitions in starting deployments </p> <p> Condition transitions in stopping deployments </p>"},{"location":"user_guides/mlops/serving/deployment/","title":"How To Create A Model Deployment","text":""},{"location":"user_guides/mlops/serving/deployment/#introduction","title":"Introduction","text":"<p>In this guide, you will learn how to create a new deployment for a trained model.</p> <p>Warning</p> <p>This guide assumes that a model has already been trained and saved into the Model Registry. To learn how to create a model in the Model Registry, see Model Registry Guide</p> <p>Deployments are used to unify the different components involved in making one or more trained models online and accessible to compute predictions on demand. For each deployment, there are four concepts to consider:</p> <ol> <li>Model files</li> <li>Artifact files</li> <li>Predictor</li> <li>Transformer</li> </ol>"},{"location":"user_guides/mlops/serving/deployment/#gui","title":"GUI","text":""},{"location":"user_guides/mlops/serving/deployment/#step-1-create-a-deployment","title":"Step 1: Create a deployment","text":"<p>If you have at least one model already trained and saved in the Model Registry, navigate to the deployments page by clicking on the <code>Deployments</code> tab on the navigation menu on the left.</p> <p> Deployments navigation tab </p> <p>Once in the deployments page, you can create a new deployment by either clicking on <code>New deployment</code> (if there are no existing deployments) or on <code>Create new deployment</code> it the top-right corner. Both options will open the deployment creation form.</p>"},{"location":"user_guides/mlops/serving/deployment/#step-2-basic-deployment-configuration","title":"Step 2: Basic deployment configuration","text":"<p>A simplified creation form will appear including the most common deployment fields from all available configurations. We provide default values for the rest of the fields, adjusted to the type of deployment you want to create.</p> <p>In the simplified form, select the model framework used to train your model. Then, select the model you want to deploy from the list of available models under <code>pick a model</code>.</p> <p>After selecting the model, the rest of fields are filled automatically. We pick the last model version and model artifact version available in the Model Registry. Moreover, we infer the deployment name from the model name.</p> <p>Deployment name validation rules</p> <p>A valid deployment name can only contain characters a-z, A-Z and 0-9.</p> <p>Predictor script for Python models</p> <p>For Python models, you must select a custom predictor script that loads and runs the trained model by clicking on <code>From project</code> or <code>Upload new file</code>, to choose an existing script in the project file system or upload a new script, respectively.</p> <p>If you prefer, change the name of the deployment, model version or artifact version. Then, click on <code>Create new deployment</code> to create the deployment for your model.</p> <p> Select the model framework </p> <p> Select the model </p>"},{"location":"user_guides/mlops/serving/deployment/#step-3-optional-advanced-configuration","title":"Step 3 (Optional): Advanced configuration","text":"<p>Optionally, you can access and adjust other parameters of the deployment configuration by clicking on <code>Advanced options</code>.</p> <p> Advanced options. Go to advanced deployment creation form </p> <p>You will be redirected to a full-page deployment creation form where you can see all the default configuration values we selected for your deployment and adjust them according to your use case. Apart from the aforementioned simplified configuration, in this form you can setup the following components:</p> <p>Deployment advanced options</p> <ol> <li>Predictor</li> <li>Transformer</li> <li>Inference logger</li> <li>Inference batcher</li> <li>Resources</li> <li>API protocol</li> </ol> <p>Once you are done with the changes, click on <code>Create new deployment</code> at the bottom of the page to create the deployment for your model.</p>"},{"location":"user_guides/mlops/serving/deployment/#step-4-deployment-creation","title":"Step 4: Deployment creation","text":"<p>Wait for the deployment creation process to finish.</p> <p> Deployment creation in progress </p>"},{"location":"user_guides/mlops/serving/deployment/#step-5-deployment-overview","title":"Step 5: Deployment overview","text":"<p>Once the deployment is created, you will be redirected to the list of all your existing deployments in the project. You can use the filters on the top of the page to easily locate your new deployment.</p> <p> List of deployments </p> <p>After that, click on the new deployment to access the overview page.</p> <p> Deployment overview </p>"},{"location":"user_guides/mlops/serving/deployment/#code","title":"Code","text":""},{"location":"user_guides/mlops/serving/deployment/#step-1-connect-to-hopsworks","title":"Step 1: Connect to Hopsworks","text":"Python <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n</code></pre>"},{"location":"user_guides/mlops/serving/deployment/#step-2-create-deployment","title":"Step 2: Create deployment","text":"<p>Retrieve the trained model you want to deploy.</p> Python <pre><code>my_model = mr.get_model(\"my_model\", version=1)\n</code></pre>"},{"location":"user_guides/mlops/serving/deployment/#option-a-using-the-model-object","title":"Option A: Using the model object","text":"Python <pre><code>my_deployment = my_model.deploy()\n</code></pre>"},{"location":"user_guides/mlops/serving/deployment/#option-b-using-the-model-serving-handle","title":"Option B: Using the Model Serving handle","text":"Python <pre><code># get Hopsworks Model Serving handle\nms = project.get_model_serving()\n\nmy_predictor = ms.create_predictor(my_model)\nmy_deployment = my_predictor.deploy()\n\n# or\nmy_deployment = ms.create_deployment(my_predictor)\nmy_deployment.save()\n</code></pre>"},{"location":"user_guides/mlops/serving/deployment/#api-reference","title":"API Reference","text":"<p>Model Serving</p>"},{"location":"user_guides/mlops/serving/deployment/#model-files","title":"Model Files","text":"<p>Model files are the files exported when a specific version of a model is saved to the model registry (see Model Registry). These files are unique for each model version, but shared across model deployments created for the same version of the model.</p> <p>Inside a model deployment, the local path to the model files is stored in the <code>MODEL_FILES_PATH</code> environment variable (see environment variables). Moreover, you can explore the model files under the <code>/Models/&lt;model-name&gt;/&lt;model-version&gt;/Files</code> directory using the File Browser.</p> <p>Warning</p> <p>All files under <code>/Models</code> are managed by Hopsworks. Changes to model files cannot be reverted and can have an impact on existing model deployments.</p>"},{"location":"user_guides/mlops/serving/deployment/#artifact-files","title":"Artifact Files","text":"<p>Artifact files are files involved in the correct startup and running of the model deployment. The most important files are the predictor and transformer scripts. The former is used to load and run the model for making predictions. The latter is typically used to apply transformations on the model inputs at inference time before making predictions. Predictor and transformer scripts run on separate components and, therefore, scale independently of each other.</p> <p>Tip</p> <p>Whenever you provide a predictor script, you can include the transformations of model inputs in the same script as far as they don't need to be scaled independently from the model inference process.</p> <p>Additionally, artifact files can also contain a server configuration file that helps detach configuration used within the model deployment from the model server or the implementation of the predictor and transformer scripts. Inside a model deployment, the local path to the configuration file is stored in the <code>CONFIG_FILE_PATH</code> environment variable (see environment variables).</p> <p>Every model deployment runs a specific version of the artifact files, commonly referred to as artifact version. One or more model deployments can use the same artifact version (i.e., same predictor and transformer scripts). Artifact versions are unique for the same model version.</p> <p>When a new deployment is created, a new artifact version is generated in two cases:</p> <ul> <li>the artifact version in the predictor is set to <code>CREATE</code> (see Artifact Version)</li> <li>no model artifact with the same files has been created before.</li> </ul> <p>Inside a model deployment, the local path to the artifact files is stored in the <code>ARTIFACT_FILES_PATH</code> environment variable (see environment variables). Moreover, you can explore the artifact files under the <code>/Models/&lt;model-name&gt;/&lt;model-version&gt;/Artifacts/&lt;artifact-version&gt;</code> directory using the File Browser. </p> <p>Warning</p> <p>All files under <code>/Models</code> are managed by Hopsworks. Changes to artifact files cannot be reverted and can have an impact on existing model deployments.</p> <p>Additional files</p> <p>Currently, the artifact files can only include predictor and transformer scripts, and a configuration file. Support for additional files (e.g., other resources) is coming soon.</p>"},{"location":"user_guides/mlops/serving/deployment/#predictor","title":"Predictor","text":"<p>Predictors are responsible for running the model server that loads the trained model, listens to inference requests and returns prediction results. To learn more about predictors, see the Predictor Guide</p> <p>Note</p> <p>Only one predictor is supported in a deployment.</p> <p>Info</p> <p>Model artifacts are assigned an incremental version number, being <code>0</code> the version reserved for model artifacts that do not contain predictor or transformer scripts (i.e., shared artifacts containing only the model files).</p>"},{"location":"user_guides/mlops/serving/deployment/#transformer","title":"Transformer","text":"<p>Transformers are used to apply transformations on the model inputs before sending them to the predictor for making predictions using the model. To learn more about transformers, see the Transformer Guide.</p> <p>Warning</p> <p>Transformers are only supported in KServe deployments.</p>"},{"location":"user_guides/mlops/serving/external-access/","title":"How To Configure External Access To A Model Deployment","text":""},{"location":"user_guides/mlops/serving/external-access/#introduction","title":"Introduction","text":"<p>Hopsworks supports role-based access control (RBAC) for project members within a project, where a project ML assets can only be accessed by Hopsworks users that are members of that project (See governance).</p> <p>However, there are cases where you might want to grant external users with access to specific model deployments without them having to register into Hopsworks or to join the project which will give them access to all project ML assets. For these cases, Hopsworks supports fine-grained access control to model deployments based on user groups managed by an external Identity Provider.</p> <p>Authentication methods</p> <p>Hopsworks can be configured to use different types of authentication methods including OAuth2, LDAP and Kerberos. See the Authentication Methods Guide for more information.</p>"},{"location":"user_guides/mlops/serving/external-access/#gui-for-hopsworks-users","title":"GUI (for Hopsworks users)","text":""},{"location":"user_guides/mlops/serving/external-access/#step-1-navigate-to-a-model-deployment","title":"Step 1: Navigate to a model deployment","text":"<p>If you have at least one model deployment already created, navigate to the model deployments page by clicking on the <code>Deployments</code> tab on the navigation menu on the left.</p> <p> Deployments navigation tab </p> <p>Once in the model deployments page, find the model deployment you want to configure external access and click on the name of the deployment to open the model deployment overview page.</p> <p> Deployment overview </p>"},{"location":"user_guides/mlops/serving/external-access/#step-2-go-to-external-access","title":"Step 2: Go to External Access","text":"<p>You can find the external access configuration by clicking on <code>External access</code> on the navigation menu on the left or scrolling down to the external access section.</p> <p> External access configuration </p>"},{"location":"user_guides/mlops/serving/external-access/#step-3-add-or-remove-user-groups","title":"Step 3: Add or remove user groups","text":"<p>In this section, you can add and remove user groups by clicking on <code>edit external user groups</code> and typing the group name in the text-free input field or selecting one of the existing ones in the dropdown list. After that, click on the <code>save</code> button to persist the changes.</p> <p>Case sensitivity</p> <p>Inference requests are authorized using a case-sensitive exact match between the group names of the user making the request and the group names granted access to the model deployment. Therefore, a user assigned to the group <code>lab1</code> won't have access to a model deployment accessible by group <code>LAB1</code>.</p> <p> External access configuration </p>"},{"location":"user_guides/mlops/serving/external-access/#gui-for-external-users","title":"GUI (for external users)","text":""},{"location":"user_guides/mlops/serving/external-access/#step-1-login-with-the-external-identity-provider","title":"Step 1: Login with the external identity provider","text":"<p>Navigate to Hopsworks, and click on the <code>Login with</code> button to sign in using the configured external identity provider (e.g., Keycloak in this example).</p> <p> Login with External Identity Provider </p>"},{"location":"user_guides/mlops/serving/external-access/#step-2-explore-the-model-deployments-you-are-granted-access-to","title":"Step 2: Explore the model deployments you are granted access to","text":"<p>Once you sign in to Hopsworks, you can see the list of model deployments you are granted access to based on your assigned groups.</p> <p> Deployments with external access </p>"},{"location":"user_guides/mlops/serving/external-access/#step-2-inspect-your-current-groups","title":"Step 2: Inspect your current groups","text":"<p>You can find the current groups you are assigned to at the top of the page.</p> <p> External user groups </p>"},{"location":"user_guides/mlops/serving/external-access/#step-3-get-an-api-key","title":"Step 3: Get an API key","text":"<p>Inference requests to model deployments are authenticated and authorized based on your external user and user groups. You can create API keys to authenticate your inference requests by clicking on the <code>Create API Key</code> button.</p> <p>Authorization header</p> <p>API keys are set in the <code>Authorization</code> header following the format <code>ApiKey &lt;api-key-value&gt;</code></p> <p> Get API key </p>"},{"location":"user_guides/mlops/serving/external-access/#step-4-send-inference-requests","title":"Step 4: Send inference requests","text":"<p>Depending on the type of model deployment, the URI of the model server can differ (e.g., <code>/chat/completions</code> for LLM deployments or <code>/predict</code> for traditional model deployments). You can find the corresponding URI on every model deployment card.</p> <p>In addition to the <code>Authorization</code> header containing the API key, the <code>Host</code> header needs to be set according to the model deployment where the inference requests are sent to. This header is used by the ingress to route the inference requests to the corresponding model deployment. You can find the <code>Host</code> header value in the model deployment card.</p> <p>Code snippets</p> <p>For clients sending inference requests using libraries similar to curl or OpenAI API-compatible libraries (e.g., LangChain), you can find code snippet examples by clicking on the <code>Curl &gt;_</code> and <code>LangChain &gt;_</code> buttons.</p> <p> Deployment endpoint </p>"},{"location":"user_guides/mlops/serving/external-access/#refreshing-external-user-groups","title":"Refreshing External User Groups","text":"<p>Every time an external user signs in to Hopsworks using a pre-configured authentication method, Hopsworks fetches the external user groups and updates the internal state accordingly. Given that groups can be added/removed from users at any time by the Identity Provider, Hopsworks needs to periodically fetch the external user groups to keep the state updated.</p> <p>Therefore, external users that want to access model deployments are required to login periodically to ensure they are still part of the allowed groups. The timespan between logins is controlled by the configuration parameter <code>requireExternalUserLoginAfterHours</code> available during the Hopsworks installation and upgrade. </p> <p>The <code>requireExternalUserLoginAfterHours</code> configuration parameter controls the number of hours after which external users are required to sign in to Hopsworks to refresh their external user groups. </p> <p>Configuring <code>requireExternalUserLoginAfterHours</code></p> <p>Allowed values are -1, 0 and greater than 0, where -1 disables the periodic login requirement and 0 disables external access completely for every model deployment.</p>"},{"location":"user_guides/mlops/serving/inference-batcher/","title":"How To Configure Inference Batcher","text":""},{"location":"user_guides/mlops/serving/inference-batcher/#introduction","title":"Introduction","text":"<p>Inference batching can be enabled to increase inference request throughput at the cost of higher latencies. The configuration of the inference batcher depends on the serving tool and the model server used in the deployment. See the compatibility matrix.</p>"},{"location":"user_guides/mlops/serving/inference-batcher/#gui","title":"GUI","text":""},{"location":"user_guides/mlops/serving/inference-batcher/#step-1-create-new-deployment","title":"Step 1: Create new deployment","text":"<p>If you have at least one model already trained and saved in the Model Registry, navigate to the deployments page by clicking on the <code>Deployments</code> tab on the navigation menu on the left.</p> <p> Deployments navigation tab </p> <p>Once in the deployments page, you can create a new deployment by either clicking on <code>New deployment</code> (if there are no existing deployments) or on <code>Create new deployment</code> it the top-right corner. Both options will open the deployment creation form.</p>"},{"location":"user_guides/mlops/serving/inference-batcher/#step-2-go-to-advanced-options","title":"Step 2: Go to advanced options","text":"<p>A simplified creation form will appear including the most common deployment fields from all available configurations. Inference batching is part of the advanced options of a deployment. To navigate to the advanced creation form, click on <code>Advanced options</code>.</p> <p> Advanced options. Go to advanced deployment creation form </p>"},{"location":"user_guides/mlops/serving/inference-batcher/#step-3-configure-inference-batching","title":"Step 3: Configure inference batching","text":"<p>To enable inference batching, click on the <code>Request batching</code> checkbox.</p> <p> Inference batching configuration (default values) </p> <p>If your deployment uses KServe, you can optionally set three additional parameters for the inference batcher: maximum batch size, maximum latency (ms) and timeout (s).</p> <p>Once you are done with the changes, click on <code>Create new deployment</code> at the bottom of the page to create the deployment for your model.</p>"},{"location":"user_guides/mlops/serving/inference-batcher/#code","title":"Code","text":""},{"location":"user_guides/mlops/serving/inference-batcher/#step-1-connect-to-hopsworks","title":"Step 1: Connect to Hopsworks","text":"Python <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n</code></pre>"},{"location":"user_guides/mlops/serving/inference-batcher/#step-2-define-an-inference-logger","title":"Step 2: Define an inference logger","text":"Python <pre><code>from hsml.inference_batcher import InferenceBatcher\n\nmy_batcher = InferenceBatcher(enabled=True,\n                              # optional\n                              max_batch_size=32,\n                              max_latency=5000, # milliseconds\n                              timeout=5 # seconds\n                              )\n</code></pre>"},{"location":"user_guides/mlops/serving/inference-batcher/#step-3-create-a-deployment-with-the-inference-batcher","title":"Step 3: Create a deployment with the inference batcher","text":"Python <pre><code>my_model = mr.get_model(\"my_model\", version=1)\n\nmy_predictor = ms.create_predictor(my_model,\n                                  inference_batcher=my_batcher\n                                  )\nmy_predictor.deploy()\n\n# or\n\nmy_deployment = ms.create_deployment(my_predictor)\nmy_deployment.save()\n</code></pre>"},{"location":"user_guides/mlops/serving/inference-batcher/#api-reference","title":"API Reference","text":"<p>Inference Batcher</p>"},{"location":"user_guides/mlops/serving/inference-batcher/#compatibility-matrix","title":"Compatibility matrix","text":"Show supported inference batcher configuration Serving tool Model server Inference batching Fine-grained configuration Docker Flask \u274c - TensorFlow Serving \u2705 \u274c Kubernetes Flask \u274c - TensorFlow Serving \u2705 \u274c KServe Flask \u2705 \u2705 TensorFlow Serving \u2705 \u2705"},{"location":"user_guides/mlops/serving/inference-logger/","title":"How To Configure Inference Logging","text":""},{"location":"user_guides/mlops/serving/inference-logger/#introduction","title":"Introduction","text":"<p>Once a model is deployed and starts making predictions as inference requests arrive, logging model inputs and predictions becomes essential to monitor the health of the model and take action if the model's performance degrades over time.</p> <p>Hopsworks supports logging both inference requests and predictions as events to a Kafka topic for analysis.</p> <p>Topic schemas vary depending on the serving tool. See below</p>"},{"location":"user_guides/mlops/serving/inference-logger/#gui","title":"GUI","text":""},{"location":"user_guides/mlops/serving/inference-logger/#step-1-create-new-deployment","title":"Step 1: Create new deployment","text":"<p>If you have at least one model already trained and saved in the Model Registry, navigate to the deployments page by clicking on the <code>Deployments</code> tab on the navigation menu on the left.</p> <p> Deployments navigation tab </p> <p>Once in the deployments page, you can create a new deployment by either clicking on <code>New deployment</code> (if there are no existing deployments) or on <code>Create new deployment</code> it the top-right corner. Both options will open the deployment creation form.</p>"},{"location":"user_guides/mlops/serving/inference-logger/#step-2-go-to-advanced-options","title":"Step 2: Go to advanced options","text":"<p>A simplified creation form will appear including the most common deployment fields from all available configurations. Inference logging is part of the advanced options of a deployment. To navigate to the advanced creation form, click on <code>Advanced options</code>.</p> <p> Advanced options. Go to advanced deployment creation form </p>"},{"location":"user_guides/mlops/serving/inference-logger/#step-3-configure-inference-logging","title":"Step 3: Configure inference logging","text":"<p>To enable inference logging, choose <code>CREATE</code> as Kafka topic name to create a new topic, or select an existing topic. If you prefer, you can disable inference logging by selecting <code>NONE</code>.</p> <p>If you decide to create a new topic, select the number of partitions and number of replicas for your topic, or use the default values.</p> <p> Inference logging configuration with a new kafka topic </p> <p>If the deployment is created with KServe enabled, you can specify which inference logs you want to send to the Kafka topic (i.e., <code>MODEL_INPUTS</code>, <code>PREDICTIONS</code> or both)</p> <p>Once you are done with the changes, click on <code>Create new deployment</code> at the bottom of the page to create the deployment for your model.</p>"},{"location":"user_guides/mlops/serving/inference-logger/#code","title":"Code","text":""},{"location":"user_guides/mlops/serving/inference-logger/#step-1-connect-to-hopsworks","title":"Step 1: Connect to Hopsworks","text":"Python <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n</code></pre>"},{"location":"user_guides/mlops/serving/inference-logger/#step-2-define-an-inference-logger","title":"Step 2: Define an inference logger","text":"Python <pre><code>from hsml.inference_logger import InferenceLogger\nfrom hsml.kafka_topic import KafkaTopic\n\nnew_topic = KafkaTopic(name=\"CREATE\",\n                      # optional\n                      num_partitions=1,\n                      num_replicas=1\n                      )\n\nmy_logger = InferenceLogger(kafka_topic=new_topic, mode=\"ALL\")\n</code></pre> <p>Use dict for simpler code</p> <p>Similarly, you can create the same logger with:</p> <pre><code>my_logger = InferenceLogger(kafka_topic={\"name\": \"CREATE\"}, mode=\"ALL\")\n</code></pre>"},{"location":"user_guides/mlops/serving/inference-logger/#step-3-create-a-deployment-with-the-inference-logger","title":"Step 3: Create a deployment with the inference logger","text":"Python <pre><code>my_model = mr.get_model(\"my_model\", version=1)\n\nmy_predictor = ms.create_predictor(my_model,\n                                  inference_logger=my_logger\n                                  )\nmy_predictor.deploy()\n\n# or\n\nmy_deployment = ms.create_deployment(my_predictor)\nmy_deployment.save()\n</code></pre>"},{"location":"user_guides/mlops/serving/inference-logger/#api-reference","title":"API Reference","text":"<p>Inference Logger</p>"},{"location":"user_guides/mlops/serving/inference-logger/#topic-schema","title":"Topic schema","text":"<p>The schema of Kafka events varies depending on the serving tool. In KServe deployments, model inputs and predictions are logged in separate events, but sharing the same <code>requestId</code> field. In non-KServe deployments, the same event contains both the model input and prediction related to the same inference request.</p> Show kafka topic schemas KServeDocker / Kubernetes <pre><code>{\n    \"fields\": [\n        { \"name\": \"servingId\", \"type\": \"int\" },\n        { \"name\": \"modelName\", \"type\": \"string\" },\n        { \"name\": \"modelVersion\", \"type\": \"int\" },\n        { \"name\": \"requestTimestamp\", \"type\": \"long\" },\n        { \"name\": \"responseHttpCode\", \"type\": \"int\" },\n        { \"name\": \"inferenceId\", \"type\": \"string\" },\n        { \"name\": \"messageType\", \"type\": \"string\" },\n        { \"name\": \"payload\", \"type\": \"string\" }\n    ],\n    \"name\": \"inferencelog\",\n    \"type\": \"record\"\n}\n</code></pre> <pre><code>{\n    \"fields\": [\n        { \"name\": \"modelId\", \"type\": \"int\" },\n        { \"name\": \"modelName\", \"type\": \"string\" },\n        { \"name\": \"modelVersion\", \"type\": \"int\" },\n        { \"name\": \"requestTimestamp\", \"type\": \"long\" },\n        { \"name\": \"responseHttpCode\", \"type\": \"int\" },\n        { \"name\": \"inferenceRequest\", \"type\": \"string\" },\n        { \"name\": \"inferenceResponse\", \"type\": \"string\" },\n        { \"name\": \"modelServer\", \"type\": \"string\" },\n        { \"name\": \"servingTool\", \"type\": \"string\" }\n    ],\n    \"name\": \"inferencelog\",\n    \"type\": \"record\"\n}\n</code></pre>"},{"location":"user_guides/mlops/serving/predictor/","title":"How To Configure A Predictor","text":""},{"location":"user_guides/mlops/serving/predictor/#introduction","title":"Introduction","text":"<p>In this guide, you will learn how to configure a predictor for a trained model.</p> <p>Warning</p> <p>This guide assumes that a model has already been trained and saved into the Model Registry. To learn how to create a model in the Model Registry, see Model Registry Guide</p> <p>Predictors are the main component of deployments. They are responsible for running a model server that loads a trained model, handles inference requests and returns predictions. They can be configured to use different model servers, serving tools, log specific inference data or scale differently. In each predictor, you can configure the following components:</p> <ol> <li>Model server</li> <li>Serving tool</li> <li>User-provided script</li> <li>Server configuration file</li> <li>Python environments</li> <li>Transformer</li> <li>Inference Logger</li> <li>Inference Batcher</li> <li>Resources</li> <li>API protocol</li> </ol>"},{"location":"user_guides/mlops/serving/predictor/#gui","title":"GUI","text":""},{"location":"user_guides/mlops/serving/predictor/#step-1-create-new-deployment","title":"Step 1: Create new deployment","text":"<p>If you have at least one model already trained and saved in the Model Registry, navigate to the deployments page by clicking on the <code>Deployments</code> tab on the navigation menu on the left.</p> <p> Deployments navigation tab </p> <p>Once in the deployments page, you can create a new deployment by either clicking on <code>New deployment</code> (if there are no existing deployments) or on <code>Create new deployment</code> it the top-right corner. Both options will open the deployment creation form.</p>"},{"location":"user_guides/mlops/serving/predictor/#step-2-choose-a-backend","title":"Step 2: Choose a backend","text":"<p>A simplified creation form will appear, including the most common deployment fields from all available configurations. The first step is to choose a backend for your model deployment. The backend will filter the models shown below according to the framework that the model was registered with in the model registry.</p> <p>For example if you registered the model as a TensorFlow model using <code>ModelRegistry.tensorflow.create_model(...)</code> you select <code>Tensorflow Serving</code> in the dropdown.</p> <p> Select the backend </p> <p>All models compatible with the selected backend will be listed in the model dropdown.</p> <p> Select the model </p> <p>Moreover, you can optionally select a predictor script (see Step 3), enable KServe (see Step 4) or change other advanced configuration (see Step 5). Otherwise, click on <code>Create new deployment</code> to create the deployment for your model.</p>"},{"location":"user_guides/mlops/serving/predictor/#step-3-optional-select-a-predictor-script","title":"Step 3 (Optional): Select a predictor script","text":"<p>For python models, if you want to use your own predictor script click on <code>From project</code> and navigate through the file system to find it, or click on <code>Upload new file</code> to upload a predictor script now.</p> <p> Select a predictor script in the simplified deployment form </p>"},{"location":"user_guides/mlops/serving/predictor/#step-4-optional-change-predictor-environment","title":"Step 4 (Optional): Change predictor environment","text":"<p>If you are using a predictor script it is also required to configure the inference environment for the predictor. This environment needs to have all the necessary dependencies installed to run your predictor script.</p> <p>By default, we provide a set of environments like <code>tensorflow-inference-pipeline</code>, <code>torch-inference-pipeline</code> and <code>pandas-inference-pipeline</code> that serves this purpose for common machine learning frameworks.</p> <p>To create your own it is recommended to clone the <code>minimal-inference-pipeline</code> and install additional dependencies for your use-case.</p> <p> Select an environment for the predictor script </p>"},{"location":"user_guides/mlops/serving/predictor/#step-5-optional-select-a-configuration-file","title":"Step 5 (Optional): Select a configuration file","text":"<p>Note</p> <p>Only available for LLM deployments.</p> <p>You can select a configuration file to be added to the artifact files. If a predictor script is provided, this configuration file will be available inside the model deployment at the local path stored in the <code>CONFIG_FILE_PATH</code> environment variable. If a predictor script is not provided, this configuration file will be directly passed to the vLLM server. You can find all configuration parameters supported by the vLLM server in the vLLM documentation.</p> <p> Select a configuration file in the simplified deployment form </p>"},{"location":"user_guides/mlops/serving/predictor/#step-6-optional-enable-kserve","title":"Step 6 (Optional): Enable KServe","text":"<p>Other configuration such as the serving tool, is part of the advanced options of a deployment. To navigate to the advanced creation form, click on <code>Advanced options</code>.</p> <p> Advanced options. Go to advanced deployment creation form </p> <p>Here, you change the serving tool for your deployment by enabling or disabling the KServe checkbox.</p> <p> KServe checkbox in the advanced deployment form </p>"},{"location":"user_guides/mlops/serving/predictor/#step-7-optional-other-advanced-options","title":"Step 7 (Optional): Other advanced options","text":"<p>Additionally, you can adjust the default values of the rest of components:</p> <p>Predictor components</p> <ol> <li>Transformer</li> <li>Inference logger</li> <li>Inference batcher</li> <li>Resources</li> <li>API protocol</li> </ol> <p>Once you are done with the changes, click on <code>Create new deployment</code> at the bottom of the page to create the deployment for your model.</p>"},{"location":"user_guides/mlops/serving/predictor/#code","title":"Code","text":""},{"location":"user_guides/mlops/serving/predictor/#step-1-connect-to-hopsworks","title":"Step 1: Connect to Hopsworks","text":"Python <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n</code></pre>"},{"location":"user_guides/mlops/serving/predictor/#step-2-optional-implement-a-predictor-script","title":"Step 2 (Optional): Implement a predictor script","text":"PredictorPredictor (vLLM deployments only) <pre><code>class Predictor():\n\n    def __init__(self):\n        \"\"\" Initialization code goes here\"\"\"\n        # Model files can be found at os.environ[\"MODEL_FILES_PATH\"]\n        # self.model = ... # load your model\n\n    def predict(self, inputs):\n        \"\"\" Serve predictions using the trained model\"\"\"\n        # Use the model to make predictions\n        # return self.model.predict(inputs)\n</code></pre> <pre><code>import os\nfrom vllm import __version__, AsyncEngineArgs, AsyncLLMEngine\nfrom typing import Iterable, AsyncIterator, Union, Optional\nfrom kserve.protocol.rest.openai import (\n    CompletionRequest,\n    ChatPrompt,\n    ChatCompletionRequestMessage,\n)\nfrom kserve.protocol.rest.openai.types import Completion\nfrom kserve.protocol.rest.openai.types.openapi import ChatCompletionTool\n\n\nclass Predictor():\n\n    def __init__(self):\n        \"\"\" Initialization code goes here\"\"\"\n\n        # (optional) if any, access the configuration file via os.environ[\"CONFIG_FILE_PATH\"]\n        config = ...\n\n        print(\"Starting vLLM backend...\")\n        engine_args = AsyncEngineArgs(\n            model=os.environ[\"MODEL_FILES_PATH\"],\n            **config\n        )\n\n        # \"self.vllm_engine\" is required as the local variable with the vllm engine handler\n        self.vllm_engine = AsyncLLMEngine.from_engine_args(engine_args)\n\n    #\n    # NOTE: Default implementations of the apply_chat_template and create_completion methods are already provided.\n    #       If needed, you can override these methods as shown below\n    #\n\n    #def apply_chat_template(\n    #    self,\n    #    messages: Iterable[ChatCompletionRequestMessage],\n    #    chat_template: Optional[str] = None,\n    #    tools: Optional[list[ChatCompletionTool]] = None,\n    #) -&gt; ChatPrompt:\n    #    \"\"\"Converts a prompt or list of messages into a single templated prompt string\"\"\"\n\n    #    prompt = ... # apply chat template on the message to build the prompt\n    #    return ChatPrompt(prompt=prompt)\n\n    #async def create_completion(\n    #    self, request: CompletionRequest\n    #) -&gt; Union[Completion, AsyncIterator[Completion]]:\n    #    \"\"\"Generate responses using the vLLM engine\"\"\"\n    #    \n    #    generators = self.vllm_engine.generate(...)\n    #\n    #    # Completion: used for returning a single answer (batch)\n    #    # AsyncIterator[Completion]: used for returning a stream of answers\n    #    return ...\n</code></pre> <p>Jupyter magic</p> <p>In a jupyter notebook, you can add <code>%%writefile my_predictor.py</code> at the top of the cell to save it as a local file.</p>"},{"location":"user_guides/mlops/serving/predictor/#step-3-optional-upload-the-script-to-your-project","title":"Step 3 (Optional): Upload the script to your project","text":"<p>You can also use the UI to upload your predictor script. See above</p> Python <pre><code>uploaded_file_path = dataset_api.upload(\"my_predictor.py\", \"Resources\", overwrite=True)\npredictor_script_path = os.path.join(\"/Projects\", project.name, uploaded_file_path)\n</code></pre>"},{"location":"user_guides/mlops/serving/predictor/#step-4-define-predictor","title":"Step 4: Define predictor","text":"Python <pre><code>my_model = mr.get_model(\"my_model\", version=1)\n\nmy_predictor = ms.create_predictor(my_model,\n                                  # optional\n                                  model_server=\"PYTHON\",\n                                  serving_tool=\"KSERVE\",\n                                  script_file=predictor_script_path\n                                  )\n</code></pre>"},{"location":"user_guides/mlops/serving/predictor/#step-5-create-a-deployment-with-the-predictor","title":"Step 5: Create a deployment with the predictor","text":"Python <pre><code>my_deployment = my_predictor.deploy()\n\n# or\nmy_deployment = ms.create_deployment(my_predictor)\nmy_deployment.save()\n</code></pre>"},{"location":"user_guides/mlops/serving/predictor/#api-reference","title":"API Reference","text":"<p>Predictor</p>"},{"location":"user_guides/mlops/serving/predictor/#model-server","title":"Model Server","text":"<p>Hopsworks Model Serving supports deploying models with a Flask server for python-based models, TensorFlow Serving for TensorFlow / Keras models and vLLM for Large Language Models (LLMs). Today, you can deploy PyTorch models as python-based models.</p> Show supported model servers Model Server Supported ML Models and Frameworks Flask \u2705 python-based (scikit-learn, xgboost, pytorch...) TensorFlow Serving \u2705 keras, tensorflow TorchServe \u274c pytorch vLLM \u2705 vLLM-supported models (see list)"},{"location":"user_guides/mlops/serving/predictor/#serving-tool","title":"Serving tool","text":"<p>In Hopsworks, model servers are deployed on Kubernetes. There are two options for deploying models on Kubernetes: using KServe inference services or Kubernetes built-in deployments. KServe is the recommended way to deploy models in Hopsworks.</p> <p>The following is a comparative table showing the features supported by each of them.</p> Show serving tools comparison Feature / requirement Kubernetes (enterprise) KServe (enterprise) Autoscaling (scale-out) \u2705 \u2705 Resource allocation \u2796 min. resources \u2705 min / max. resources Inference logging \u2796 simple \u2705 fine-grained Inference batching \u2796 partially \u2705 Scale-to-zero \u274c \u2705 after 30s of inactivity Transformers \u274c \u2705 Low-latency predictions \u274c \u2705 Multiple models \u274c \u2796 (python-based) User-provided predictor required  (python-only) \u2705 \u274c"},{"location":"user_guides/mlops/serving/predictor/#user-provided-script","title":"User-provided script","text":"<p>Depending on the model server and serving platform used in the model deployment, you can (or need) to provide your own python script to load the model and make predictions. This script is referred to as predictor script, and is included in the artifact files of the model deployment.</p> <p>The predictor script needs to implement a given template depending on the model server of the model deployment. See the templates in Step 2.</p> Show supported user-provided predictors Serving tool Model server User-provided predictor script Kubernetes Flask server \u2705 (required) TensorFlow Serving \u274c KServe Fast API \u2705 (only required for artifacts with multiple models) TensorFlow Serving \u274c vLLM \u2705 (optional)"},{"location":"user_guides/mlops/serving/predictor/#server-configuration-file","title":"Server configuration file","text":"<p>Depending on the model server, a server configuration file can be selected to help detach configuration used within the model deployment from the model server or the implementation of the predictor and transformer scripts. In other words, by modifying the configuration file of an existing model deployment you can adjust its settings without making changes to the predictor or transformer scripts. Inside a model deployment, the local path to the configuration file is stored in the <code>CONFIG_FILE_PATH</code> environment variable (see environment variables). </p> <p>Configuration file format</p> <p>The configuration file can be of any format, except in vLLM deployments without a predictor script for which a YAML file is required.</p> <p>Passing arguments to vLLM via configuration file</p> <p>For vLLM deployments without a predictor script, the server configuration file is required and it is used to configure the vLLM server. For example, you can use this configuration file to specify the chat template  or LoRA modules to be loaded by the vLLM server. See all available parameters in the official documentation.</p>"},{"location":"user_guides/mlops/serving/predictor/#environment-variables","title":"Environment variables","text":"<p>A number of different environment variables is available in the predictor to ease its implementation.</p> Show environment variables Name Description MODEL_FILES_PATH Local path to the model files ARTIFACT_FILES_PATH Local path to the artifact files CONFIG_FILE_PATH Local path to the configuration file DEPLOYMENT_NAME Name of the current deployment MODEL_NAME Name of the model being served by the current deployment MODEL_VERSION Version of the model being served by the current deployment ARTIFACT_VERSION Version of the model artifact being served by the current deployment"},{"location":"user_guides/mlops/serving/predictor/#python-environments","title":"Python environments","text":"<p>Depending on the model server and serving tool used in the model deployment, you can select the Python environment where the predictor and transformer scripts will run. To create a new Python environment see Python Environments.</p> Show supported Python environments Serving tool Model server Editable Predictor Transformer Kubernetes Flask server \u274c <code>pandas-inference-pipeline</code> only \u274c TensorFlow Serving \u274c (official) tensorflow serving image \u274c KServe Fast API \u2705 any <code>inference-pipeline</code> image any <code>inference-pipeline</code> image TensorFlow Serving \u2705 (official) tensorflow serving image any <code>inference-pipeline</code> image vLLM \u2705 <code>vllm-inference-pipeline</code> or <code>vllm-openai</code> any <code>inference-pipeline</code> image <p>Note</p> <p>The selected Python environment is used for both predictor and transformer. Support for selecting a different Python environment for the predictor and transformer is coming soon.</p>"},{"location":"user_guides/mlops/serving/predictor/#transformer","title":"Transformer","text":"<p>Transformers are used to apply transformations on the model inputs before sending them to the predictor for making predictions using the model. To learn more about transformers, see the Transformer Guide.</p> <p>Note</p> <p>Transformers are only supported in KServe deployments.</p>"},{"location":"user_guides/mlops/serving/predictor/#inference-logger","title":"Inference logger","text":"<p>Inference loggers are deployment components that log inference requests into a Kafka topic for later analysis. To learn about the different logging modes, see the Inference Logger Guide</p>"},{"location":"user_guides/mlops/serving/predictor/#inference-batcher","title":"Inference batcher","text":"<p>Inference batcher are deployment component that apply batching to the incoming inference requests for a better throughput-latency trade-off. To learn about the different configuration available for the inference batcher, see the Inference Batcher Guide.</p>"},{"location":"user_guides/mlops/serving/predictor/#resources","title":"Resources","text":"<p>Resources include the number of replicas for the deployment as well as the resources (i.e., memory, CPU, GPU) to be allocated per replica. To learn about the different combinations available, see the Resources Guide.</p>"},{"location":"user_guides/mlops/serving/predictor/#api-protocol","title":"API protocol","text":"<p>Hopsworks supports both REST and gRPC as the API protocols to send inference requests to model deployments. In general, you use gRPC when you need lower latency inference requests. To learn more about the REST and gRPC API protocols for model deployments, see the API Protocol Guide.</p>"},{"location":"user_guides/mlops/serving/resources/","title":"How To Allocate Resources To A Model Deployment","text":""},{"location":"user_guides/mlops/serving/resources/#introduction","title":"Introduction","text":"<p>Depending on the serving tool used to deploy a trained model, resource allocation can be configured at different levels. While deployments on Docker containers only support a fixed number of resources (CPU and memory), using Kubernetes or KServe allows a better exploitation of the resources available in the platform, by enabling you to specify how many CPUs, GPUs, and memory are allocated to a deployment. See the compatibility matrix.</p>"},{"location":"user_guides/mlops/serving/resources/#gui","title":"GUI","text":""},{"location":"user_guides/mlops/serving/resources/#step-1-create-new-deployment","title":"Step 1: Create new deployment","text":"<p>If you have at least one model already trained and saved in the Model Registry, navigate to the deployments page by clicking on the <code>Deployments</code> tab on the navigation menu on the left.</p> <p> Deployments navigation tab </p> <p>Once in the deployments page, you can create a new deployment by either clicking on <code>New deployment</code> (if there are no existing deployments) or on <code>Create new deployment</code> it the top-right corner. Both options will open the deployment creation form.</p>"},{"location":"user_guides/mlops/serving/resources/#step-2-go-to-advanced-options","title":"Step 2: Go to advanced options","text":"<p>A simplified creation form will appear including the most common deployment fields from all available configurations. Resource allocation is part of the advanced options of a deployment. To navigate to the advanced creation form, click on <code>Advanced options</code>.</p> <p> Advanced options. Go to advanced deployment creation form </p>"},{"location":"user_guides/mlops/serving/resources/#step-3-configure-resource-allocation","title":"Step 3: Configure resource allocation","text":"<p>In the <code>Resource allocation</code> section of the form, you can optionally set the resources to be allocated to the predictor and/or the transformer (if available). Moreover, you can choose the minimum number of replicas for each of these components.</p> Scale-to-zero capabilities <p>Deployments with KServe enabled can scale to zero by choosing <code>0</code> as the number of instances.</p> <p> Resource allocation for the predictor and transformer </p> <p>Once you are done with the changes, click on <code>Create new deployment</code> at the bottom of the page to create the deployment for your model.</p>"},{"location":"user_guides/mlops/serving/resources/#code","title":"Code","text":""},{"location":"user_guides/mlops/serving/resources/#step-1-connect-to-hopsworks","title":"Step 1: Connect to Hopsworks","text":"Python <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n</code></pre>"},{"location":"user_guides/mlops/serving/resources/#step-2-define-the-predictor-resource-configuration","title":"Step 2: Define the predictor resource configuration","text":"Python <pre><code>from hsml.resources import PredictorResources, Resources\n\nminimum_res = Resources(cores=1, memory=128, gpus=1)\nmaximum_res = Resources(cores=2, memory=256, gpus=1)\n\npredictor_res = PredictorResources(num_instances=1, requests=minimum_res, limits=maximum_res)\n</code></pre>"},{"location":"user_guides/mlops/serving/resources/#step-3-optional-define-the-transformer-resource-configuration","title":"Step 3 (Optional): Define the transformer resource configuration","text":"Python <pre><code>from hsml.resources import TransformerResources\n\nminimum_res = Resources(cores=1, memory=128, gpus=1)\nmaximum_res = Resources(cores=2, memory=256, gpus=1)\n\ntransformer_res = TransformerResources(num_instances=2, requests=minimum_res, limits=maximum_res)\n</code></pre>"},{"location":"user_guides/mlops/serving/resources/#step-4-create-a-deployment-with-the-resource-configuration","title":"Step 4: Create a deployment with the resource configuration","text":"Python <pre><code>my_model = mr.get_model(\"my_model\", version=1)\n\nmy_predictor = ms.create_predictor(my_model,\n                                  resources=predictor_res,\n                                  # transformer=Transformer(script_file,\n                                  #                         resources=transformer_res)\n                                  )\nmy_predictor.deploy()\n\n# or\n\nmy_deployment = ms.create_deployment(my_predictor)\nmy_deployment.save()\n</code></pre>"},{"location":"user_guides/mlops/serving/resources/#api-reference","title":"API Reference","text":"<p>Resource Allocation</p>"},{"location":"user_guides/mlops/serving/resources/#compatibility-matrix","title":"Compatibility matrix","text":"Show supported resource allocation configuration Serving tool Component Resources Docker Predictor Fixed Transformer \u274c Kubernetes Predictor Minimum resources Transformer \u274c KServe Predictor Minimum / maximum resources Transformer Minimum / maximum resources"},{"location":"user_guides/mlops/serving/transformer/","title":"How To Configure A Transformer","text":""},{"location":"user_guides/mlops/serving/transformer/#introduction","title":"Introduction","text":"<p>In this guide, you will learn how to configure a transformer in a deployment.</p> <p>Transformers are used to apply transformations on the model inputs before sending them to the predictor for making predictions using the model. They run on a built-in Flask server provided by Hopsworks and require a user-provided python script implementing the Transformer class.</p> Warning <p>Transformers are only supported in deployments using KServe as serving tool.</p> <p>A transformer has two configurable components:</p> <ol> <li>User-provided script</li> <li>Resources</li> </ol> <p>See examples of transformer scripts in the serving example notebooks.</p>"},{"location":"user_guides/mlops/serving/transformer/#gui","title":"GUI","text":""},{"location":"user_guides/mlops/serving/transformer/#step-1-create-new-deployment","title":"Step 1: Create new deployment","text":"<p>If you have at least one model already trained and saved in the Model Registry, navigate to the deployments page by clicking on the <code>Deployments</code> tab on the navigation menu on the left.</p> <p> Deployments navigation tab </p> <p>Once in the deployments page, you can create a new deployment by either clicking on <code>New deployment</code> (if there are no existing deployments) or on <code>Create new deployment</code> it the top-right corner. Both options will open the deployment creation form.</p>"},{"location":"user_guides/mlops/serving/transformer/#step-2-go-to-advanced-options","title":"Step 2: Go to advanced options","text":"<p>A simplified creation form will appear including the most common deployment fields from all available configurations. Transformers are part of the advanced options of a deployment. To navigate to the advanced creation form, click on <code>Advanced options</code>.</p> <p> Advanced options. Go to advanced deployment creation form </p>"},{"location":"user_guides/mlops/serving/transformer/#step-3-select-a-transformer-script","title":"Step 3: Select a transformer script","text":"<p>Transformers require KServe as the serving platform for the deployment. Make sure that KServe is enabled for this deployment by activating the corresponding checkbox.</p> <p> Enable KServe in the advanced deployment form </p> <p>Then, if the transformer script is already located in Hopsworks, click on <code>From project</code> and navigate through the file system to find your script. Otherwise, you can click on <code>Upload new file</code> to upload the transformer script now.</p> <p> Choose a transformer script in the advanced deployment form </p> <p>After selecting the transformer script, you can optionally configure resource allocation for your transformer (see Step 4). Otherwise, click on <code>Create new deployment</code> to create the deployment for your model.</p>"},{"location":"user_guides/mlops/serving/transformer/#step-4-optional-configure-resource-allocation","title":"Step 4 (Optional): Configure resource allocation","text":"<p>At the end of the page, you can configure the resources to be allocated for the transformer, as well as the minimum and maximum number of replicas to be deployed.</p> Scale-to-zero capabilities <p>Deployments with KServe enabled can scale to zero by choosing <code>0</code> as the number of instances.</p> <p> Resource allocation for the transformer </p> <p>Once you are done with the changes, click on <code>Create new deployment</code> at the bottom of the page to create the deployment for your model.</p>"},{"location":"user_guides/mlops/serving/transformer/#code","title":"Code","text":""},{"location":"user_guides/mlops/serving/transformer/#step-1-connect-to-hopsworks","title":"Step 1: Connect to Hopsworks","text":"Python <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# get Dataset API instance\ndataset_api = project.get_dataset_api()\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n</code></pre>"},{"location":"user_guides/mlops/serving/transformer/#step-2-implement-transformer-script","title":"Step 2: Implement transformer script","text":"Transformer <pre><code>class Transformer():\n    def __init__(self):\n        \"\"\" Initialization code goes here\"\"\"\n        pass\n\n    def preprocess(self, inputs):\n        \"\"\" Transform the requests inputs here. The object returned by this method will be used as model input to make predictions. \"\"\"\n        return inputs\n\n    def postprocess(self, outputs):\n        \"\"\" Transform the predictions computed by the model before returning a response \"\"\"\n        return outputs\n</code></pre> <p>Jupyter magic</p> <p>In a jupyter notebook, you can add <code>%%writefile my_transformer.py</code> at the top of the cell to save it as a local file.</p>"},{"location":"user_guides/mlops/serving/transformer/#step-3-upload-the-script-to-your-project","title":"Step 3: Upload the script to your project","text":"<p>You can also use the UI to upload your transformer script. See above</p> Python <pre><code>uploaded_file_path = dataset_api.upload(\"my_transformer.py\", \"Resources\", overwrite=True)\ntransformer_script_path = os.path.join(\"/Projects\", project.name, uploaded_file_path)\n</code></pre>"},{"location":"user_guides/mlops/serving/transformer/#step-4-define-a-transformer","title":"Step 4: Define a transformer","text":"Python <pre><code>my_transformer = ms.create_transformer(script_file=uploaded_file_path)\n\n# or\n\nfrom hsml.transformer import Transformer\n\nmy_transformer = Transformer(script_file)\n</code></pre>"},{"location":"user_guides/mlops/serving/transformer/#step-5-create-a-deployment-with-the-transformer","title":"Step 5: Create a deployment with the transformer","text":"Python <pre><code>my_predictor = ms.create_predictor(transformer=my_transformer)\nmy_deployment = my_predictor.deploy()\n\n# or\nmy_deployment = ms.create_deployment(my_predictor, transformer=my_transformer)\nmy_deployment.save()\n</code></pre>"},{"location":"user_guides/mlops/serving/transformer/#api-reference","title":"API Reference","text":"<p>Transformer</p>"},{"location":"user_guides/mlops/serving/transformer/#resources","title":"Resources","text":"<p>Resources include the number of replicas for the deployment as well as the resources (i.e., memory, CPU, GPU) to be allocated per replica. To learn about the different combinations available, see the Resources Guide.</p>"},{"location":"user_guides/mlops/serving/transformer/#environment-variables","title":"Environment variables","text":"<p>A number of different environment variables is available in the transformer to ease its implementation.</p> Show environment variables Name Description ARTIFACT_FILES_PATH Local path to the model artifact files DEPLOYMENT_NAME Name of the current deployment MODEL_NAME Name of the model being served by the current deployment MODEL_VERSION Version of the model being served by the current deployment ARTIFACT_VERSION Version of the model artifact being served by the current deployment"},{"location":"user_guides/mlops/serving/troubleshooting/","title":"How To Troubleshoot A Model Deployment","text":""},{"location":"user_guides/mlops/serving/troubleshooting/#introduction","title":"Introduction","text":"<p>In this guide, you will learn how to troubleshoot a deployment that is having issues to serve a trained model. But before that, it is important to understand how deployment states are defined and the possible transitions between conditions.</p> <p>When a deployment is starting, it follows an ordered sequence of states before becoming ready for serving predictions. Similarly, it follows an ordered sequence of states when being stopped, although with fewer steps.</p>"},{"location":"user_guides/mlops/serving/troubleshooting/#gui","title":"GUI","text":""},{"location":"user_guides/mlops/serving/troubleshooting/#step-1-inspect-deployment-status","title":"Step 1: Inspect deployment status","text":"<p>If you have at least one deployment already created, navigate to the deployments page by clicking on the <code>Deployments</code> tab on the navigation menu on the left.</p> <p> Deployments navigation tab </p> <p>Once in the deployments page, find the deployment you want to inspect. Next to the actions buttons, you can find an indicator showing the current status of the deployment. For a more descriptive representation, this indicator changes its color based on the status.</p> <p>To inspect the condition of the deployment, click on the name of the deployment to open the deployment overview page.</p>"},{"location":"user_guides/mlops/serving/troubleshooting/#step-2-inspect-condition","title":"Step 2: Inspect condition","text":"<p>At the top of page, you can find the same status indicator mentioned in the previous step. Below it, a one-line message is shown with a more detailed description of the deployment status. This message is built using the current status condition of the deployment.</p> <p>Oftentimes, the status and the one-line description are enough to understand the current state of a deployment. For instance, when the cluster lacks enough allocatable resources to meet the deployment requirements, a meaningful error message will be shown with the root cause.</p> <p> Condition of a deployment that cannot be scheduled </p> <p>However, when the deployment fails to start futher details might be needed depending on the source of failure. For example, failures in the initialization or starting steps will show a less relevant message. In those cases, you can explore the deployments logs in search of the cause of the problem.</p> <p> Condition of a deployment that fails to start </p>"},{"location":"user_guides/mlops/serving/troubleshooting/#step-3-explore-transient-logs","title":"Step 3: Explore transient logs","text":"<p>Each deployment is composed of several components depending on its configuration and the model being served. Transient logs refer to component-specific logs that are directly retrieved from the component itself. Therefore, these logs can only be retrieved as long as the deployment components are reachable.</p> <p>Transient logs are informative and fast to retrieve, facilitating the troubleshooting of deployment components at a glance</p> <p>Transient logs are convenient when access to the most recent logs of a deployment is needed.</p> <p>Info</p> <p>When a deployment is in idle state, there are no components running (i.e., scaled to zero) and, thus, no transient logs are available.</p> <p>Note</p> <p>In the current version of Hopsworks, transient logs can only be accessed using the Hopsworks Machine Learning Python library. See an example here.</p>"},{"location":"user_guides/mlops/serving/troubleshooting/#step-4-explore-historical-logs","title":"Step 4: Explore historical logs","text":"<p>Transient logs are continuously collected and stored in OpenSearch, where they become historical logs accessible using the integrated OpenSearch Dashboards. Therefore, historical logs contain the same information than transient logs. However, there might be cases where transient logs could not be collected in time for a specific component and, thus, not included in the historical logs.</p> <p>Historical logs are persisted transient logs that can be queried, filtered and sorted using OpenSearch Dashboards, facilitating a more sophisticated exploration of past records.</p> <p>Historical logs are convenient when a deployment fails occasionally, either at inference time or without a clear reason. In this case, narrowing the inspection of component-specific logs at a concrete point in time and searching for keywords can be helpful.</p> <p>To access the OpenSearch Dashboards, click on the <code>See logs</code> button at the top of the deployment overview page.</p> <p> Access to historical logs of a deployment </p> <p>Note</p> <p>In case you are not familiar with the interface, you may find the official documentation useful.</p> <p>Once in the OpenSearch Dashboards, you can search for keywords, apply multiple filters and sort the records by timestamp.</p> Show available filters Filter Description component Name of the deployment component (i.e., predictor or transformer) container_name Name of the container within a component (i.e., kserve-container, storage-initializer, inference-logger) serving_name Name of the deployment model_name Name of the model being served model_version Version of the model being served timestamp Timestamp when the record was reported"},{"location":"user_guides/mlops/serving/troubleshooting/#code","title":"Code","text":""},{"location":"user_guides/mlops/serving/troubleshooting/#step-1-connect-to-hopsworks","title":"Step 1: Connect to Hopsworks","text":"Python <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n</code></pre>"},{"location":"user_guides/mlops/serving/troubleshooting/#step-2-retrieve-an-existing-deployment","title":"Step 2: Retrieve an existing deployment","text":"Python <pre><code>deployment = ms.get_deployment(\"mydeployment\")\n</code></pre>"},{"location":"user_guides/mlops/serving/troubleshooting/#step-3-get-current-deployments-predictor-state","title":"Step 3: Get current deployment's predictor state","text":"Python <pre><code>state = deployment.get_state()\n\nstate.describe()\n</code></pre>"},{"location":"user_guides/mlops/serving/troubleshooting/#step-4-explore-transient-logs","title":"Step 4: Explore transient logs","text":"Python <pre><code>deployment.get_logs(component=\"predictor|transformer\", tail=10)\n</code></pre>"},{"location":"user_guides/mlops/serving/troubleshooting/#api-reference","title":"API Reference","text":"<p>Deployment</p> <p>PredictorState</p>"},{"location":"user_guides/mlops/vector_database/","title":"How To Use OpenSearch k-NN plugin","text":""},{"location":"user_guides/mlops/vector_database/#introduction","title":"Introduction","text":"<p>The k-NN plugin enables users to search for the k-nearest neighbors to a query point across an index of vectors. To determine the neighbors, you can specify the space (the distance function) you want to use to measure the distance between points.</p> <p>Use cases include recommendations (for example, an \u201cother songs you might like\u201d feature in a music application), image recognition, and fraud detection.</p> <p>Limited to internal Jobs and Notebooks</p> <p>Currently it's only possible to configure the opensearch-py client in a job or jupyter notebook running inside the Hopsworks cluster.</p>"},{"location":"user_guides/mlops/vector_database/#code","title":"Code","text":"<p>In this guide, you will learn how to create a simple recommendation application, using the <code>k-NN plugin</code> in OpenSearch.</p>"},{"location":"user_guides/mlops/vector_database/#step-1-get-the-opensearch-api","title":"Step 1: Get the OpenSearch API","text":"Python <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nopensearch_api = project.get_opensearch_api()\n</code></pre>"},{"location":"user_guides/mlops/vector_database/#step-2-configure-the-opensearch-py-client","title":"Step 2: Configure the opensearch-py client","text":"Python <pre><code>from opensearchpy import OpenSearch\n\nclient = OpenSearch(**opensearch_api.get_default_py_config())\n</code></pre>"},{"location":"user_guides/mlops/vector_database/#step-3-create-an-index","title":"Step 3: Create an index","text":"<p>Create an index to use by calling <code>opensearch_api.get_project_index(..)</code>.</p> Python <pre><code>knn_index_name = opensearch_api.get_project_index(\"demo_knn_index\")\n\nindex_body = {\n    \"settings\": {\n        \"knn\": True,\n        \"knn.algo_param.ef_search\": 100,\n    },\n    \"mappings\": {\n        \"properties\": {\n            \"my_vector1\": {\n                \"type\": \"knn_vector\",\n                \"dimension\": 2\n            }\n        }\n    }\n}\n\nresponse = client.indices.create(knn_index_name, body=index_body)\n\nprint(response)\n</code></pre>"},{"location":"user_guides/mlops/vector_database/#step-4-bulk-ingestion-of-vectors","title":"Step 4: Bulk ingestion of vectors","text":"<p>Ingest 10 vectors in a bulk fashion to the index. These vectors represent the list of vectors to calculate the similarity for.</p> Python <pre><code>from opensearchpy.helpers import bulk\nimport random\n\nactions = [\n    {\n        \"_index\": knn_index_name,\n        \"_id\": count,\n        \"_source\": {\n            \"my_vector1\": [random.uniform(0, 10), random.uniform(0, 10)],\n        }\n    }\n    for count in range(0, 10)\n]\n\nbulk(\n    client,\n    actions,\n)\n</code></pre>"},{"location":"user_guides/mlops/vector_database/#step-5-score-vector-similarity","title":"Step 5: Score vector similarity","text":"<p>Score the vector <code>[2.5, 3]</code> and find the 3 most similar vectors.</p> Python <pre><code># Define the search request\nquery = {\n    \"size\": 3,\n    \"query\": {\n        \"knn\": {\n            \"my_vector1\": {\n                \"vector\": [2.5, 3],\n                \"k\": 3\n            }\n        }\n    }\n}\n\n# Perform the similarity search\nresponse = client.search(\n    body = query,\n    index = knn_index_name\n)\n\n# Pretty print response\nimport pprint\npp = pprint.PrettyPrinter()\npp.pprint(response)\n</code></pre> <p><code>Output</code> from the above script shows the score for each of the three most similar vectors that have been indexed.</p> <p><code>[4.798869166444522, 4.069064892468535]</code> is the most similar vector to <code>[2.5, 3]</code> with a score of <code>0.1346312</code>.</p> Bash <pre><code>2022-05-30 09:55:50,529 INFO: POST https://10.0.2.15:9200/my_project_demo_knn_index/_search [status:200 request:0.017s]\n{'_shards': {'failed': 0, 'skipped': 0, 'successful': 1, 'total': 1},\n 'hits': {'hits': [{'_id': '9',\n                    '_index': 'my_project_demo_knn_index',\n                    '_score': 0.1346312,\n                    '_source': {'my_vector1': [4.798869166444522,\n                                               4.069064892468535]},\n                    '_type': '_doc'},\n                   {'_id': '0',\n                    '_index': 'my_project_demo_knn_index',\n                    '_score': 0.040784083,\n                    '_source': {'my_vector1': [6.267438489652193,\n                                               6.0538134453735175]},\n                    '_type': '_doc'},\n                   {'_id': '7',\n                    '_index': 'my_project_demo_knn_index',\n                    '_score': 0.03222388,\n                    '_source': {'my_vector1': [7.973873201006634,\n                                               2.7361877621502115]},\n                    '_type': '_doc'}],\n          'max_score': 0.1346312,\n          'total': {'relation': 'eq', 'value': 3}},\n 'timed_out': False,\n 'took': 9}\n</code></pre>"},{"location":"user_guides/mlops/vector_database/#api-reference","title":"API Reference","text":"<p>k-NN plugin</p> <p>OpenSearch</p>"},{"location":"user_guides/projects/","title":"Projects Guides","text":"<p>This section serves to provide guides and examples for the common usage of services in a Project through the Hopsworks UI and APIs.</p> <ul> <li>Projects</li> <li>Authentication</li> <li>API Keys</li> <li>Jupyter</li> <li>Jobs</li> <li>Git</li> <li>Python Environment</li> <li>Kafka</li> <li>OpenSearch</li> <li>Secrets</li> </ul>"},{"location":"user_guides/projects/airflow/airflow/","title":"Orchestrate Jobs using Apache Airflow","text":""},{"location":"user_guides/projects/airflow/airflow/#introduction","title":"Introduction","text":"<p>Hopsworks jobs can be orchestrated using Apache Airflow. You can define a Airflow DAG (Directed Acyclic Graph) containing the dependencies between Hopsworks jobs.  You can then schedule the DAG to be executed at a specific schedule using a cron expression.</p> <p>Airflow DAGs are defined as Python files. Within the Python file, different operators can be used to trigger different actions. Hopsworks provides an operator to execute jobs on Hopsworks and a sensor to wait for a specific job to finish.</p>"},{"location":"user_guides/projects/airflow/airflow/#use-apache-airflow-in-hopsworks","title":"Use Apache Airflow in Hopsworks","text":"<p>Hopsworks deployments include a deployment of Apache Airflow. You can access it from the Hopsworks UI by clicking on the Airflow button on the left menu.</p> <p>Airfow is configured to enforce Role Based Access Control (RBAC) to the Airflow DAGs. Admin users on Hopsworks have access to all the DAGs in the deployment. Regular users can access all the DAGs of the projects they are a member of.</p> <p>Access Control</p> <p>Airflow does not have any knowledge of the Hopsworks project you are currently working on. As such, when opening the Airflow UI, you will see all the DAGs all of the projects you are a member of.</p>"},{"location":"user_guides/projects/airflow/airflow/#hopsworks-dag-builder","title":"Hopsworks DAG Builder","text":"Airflow DAG Builder <p>You can create a new Airflow DAG to orchestrate jobs using the Hopsworks DAG builder tool. Click on New Workflow to create a new Airflow DAG. You should provide a name for the DAG as well as a schedule interval. You can define the schedule using the dropdown menus or by providing a cron expression.</p> <p>You can add to the DAG Hopsworks operators and sensors:</p> <ul> <li> <p>Operator: The operator is used to trigger a job execution. When configuring the operator you select the job you want to execute and you can optionally provide execution arguments. You can decide whether or not the operator should wait for the execution to be completed. If you select the wait option, the operator will block and Airflow will not execute any parallel task. If you select the wait option the Airflow task fails if the job fails. If you want to execute tasks in parallel, you should not select the wait option but instead use the sensor. When configuring the operator, you can can also provide which other Airflow tasks it depends on. If you add a dependency, the task will be executed only after the upstream tasks have been executed successfully. </p> </li> <li> <p>Sensor: The sensor can be used to wait for executions to be completed. Similarly to the wait option of the operator, the sensor blocks until the job execution is completed. The sensor can be used to launch several jobs in parallel and wait for their execution to be completed. Please note that the sensor is defined at the job level rather than the execution level. The sensor will wait for the most recent execution to be completed and it will fail the Airflow task if the execution was not successful. </p> </li> </ul> <p>You can then create the DAG and Hopsworks will generate the Python file. </p>"},{"location":"user_guides/projects/airflow/airflow/#write-your-own-dag","title":"Write your own DAG","text":"<p>If you prefer to code the DAGs or you want to edit a DAG built with the builder tool, you can do so. The Airflow DAGs are stored in the Airflow dataset which you can access using the file browser in the project settings.</p> <p>When writing the code for the DAG you can invoke the operator as follows:</p> <pre><code>HopsworksLaunchOperator(dag=dag,\n    task_id=\"profiles_fg_0\",\n    project_name=\"airflow_doc\",\n    job_name=\"profiles_fg\",\n    job_arguments=\"\",\n    wait_for_completion=True)\n</code></pre> <p>You should provide the name of the Airflow task (<code>task_id</code>) and the Hopsworks job information (<code>project_name</code>, <code>job_name</code>, <code>job_arguments</code>). You can set the <code>wait_for_completion</code> flag to <code>True</code> if you want the operator to block and wait for the job execution to be finished.</p> <p>Similarly, you can invoke the sensor as shown below. You should provide the name of the Airflow task (<code>task_id</code>) and the Hopsworks job information (<code>project_name</code>, <code>job_name</code>) </p> <pre><code>HopsworksJobSuccessSensor(dag=dag,\n    task_id='wait_for_profiles_fg',\n    project_name=\"airflow_doc\",\n    job_name='profiles_fg')\n</code></pre> <p>When writing the DAG file, you should also add the <code>access_control</code> parameter to the DAG configuration. The <code>access_control</code> parameter specifies which projects have access to the DAG and which actions the project members can perform on it. If you do not specify the <code>access_control</code> option, project members will not be able to see the DAG in the Airflow UI.</p> <p>Admin access</p> <p>The <code>access_control</code> configuration does not apply to Hopsworks admin users which have full access to all the DAGs even if they are not member of the project. </p> <pre><code>    dag = DAG(\n        dag_id = \"example_dag\",\n        default_args = args,\n        access_control = {\n            \"project_name\": {\"can_dag_read\", \"can_dag_edit\"},\n        },\n\n        schedule_interval = \"0 4 * * *\"\n    )\n</code></pre> <p>Project Name</p> <p>You should replace the <code>project_name</code> in the snippet above with the name of your own project</p>"},{"location":"user_guides/projects/airflow/airflow/#manage-airflow-dags-using-git","title":"Manage Airflow DAGs using Git","text":"<p>You can leverage the Git integration to track your Airflow DAGs in a git repository. Airflow will only consider the DAG files which are stored in the Airflow Dataset in Hopsworks.  After cloning the git repository in Hopsworks, you can automate the process of copying the DAG file in the Airflow Dataset using the copy method of the Hopsworks API.</p>"},{"location":"user_guides/projects/api_key/create_api_key/","title":"How To Create An API Key","text":""},{"location":"user_guides/projects/api_key/create_api_key/#introduction","title":"Introduction","text":"<p>An API key allows a user or a program to make API calls without having to authenticate with a username and password. To access an endpoint using an API key, a client should send the access token using the ApiKey authentication scheme.</p> <p>The API Key can now be used when connecting to your Hopsworks instance using the <code>hopsworks</code>, <code>hsfs</code> or <code>hsml</code> python library or set in the <code>ApiKey</code> header for the REST API.</p> <pre><code>GET /resource HTTP/1.1\nHost: server.hopsworks.ai\nAuthorization: ApiKey &lt;api_key&gt;\n</code></pre>"},{"location":"user_guides/projects/api_key/create_api_key/#ui","title":"UI","text":"<p>In this guide, you will learn how to create an API key.</p>"},{"location":"user_guides/projects/api_key/create_api_key/#step-1-navigate-to-api-keys","title":"Step 1: Navigate to API Keys","text":"<p>In the Account Settings page you can find the API section showing a list of all API keys.</p> <p> List of API Keys </p>"},{"location":"user_guides/projects/api_key/create_api_key/#step-2-create-an-api-key","title":"Step 2: Create an API Key","text":"<p>Click <code>New Api key</code>, select the required scopes and create it by clicking <code>Create Api Key</code>. </p> <p>Copy the value and save it in a secure location, such as a password manager.</p> <p> Create new API Key </p>"},{"location":"user_guides/projects/api_key/create_api_key/#login-with-api-key-using-sdk","title":"Login with API Key using SDK","text":"<p>In this guide you learned how to create an API Key. You can now use the API Key to login using the <code>hopsworks</code> python SDK.</p>"},{"location":"user_guides/projects/auth/krb/","title":"Login using Kerberos","text":""},{"location":"user_guides/projects/auth/krb/#introduction","title":"Introduction","text":"<p>Hopsworks supports different methods of authentication. Here we will look at authentication using Kerberos.</p>"},{"location":"user_guides/projects/auth/krb/#prerequisites","title":"Prerequisites","text":"<p>A Hopsworks cluster with Kerberos authentication.  See Configure Kerberos on how to configure Kerberos on your cluster.</p>"},{"location":"user_guides/projects/auth/krb/#step-1-log-in-with-kerberos","title":"Step 1: Log in with Kerberos","text":"<p>If Kerberos is configured you will see a Log in using alternative on the login page. Choose Kerberos and click on  Go to Hopsworks to login. </p> Log in using Kerberos <p>If password login is disabled you only see the Log in using Kerberos/SSO alternative. Click on Go to Hopsworks to login.</p> Kerberos only authentication <p>To be able to authenticate with Kerberos you need to configure your browser to use Kerberos.  Note that without a properly configured browser, the Kerberos token is not sent to the server and so SSO will not work.</p> <p>If Kerberos is not configured properly you will see Wrong credentials message when trying to log in.</p> Missing Kerberos ticket"},{"location":"user_guides/projects/auth/krb/#step-2-give-consent","title":"Step 2: Give consent","text":"<p>When logging in with Kerberos for the first time Hopsworks will retrieve and save consented claims (firstname, lastname and email), about the logged in end-user. If you have multiple email addresses registered in Kerberos you can choose  one to use with Hopsworks.</p> <p>If you do not want your information to be saved in Hopsworks you can click Cancel. This will redirect you back to the login page.</p> Give consent <p>After clicking on Register you will be redirected to the landing page:  Landing page </p> <p>In the landing page, you will find two buttons. Use these buttons to either create a  demo project or a new project.</p>"},{"location":"user_guides/projects/auth/ldap/","title":"Login using LDAP","text":""},{"location":"user_guides/projects/auth/ldap/#introduction","title":"Introduction","text":"<p>Hopsworks supports different methods of authentication. Here we will look at authentication using LDAP.</p>"},{"location":"user_guides/projects/auth/ldap/#prerequisites","title":"Prerequisites","text":"<p>A Hopsworks cluster with LDAP authentication.  See Configure LDAP on how to configure LDAP on your cluster.</p>"},{"location":"user_guides/projects/auth/ldap/#step-1-log-in-with-ldap","title":"Step 1: Log in with LDAP","text":"<p>If LDAP is configured you will see a Log in using alternative on the login page. Choose LDAP and type in your  username and password then click on Login.</p> <p>Note that you need to use your LDAP credentials.</p> Log in using LDAP"},{"location":"user_guides/projects/auth/ldap/#step-2-give-consent","title":"Step 2: Give consent","text":"<p>When logging in with LDAP for the first time Hopsworks will retrieve and save consented claims (firstname, lastname and email), about the logged in end-user. If you have multiple email addresses registered in LDAP you can choose one to use with Hopsworks. </p> <p>If you do not want your information to be saved in Hopsworks you can click Cancel. This will redirect you back  to the login page.</p> Give consent <p>After clicking on Register you will be redirected to the landing page:  Landing page </p> <p>In the landing page, you will find two buttons. Use these buttons to either create a  demo project or a new project.</p>"},{"location":"user_guides/projects/auth/login/","title":"Log in To Hopsworks","text":""},{"location":"user_guides/projects/auth/login/#introduction","title":"Introduction","text":"<p>Hopsworks supports different methods of authentication. Here we will look at authentication using username and password.</p>"},{"location":"user_guides/projects/auth/login/#prerequisites","title":"Prerequisites","text":"<p>An account on a Hopsworks cluster.</p>"},{"location":"user_guides/projects/auth/login/#step-1-log-in-with-email-and-password","title":"Step 1: Log in with email and password","text":"<p>After your account is validated by an administrator you can use your email and password to login.</p> Login with password"},{"location":"user_guides/projects/auth/login/#step-2-two-factor-authentication","title":"Step 2: Two-factor authentication","text":"<p>If two-factor authentication is enabled you will be presented with a two-factor authentication window after you  enter your password. Use your authenticator app (example. Google Authenticator) on your phone to get a one-time password.</p> One time password <p>Upon successful login, you will arrive at the landing page:</p> Landing page <p>In the landing page, you will find two buttons. Use these buttons to either create a  demo project or a new project.</p>"},{"location":"user_guides/projects/auth/oauth/","title":"Login Using A Third-party Identity Provider","text":""},{"location":"user_guides/projects/auth/oauth/#introduction","title":"Introduction","text":"<p>Hopsworks supports different methods of authentication. Here we will look at authentication using Third-party Identity Provider.</p>"},{"location":"user_guides/projects/auth/oauth/#prerequisites","title":"Prerequisites","text":"<p>A Hopsworks cluster with OAuth authentication.  See Configure OAuth2 on how to configure OAuth on your cluster.</p>"},{"location":"user_guides/projects/auth/oauth/#step-1-log-in-with-oauth","title":"Step 1: Log in with OAuth","text":"<p>If OAuth is configured a **Login with ** button will appear in the login page. Use this button to log in to Hopsworks using your OAuth credentials.</p> Login with OAuth2"},{"location":"user_guides/projects/auth/oauth/#step-2-give-consent","title":"Step 2: Give consent","text":"<p>When logging in with OAuth for the first time Hopsworks will retrieve and save consented claims (firstname, lastname  and email), about the logged in end-user.</p> Give consent <p>After clicking on Register you will be redirected to the landing page:  Landing page </p> <p>In the landing page, you will find two buttons. Use these buttons to either create a  demo project or a new project.</p>"},{"location":"user_guides/projects/auth/profile/","title":"Update Your Profile and Credentials","text":""},{"location":"user_guides/projects/auth/profile/#introduction","title":"Introduction","text":"<p>A profile is required to access Hopsworks. A profile is created when a user registers and can be updated via Account settings.</p>"},{"location":"user_guides/projects/auth/profile/#prerequisites","title":"Prerequisites","text":"<p>An account on a Hopsworks cluster.</p> <p>Updating profile and credentials is not supported if you are using Third-party Identity Providers like Kerberos, LDAP, or OAuth  to authenticate to Hopsworks.</p>"},{"location":"user_guides/projects/auth/profile/#step-1-go-to-your-account-settings","title":"Step 1: Go to your Account settings","text":"<p>After you have logged in, in the upper right-hand corner of the screen, you will see your name. Click on your name, then click on the menu item Account settings. The account settings page will open with profile tab selected. In this tab you can change your first and last name. You cannot change your email address and will need to create a new  account if you wish to change your email address.</p> Update profile"},{"location":"user_guides/projects/auth/profile/#step-2-update-credential","title":"Step 2: Update credential","text":"<p>To update your credential go to the Authentication tab as shown in the image below.  Update credential </p>"},{"location":"user_guides/projects/auth/profile/#step-3-enablereset-two-factor-authentication","title":"Step 3: Enable/Reset Two-factor Authentication","text":"<p>You can also change your two-factor setting in the Authentication tab. Two-factor authentication is only available if it is enabled from the cluster administration page.</p> Enable Two-factor Authentication <p>After enabling or resetting two-factor you will be presented with a QR Code. You will then need to scan the QR code  to add it on your phone's authenticator application  (example. Google Authenticator). </p> <p>If you miss this step, you will have to recover your smartphone credentials at a later stage.</p> Register Two-factor Authentication <p>Use the one time password generated by your authenticator app to confirm the registration.</p>"},{"location":"user_guides/projects/auth/recovery/","title":"Password Recovery","text":""},{"location":"user_guides/projects/auth/recovery/#introduction","title":"Introduction","text":"<p>This topic describes how to recover a forgotten password.</p>"},{"location":"user_guides/projects/auth/recovery/#prerequisites","title":"Prerequisites","text":"<p>An account on a Hopsworks cluster.</p>"},{"location":"user_guides/projects/auth/recovery/#step-1-request-password-reset","title":"Step 1: Request password reset","text":"<p>If you forget your password start by clicking on Forgot password on the login page. Enter your email and click on the  Send reset link button.  Password reset </p>"},{"location":"user_guides/projects/auth/recovery/#step-2-use-the-password-reset-link","title":"Step 2: Use the password reset link","text":"<p>A password reset link will be sent to the email address you entered if the email is found in the system. Click on the reset link to set your new password.</p>"},{"location":"user_guides/projects/auth/registration/","title":"Register A New Account On Hopsworks","text":""},{"location":"user_guides/projects/auth/registration/#introduction","title":"Introduction","text":"<p>Hopsworks supports different methods of authentication.  To use username and password as the method of authentication, you first need to register.</p>"},{"location":"user_guides/projects/auth/registration/#prerequisites","title":"Prerequisites","text":"<p>Registration enabled Hopsworks cluster.</p> <p>The process for registering a new account is as follows</p>"},{"location":"user_guides/projects/auth/registration/#step-1-register-a-new-account","title":"Step 1: Register a new account","text":"<p>Click on the Register button on the login page and register your email address and details.</p> Register new account"},{"location":"user_guides/projects/auth/registration/#step-2-enable-two-factor-authentication","title":"Step 2: Enable Two-Factor Authentication","text":"<p>If two-factor authentication is required you will be presented with a page like in the figure below. Scan the QR  code or type the code in bold to register your account in your authenticator app  (example. Google Authenticator). </p> Add two-factor authentication"},{"location":"user_guides/projects/auth/registration/#step-3-validate-your-email-address","title":"Step 3: Validate your email address","text":"<p>Validate your email address by clicking on the link in the validation email you received. After your account is created an administrator needs to validate your account before you can log in.</p> Account created"},{"location":"user_guides/projects/git/clone_repo/","title":"How To Clone a Git Repository","text":""},{"location":"user_guides/projects/git/clone_repo/#introduction","title":"Introduction","text":"<p>Repositories are cloned and managed within the scope of a project. The content of the repository will reside on the Hopsworks File System. The content of the repository can be edited from Jupyter notebooks and can for example be used to configure Jobs. Repositories can be managed from the Git section in the project settings. The Git overview in the project settings provides a list of repositories currently cloned within the project, the location of their content as well which branch and commit their HEAD is currently at.</p>"},{"location":"user_guides/projects/git/clone_repo/#prerequisites","title":"Prerequisites","text":"<ul> <li>For cloning a private repository, you should configure a Git Provider with your git credentials. You can clone a GitHub and GitLab public repository without configuring the provider. However, for BitBucket you always need to configure the username and token to clone a repository.</li> </ul>"},{"location":"user_guides/projects/git/clone_repo/#ui","title":"UI","text":""},{"location":"user_guides/projects/git/clone_repo/#step-1-navigate-to-repositories","title":"Step 1: Navigate to repositories","text":"<p>In the left-hand sidebar found in your project click on <code>Project settings</code>, and then navigate to the <code>Git</code> section.</p> <p>This page lists all the cloned git repositories under <code>Repositories</code>, while operations performed on those repositories, e.g <code>push</code>/<code>pull</code>/<code>commit</code> are listed under <code>Git Executions</code>.</p> <p> Git repository overview </p>"},{"location":"user_guides/projects/git/clone_repo/#step-2-clone-a-repository","title":"Step 2: Clone a repository","text":"<p>To clone a new repository, click on the <code>Clone repository</code> button on the Git overview page.</p> <p> Git clone </p> <p>You should first choose the git provider e.g., GitHub, GitLab or BitBucket. If you are cloning a private repository, remember to configure the username and token for the provider first in Git Provider. The clone dialog also asks you to specify the URL of the repository to clone. The supported protocol is HTTPS. As an example, if the repository is hosted on GitHub, the URL should look like: <code>https://github.com/logicalclocks/hops-examples.git</code>.</p> <p>Then specify which branch you want to clone. By default the <code>main</code> branch will be used, however a different branch or commit can be specified by selecting <code>Clone from a specific branch</code>.</p> <p>You can select the folder, within your project, in which the repository should be cloned. By default, the repository is going to be cloned within the <code>Jupyter</code> dataset. However, by clicking on the location button, a different location can be selected.</p> <p>Finally, click on the <code>Clone repository</code> button to trigger the cloning of the repository.</p>"},{"location":"user_guides/projects/git/clone_repo/#step-3-track-progress-of-the-clone","title":"Step 3: Track progress of the clone","text":"<p>The progress of the git clone can be tracked under <code>Git Executions</code>.</p> <p> Track progress of clone </p>"},{"location":"user_guides/projects/git/clone_repo/#step-4-browse-repository-files","title":"Step 4: Browse repository files","text":"<p>In the <code>File browser</code> page you can now browse the files of the cloned repository. In the figure below, the repository is located in <code>Jupyter/hops-examples</code> directory.</p> <p> Browse repository files </p>"},{"location":"user_guides/projects/git/clone_repo/#code","title":"Code","text":"<p>You can also clone a repository through the hopsworks git API in python.</p>"},{"location":"user_guides/projects/git/clone_repo/#step-1-get-the-git-api","title":"Step 1: Get the git API","text":"<pre><code>import hopsworks\n\nproject = hopsworks.login()\n\ngit_api = project.get_git_api()\n</code></pre>"},{"location":"user_guides/projects/git/clone_repo/#step-2-clone-the-repository","title":"Step 2: Clone the repository","text":"<pre><code>REPO_URL=\"https://github.com/logicalclocks/hops-examples.git\" # git repository\nHOPSWORKS_FOLDER=\"Jupyter\" # path in Hopsworks filesystem to clone to\nPROVIDER=\"GitHub\"\nBRANCH=\"master\" # optional branch to clone\n\nexamples_repo = git_api.clone(REPO_URL, HOPSWORKS_FOLDER, PROVIDER, branch=BRANCH)\n</code></pre>"},{"location":"user_guides/projects/git/clone_repo/#api-reference","title":"API Reference","text":"<p>Api reference for git repositories is available here: GitRepo</p> <p>A notebook for managing git can be found here.</p>"},{"location":"user_guides/projects/git/clone_repo/#errors-and-troubleshooting","title":"Errors and Troubleshooting","text":""},{"location":"user_guides/projects/git/clone_repo/#invalid-credentials","title":"Invalid credentials","text":"<p>This might happen when the credentials entered for the provider are incorrect. Try the following:</p> <ul> <li>Confirm that the settings for the provider ( in Account Settings &gt; Git providers) are correct. You must enter both your Git provider username and token.</li> <li>Confirm that you have selected the correct Git provider when cloning the repository.</li> <li>Ensure your personal access token has the correct repository access rights.</li> <li>Ensure your personal access token has not expired.</li> </ul>"},{"location":"user_guides/projects/git/clone_repo/#timeout-errors","title":"Timeout errors","text":"<p>Cloning a large repo or checking out a large branch may hit timeout errors. You can try again later if the system was under heavy load at the time.</p>"},{"location":"user_guides/projects/git/clone_repo/#symlink-errors","title":"Symlink errors","text":"<p>Git repositories with symlinks are not yet supported, therefore cloning repositories with symlinks will fail. You can create a separate branch to remove the symlinks, and clone from this branch.</p>"},{"location":"user_guides/projects/git/clone_repo/#going-further","title":"Going Further","text":"<p>You can now start Jupyter from the cloned git repository path to work with the files.</p>"},{"location":"user_guides/projects/git/configure_git_provider/","title":"How To Configure a Git Provider","text":""},{"location":"user_guides/projects/git/configure_git_provider/#introduction","title":"Introduction","text":"<p>When you perform Git operations on Hopsworks that need to interact with the remote repository, Hopsworks relies on the Git HTTPS protocol to perform those operations. Authentication with the remote repository happens through a token generated by the Git repository hosting service (GitHub, GitLab, BitBucket).</p> <p>Token permissions</p> <p>The token permissions should grant access to public and private repositories including read and write access to repository contents and commit statuses.  If you are using the new GitHub access tokens, make sure you choose the correct <code>Resource owner</code> when generating the token for the repositories you will want to clone. For the <code>Repository permissions</code> of the new GitHub fine-grained token, you should atleast give read and write access to <code>Commit statuses</code> and <code>Contents</code>.</p>"},{"location":"user_guides/projects/git/configure_git_provider/#ui","title":"UI","text":"<p>Documentation on how to generate a token for the supported Git hosting services is available here:</p> <ul> <li>GitHub</li> <li>GitLab</li> <li>BitBucket</li> </ul>"},{"location":"user_guides/projects/git/configure_git_provider/#step-1-navigate-to-git-providers","title":"Step 1: Navigate to Git Providers","text":"<p>You can access the <code>Git Providers</code> page of your Hopsworks cluster by clicking on your name, in the top right corner, and choosing <code>Account Settings</code> from the dropdown menu. The <code>Git providers</code> section displays which providers have been already configured and can be used to clone new repositories.</p> <p> Git provider configuration list </p>"},{"location":"user_guides/projects/git/configure_git_provider/#step-2-configure-a-provider","title":"Step 2: Configure a provider","text":"<p>Click on <code>Edit Configuration</code> to change a provider username or token, or to configure a new provider.</p> <p>Tick the checkbox next to the provider you want to configure and insert the username and the token to use for that provider.</p> <p> Git provider configuration </p> <p>Click <code>Create Configuration</code> to save the configuration.</p>"},{"location":"user_guides/projects/git/configure_git_provider/#step-3-provider-is-configured","title":"Step 3: Provider is configured","text":"<p>The configured provider should now be marked as configured.</p> <p> Git provider configured </p>"},{"location":"user_guides/projects/git/configure_git_provider/#code","title":"Code","text":"<p>You can also configure a git provider using the hopsworks git API in python.</p>"},{"location":"user_guides/projects/git/configure_git_provider/#step-1-get-the-git-api","title":"Step 1: Get the git API","text":"<pre><code>import hopsworks\n\nproject = hopsworks.login()\n\ngit_api = project.get_git_api()\n</code></pre>"},{"location":"user_guides/projects/git/configure_git_provider/#step-2-configure-git-provider","title":"Step 2: Configure git provider","text":"<pre><code>PROVIDER=\"GitHub\"\nGITHUB_USER=\"my_user\"\nAPI_TOKEN=\"my_token\"\n\ngit_api.set_provider(PROVIDER, GITHUB_USER, API_TOKEN)\n</code></pre>"},{"location":"user_guides/projects/git/configure_git_provider/#api-reference","title":"API Reference","text":"<p>GitProvider</p>"},{"location":"user_guides/projects/git/configure_git_provider/#going-further","title":"Going Further","text":"<p>You can now use the credentials to clone a repository from the configured provider.</p>"},{"location":"user_guides/projects/git/repository_actions/","title":"Repository actions","text":""},{"location":"user_guides/projects/git/repository_actions/#introduction","title":"Introduction","text":"<p>This section explains the git operations or commands you can perform on hopsworks git repositories. These commands include commit, pull, push, create branches and many more.</p> <p>Repository permissions</p> <p>Git repositories are private. Only the owner of the repository can perform git actions on the repository such as commit, push, pull e.t.c.</p>"},{"location":"user_guides/projects/git/repository_actions/#ui","title":"UI","text":"<p>The operations to perform on the cloned repository can be found in the dropdown as shown below.</p> <p> Repository actions </p> <p>Note that some repository actions will require the username and token to be configured first depending on the provider. For example to be able to perform a push action in any repository, you must configure the provider for the repository first. To be able to perform a pull action for the for a GitLab repository, you must configure the GitLab provider first. You will see the dialog below in the case you need to configure the provider first to perform the repository action.</p> <p> Configure provider prompt </p>"},{"location":"user_guides/projects/git/repository_actions/#read-only-repositories","title":"Read only repositories","text":"<p>In read only repositories, the following actions are disabled: commit, push and file checkout. The read only property can be enabled or disabled in the Cluster settings &gt; Configuration, by updating the <code>enable_read_only_git_repositories</code> variable to true or false. Note that you need administrator privileges to update this property.</p>"},{"location":"user_guides/projects/git/repository_actions/#code","title":"Code","text":"<p>You can also perform the repository actions using the hopsworks git API in python. </p>"},{"location":"user_guides/projects/git/repository_actions/#step-1-get-the-git-api","title":"Step 1: Get the git API","text":"<pre><code>import hopsworks\n\nproject = hopsworks.login()\n\ngit_api = project.get_git_api()\n</code></pre>"},{"location":"user_guides/projects/git/repository_actions/#step-2-get-the-git-repository","title":"Step 2: Get the git repository","text":"<pre><code>git_repo = git_api.get_repo(REPOSITORY_NAME)\n</code></pre>"},{"location":"user_guides/projects/git/repository_actions/#step-3-perform-the-git-repository-action-eg-commit","title":"Step 3: Perform the git repository action e.g commit","text":"<pre><code>git_repo = git_api.commit(\"Test commit\")\n</code></pre>"},{"location":"user_guides/projects/git/repository_actions/#api-reference","title":"API Reference","text":"<p>Api reference for repository actions is available here: GitRepo</p>"},{"location":"user_guides/projects/iam_role/iam_role_chaining/","title":"How To Use AWS IAM Roles on EC2 instances","text":""},{"location":"user_guides/projects/iam_role/iam_role_chaining/#introduction","title":"Introduction","text":"<p>When deploying Hopsworks on EC2 instances you might need to assume different roles to access resources on AWS.  These roles can be configured in AWS and mapped to a project in Hopsworks.</p>"},{"location":"user_guides/projects/iam_role/iam_role_chaining/#prerequisites","title":"Prerequisites","text":"<p>Before you begin this guide you'll need the following:</p> <ul> <li>A Hopsworks cluster running on EC2.</li> <li>Role chaining setup in AWS.</li> <li>Configure role mappings in Hopsworks. For a guide on how to configure this see AWS IAM Role Chaining.</li> </ul>"},{"location":"user_guides/projects/iam_role/iam_role_chaining/#ui","title":"UI","text":"<p>In this guide, you will learn how to use a mapped IAM role in your project.</p>"},{"location":"user_guides/projects/iam_role/iam_role_chaining/#step-1-navigate-to-your-projects-iam-role-chaining-tab","title":"Step 1: Navigate to your project's IAM Role Chaining tab","text":"<p>In the Project Settings page you can find the IAM Role Chaining section showing a list of all IAM roles mapped to your project.</p> Role Chaining"},{"location":"user_guides/projects/iam_role/iam_role_chaining/#step-2-use-the-iam-role","title":"Step 2: Use the IAM role","text":"<p>You can now use the IAM roles listed in your project when creating a storage connector with Temporary Credentials.</p>"},{"location":"user_guides/projects/jobs/notebook_job/","title":"How To Run A Jupyter Notebook Job","text":""},{"location":"user_guides/projects/jobs/notebook_job/#introduction","title":"Introduction","text":"<p>All members of a project in Hopsworks can launch the following types of applications through a project's Jobs service:</p> <ul> <li>Python</li> <li>Apache Spark</li> </ul> <p>Launching a job of any type is very similar process, what mostly differs between job types is the various configuration parameters each job type comes with. After following this guide you will be able to create a Jupyter Notebook job.</p>"},{"location":"user_guides/projects/jobs/notebook_job/#ui","title":"UI","text":""},{"location":"user_guides/projects/jobs/notebook_job/#step-1-jobs-overview","title":"Step 1: Jobs overview","text":"<p>The image below shows the Jobs overview page in Hopsworks and is accessed by clicking <code>Jobs</code> in the sidebar.</p> <p> Jobs overview </p>"},{"location":"user_guides/projects/jobs/notebook_job/#step-2-create-new-job-dialog","title":"Step 2: Create new job dialog","text":"<p>Click <code>New Job</code> and the following dialog will appear.</p> <p> Create new job dialog </p>"},{"location":"user_guides/projects/jobs/notebook_job/#step-3-set-the-job-type","title":"Step 3: Set the job type","text":"<p>By default, the dialog will create a Spark job. To instead configure a Jupyter Notebook job, select <code>PYTHON</code>.</p> <p> Select Python job type </p>"},{"location":"user_guides/projects/jobs/notebook_job/#step-4-set-the-notebook","title":"Step 4: Set the notebook","text":"<p>Next step is to select the Jupyter Notebook to run. You can either select <code>From project</code>, if the file was previously uploaded to Hopsworks, or <code>Upload new file</code> which lets you select a file from your local filesystem as demonstrated below. By default, the job name is the same as the file name, but you can customize it as shown. </p> <p> Configure program </p> <p>Then click <code>Create job</code> to create the job.</p>"},{"location":"user_guides/projects/jobs/notebook_job/#step-5-optional-set-the-jupyter-notebook-arguments","title":"Step 5 (optional): Set the Jupyter Notebook arguments","text":"<p>In the job settings, you can specify arguments for your notebook script. Arguments must be in the format of <code>-p arg1 value1 -p arg2 value2</code>. For each argument, you must first provide <code>-p</code>, followed by the parameter name (e.g. <code>arg1</code>), followed by its value (e.g. <code>value1</code>). The next step is to read the arguments in the notebook which is explained in this guide.</p> <p> Configure notebook arguments </p>"},{"location":"user_guides/projects/jobs/notebook_job/#step-6-optional-additional-configuration","title":"Step 6 (optional): Additional configuration","text":"<p>It is possible to also set following configuration settings for a <code>PYTHON</code> job.</p> <ul> <li><code>Environment</code>: The python environment to use</li> <li><code>Container memory</code>: The amount of memory in MB to be allocated to the Jupyter Notebook script</li> <li><code>Container cores</code>: The number of cores to be allocated for the Jupyter Notebook script</li> <li><code>Additional files</code>: List of files that will be locally accessible in the working directory of the application. Only recommended to use if project datasets are not mounted under <code>/hopsfs</code>. You can always modify the arguments in the job settings.</li> </ul> <p> Set the job type </p>"},{"location":"user_guides/projects/jobs/notebook_job/#step-7-execute-the-job","title":"Step 7: Execute the job","text":"<p>Now click the <code>Run</code> button to start the execution of the job. You will be redirected to the <code>Executions</code> page where you can see the list of all executions.</p> <p> Start job execution </p>"},{"location":"user_guides/projects/jobs/notebook_job/#step-8-visualize-output-notebook","title":"Step 8: Visualize output notebook","text":"<p>Once the execution is finished, click <code>Logs</code> and then <code>notebook out</code> to see the logs for the execution.</p> <p> Visualize output notebook </p> <p>You can directly edit and save the output notebook by clicking <code>Open Notebook</code>.</p>"},{"location":"user_guides/projects/jobs/notebook_job/#code","title":"Code","text":""},{"location":"user_guides/projects/jobs/notebook_job/#step-1-upload-the-jupyter-notebook-script","title":"Step 1: Upload the Jupyter Notebook script","text":"<p>This snippet assumes the Jupyter Notebook script is in the current working directory and named <code>notebook.ipynb</code>. </p> <p>It will upload the Jupyter Notebook script to the <code>Resources</code> dataset in your project.</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\ndataset_api = project.get_dataset_api()\n\nuploaded_file_path = dataset_api.upload(\"notebook.ipynb\", \"Resources\")\n</code></pre>"},{"location":"user_guides/projects/jobs/notebook_job/#step-2-create-jupyter-notebook-job","title":"Step 2: Create Jupyter Notebook job","text":"<p>In this snippet we get the <code>JobsApi</code> object to get the default job configuration for a <code>PYTHON</code> job, set the jupyter notebook file and override the environment to run in, and finally create the <code>Job</code> object.</p> <pre><code>jobs_api = project.get_job_api()\n\nnotebook_job_config = jobs_api.get_configuration(\"PYTHON\")\n\n# Set the application file\nnotebook_job_config['appPath'] = uploaded_file_path\n\n# Override the python job environment\nnotebook_job_config['environmentName'] = \"python-feature-pipeline\"\n\njob = jobs_api.create_job(\"notebook_job\", notebook_job_config)\n</code></pre>"},{"location":"user_guides/projects/jobs/notebook_job/#step-3-execute-the-job","title":"Step 3: Execute the job","text":"<p>In this code snippet, we execute the job with arguments and wait until it reaches a terminal state.</p> <pre><code># Run the job\nexecution = job.run(args='-p a 2 -p b 5', await_termination=True)\n</code></pre>"},{"location":"user_guides/projects/jobs/notebook_job/#configuration","title":"Configuration","text":"<p>The following table describes the JSON payload returned by <code>jobs_api.get_configuration(\"PYTHON\")</code></p> Field Type Description Default <code>type</code> string Type of the job configuration <code>\"pythonJobConfiguration\"</code> <code>appPath</code> string Project path to notebook (e.g <code>Resources/foo.ipynb</code>) <code>null</code> <code>environmentName</code> string Name of the python environment <code>\"pandas-training-pipeline\"</code> <code>resourceConfig.cores</code> number (float) Number of CPU cores to be allocated <code>1.0</code> <code>resourceConfig.memory</code> number (int) Number of MBs to be allocated <code>2048</code> <code>resourceConfig.gpus</code> number (int) Number of GPUs to be allocated <code>0</code> <code>logRedirection</code> boolean Whether logs are redirected <code>true</code> <code>jobType</code> string Type of job <code>\"PYTHON\"</code>"},{"location":"user_guides/projects/jobs/notebook_job/#accessing-project-data","title":"Accessing project data","text":"<p>Recommended approach if <code>/hopsfs</code> is mounted</p> <p>If your Hopsworks installation is configured to mount the project datasets under <code>/hopsfs</code>, which it is in most cases, then please refer to this section instead of the <code>Additional files</code> property to reference file resources.</p>"},{"location":"user_guides/projects/jobs/notebook_job/#absolute-paths","title":"Absolute paths","text":"<p>The project datasets are mounted under <code>/hopsfs</code>, so you can access <code>data.csv</code> from the <code>Resources</code> dataset using <code>/hopsfs/Resources/data.csv</code> in your notebook.</p>"},{"location":"user_guides/projects/jobs/notebook_job/#relative-paths","title":"Relative paths","text":"<p>The notebook's working directory is the folder it is located in. For example, if it is located in the <code>Resources</code> dataset, and you have a file named <code>data.csv</code> in that dataset, you simply access it using <code>data.csv</code>. Also, if you write a local file, for example <code>output.txt</code>, it will be saved in the <code>Resources</code> dataset.</p>"},{"location":"user_guides/projects/jobs/notebook_job/#api-reference","title":"API Reference","text":"<p>Jobs</p> <p>Executions</p>"},{"location":"user_guides/projects/jobs/pyspark_job/","title":"How To Run A PySpark Job","text":""},{"location":"user_guides/projects/jobs/pyspark_job/#introduction","title":"Introduction","text":"<p>All members of a project in Hopsworks can launch the following types of applications through a project's Jobs service:</p> <ul> <li>Python</li> <li>Apache Spark</li> </ul> <p>Launching a job of any type is very similar process, what mostly differs between job types is the various configuration parameters each job type comes with. Hopsworks clusters support scheduling to run jobs on a regular basis, e.g backfilling a Feature Group by running your feature engineering pipeline nightly. Scheduling can be done both through the UI and the python API, checkout our Scheduling guide.</p> <p>PySpark program can either be a <code>.py</code> script or a <code>.ipynb</code> file, however be mindful of how to access/create the spark session based on the extension you provide.</p> <p>Instantiate the SparkSession</p> <p>For a <code>.py</code> file, remember to instantiate the SparkSession i.e <code>spark=SparkSession.builder.getOrCreate()</code></p> <p>For a <code>.ipynb</code> file, the <code>SparkSession</code> is already available as <code>spark</code> when the job is started.</p>"},{"location":"user_guides/projects/jobs/pyspark_job/#ui","title":"UI","text":""},{"location":"user_guides/projects/jobs/pyspark_job/#step-1-jobs-overview","title":"Step 1: Jobs overview","text":"<p>The image below shows the Jobs overview page in Hopsworks and is accessed by clicking <code>Jobs</code> in the sidebar.</p> <p> Jobs overview </p>"},{"location":"user_guides/projects/jobs/pyspark_job/#step-2-create-new-job-dialog","title":"Step 2: Create new job dialog","text":"<p>Click <code>New Job</code> and the following dialog will appear.</p> <p> Create new job dialog </p>"},{"location":"user_guides/projects/jobs/pyspark_job/#step-3-set-the-job-type","title":"Step 3: Set the job type","text":"<p>By default, the dialog will create a Spark job. Make sure <code>SPARK</code> is chosen.</p>"},{"location":"user_guides/projects/jobs/pyspark_job/#step-4-set-the-script","title":"Step 4: Set the script","text":"<p>Next step is to select the program to run. You can either select <code>From project</code>, if the file was previously uploaded to Hopsworks, or <code>Upload new file</code> which lets you select a file from your local filesystem as demonstrated below. By default, the job name is the same as the file name, but you can customize it as shown. </p> <p> Configure program </p> <p>Then click <code>Create job</code> to create the job.</p>"},{"location":"user_guides/projects/jobs/pyspark_job/#step-5-optional-set-the-pyspark-script-arguments","title":"Step 5 (optional): Set the PySpark script arguments","text":"<p>In the job settings, you can specify arguments for your PySpark script. Remember to handle the arguments inside your PySpark script.</p> <p> Configure PySpark script arguments </p>"},{"location":"user_guides/projects/jobs/pyspark_job/#step-6-optional-advanced-configuration","title":"Step 6 (optional): Advanced configuration","text":"<p>Resource allocation for the Spark driver and executors can be configured, also the number of executors and whether dynamic execution should be enabled.</p> <ul> <li> <p><code>Environment</code>: The python environment to use, must be based on <code>spark-feature-pipeline</code></p> </li> <li> <p><code>Driver memory</code>: Number of cores to allocate for the Spark driver</p> </li> <li> <p><code>Driver virtual cores</code>: Number of MBs to allocate for the Spark driver</p> </li> <li> <p><code>Executor memory</code>: Number of cores to allocate for each Spark executor</p> </li> <li> <p><code>Executor virtual cores</code>: Number of MBs to allocate for each Spark executor</p> </li> <li> <p><code>Dynamic/Static</code>: Run the Spark application in static or dynamic allocation mode (see spark docs for details).</p> </li> </ul> <p> Resource configuration for the PySpark job </p> <p>Additional files or dependencies required for the Spark job can be configured.</p> <ul> <li> <p><code>Additional archives</code>: List of archives to be extracted into the working directory of each executor.</p> </li> <li> <p><code>Additional jars</code>: List of jars to be placed in the working directory of each executor.</p> </li> <li> <p><code>Additional python dependencies</code>: List of python files and archives to be placed on each executor and added to PATH.</p> </li> <li> <p><code>Additional files</code>: List of files to be placed in the working directory of each executor.</p> </li> </ul> <p> File configuration for the PySpark job </p> <p>Line-separates properties to be set for the Spark application. For example, changing the configuration variables for the Kryo Serializer or setting environment variables for the driver, you can set the properties as shown below.</p> <p> Additional Spark configuration </p>"},{"location":"user_guides/projects/jobs/pyspark_job/#step-7-execute-the-job","title":"Step 7: Execute the job","text":"<p>Now click the <code>Run</code> button to start the execution of the job. You will be redirected to the <code>Executions</code> page where you can see the list of all executions.</p> <p> Start job execution </p>"},{"location":"user_guides/projects/jobs/pyspark_job/#step-8-application-logs","title":"Step 8: Application logs","text":"<p>To monitor logs while the execution is running, click <code>Spark UI</code> to open the Spark UI in a separate tab. </p> <p>Once the execution is finished, you can click on <code>Logs</code> to see the full logs for execution.</p> <p> Access Spark logs </p>"},{"location":"user_guides/projects/jobs/pyspark_job/#code","title":"Code","text":""},{"location":"user_guides/projects/jobs/pyspark_job/#step-1-upload-the-pyspark-program","title":"Step 1: Upload the PySpark program","text":"<p>This snippet assumes the program to run is in the current working directory and named <code>script.py</code>. </p> <p>It will upload the python script to the <code>Resources</code> dataset in your project.</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\ndataset_api = project.get_dataset_api()\n\nuploaded_file_path = dataset_api.upload(\"script.py\", \"Resources\")\n</code></pre>"},{"location":"user_guides/projects/jobs/pyspark_job/#step-2-create-pyspark-job","title":"Step 2: Create PySpark job","text":"<p>In this snippet we get the <code>JobsApi</code> object to get the default job configuration for a <code>PYSPARK</code> job, set the pyspark script and override the environment to run in, and finally create the <code>Job</code> object.</p> <pre><code>jobs_api = project.get_job_api()\n\nspark_config = jobs_api.get_configuration(\"PYSPARK\")\n\n# Set the application file\nspark_config['appPath'] = uploaded_file_path\n\n# Override the python job environment\nspark_config['environmentName'] = \"spark-feature-pipeline\"\n\njob = jobs_api.create_job(\"pyspark_job\", spark_config)\n</code></pre>"},{"location":"user_guides/projects/jobs/pyspark_job/#step-3-execute-the-job","title":"Step 3: Execute the job","text":"<p>In this snippet we execute the job synchronously, that is wait until it reaches a terminal state, and then download and print the logs.</p> <pre><code>execution = job.run(await_termination=True)\n\nout, err = execution.download_logs()\n\nf_out = open(out, \"r\")\nprint(f_out.read())\n\nf_err = open(err, \"r\")\nprint(f_err.read())\n</code></pre>"},{"location":"user_guides/projects/jobs/pyspark_job/#configuration","title":"Configuration","text":"<p>The following table describes the JSON payload returned by <code>jobs_api.get_configuration(\"PYSPARK\")</code></p> Field Type Description Default <code>type</code> string Type of the job configuration <code>\"sparkJobConfiguration\"</code> <code>appPath</code> string Project path to script (e.g <code>Resources/foo.py</code>) <code>null</code> <code>environmentName</code> string Name of the project spark environment <code>\"spark-feature-pipeline\"</code> <code>spark.driver.cores</code> number (float) Number of CPU cores allocated for the driver <code>1.0</code> <code>spark.driver.memory</code> number (int) Memory allocated for the driver (in MB) <code>2048</code> <code>spark.executor.instances</code> number (int) Number of executor instances <code>1</code> <code>spark.executor.cores</code> number (float) Number of CPU cores per executor <code>1.0</code> <code>spark.executor.memory</code> number (int) Memory allocated per executor (in MB) <code>4096</code> <code>spark.dynamicAllocation.enabled</code> boolean Enable dynamic allocation of executors <code>true</code> <code>spark.dynamicAllocation.minExecutors</code> number (int) Minimum number of executors with dynamic allocation <code>1</code> <code>spark.dynamicAllocation.maxExecutors</code> number (int) Maximum number of executors with dynamic allocation <code>2</code> <code>spark.dynamicAllocation.initialExecutors</code> number (int) Initial number of executors with dynamic allocation <code>1</code> <code>spark.blacklist.enabled</code> boolean Whether executor/node blacklisting is enabled <code>false</code>"},{"location":"user_guides/projects/jobs/pyspark_job/#accessing-project-data","title":"Accessing project data","text":""},{"location":"user_guides/projects/jobs/pyspark_job/#read-directly-from-the-filesystem-recommended","title":"Read directly from the filesystem (recommended)","text":"<p>To read a dataset in your project using Spark, use the full filesystem path where the data is stored. For example, to read a CSV file named <code>data.csv</code> located in the <code>Resources</code> dataset of a project called <code>my_project</code>:</p> <pre><code>df = spark.read.csv(\"/Projects/my_project/Resources/data.csv\", header=True, inferSchema=True)\ndf.show()\n</code></pre>"},{"location":"user_guides/projects/jobs/pyspark_job/#additional-files","title":"Additional files","text":"<p>Different file types can be attached to the spark job and made available in the <code>/srv/hops/artifacts</code> folder when the PySpark job is started. This configuration is mainly useful when you need to add additional setup, such as jars that needs to be added to the CLASSPATH.</p> <p>When reading data in your Spark job it is recommended to use the Spark read API as previously demonstrated, since this reads from the filesystem directly, whereas <code>Additional files</code> configuration options will download the files in its entirety and is not a scalable option.</p>"},{"location":"user_guides/projects/jobs/pyspark_job/#api-reference","title":"API Reference","text":"<p>Jobs</p> <p>Executions</p>"},{"location":"user_guides/projects/jobs/python_job/","title":"How To Run A Python Job","text":""},{"location":"user_guides/projects/jobs/python_job/#introduction","title":"Introduction","text":"<p>All members of a project in Hopsworks can launch the following types of applications through a project's Jobs service:</p> <ul> <li>Python</li> <li>Apache Spark</li> </ul> <p>Launching a job of any type is very similar process, what mostly differs between job types is the various configuration parameters each job type comes with. Hopsworks support scheduling jobs to run on a regular basis, e.g backfilling a Feature Group by running your feature engineering pipeline nightly. Scheduling can be done both through the UI and the python API, checkout our Scheduling guide.</p>"},{"location":"user_guides/projects/jobs/python_job/#ui","title":"UI","text":""},{"location":"user_guides/projects/jobs/python_job/#step-1-jobs-overview","title":"Step 1: Jobs overview","text":"<p>The image below shows the Jobs overview page in Hopsworks and is accessed by clicking <code>Jobs</code> in the sidebar.</p> <p> Jobs overview </p>"},{"location":"user_guides/projects/jobs/python_job/#step-2-create-new-job-dialog","title":"Step 2: Create new job dialog","text":"<p>Click <code>New Job</code> and the following dialog will appear.</p> <p> Create new job dialog </p>"},{"location":"user_guides/projects/jobs/python_job/#step-3-set-the-job-type","title":"Step 3: Set the job type","text":"<p>By default, the dialog will create a Spark job. To instead configure a Python job, select <code>PYTHON</code>.</p> <p> Select Python job type </p>"},{"location":"user_guides/projects/jobs/python_job/#step-4-set-the-script","title":"Step 4: Set the script","text":"<p>Next step is to select the python script to run. You can either select <code>From project</code>, if the file was previously uploaded to Hopsworks, or <code>Upload new file</code> which lets you select a file from your local filesystem as demonstrated below. By default, the job name is the same as the file name, but you can customize it as shown. </p> <p> Configure program </p>"},{"location":"user_guides/projects/jobs/python_job/#step-5-optional-set-the-python-script-arguments","title":"Step 5 (optional): Set the Python script arguments","text":"<p>In the job settings, you can specify arguments for your Python script. Remember to handle the arguments inside your Python script.</p> <p> Configure Python script arguments </p>"},{"location":"user_guides/projects/jobs/python_job/#step-6-optional-additional-configuration","title":"Step 6 (optional): Additional configuration","text":"<p>It is possible to also set following configuration settings for a <code>PYTHON</code> job.</p> <ul> <li><code>Environment</code>: The python environment to use</li> <li><code>Container memory</code>: The amount of memory in MB to be allocated to the Python script</li> <li><code>Container cores</code>: The number of cores to be allocated for the Python script</li> <li><code>Additional files</code>: List of files that will be locally accessible in the working directory of the application. Only recommended to use if project datasets are not mounted under <code>/hopsfs</code>.   You can always modify the arguments in the job settings.</li> </ul> <p> Additional configuration </p>"},{"location":"user_guides/projects/jobs/python_job/#step-7-execute-the-job","title":"Step 7: Execute the job","text":"<p>Now click the <code>Run</code> button to start the execution of the job. You will be redirected to the <code>Executions</code> page where you can see the list of all executions.</p> <p>Once the execution is finished, click on <code>Logs</code> to see the logs for the execution.</p> <p> Start job execution </p>"},{"location":"user_guides/projects/jobs/python_job/#code","title":"Code","text":""},{"location":"user_guides/projects/jobs/python_job/#step-1-upload-the-python-script","title":"Step 1: Upload the Python script","text":"<p>This snippet assumes the python script is in the current working directory and named <code>script.py</code>. </p> <p>It will upload the python script to the <code>Resources</code> dataset in your project.</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\ndataset_api = project.get_dataset_api()\n\nuploaded_file_path = dataset_api.upload(\"script.py\", \"Resources\")\n</code></pre>"},{"location":"user_guides/projects/jobs/python_job/#step-2-create-python-job","title":"Step 2: Create Python job","text":"<p>In this snippet we get the <code>JobsApi</code> object to get the default job configuration for a <code>PYTHON</code> job, set the python script and override the environment to run in, and finally create the <code>Job</code> object.</p> <pre><code>jobs_api = project.get_job_api()\n\npy_job_config = jobs_api.get_configuration(\"PYTHON\")\n\n# Set the application file\npy_job_config['appPath'] = uploaded_file_path\n\n# Override the python job environment\npy_job_config['environmentName'] = \"python-feature-pipeline\"\n\njob = jobs_api.create_job(\"py_job\", py_job_config)\n</code></pre>"},{"location":"user_guides/projects/jobs/python_job/#step-3-execute-the-job","title":"Step 3: Execute the job","text":"<p>In this snippet we execute the job synchronously, that is wait until it reaches a terminal state, and then download and print the logs.</p> <pre><code># Run the job\nexecution = job.run(await_termination=True)\n\n# Download logs\nout, err = execution.download_logs()\n\nf_out = open(out, \"r\")\nprint(f_out.read())\n\nf_err = open(err, \"r\")\nprint(f_err.read())\n</code></pre>"},{"location":"user_guides/projects/jobs/python_job/#configuration","title":"Configuration","text":"<p>The following table describes the JSON payload returned by <code>jobs_api.get_configuration(\"PYTHON\")</code></p> Field Type Description Default <code>type</code> string Type of the job configuration <code>\"pythonJobConfiguration\"</code> <code>appPath</code> string Project path to script (e.g <code>Resources/foo.py</code>) <code>null</code> <code>environmentName</code> string Name of the project python environment <code>\"pandas-training-pipeline\"</code> <code>resourceConfig.cores</code> number (float) Number of CPU cores to be allocated <code>1.0</code> <code>resourceConfig.memory</code> number (int) Number of MBs to be allocated <code>2048</code> <code>resourceConfig.gpus</code> number (int) Number of GPUs to be allocated <code>0</code> <code>logRedirection</code> boolean Whether logs are redirected <code>true</code> <code>jobType</code> string Type of job <code>\"PYTHON\"</code>"},{"location":"user_guides/projects/jobs/python_job/#accessing-project-data","title":"Accessing project data","text":"<p>Recommended approach if <code>/hopsfs</code> is mounted</p> <p>If your Hopsworks installation is configured to mount the project datasets under <code>/hopsfs</code>, which it is in most cases, then please refer to this section instead of the <code>Additional files</code> property to reference file resources.</p>"},{"location":"user_guides/projects/jobs/python_job/#absolute-paths","title":"Absolute paths","text":"<p>The project datasets are mounted under <code>/hopsfs</code>, so you can access <code>data.csv</code> from the <code>Resources</code> dataset using <code>/hopsfs/Resources/data.csv</code> in your script.</p>"},{"location":"user_guides/projects/jobs/python_job/#relative-paths","title":"Relative paths","text":"<p>The script's working directory is the folder it is located in. For example, if it is located in the <code>Resources</code> dataset, and you have a file named <code>data.csv</code> in that dataset, you simply access it using <code>data.csv</code>. Also, if you write a local file, for example <code>output.txt</code>, it will be saved in the <code>Resources</code> dataset.</p>"},{"location":"user_guides/projects/jobs/python_job/#api-reference","title":"API Reference","text":"<p>Jobs</p> <p>Executions</p>"},{"location":"user_guides/projects/jobs/ray_job/","title":"How To Run A Ray Job","text":""},{"location":"user_guides/projects/jobs/ray_job/#introduction","title":"Introduction","text":"<p>All members of a project in Hopsworks can launch the following types of applications through a project's Jobs service:</p> <ul> <li>Python</li> <li>Apache Spark</li> <li>Ray </li> </ul> <p>Launching a job of any type is very similar process, what mostly differs between job types is the various configuration parameters each job type comes with. Hopsworks support scheduling to run jobs on a regular basis, e.g backfilling a Feature Group by running your feature engineering pipeline nightly. Scheduling can be done both through the UI and the python API, checkout our Scheduling guide.</p> <p>Enable Ray</p> <p>Support for Ray needs to be explicitly enabled by adding the following option in the <code>values.yaml</code> file for the deployment:</p> <pre><code>global:\n  ray:\n    enabled: true\n</code></pre>"},{"location":"user_guides/projects/jobs/ray_job/#ui","title":"UI","text":""},{"location":"user_guides/projects/jobs/ray_job/#step-1-jobs-overview","title":"Step 1: Jobs overview","text":"<p>The image below shows the Jobs overview page in Hopsworks and is accessed by clicking <code>Jobs</code> in the sidebar.</p> <p> Jobs overview </p>"},{"location":"user_guides/projects/jobs/ray_job/#step-2-create-new-job-dialog","title":"Step 2: Create new job dialog","text":"<p>Click <code>New Job</code> and the following dialog will appear.</p> <p> Create new job dialog </p>"},{"location":"user_guides/projects/jobs/ray_job/#step-3-set-the-job-type","title":"Step 3: Set the job type","text":"<p>By default, the dialog will create a Spark job. Make sure <code>RAY</code> is chosen.</p>"},{"location":"user_guides/projects/jobs/ray_job/#step-4-set-the-script","title":"Step 4: Set the script","text":"<p>Next step is to select the program to run. You can either select <code>From project</code>, if the file was previously uploaded to Hopsworks, or <code>Upload new file</code> which lets you select a file from your local filesystem as demonstrated below. After that set the name for the job. By default, the job name is the same as the file name, but you can customize it here.</p> <p> Configure program </p>"},{"location":"user_guides/projects/jobs/ray_job/#step-5-optional-advanced-configuration","title":"Step 5 (optional): Advanced configuration","text":"<p>Resource allocation for the Driver and Workers can be configured.</p> <p>Using the resources in the Ray script</p> <p>The resource configurations describe the cluster that will be provisioned when launching the Ray job. User can still  provide extra configurations in the job script using <code>ScalingConfig</code>, i.e. <code>ScalingConfig(num_workers=4, trainer_resources={\"CPU\": 1}, use_gpu=True)</code>.</p> <ul> <li> <p><code>Driver memory</code>: Memory in MBs to allocate for Driver</p> </li> <li> <p><code>Driver virtual cores</code>: Number of cores to allocate for the Driver</p> </li> <li> <p><code>Worker memory</code>: Memory in MBs to allocate for each worker</p> </li> <li> <p><code>Worker cores</code>: Number of cores to allocate for each worker</p> </li> <li> <p><code>Min workers</code>: Minimum number of workers to start with</p> </li> <li> <p><code>Max workers</code>: Maximum number of workers to scale up to</p> </li> </ul> <p> Resource configuration for the Ray Job </p> <p>Runtime environment and Additional files required for the Ray job can also be provided.</p> <ul> <li> <p><code>Runtime Environment (Optional)</code>:  A runtime environment describes the dependencies required for the Ray job including files, packages, environment variables, and more. This is useful when you need to install specific packages and set environment variables for this particular Ray job. It should be provided as a YAML file. You can select the file from the project or upload a new one.</p> </li> <li> <p><code>Additional files</code>: List of other files required for the Ray job. These files will be placed in <code>/srv/hops/ray/job</code>.</p> </li> </ul> <p> Runtime configuration and additional files for Ray job </p>"},{"location":"user_guides/projects/jobs/ray_job/#step-6-execute-the-job","title":"Step 6: Execute the job","text":"<p>Now click the <code>Run</code> button to start the execution of the job, and then click on <code>Executions</code> to see the list of all executions.</p> <p> Start job execution </p>"},{"location":"user_guides/projects/jobs/ray_job/#ray-dashboard","title":"Ray Dashboard","text":"<p>When the Ray job is running, you can access the Ray dashboard to monitor the job. The Ray dashboard is accessible from the  <code>Executions</code> page. Please note that the Ray dashboard is only available when the job execution is running. In the Ray Dashboard,  you can monitor the resources used by the job, the number of workers, logs, and the tasks that are running.</p> <p> Access Ray Dashboard </p>"},{"location":"user_guides/projects/jobs/ray_job/#step-9-application-logs","title":"Step 9: Application logs","text":"<p>Once the execution is finished, you can click on <code>Logs</code> to see the full logs for execution.</p> <p> Access Ray job execution logs </p>"},{"location":"user_guides/projects/jobs/ray_job/#code","title":"Code","text":""},{"location":"user_guides/projects/jobs/ray_job/#step-1-upload-the-ray-script","title":"Step 1: Upload the Ray script","text":"<p>This snippet assumes the Ray program is in the current working directory and named <code>ray_job.py</code>. If the file is already in the project, you can skip this step.</p> <p>It will upload the jar to the <code>Resources</code> dataset in your project.</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\ndataset_api = project.get_dataset_api()\n\nuploaded_file_path = dataset_api.upload(\"ray_job.py\", \"Resources\")\n</code></pre>"},{"location":"user_guides/projects/jobs/ray_job/#step-2-create-ray-job","title":"Step 2: Create Ray job","text":"<p>In this snippet we get the <code>JobsApi</code> object to get the default job configuration for a <code>RAY</code> job, set the python script to run and create the <code>Job</code> object.</p> <pre><code>jobs_api = project.get_job_api()\n\nray_config = jobs_api.get_configuration(\"RAY\")\n\nray_config['appPath'] = uploaded_file_path\nray_config['environmentName'] = \"ray-training-pipeline\"\nray_config['driverCores'] = 2\nray_config['driverMemory'] = 2048\nray_config['workerCores'] = 2\nray_config['workerMemory'] = 4096\nray_config['minWorkers'] = 1\nray_config['maxWorkers'] = 4\n\njob = jobs_api.create_job(\"ray_job\", ray_config)\n</code></pre>"},{"location":"user_guides/projects/jobs/ray_job/#step-3-execute-the-job","title":"Step 3: Execute the job","text":"<p>In this snippet we execute the job synchronously, that is wait until it reaches a terminal state, and then download and print the logs.</p> <pre><code>execution = job.run(await_termination=True)\n\nout, err = execution.download_logs()\n\nf_out = open(out, \"r\")\nprint(f_out.read())\n\nf_err = open(err, \"r\")\nprint(f_err.read())\n</code></pre>"},{"location":"user_guides/projects/jobs/ray_job/#accessing-project-data","title":"Accessing project data","text":"<p>The project datasets are mounted under <code>/hopsfs</code>, so you can access <code>data.csv</code> from the <code>Resources</code> dataset using <code>/hopsfs/Resources/data.csv</code> in your script.</p>"},{"location":"user_guides/projects/jobs/ray_job/#api-reference","title":"API Reference","text":"<p>Jobs</p> <p>Executions</p>"},{"location":"user_guides/projects/jobs/schedule_job/","title":"How To Schedule a Job","text":""},{"location":"user_guides/projects/jobs/schedule_job/#introduction","title":"Introduction","text":"<p>Hopsworks clusters can run jobs on a schedule, allowing you to automate the execution. Whether you need to backfill your feature groups on a nightly basis or run a model training pipeline every week, the Hopsworks scheduler will help you automate these tasks. Each job can be configured to have a single schedule. For more advanced use cases, Hopsworks integrates with any DAG manager and directly with the open-source Apache Airflow, check out our Airflow Guide.</p> <p>Schedules can be defined using the drop down menus in the UI or a Quartz cron expression. </p> <p>Schedule frequency</p> <p>The Hopsworks scheduler runs every minute. As such, the scheduling frequency should be of at least 1 minute.</p> <p>Parallel executions</p> <p>If a job execution needs to be scheduled, the scheduler will first check that there are no active executions for that job. If there is an execution running, the scheduler will postpone the execution until the running one is done. </p>"},{"location":"user_guides/projects/jobs/schedule_job/#ui","title":"UI","text":""},{"location":"user_guides/projects/jobs/schedule_job/#scheduling-jobs","title":"Scheduling Jobs","text":"<p>You can define a schedule for a job during the creation of the job itself or after the job has been created from the job overview UI. </p> <p> Schedule a Job </p> <p>The add schedule prompt requires you to select a frequency either through the drop down menus or by using a cron expression. You can also provide a start time to specify from when the schedule should have effect.  The start time can also be in the past. If that's the case, the scheduler will backfill the executions from the specified start time. As mentioned above, the execution backfilling will happen one execution at the time.</p> <p>You can optionally provide an end date time to specify until when the scheduling should continue. The end time can also be in the past.</p> <p>In the job overview, you can see the current scheduling configuration, whether or not it is enabled and when the next execution is planned for.</p> <p>All times will be considered as UTC time.</p> <p> Job scheduling overview </p>"},{"location":"user_guides/projects/jobs/schedule_job/#job-argument","title":"Job argument","text":"<p>When a job execution is triggered by the scheduler, a <code>-start_time</code> argument is added to the job arguments. The <code>-start_time</code> value will be the time of the scheduled execution in UTC in the ISO-8601 format (e.g.: <code>-start_time 2023-08-19T18:00:00Z</code>).</p> <p>The <code>-start_time</code> value passed as argument represents the time when the execution was scheduled, not when the execution was started. For example, if the scheduled execution time was in the past (e.g. in the case of backfilling), the <code>-start_time</code> passed to the execution is the time in the past, not the current time when the execution is running.  Similarly, if the scheduler was not running for a period of time, when it comes back online, it will start the executions it missed to schedule while offline. Even in that case, the <code>-start_time</code> value will contain the time at which the execution was supposed to be started, not the current time.</p>"},{"location":"user_guides/projects/jobs/schedule_job/#disable-enable-a-schedule","title":"Disable / Enable a schedule","text":"<p>You can decide to pause the scheduling of a job and avoid new executions to be started. You can later on re-enable the same scheduling configuration, and the scheduler will run the executions that were skipped while the schedule was disabled, if any, sequentially. In this way you will backfill the executions in between. </p> <p>You can skip the backfilling of the executions by editing the scheduling configuration and bringing forward the schedule start time for the job. </p>"},{"location":"user_guides/projects/jobs/schedule_job/#delete-a-scheduling","title":"Delete a scheduling","text":"<p>You can remove the schedule for a job using the UI and by clicking on the trash icon on the schedule section of the job overview. If you re-schedule a job after having deleted the previous schedule, even with the same options, it will not take into account previous scheduled executions.</p>"},{"location":"user_guides/projects/jobs/spark_job/","title":"How To Run A Spark Job","text":""},{"location":"user_guides/projects/jobs/spark_job/#introduction","title":"Introduction","text":"<p>All members of a project in Hopsworks can launch the following types of applications through a project's Jobs service:</p> <ul> <li>Python</li> <li>Apache Spark</li> </ul> <p>Launching a job of any type is very similar process, what mostly differs between job types is the various configuration parameters each job type comes with. Hopsworks support scheduling to run jobs on a regular basis, e.g backfilling a Feature Group by running your feature engineering pipeline nightly. Scheduling can be done both through the UI and the python API, checkout our Scheduling guide.</p>"},{"location":"user_guides/projects/jobs/spark_job/#ui","title":"UI","text":""},{"location":"user_guides/projects/jobs/spark_job/#step-1-jobs-overview","title":"Step 1: Jobs overview","text":"<p>The image below shows the Jobs overview page in Hopsworks and is accessed by clicking <code>Jobs</code> in the sidebar.</p> <p> Jobs overview </p>"},{"location":"user_guides/projects/jobs/spark_job/#step-2-create-new-job-dialog","title":"Step 2: Create new job dialog","text":"<p>Click <code>New Job</code> and the following dialog will appear.</p> <p> Create new job dialog </p>"},{"location":"user_guides/projects/jobs/spark_job/#step-3-set-the-job-type","title":"Step 3: Set the job type","text":"<p>By default, the dialog will create a Spark job. Make sure <code>SPARK</code> is chosen.</p>"},{"location":"user_guides/projects/jobs/spark_job/#step-4-set-the-jar","title":"Step 4: Set the jar","text":"<p>Next step is to select the program to run. You can either select <code>From project</code>, if the file was previously uploaded to Hopsworks, or <code>Upload new file</code> which lets you select a file from your local filesystem as demonstrated below. After that set the name for the job. By default, the job name is the same as the file name, but you can customize it here.</p> <p> Configure program </p>"},{"location":"user_guides/projects/jobs/spark_job/#step-5-set-the-main-class","title":"Step 5: Set the main class","text":"<p>Next step is to set the main class for the application. Then specify advanced configuration or click <code>Create New Job</code> to create the job.</p> <p> Set the main class </p> <p>Then click <code>Create job</code> to create the job.</p>"},{"location":"user_guides/projects/jobs/spark_job/#step-6-optional-set-the-spark-script-arguments","title":"Step 6 (optional): Set the Spark script arguments","text":"<p>In the job settings, you can specify arguments for your Spark script. Remember to handle the arguments inside your Spark script.</p> <p> Configure Spark script arguments </p>"},{"location":"user_guides/projects/jobs/spark_job/#step-7-optional-advanced-configuration","title":"Step 7 (optional): Advanced configuration","text":"<p>Resource allocation for the Spark driver and executors can be configured, also the number of executors and whether dynamic execution should be enabled.</p> <ul> <li> <p><code>Environment</code>: The environment to use, must be based on <code>spark-feature-pipeline</code></p> </li> <li> <p><code>Driver memory</code>: Number of cores to allocate for the Spark driver</p> </li> <li> <p><code>Driver virtual cores</code>: Number of MBs to allocate for the Spark driver</p> </li> <li> <p><code>Executor memory</code>: Number of cores to allocate for each Spark executor</p> </li> <li> <p><code>Executor virtual cores</code>: Number of MBs to allocate for each Spark executor</p> </li> <li> <p><code>Dynamic/Static</code>: Run the Spark application in static or dynamic allocation mode (see spark docs for details).</p> </li> </ul> <p> Resource configuration for the Spark job </p> <p>Additional files or dependencies required for the Spark job can be configured.</p> <ul> <li> <p><code>Additional archives</code>: List of archives to be extracted into the working directory of each executor.</p> </li> <li> <p><code>Additional jars</code>: List of jars to be placed in the working directory of each executor.</p> </li> <li> <p><code>Additional python dependencies</code>: List of python files and archives to be placed on each executor and added to PATH.</p> </li> <li> <p><code>Additional files</code>: List of files to be placed in the working directory of each executor.</p> </li> </ul> <p> File configuration for the Spark job </p> <p>Line-separates properties to be set for the Spark application. For example, changing the configuration variables for the Kryo Serializer or setting environment variables for the driver, you can set the properties as shown below.</p> <p> Additional Spark configuration </p>"},{"location":"user_guides/projects/jobs/spark_job/#step-8-execute-the-job","title":"Step 8: Execute the job","text":"<p>Now click the <code>Run</code> button to start the execution of the job, and then click on <code>Executions</code> to see the list of all executions.</p> <p> Start job execution </p>"},{"location":"user_guides/projects/jobs/spark_job/#step-9-application-logs","title":"Step 9: Application logs","text":"<p>To monitor logs while the execution is running, click <code>Spark UI</code> to open the Spark UI in a separate tab.</p> <p>Once the execution is finished, you can click on <code>Logs</code> to see the full logs for execution.</p> <p> Access Spark logs </p>"},{"location":"user_guides/projects/jobs/spark_job/#code","title":"Code","text":""},{"location":"user_guides/projects/jobs/spark_job/#step-1-upload-the-spark-jar","title":"Step 1: Upload the Spark jar","text":"<p>This snippet assumes the Spark program is in the current working directory and named <code>sparkpi.jar</code>. </p> <p>It will upload the jar to the <code>Resources</code> dataset in your project.</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\ndataset_api = project.get_dataset_api()\n\nuploaded_file_path = dataset_api.upload(\"sparkpi.jar\", \"Resources\")\n</code></pre>"},{"location":"user_guides/projects/jobs/spark_job/#step-2-create-spark-job","title":"Step 2: Create Spark job","text":"<p>In this snippet we get the <code>JobsApi</code> object to get the default job configuration for a <code>SPARK</code> job, set the python script to run and create the <code>Job</code> object.</p> <pre><code>jobs_api = project.get_job_api()\n\nspark_config = jobs_api.get_configuration(\"SPARK\")\n\nspark_config['appPath'] = uploaded_file_path\nspark_config['mainClass'] = 'org.apache.spark.examples.SparkPi'\n\njob = jobs_api.create_job(\"pyspark_job\", spark_config)\n</code></pre>"},{"location":"user_guides/projects/jobs/spark_job/#step-3-execute-the-job","title":"Step 3: Execute the job","text":"<p>In this snippet we execute the job synchronously, that is wait until it reaches a terminal state, and then download and print the logs.</p> <pre><code>execution = job.run(await_termination=True)\n\nout, err = execution.download_logs()\n\nf_out = open(out, \"r\")\nprint(f_out.read())\n\nf_err = open(err, \"r\")\nprint(f_err.read())\n</code></pre>"},{"location":"user_guides/projects/jobs/spark_job/#configuration","title":"Configuration","text":"<p>The following table describes the JSON payload returned by <code>jobs_api.get_configuration(\"SPARK\")</code></p> Field Type Description Default <code>type</code> string Type of the job configuration <code>\"sparkJobConfiguration\"</code> <code>appPath</code> string Project path to spark program (e.g <code>Resources/foo.jar</code>) <code>null</code> <code>mainClass</code> string Name of the main class to run  (e.g <code>org.company.Main</code>) <code>null</code> <code>environmentName</code> string Name of the project spark environment <code>\"spark-feature-pipeline\"</code> <code>spark.driver.cores</code> number (float) Number of CPU cores allocated for the driver <code>1.0</code> <code>spark.driver.memory</code> number (int) Memory allocated for the driver (in MB) <code>2048</code> <code>spark.executor.instances</code> number (int) Number of executor instances <code>1</code> <code>spark.executor.cores</code> number (float) Number of CPU cores per executor <code>1.0</code> <code>spark.executor.memory</code> number (int) Memory allocated per executor (in MB) <code>4096</code> <code>spark.dynamicAllocation.enabled</code> boolean Enable dynamic allocation of executors <code>true</code> <code>spark.dynamicAllocation.minExecutors</code> number (int) Minimum number of executors with dynamic allocation <code>1</code> <code>spark.dynamicAllocation.maxExecutors</code> number (int) Maximum number of executors with dynamic allocation <code>2</code> <code>spark.dynamicAllocation.initialExecutors</code> number (int) Initial number of executors with dynamic allocation <code>1</code> <code>spark.blacklist.enabled</code> boolean Whether executor/node blacklisting is enabled <code>false</code>"},{"location":"user_guides/projects/jobs/spark_job/#accessing-project-data","title":"Accessing project data","text":""},{"location":"user_guides/projects/jobs/spark_job/#read-directly-from-the-filesystem-recommended","title":"Read directly from the filesystem (recommended)","text":"<p>To read a dataset in your project using Spark, use the full filesystem path where the data is stored. For example, to read a CSV file named <code>data.csv</code> located in the <code>Resources</code> dataset of a project called <code>my_project</code>:</p> <pre><code>Dataset&lt;Row&gt; df = spark.read()\n    .option(\"header\", \"true\")       // CSV has header\n    .option(\"inferSchema\", \"true\")  // Infer data types\n    .csv(\"/Projects/my_project/Resources/data.csv\");\n\ndf.show();\n</code></pre>"},{"location":"user_guides/projects/jobs/spark_job/#additional-files","title":"Additional files","text":"<p>Different file types can be attached to the spark job and made available in the <code>/srv/hops/artifacts</code> folder when the Spark job is started. This configuration is mainly useful when you need to add additional configuration such as jars that needs to be added to the CLASSPATH. </p> <p>When reading data in your Spark job it is recommended to use the Spark read API as previously demonstrated, since this reads from the filesystem directly, whereas <code>Additional files</code> configuration options will download the files in its entirety and is not a scalable option.</p>"},{"location":"user_guides/projects/jobs/spark_job/#api-reference","title":"API Reference","text":"<p>Jobs</p> <p>Executions</p>"},{"location":"user_guides/projects/jupyter/python_notebook/","title":"How To Run A Python Notebook","text":""},{"location":"user_guides/projects/jupyter/python_notebook/#introduction","title":"Introduction","text":"<p>Jupyter is provided as a service in Hopsworks, providing the same user experience and features as if run on your laptop.</p> <ul> <li>Supports JupyterLab and the classic Jupyter front-end</li> <li>Configured with Python3, PySpark and Ray kernels</li> </ul>"},{"location":"user_guides/projects/jupyter/python_notebook/#step-1-jupyter-dashboard","title":"Step 1: Jupyter dashboard","text":"<p>The image below shows the Jupyter service page in Hopsworks and is accessed by clicking <code>Jupyter</code> in the sidebar.</p> <p> Jupyter dashboard in Hopsworks </p> <p>From this page, you can configure various options and settings to start Jupyter with as described in the sections below.</p>"},{"location":"user_guides/projects/jupyter/python_notebook/#step-2-optional-configure-resources","title":"Step 2 (Optional): Configure resources","text":"<p>Next step is to configure Jupyter, Click <code>edit configuration</code> to get to the configuration page and select <code>Python</code>.</p> <ul> <li> <p><code>Container cores</code>: Number of cores to allocate for the Jupyter instance</p> </li> <li> <p><code>Container memory</code>: Number of MBs to allocate for the Jupyter instance</p> </li> </ul> <p>Configured resource pool is shared by all running kernels. If a kernel crashes while executing a cell, try increasing the Container memory.</p> <p> Resource configuration for the Python kernel </p> <p>Click <code>Save</code> to save the new configuration.</p>"},{"location":"user_guides/projects/jupyter/python_notebook/#step-3-optional-configure-environment-root-folder-and-automatic-shutdown","title":"Step 3 (Optional): Configure environment, root folder and automatic shutdown","text":"<p>Before starting the server there are three additional configurations that can be set next to the <code>Run Jupyter</code> button.</p> <p>The environment that Jupyter should run in needs to be configured. Select the environment that contains the necessary dependencies for your code.</p> <p> Configure environment </p> <p>The runtime of the Jupyter instance can be configured, this is useful to ensure that idle instances will not be hanging around and keep allocating resources. If a limited runtime is not desirable, this can be disabled by setting <code>no limit</code>. </p> <p> Configure maximum runtime </p> <p>The root path from which to start the Jupyter instance can be configured. By default it starts by setting the <code>/Jupyter</code> folder as the root.</p> <p> Configure root folder </p>"},{"location":"user_guides/projects/jupyter/python_notebook/#step-4-start-jupyter","title":"Step 4: Start Jupyter","text":"<p>Start the Jupyter instance by clicking the <code>Run Jupyter</code> button.</p> <p> Starting Jupyter and running a Python notebook </p>"},{"location":"user_guides/projects/jupyter/python_notebook/#accessing-project-data","title":"Accessing project data","text":"<p>Recommended approach if <code>/hopsfs</code> is mounted</p> <p>If your Hopsworks installation is configured to mount the project datasets under <code>/hopsfs</code>, which it is in most cases, then please refer to this section. If the file system is not mounted, then project files can be localized using the download api to localize files in the current working directory.</p>"},{"location":"user_guides/projects/jupyter/python_notebook/#absolute-paths","title":"Absolute paths","text":"<p>The project datasets are mounted under <code>/hopsfs</code>, so you can access <code>data.csv</code> from the <code>Resources</code> dataset using <code>/hopsfs/Resources/data.csv</code> in your notebook.</p>"},{"location":"user_guides/projects/jupyter/python_notebook/#relative-paths","title":"Relative paths","text":"<p>The notebook's working directory is the folder it is located in. For example, if it is located in the <code>Resources</code> dataset, and you have a file named <code>data.csv</code> in that dataset, you simply access it using <code>data.csv</code>. Also, if you write a local file, for example <code>output.txt</code>, it will be saved in the <code>Resources</code> dataset.</p>"},{"location":"user_guides/projects/jupyter/python_notebook/#going-further","title":"Going Further","text":"<p>You can learn how to install a library so that it can be used in a notebook.</p>"},{"location":"user_guides/projects/jupyter/ray_notebook/","title":"How To Run A Ray Notebook","text":""},{"location":"user_guides/projects/jupyter/ray_notebook/#introduction","title":"Introduction","text":"<p>Jupyter is provided as a service in Hopsworks, providing the same user experience and features as if run on your laptop.</p> <ul> <li>Supports JupyterLab and the classic Jupyter front-end</li> <li>Configured with Python3, PySpark and Ray kernels</li> </ul> <p>Enable Ray</p> <p>Support for Ray needs to be explicitly enabled by adding the following option in the <code>values.yaml</code> file for the deployment:</p> <pre><code>global:\n  ray:\n    enabled: true\n</code></pre>"},{"location":"user_guides/projects/jupyter/ray_notebook/#step-1-jupyter-dashboard","title":"Step 1: Jupyter dashboard","text":"<p>The image below shows the Jupyter service page in Hopsworks and is accessed by clicking <code>Jupyter</code> in the sidebar.</p> <p> Jupyter dashboard in Hopsworks </p> <p>From this page, you can configure various options and settings to start Jupyter with as described in the sections below.</p>"},{"location":"user_guides/projects/jupyter/ray_notebook/#step-2-optional-configure-ray","title":"Step 2 (Optional): Configure Ray","text":"<p>Next step is to configure the Ray cluster configuration that will be created when you start Ray session later in  Jupyter. Click <code>edit configuration</code> to get to the configuration page and select <code>Ray</code>.</p>"},{"location":"user_guides/projects/jupyter/ray_notebook/#resource-and-compute","title":"Resource and compute","text":"<p>Resource allocation for the Driver and Workers can be configured.</p> <p>Using the resources in the Ray script</p> <p>The resource configurations describe the cluster that will be provisioned when launching the Ray job. User can still  provide extra configurations in the job script using <code>ScalingConfig</code>, i.e. <code>ScalingConfig(num_workers=4, trainer_resources={\"CPU\": 1}, use_gpu=True)</code>.</p> <ul> <li> <p><code>Driver memory</code>: Memory in MBs to allocate for Driver</p> </li> <li> <p><code>Driver virtual cores</code>: Number of cores to allocate for the Driver</p> </li> <li> <p><code>Worker memory</code>: Memory in MBs to allocate for each worker</p> </li> <li> <p><code>Worker cores</code>: Number of cores to allocate for each worker</p> </li> <li> <p><code>Min workers</code>: Minimum number of workers to start with</p> </li> <li> <p><code>Max workers</code>: Maximum number of workers to scale up to</p> </li> </ul> <p> Resource configuration for the Ray kernels </p> <p>Runtime environment and Additional files required for the Ray job can also be provided.</p> <ul> <li> <p><code>Runtime Environment (Optional)</code>:  A runtime environment describes the dependencies required for the Ray job including files, packages, environment variables, and more. This is useful when you need to install specific packages and set environment variables for this particular Ray job. It should be provided as a YAML file. You can select the file from the project or upload a new one.</p> </li> <li> <p><code>Additional files</code>: List of other files required for the Ray job. These files will be placed in <code>/srv/hops/ray/job</code>.</p> </li> </ul> <p> Runtime configuration and additional files for Ray jupyter session </p> <p>Click <code>Save</code> to save the new configuration.</p>"},{"location":"user_guides/projects/jupyter/ray_notebook/#step-3-optional-configure-max-runtime-and-root-path","title":"Step 3 (Optional): Configure max runtime and root path","text":"<p>Before starting the server there are two additional configurations that can be set next to the <code>Run Jupyter</code> button.</p> <p>The runtime of the Jupyter instance can be configured, this is useful to ensure that idle instances will not be hanging around and keep allocating resources. If a limited runtime is not desirable, this can be disabled by setting <code>no limit</code>.</p> <p> Configure maximum runtime </p> <p>The root path from which to start the Jupyter instance can be configured. By default it starts by setting the <code>/Jupyter</code> folder as the root.</p> <p> Configure root folder </p>"},{"location":"user_guides/projects/jupyter/ray_notebook/#step-4-select-the-environment","title":"Step 4: Select the environment","text":"<p>Hopsworks provides a variety of environments to run Jupyter notebooks. Select the environment you want to use by clicking on the dropdown menu. In order to be able to run a Ray notebook, you need to select the environment that has the Ray kernel installed.  Environment with Ray kernel have a <code>Ray Enabled</code> label next to them.</p>"},{"location":"user_guides/projects/jupyter/ray_notebook/#step-5-start-jupyter","title":"Step 5: Start Jupyter","text":"<p>Start the Jupyter instance by clicking the <code>Run Jupyter</code> button.</p>"},{"location":"user_guides/projects/jupyter/ray_notebook/#running-ray-code-in-jupyter","title":"Running Ray code in Jupyter","text":"<p>Once the Jupyter instance is started, you can create a new notebook by clicking on the <code>New</code> button and selecting  <code>Ray</code> kernel. You can now write and run Ray code in the notebook. When you first run a cell with Ray code, a Ray session will be started and you can monitor the resources used by the job in the Ray dashboard.</p> <p> Ray Kernel </p>"},{"location":"user_guides/projects/jupyter/ray_notebook/#step-6-access-ray-dashboard","title":"Step 6: Access Ray Dashboard","text":"<p>When you start a Ray session in Jupyter, a new application will appear in the Jupyter page. The notebook name from which the session was started is displayed. You can access the Ray UI by clicking on the <code>Ray Dashboard</code> and a new  tab will be opened. The Ray dashboard is only available while the Ray kernel is running.  You can kill the Ray session to free up resources by shutting down the kernel in Jupyter.  In the Ray Dashboard, you can monitor the resources used  by code you are running, the number of workers, logs, and the tasks that are running.</p> <p> Access Ray Dashboard for Jupyter Ray session </p>"},{"location":"user_guides/projects/jupyter/ray_notebook/#accessing-project-data","title":"Accessing project data","text":"<p>The project datasets are mounted under <code>/hopsfs</code> in the Ray containers, so you can access <code>data.csv</code> from the <code>Resources</code> dataset using <code>/hopsfs/Resources/data.csv</code>.</p>"},{"location":"user_guides/projects/jupyter/remote_filesystem_driver/","title":"Configuring remote filesystem driver","text":""},{"location":"user_guides/projects/jupyter/remote_filesystem_driver/#introduction","title":"Introduction","text":"<p>We provide two ways to access and persist files in HopsFS from a jupyter notebook:</p> <ul> <li><code>hdfscontentsmanager</code>: With <code>hdfscontentsmanager</code> you interact with the project datasets using the dataset api. When you   start a notebook using the <code>hdfscontentsmanager</code> you will only see the files in the configured root path.</li> <li><code>hopsfsmount</code>: With <code>hopsfsmount</code> all the project datasets are available in the jupyter notebook as a local filesystem.   This means you can use native Python file I/O operations (copy, move, create, open, etc.) to interact with the project datasets.   When you open the jupyter notebook you will see all the project datasets.</li> </ul>"},{"location":"user_guides/projects/jupyter/remote_filesystem_driver/#configuring-the-driver","title":"Configuring the driver","text":"<p>To configure the driver you need to have admin role and set the <code>jupyter_remote_fs_driver</code> to either <code>hdfscontentsmanager</code> or <code>hopsfsmount</code>. The default driver is <code>hdfscontentsmanager</code>.</p>"},{"location":"user_guides/projects/jupyter/spark_notebook/","title":"How To Run A PySpark Notebook","text":""},{"location":"user_guides/projects/jupyter/spark_notebook/#introduction","title":"Introduction","text":"<p>Jupyter is provided as a service in Hopsworks, providing the same user experience and features as if run on your laptop.</p> <ul> <li>Supports JupyterLab and the classic Jupyter front-end</li> <li>Configured with Python3, PySpark and Ray kernels</li> </ul>"},{"location":"user_guides/projects/jupyter/spark_notebook/#step-1-jupyter-dashboard","title":"Step 1: Jupyter dashboard","text":"<p>The image below shows the Jupyter service page in Hopsworks and is accessed by clicking <code>Jupyter</code> in the sidebar.</p> <p> Jupyter dashboard in Hopsworks </p> <p>From this page, you can configure various options and settings to start Jupyter with as described in the sections below.</p>"},{"location":"user_guides/projects/jupyter/spark_notebook/#step-2-a-spark-environment-must-be-configured","title":"Step 2: A Spark environment must be configured","text":"<p>The PySpark kernel will only be available if Jupyter is configured to use the <code>spark-feature-pipeline</code> or an environment cloned from it. You can easily refer to the green ticks as to what kernels are available in which environment.</p> <p> Select an environment with PySpark kernel enabled </p>"},{"location":"user_guides/projects/jupyter/spark_notebook/#step-3-optional-configure-spark-properties","title":"Step 3 (Optional): Configure spark properties","text":"<p>Next step is to configure the Ray properties to be used in Jupyter, Click <code>edit configuration</code> to get to the  configuration page and select <code>Ray</code>.</p>"},{"location":"user_guides/projects/jupyter/spark_notebook/#resource-and-compute","title":"Resource and compute","text":"<p>Resource allocation for the Spark driver and executors can be configured, also the number of executors and whether dynamic execution should be enabled.</p> <ul> <li> <p><code>Driver memory</code>: Number of cores to allocate for the Spark driver</p> </li> <li> <p><code>Driver virtual cores</code>: Number of MBs to allocate for the Spark driver</p> </li> <li> <p><code>Executor memory</code>: Number of cores to allocate for each Spark executor</p> </li> <li> <p><code>Executor virtual cores</code>: Number of MBs to allocate for each Spark executor</p> </li> <li> <p><code>Dynamic/Static</code>: Run the Spark application in static or dynamic allocation mode (see spark docs for details).</p> </li> </ul> <p> Resource configuration for the Spark kernels </p>"},{"location":"user_guides/projects/jupyter/spark_notebook/#attach-files-or-dependencies","title":"Attach files or dependencies","text":"<p>Additional files or dependencies required for the Spark job can be configured.</p> <ul> <li> <p><code>Additional archives</code>: List of zip or .tgz files that will be locally accessible by the application</p> </li> <li> <p><code>Additional jars</code>: List of .jar files to add to the CLASSPATH of the application</p> </li> <li> <p><code>Additional python dependencies</code>: List of .py, .zip or .egg files that will be locally accessible by the application</p> </li> <li> <p><code>Additional files</code>: List of files that will be locally accessible by the application</p> </li> </ul> <p> File configuration for the Spark kernels </p> <p>Line-separates properties to be set for the Spark application. For example, changing the configuration variables for the Kryo Serializer or setting environment variables for the driver, you can set the properties as shown below.</p> <p> Additional Spark configuration </p> <p>Click <code>Save</code> to save the new configuration.</p>"},{"location":"user_guides/projects/jupyter/spark_notebook/#step-4-optional-configure-root-folder-and-automatic-shutdown","title":"Step 4 (Optional): Configure root folder and automatic shutdown","text":"<p>Before starting the server there are two additional configurations that can be set next to the <code>Run Jupyter</code> button.</p> <p>The runtime of the Jupyter instance can be configured, this is useful to ensure that idle instances will not be hanging around and keep allocating resources. If a limited runtime is not desirable, this can be disabled by setting <code>no limit</code>.</p> <p> Configure maximum runtime </p> <p>The root path from which to start the Jupyter instance can be configured. By default it starts by setting the <code>/Jupyter</code> folder as the root.</p> <p> Configure root folder </p>"},{"location":"user_guides/projects/jupyter/spark_notebook/#step-5-start-jupyter","title":"Step 5: Start Jupyter","text":"<p>Start the Jupyter instance by clicking the <code>Run Jupyter</code> button.</p> <p> Starting Jupyter and running a Spark notebook </p>"},{"location":"user_guides/projects/jupyter/spark_notebook/#step-6-access-spark-ui","title":"Step 6: Access Spark UI","text":"<p>Navigate back to Hopsworks and a Spark session will have appeared, click on the <code>Spark UI</code> button to go to the Spark UI.</p> <p> Access Spark UI and see application logs </p>"},{"location":"user_guides/projects/jupyter/spark_notebook/#accessing-project-data","title":"Accessing project data","text":""},{"location":"user_guides/projects/jupyter/spark_notebook/#read-directly-from-the-filesystem-recommended","title":"Read directly from the filesystem (recommended)","text":"<p>To read a dataset in your project using Spark, use the full filesystem path where the data is stored. For example, to read a CSV file named <code>data.csv</code> located in the <code>Resources</code> dataset of a project called <code>my_project</code>:</p> <pre><code>df = spark.read.csv(\"/Projects/my_project/Resources/data.csv\", header=True, inferSchema=True)\ndf.show()\n</code></pre>"},{"location":"user_guides/projects/jupyter/spark_notebook/#additional-files","title":"Additional files","text":"<p>Different files can be attached to the jupyter session and made available in the <code>/srv/hops/artifacts</code> folder when the PySpark kernel is started. This configuration is mainly useful when you need to add additional configuration such as jars that needs to be added to the CLASSPATH.</p> <p>When reading data in your Spark application, it is recommended to use the Spark read API as previously demonstrated, since this reads from the filesystem directly, whereas <code>Additional files</code> configuration options will download the files in its entirety and is not a scalable option.</p>"},{"location":"user_guides/projects/jupyter/spark_notebook/#going-further","title":"Going Further","text":"<p>You can learn how to install a library so that it can be used in a notebook.</p>"},{"location":"user_guides/projects/kafka/consume_messages/","title":"How To Consume Message From A Topic","text":""},{"location":"user_guides/projects/kafka/consume_messages/#introduction","title":"Introduction","text":"<p>A Consumer is a process which reads messages from a Kafka topic. In Hopsworks, all user roles are capable of performing 'Read' and 'Describe' actions on Kafka topics within projects that they are a member of or are shared with them.</p>"},{"location":"user_guides/projects/kafka/consume_messages/#prerequisites","title":"Prerequisites","text":"<p>This guide requires that you have previously produced messages to a kafka topic.</p>"},{"location":"user_guides/projects/kafka/consume_messages/#code","title":"Code","text":"<p>In this guide, you will learn how to consume messages from a kafka topic.</p>"},{"location":"user_guides/projects/kafka/consume_messages/#step-1-get-the-kafka-api","title":"Step 1: Get the Kafka API","text":"<pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nkafka_api = project.get_kafka_api()\n</code></pre>"},{"location":"user_guides/projects/kafka/consume_messages/#step-2-configure-confluent-kafka-client","title":"Step 2: Configure confluent-kafka client","text":"<pre><code>consumer_config = kafka_api.get_default_config()\nconsumer_config['default.topic.config'] = {'auto.offset.reset': 'earliest'}\n\nfrom confluent_kafka import Consumer\n\nconsumer = Consumer(consumer_config)\n</code></pre>"},{"location":"user_guides/projects/kafka/consume_messages/#step-3-consume-messages-from-a-topic","title":"Step 3: Consume messages from a topic","text":"<pre><code># Subscribe to topic\nconsumer.subscribe([\"my_topic\"])\n\nfor i in range(0, 10):\n    msg = consumer.poll(timeout=10.0)\n    print(msg.value())\n</code></pre>"},{"location":"user_guides/projects/kafka/consume_messages/#api-reference","title":"API Reference","text":"<p>KafkaTopic</p>"},{"location":"user_guides/projects/kafka/create_schema/","title":"How To Create A Kafka Schema","text":""},{"location":"user_guides/projects/kafka/create_schema/#introduction","title":"Introduction","text":""},{"location":"user_guides/projects/kafka/create_schema/#code","title":"Code","text":"<p>In this guide, you will learn how to create a Kafka Avro Schema in the Hopsworks Schema Registry.</p>"},{"location":"user_guides/projects/kafka/create_schema/#step-1-get-the-kafka-api","title":"Step 1: Get the Kafka API","text":"<pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nkafka_api = project.get_kafka_api()\n</code></pre>"},{"location":"user_guides/projects/kafka/create_schema/#step-2-define-the-schema","title":"Step 2: Define the schema","text":"<p>Define the Avro Schema, see types for the format of the schema.</p> <pre><code>schema = {\n    \"type\": \"record\",\n    \"name\": \"tutorial\",\n    \"fields\": [\n        {\n            \"name\": \"id\",\n            \"type\": \"int\"\n        },\n        {\n            \"name\": \"data\",\n            \"type\": \"string\"\n        }\n    ]\n}\n</code></pre>"},{"location":"user_guides/projects/kafka/create_schema/#step-3-create-the-schema","title":"Step 3: Create the schema","text":"<p>Create the schema in the Schema Registry.</p> <pre><code>SCHEMA_NAME=\"schema_example\"\n\nmy_schema = kafka_api.create_schema(SCHEMA_NAME, schema)\n</code></pre>"},{"location":"user_guides/projects/kafka/create_schema/#api-reference","title":"API Reference","text":"<p>KafkaSchema</p>"},{"location":"user_guides/projects/kafka/create_topic/","title":"How To Create A Kafka Topic","text":""},{"location":"user_guides/projects/kafka/create_topic/#introduction","title":"Introduction","text":"<p>A Topic is a queue to which records are stored and published. Producer applications write data to topics and consumer applications read from topics.</p>"},{"location":"user_guides/projects/kafka/create_topic/#prerequisites","title":"Prerequisites","text":"<p>This guide requires that you have 'Data owner' role and have previously created a Kafka Schema to be used for the topic.</p>"},{"location":"user_guides/projects/kafka/create_topic/#code","title":"Code","text":"<p>In this guide, you will learn how to create a Kafka Topic.</p>"},{"location":"user_guides/projects/kafka/create_topic/#step-1-get-the-kafka-api","title":"Step 1: Get the Kafka API","text":"<pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nkafka_api = project.get_kafka_api()\n</code></pre>"},{"location":"user_guides/projects/kafka/create_topic/#step-2-define-the-schema","title":"Step 2: Define the schema","text":"<pre><code>TOPIC_NAME=\"topic_example\"\nSCHEMA_NAME=\"schema_example\"\n\nmy_topic = kafka_api.create_topic(TOPIC_NAME, SCHEMA_NAME, 1, replicas=1, partitions=1)\n</code></pre>"},{"location":"user_guides/projects/kafka/create_topic/#api-reference","title":"API Reference","text":"<p>KafkaTopic</p>"},{"location":"user_guides/projects/kafka/produce_messages/","title":"How To Produce To A Topic","text":""},{"location":"user_guides/projects/kafka/produce_messages/#introduction","title":"Introduction","text":"<p>A Producer is a process which produces messages to a Kafka topic. In Hopsworks, only users with the 'Data owner' role are capable of performing the 'Write' action on Kafka topics within the project that they are a member of.</p>"},{"location":"user_guides/projects/kafka/produce_messages/#prerequisites","title":"Prerequisites","text":"<p>This guide requires that you have 'Data owner' role and have previously created a Kafka Topic.</p>"},{"location":"user_guides/projects/kafka/produce_messages/#code","title":"Code","text":"<p>In this guide, you will learn how to produce messages to a kafka topic.</p>"},{"location":"user_guides/projects/kafka/produce_messages/#step-1-get-the-kafka-api","title":"Step 1: Get the Kafka API","text":"<pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nkafka_api = project.get_kafka_api()\n</code></pre>"},{"location":"user_guides/projects/kafka/produce_messages/#step-2-configure-confluent-kafka-client","title":"Step 2: Configure confluent-kafka client","text":"<pre><code>producer_config = kafka_api.get_default_config()\n\nfrom confluent_kafka import Producer\n\nproducer = Producer(producer_config)\n</code></pre>"},{"location":"user_guides/projects/kafka/produce_messages/#step-3-produce-messages-to-topic","title":"Step 3: Produce messages to topic","text":"<pre><code>import uuid\nimport json\n\n# Send a few messages\nfor i in range(0, 10):\n    producer.produce(\"my_topic\", json.dumps({\"id\": i, \"data\": str(uuid.uuid1())}), \"key\")\n\n# Trigger the sending of all messages to the brokers, 10 sec timeout\nproducer.flush(10)\n</code></pre>"},{"location":"user_guides/projects/kafka/produce_messages/#api-reference","title":"API Reference","text":"<p>KafkaTopic</p>"},{"location":"user_guides/projects/kafka/produce_messages/#going-further","title":"Going Further","text":"<p>Now you can create a Consumer to read the messages from the topic.</p>"},{"location":"user_guides/projects/opensearch/connect/","title":"How To Connect To OpenSearch","text":""},{"location":"user_guides/projects/opensearch/connect/#introduction","title":"Introduction","text":"<p>Text here</p> <p>Limited to internal Jobs and Notebooks</p> <p>Currently it's only possible to configure the opensearch-py client in a job or jupyter notebook running inside the Hopsworks cluster.</p>"},{"location":"user_guides/projects/opensearch/connect/#code","title":"Code","text":"<p>In this guide, you will learn how to connect to the OpenSearch cluster using an opensearch-py client. </p>"},{"location":"user_guides/projects/opensearch/connect/#step-1-get-the-opensearch-api","title":"Step 1: Get the OpenSearch API","text":"<pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nopensearch_api = project.get_opensearch_api()\n</code></pre>"},{"location":"user_guides/projects/opensearch/connect/#step-2-configure-the-opensearch-py-client","title":"Step 2: Configure the opensearch-py client","text":"<pre><code>from opensearchpy import OpenSearch\n\nclient = OpenSearch(**opensearch_api.get_default_py_config())\n</code></pre>"},{"location":"user_guides/projects/opensearch/connect/#api-reference","title":"API Reference","text":"<p>OpenSearch</p>"},{"location":"user_guides/projects/opensearch/connect/#going-further","title":"Going Further","text":"<p>You can now use the client to interact directly with the OpenSearch cluster, such as vector database.</p>"},{"location":"user_guides/projects/project/add_members/","title":"How To Add Members To A Project","text":""},{"location":"user_guides/projects/project/add_members/#introduction","title":"Introduction","text":"<p>In this guide, you will learn how to add new members to your project as well as the different roles within a project.</p>"},{"location":"user_guides/projects/project/add_members/#step-1-members-list","title":"Step 1: Members list","text":"<p>On the <code>Project settings</code> page, you can find the <code>General</code> section, which lists the members of the project.</p> <p> List of project members </p>"},{"location":"user_guides/projects/project/add_members/#step-2-add-a-new-member","title":"Step 2: Add a new member","text":"<p>Next click <code>Add members</code> and a dialog where users can be invited will appear. Select the users to invite.</p> <p> Add new member dialog </p> <p>Subsequently, the selected project members can be assigned to 2 different roles, depending on the privileges necessary for him/her to fulfill their needs.</p>"},{"location":"user_guides/projects/project/add_members/#data-owner","title":"Data owner","text":"<p>Data owners hold the highest authority in the project, having full control of its contents.</p> <p>They are allowed to: - Share a project - Manage the project and its members - Work with all feature store abstractions (such as Feature groups, Feature views, Storage connectors, etc.)</p> <p>It is worth mentioning that the project's creator (aka. <code>author</code>) is a special type of <code>Data owner</code>. He is the only user capable of deleting the project and it is impossible to change his role to <code>Data scientist</code>.</p>"},{"location":"user_guides/projects/project/add_members/#data-scientist","title":"Data scientist","text":"<p>Data scientists can be viewed as the users of data.</p> <p>They are allowed to: - Create feature views/training datasets using existing features - Manage the feature views/training datasets they have created</p>"},{"location":"user_guides/projects/project/add_members/#step-3-member-invited","title":"Step 3: Member invited","text":"<p>The invited user will now appear in the list of members and will have access to the project.</p> <p> List of project members </p>"},{"location":"user_guides/projects/project/create_project/","title":"How To Create A Project","text":""},{"location":"user_guides/projects/project/create_project/#introduction","title":"Introduction","text":"<p>In this guide, you will learn how to create a new project.</p> <p>Project name validation rules</p> <p>A valid project name can only contain characters a-z, A-Z, 0-9 and special characters \u2018_\u2019 and \u2018.\u2019 but not \u2018__\u2019 (double underscore). There is also a number of reserved project names that can not be used.</p>"},{"location":"user_guides/projects/project/create_project/#gui","title":"GUI","text":""},{"location":"user_guides/projects/project/create_project/#step-1-create-a-project","title":"Step 1: Create a project","text":"<p>If you log in to the platform and do not have any projects, you are presented with the following view. To run the Feature Store tour click <code>Run a demo project</code>, to create a new project click <code>Create new project</code>.</p> <p>For this guide click <code>Create new project</code> to continue.</p> <p> Landing page </p>"},{"location":"user_guides/projects/project/create_project/#step-2-project-creation-form","title":"Step 2: Project creation form","text":"<p>In the creation form in which you enter the project name, an optional description and set of members to invite to the project.</p> <p> Project creation form </p>"},{"location":"user_guides/projects/project/create_project/#step-3-project-creation","title":"Step 3: Project creation","text":"<p>Then wait for the project creation process to finish.</p> <p> List of created API Keys </p>"},{"location":"user_guides/projects/project/create_project/#step-4-project-overview","title":"Step 4: Project overview","text":"<p>Once the project is created the overview page for it will appear.</p> <p> List of created API Keys </p>"},{"location":"user_guides/projects/project/create_project/#code","title":"Code","text":""},{"location":"user_guides/projects/project/create_project/#step-1-connect-to-hopsworks","title":"Step 1: Connect to Hopsworks","text":"<pre><code>import hopsworks\n\nhopsworks.login()\n</code></pre>"},{"location":"user_guides/projects/project/create_project/#step-2-create-project","title":"Step 2: Create project","text":"<pre><code>project = hopsworks.create_project(\"my_project\")\n</code></pre>"},{"location":"user_guides/projects/project/create_project/#api-reference","title":"API Reference","text":"<p>Projects</p>"},{"location":"user_guides/projects/project/create_project/#reserved-project-names","title":"Reserved project names","text":"<pre><code>PROJECTS, HOPS-SYSTEM, HOPSWORKS, INFORMATION_SCHEMA, AIRFLOW, GLASSFISH_TIMERS, GRAFANA, HOPS, METASTORE,\nMYSQL, NDBINFO, RONDB_REPLICATION, PERFORMANCE_SCHEMA, SQOOP, SYS, GLASSFISH_TIMERS, GRAFANA, HOPS, METASTORE, MYSQL, NDBINFO,\nPERFORMANCE_SCHEMA, SQOOP, SYS, BIGINT, BINARY, BOOLEAN, BOTH, BY, CASE, CAST, CHAR, COLUMN, CONF, CREATE, CROSS, CUBE, CURRENT, CURRENT_DATE,\nCURRENT_TIMESTAMP, CURSOR, DATABASE, DATE, DECIMAL, DELETE, DESCRIBE, DISTINCT, DOUBLE, DROP, ELSE, END,\nEXCHANGE, EXISTS, EXTENDED, EXTERNAL, FALSE, FETCH, FLOAT, FOLLOWING, FOR, FROM, FULL, FUNCTION, GRANT, GROUP,\nGROUPING, HAVING, IF, IMPORT, IN, INNER, INSERT, INT, INTERSECT, INTERVAL, INTO, IS, JOIN, LATERAL, LEFT, LESS,\nLIKE, LOCAL, MACRO, MAP, MORE, NONE, NOT, NULL, OF, ON, OR, ORDER, OUT, OUTER, OVER, PARTIALSCAN, PARTITION,\nPERCENT, PRECEDING, PRESERVE, PROCEDURE, RANGE, READS, REDUCE, REVOKE, RIGHT, ROLLUP, ROW, ROWS, SELECT, SET,\nSMALLINT, TABLE, TABLESAMPLE, THEN, TIMESTAMP, TO, TRANSFORM, TRIGGER, TRUE, TRUNCATE, UNBOUNDED, UNION,\nUNIQUEJOIN, UPDATE, USER, USING, UTC_TMESTAMP, VALUES, VARCHAR, WHEN, WHERE, WINDOW, WITH, COMMIT, ONLY,\nREGEXP, RLIKE, ROLLBACK, START, CACHE, CONSTRAINT, FOREIGN, PRIMARY, REFERENCES, DAYOFWEEK, EXTRACT, FLOOR,\nINTEGER, PRECISION, VIEWS, TIME, NUMERIC, SYNC, BASE, PYTHON37, FILEBEAT.\n\nAnd any word containing _FEATURESTORE.\n</code></pre>"},{"location":"user_guides/projects/python/custom_commands/","title":"Adding extra configuration with generic bash commands","text":""},{"location":"user_guides/projects/python/custom_commands/#introduction","title":"Introduction","text":"<p>Hopsworks comes with several prepackaged Python environments that contain libraries for data engineering, machine learning, and more general data science use-cases. Hopsworks also offers the ability to install additional packages from various sources, such as using the pip or conda package managers and public or private git repository.</p> <p>Some Python libraries require the installation of some OS-Level libraries. In some cases, you may need to add more complex configuration to your environment. This demands writing your own commands and executing them on top of the existing environment.</p> <p>In this guide, you will learn how to run custom bash commands that can be used to add more complex configuration to your environment e.g., installing OS-Level packages or configuring an oracle database.</p>"},{"location":"user_guides/projects/python/custom_commands/#prerequisites","title":"Prerequisites","text":"<p>In order to install a custom dependency one of the base environments must first be cloned, follow this guide for that.</p>"},{"location":"user_guides/projects/python/custom_commands/#running-bash-commands","title":"Running bash commands","text":"<p>In this section, we will see how you can run custom bash commands in Hopsworks to configure your Python environment.</p> <p>In Hopsworks, we maintain a docker image built on top of Ubuntu Linux distribution. You can run generic bash commands on top of the project environment from the UI or REST API.</p>"},{"location":"user_guides/projects/python/custom_commands/#setting-up-the-bash-script-and-artifacts-from-the-ui","title":"Setting up the bash script and artifacts from the UI","text":"<p>To use the UI, navigate to the Python environment in the Project settings. In the Python environment page, navigate to custom commands. From the UI, you can write the bash commands in the textbox provided. These bash commands will be uploaded and executed when building your new environment. You can include build artifacts e.g., binaries that you would like to execute or include when building the environment. See Figure 1.</p> <p> Figure 1: You can write custom commands and upload build artifacts from the UI </p>"},{"location":"user_guides/projects/python/custom_commands/#code","title":"Code","text":"<p>You can also run the custom commands using the REST API. From the REST API, you should provide the path, in HOPSFS, to the bash script and the artifacts(comma separated string of paths in HopsFs). The REST API endpoint for running custom commands is: <code>hopsworks-api/api/project/&lt;projectId&gt;/python/environments/&lt;environmentName&gt;/commands/custom</code> and the body should look like this: <pre><code>{\n    \"commandsFile\": \"&lt;pathToYourBashScriptInHopsFS&gt;\",\n    \"artifacts\": \"&lt;commaSeparatedListOfPathsToArtifactsInHopsFS&gt;\"\n}\n</code></pre></p>"},{"location":"user_guides/projects/python/custom_commands/#what-to-include-in-the-bash-script","title":"What to include in the bash script","text":"<p>There are few important things to be aware of when writing the bash script:</p> <ul> <li>The first line of your bash script should always be <code>#!/bin/bash</code> (known as shebang) so that the script can be interpreted and executed using the Bash shell.</li> <li>You can use <code>apt</code>, <code>apt-get</code> and <code>deb</code> commands to install packages. You should always run these commands with <code>sudo</code>. In some cases, these commands will ask for user input, therefore you should provide the input of what the command expects, e.g., <code>sudo apt -y install</code>, otherwise the build will fail. We have already configured <code>apt-get</code> to be non-interactive</li> <li>The build artifacts will be copied to <code>srv/hops/build</code>. You can use them in your script via this path. This path is also available via the environmental variable <code>BUILD_PATH</code>. If you want to use many artifacts it is advisable to create a zip file and upload it to HopsFS in one of your project datasets. You can then include the zip file as one of the artifacts.</li> <li>The conda environment is located in <code>/srv/hops/anaconda/envs/hopsworks_environment</code>. You can install or uninstall packages in the conda environment using pip like: <code>/srv/hops/anaconda/envs/hopsworks_environment/bin/pip install spotify==0.10.2</code>. If the command requires some input, write the command together with the expected input otherwise the build will fail.</li> </ul>"},{"location":"user_guides/projects/python/environment_history/","title":"Environment History","text":"<p>Hopsworks comes with several prepackaged Python environments that contain libraries for data engineering, machine learning, and more general data science use-cases. Hopsworks also offers the ability to install additional packages from various sources, such as using the pip or conda package managers and public or private git repository.</p> <p>The Python virtual environment is shared by different members of the project. When a member of the project introduces a change to the environment i.e., installs/uninstalls a library, a new environment is created and it becomes a defacto environment for everyone in the project. It is therefore important to track how the environment has been changing over time i.e., what libraries were installed, uninstalled, upgraded, or downgraded when the environment was created and who introduced the changes. </p> <p>In this guide, you will learn how you can track the changes of your Python environment.</p>"},{"location":"user_guides/projects/python/environment_history/#viewing-python-environment-history-in-the-ui","title":"Viewing python environment history in the UI","text":"<p>The Python environment evolves over time as libraries are installed, uninstalled, upgraded, and downgraded. To assist in tracking the changes in the environment, you can see the environment history in the UI. You can view what changes were introduced at each point a new environment was created. Hopsworks will keep a version of a YAML file for each environment so that if you want to restore an older environment you can use it. To see the differences between environments click on the button as shown in figure 1. You will then see the difference between the environment and the previous environment it was created from.</p> <p> Figure 1: You can see the difference between the two environments by clicking on the button pointed.  </p> <p>If you had built the environment using custom commands you can go back to see what commands were run during the build as shown in figure 2. </p> <p> Figure 2:  You can see custom commands that were used to create the environment by clicking on the button pointed.  </p>"},{"location":"user_guides/projects/python/python_env_clone/","title":"How To Clone Python Environment","text":""},{"location":"user_guides/projects/python/python_env_clone/#introduction","title":"Introduction","text":"<p>Cloning an environment in Hopsworks means creating a copy of one of the base environments. The base environments are immutable, meaning that it is required to clone an environment before you can make any change to it, such as installing your own libraries. This ensures that the project maintains a set of stable environments that are tested with the capabilities of the platform, meanwhile through cloning, allowing users to further customize an environment without affecting the base environments.</p> <p>In this guide, you will learn how to clone an environment.</p>"},{"location":"user_guides/projects/python/python_env_clone/#step-1-select-an-environment","title":"Step 1: Select an environment","text":"<p>Under the <code>Project settings</code> section you can find the <code>Python environment</code> setting.</p> <p>First select an environment, for example the <code>python-feature-pipeline</code>.</p> <p> Select a base environment </p>"},{"location":"user_guides/projects/python/python_env_clone/#step-2-clone-environment","title":"Step 2: Clone environment","text":"<p>The environment can now be cloned by clicking <code>Clone env</code> and entering a name and description. The interface will show <code>Syncing packages</code> while creating the environment.</p> <p> Clone a base environment </p>"},{"location":"user_guides/projects/python/python_env_clone/#step-3-environment-is-now-ready","title":"Step 3: Environment is now ready","text":"<p> Environment is now cloned </p> <p>What does the CUSTOM mean?</p> <p>Notice that the cloned environment is tagged as <code>CUSTOM</code>, it means that it is a base environment which has been cloned.</p> <p>Base environment also marked</p> <p>When you select a <code>CUSTOM</code> environment the base environment it was cloned from is also shown.</p>"},{"location":"user_guides/projects/python/python_env_clone/#concerning-upgrades","title":"Concerning upgrades","text":"<p>Please note</p> <p>The base environments are automatically upgraded when Hopsworks is upgraded and application code should keep functioning provided that no breaking changes were made in the upgraded version of the environment. A <code>CUSTOM</code> environment is not automatically upgraded and the users is recommended to reapply the modifications to a base environment if they encounter issues after an upgrade.</p>"},{"location":"user_guides/projects/python/python_env_clone/#next-steps","title":"Next steps","text":"<p>In this guide you learned how to clone a new environment. The next step is to install a library in the environment.</p>"},{"location":"user_guides/projects/python/python_env_export/","title":"How To Export Python Environment","text":""},{"location":"user_guides/projects/python/python_env_export/#introduction","title":"Introduction","text":"<p>Each of the python environments in a project can be exported to an <code>environment.yml</code> file. It can be useful to export it to keep a snapshot of all the installed libraries and their versions.</p> <p>In this guide, you will learn how to export a python environment. </p>"},{"location":"user_guides/projects/python/python_env_export/#step-1-go-to-environment","title":"Step 1: Go to environment","text":"<p>Under the <code>Project settings</code> section you can find the <code>Python environment</code> setting.</p>"},{"location":"user_guides/projects/python/python_env_export/#step-2-select-a-custom-environment","title":"Step 2: Select a CUSTOM environment","text":"<p>Select the environment that you have previously cloned and want to export. Only a <code>CUSTOM</code> environment can be exported.</p>"},{"location":"user_guides/projects/python/python_env_export/#step-3-click-export-env","title":"Step 3: Click Export env","text":"<p>An existing Anaconda environment can be exported as a yml file, clicking the <code>Export env</code> will download the <code>environment.yml</code> file in your browser.</p> <p> Export environment </p>"},{"location":"user_guides/projects/python/python_env_overview/","title":"Python Environments","text":""},{"location":"user_guides/projects/python/python_env_overview/#introduction","title":"Introduction","text":"<p>Hopsworks postulates that building ML systems following the FTI pipeline architecture is best practice. This architecture consists of three independently developed and operated ML pipelines:</p> <ul> <li>Feature Pipeline: takes as input raw data that it transforms into features (and labels)</li> <li>Training Pipeline: takes as input features (and labels) and outputs a trained model</li> <li>Inference Pipeline: takes new feature data and a trained model and makes predictions.</li> </ul> <p>In order to facilitate the development of these pipelines Hopsworks bundles several python environments containing necessary dependencies.  Each environment can also be customized further by installing additional dependencies from PyPi, Conda, Wheel files, GitHub repos or applying custom Dockerfiles on top.</p>"},{"location":"user_guides/projects/python/python_env_overview/#step-1-go-to-environments-page","title":"Step 1: Go to environments page","text":"<p>Under the <code>Project settings</code> section you can find the <code>Python environment</code> setting.</p>"},{"location":"user_guides/projects/python/python_env_overview/#step-2-list-available-environments","title":"Step 2: List available environments","text":"<p>Environments listed under <code>FEATURE ENGINEERING</code> corresponds to environments you would use in a feature pipeline, <code>MODEL TRAINING</code> maps to environments used in a training pipeline and <code>MODEL INFERENCE</code> are what you would use in inference pipelines. </p> <p> Bundled python environments </p> <p>Python version</p> <p>The python version used in all the environments is 3.10.</p>"},{"location":"user_guides/projects/python/python_env_overview/#feature-engineering","title":"Feature engineering","text":"<p>The <code>FEATURE ENGINEERING</code> environments can be used in Jupyter notebooks, a Python job or a PySpark job.</p> <ul> <li><code>python-feature-pipeline</code> for writing feature pipelines using Python</li> <li><code>spark-feature-pipeline</code> for writing feature pipelines using PySpark</li> </ul>"},{"location":"user_guides/projects/python/python_env_overview/#model-training","title":"Model training","text":"<p>The <code>MODEL TRAINING</code> environments can be used in Jupyter notebooks or a Python job or in a Ray job. </p> <ul> <li><code>tensorflow-training-pipeline</code> to train TensorFlow models</li> <li><code>torch-training-pipeline</code> to train PyTorch models</li> <li><code>pandas-training-pipeline</code> to train XGBoost, Catboost and Sklearn models</li> <li><code>ray_training_pipeline</code> a general purpose environment for distributed training using Ray framework to train    XGBoost and Sklearn models. Should be used in Ray job. It can be customized to install    additional dependencies of your choice.</li> <li><code>ray_torch_training_pipeline</code> for distributed training of PyTorch models using Ray framework in a Ray job</li> <li><code>ray_tensorflow_training_pipeline</code> for distributed training of TensorFlow models using Ray framework in a Ray job</li> </ul>"},{"location":"user_guides/projects/python/python_env_overview/#model-inference","title":"Model inference","text":"<p>The <code>MODEL INFERENCE</code> environments can be used in a deployment using a custom predictor script.</p> <ul> <li><code>tensorflow-inference-pipeline</code> to load and serve TensorFlow models</li> <li><code>torch-inference-pipeline</code> to load and serve PyTorch models</li> <li><code>pandas-inference-pipeline</code> to load and serve XGBoost, Catboost and Sklearn models</li> <li><code>vllm-inference-pipeline</code> to load and serve LLMs with vLLM inference engine</li> <li><code>minimal-inference-pipeline</code> to install your own custom framework, contains a minimal set of dependencies</li> </ul>"},{"location":"user_guides/projects/python/python_env_overview/#next-steps","title":"Next steps","text":"<p>In this guide you learned how to find the bundled python environments and where they can be used. Now you can test out the environment in a Jupyter notebook.</p>"},{"location":"user_guides/projects/python/python_install/","title":"How To Install Python Libraries","text":""},{"location":"user_guides/projects/python/python_install/#introduction","title":"Introduction","text":"<p>Hopsworks comes with several prepackaged Python environments that contain libraries for data engineering, machine learning, and more general data science use-cases. Hopsworks also offers the ability to install additional packages from various sources, such as using the pip or conda package managers and public or private git repository.</p> <p>In this guide, you will learn how to install Python packages using these different options.</p> <ul> <li>PyPi, using pip package manager</li> <li>A conda channel, using conda package manager</li> <li>Packages contained in .whl format</li> <li>A public or private git repository</li> <li>A requirements.txt file to install multiple libraries at the same time using pip</li> </ul> <p>Notice</p> <p>If your libraries require installing some extra OS-Level packages, refer to the guide custom commands guide on how to install OS-Level packages.</p>"},{"location":"user_guides/projects/python/python_install/#prerequisites","title":"Prerequisites","text":"<p>In order to install a custom dependency one of the base environments must first be cloned, follow this guide for that.</p>"},{"location":"user_guides/projects/python/python_install/#step-1-go-to-environments-page","title":"Step 1: Go to environments page","text":"<p>Under the <code>Project settings</code> section select the <code>Python environment</code> setting.</p>"},{"location":"user_guides/projects/python/python_install/#step-2-select-a-custom-environment","title":"Step 2: Select a CUSTOM environment","text":"<p>Select the environment that you have previously cloned and want to modify.</p>"},{"location":"user_guides/projects/python/python_install/#step-3-installation-options","title":"Step 3: Installation options","text":""},{"location":"user_guides/projects/python/python_install/#name-and-version","title":"Name and version","text":"<p>Enter the name and, optionally, the desired version to install.</p> <p> Installing library by name and version </p>"},{"location":"user_guides/projects/python/python_install/#search","title":"Search","text":"<p>Enter the search term and select a library and version to install.</p> <p> Installing library using search </p>"},{"location":"user_guides/projects/python/python_install/#distribution-whl-egg","title":"Distribution (.whl, .egg..)","text":"<p>Install a python package by uploading the corresponding package file and selecting it in the file browser.</p> <p> Installing library from file </p>"},{"location":"user_guides/projects/python/python_install/#git-source","title":"Git source","text":"<p>The URL you should provide is the same as you would enter on the command line using <code>pip install git+{repo_url}</code>, where <code>repo_url</code> is the part that you enter in <code>Git URL</code>.</p> <p>For example to install matplotlib 3.7.2, the following are correct inputs:</p> <p><code>matplotlib @ git+https://github.com/matplotlib/matplotlib@v3.7.2</code></p> <p><code>git+https://github.com/matplotlib/matplotlib@v3.7.2</code></p> <p>In the case of a private git repository, also select whether it is a GitHub or GitLab repository and the preconfigured access token for the repository.</p> <p>Keep your secrets safe</p> <p>If you are installing from a git repository which is not GitHub or GitLab simply supply the access token in the URL. Keep in mind that in this case the access token may be visible in logs for other users in the same project to see.</p> <p> Installing library from git repo </p>"},{"location":"user_guides/projects/python/python_install/#going-further","title":"Going Further","text":"<p>Now you can use the library in a Jupyter notebook or a Job.</p>"},{"location":"user_guides/projects/scheduling/kube_scheduler/","title":"Scheduler","text":""},{"location":"user_guides/projects/scheduling/kube_scheduler/#introduction","title":"Introduction","text":"<p>Hopsworks allows users to configure some Kubernetes scheduler abstractions, such as Affinity and Priority Classes. Hopsworks also supports additional scheduling abstractions backed by Kueue. This includes Queues, Cohorts and Topologies. All these scheduling abstractions are supported in jobs, jupyter notebooks and model deployments. Kueue abstractions however, are currently not supported for Spark jobs.</p> <p>Hopsworks Admins can control which labels and priority classes can be used the cluster (see Cluster configuration section) and by which project (see Default Project configuration section)</p> <p>Within a project, data owners can set defaults for jobs and Jupyter notebooks running within that project (see: Project defaults section).</p>"},{"location":"user_guides/projects/scheduling/kube_scheduler/#node-labels-node-affinity-and-node-anti-affinity","title":"Node Labels, Node Affinity and Node Anti-Affinity","text":"<p>Labels in Kubernetes are key-value pairs used to organize and select resources. Hopsworks relies on labels applied to nodes for pod-node affinity to determine where the pod can (or cannot) run. Some uses cases where labels and affinity can be used include:</p> <ul> <li>Hardware constraints (GPU, SSD)</li> <li>Environment separation (prod/dev)</li> <li>Co-locating related pods</li> <li>Spreading pods for high availability</li> </ul> <p>Hopsworks uses the node affinity <code>IN</code> operator for the Hopsworks Node Affinity and the <code>NOT IN</code> operator for the Hopsworks Node Anti Affinity.</p> <p>For more information on Kubernetes Affinity, you can check the Kubernetes Affinity documentation page.</p>"},{"location":"user_guides/projects/scheduling/kube_scheduler/#priority-classes","title":"Priority Classes","text":"<p>Priority classes in Kubernetes determine the scheduling and eviction priority of pods.</p> <p>Pods with higher priority:</p> <ul> <li>Get scheduled first</li> <li>Can preempt (evict) lower priority pods</li> <li>Less likely to be evicted under resource pressure</li> </ul> <p>Common uses:</p> <ul> <li>Protecting critical workloads</li> <li>Ensuring core services stay running</li> <li>Managing resource competition</li> <li>Guaranteeing QoS for important applications</li> </ul> <p>For more information on Priority Classes, you can check the Kubernetes Priority Classes documentation page.</p>"},{"location":"user_guides/projects/scheduling/kube_scheduler/#kueue","title":"Kueue","text":"<p>Hopsworks adds the integration with Kueue to offer more advanced scheduling abstractions such as queues, cohorts and topologies.</p> <p>For a more detailed view on how Hopsworks uses the Kueue abstractions you can check the Kueue details section.</p>"},{"location":"user_guides/projects/scheduling/kube_scheduler/#queues-cohorts","title":"Queues, Cohorts","text":"<p>Jobs, notebooks and model deployments are submitted to these queues. Hopsworks administrator can define quotas on how many resources a queue can use. Queues can be grouped together in cohorts in order to add the ability to borrow resources from each other when the other queue does not use its resources.</p> <p>When creating a new job, the user can select a queue for the job in the <code>Advance configuration -&gt; Scheduler section</code>.</p> <p></p>"},{"location":"user_guides/projects/scheduling/kube_scheduler/#topologies","title":"Topologies","text":"<p>The integration of Hopsworks with Kueue, also provides access to the topology abstraction. Topologies can be defined, so that the user can decide for the pods of jobs or model deployments to run somehow grouped together. The user could decide for example, that all pods of a job should run on the same host, because the pods need to transfer a lot of data between each other, and we want to avoid network traffic to lower the latency.</p> <p>The user can select the topology unit for jobs, notebooks and model deployments in the <code>Advance configuration -&gt; Scheduler section</code>.</p> <p></p>"},{"location":"user_guides/projects/scheduling/kube_scheduler/#admin-configuration","title":"Admin configuration","text":""},{"location":"user_guides/projects/scheduling/kube_scheduler/#affinity-and-priority-classes","title":"Affinity and priority classes","text":"<p>Hopsworks admins can control the affinity labels and priority classes available on the Hopsworks cluster from the <code>Cluster Settings -&gt; Scheduler</code> page:</p> <p></p> <p>Hopsworks Cluster can run within a shared Kubernets Cluster. The first configuration level is to limit the subset of labels and priority classes that can be used within the Hopsworks Cluster. This can be done from the <code>Available in Hopsworks</code> sub-section.</p> <p>Permissions</p> <p>In order to be able to list all the Kubernetes Node Labels, Hopsworks requires the following cluster role:</p> <pre><code>    - apiGroups: [\"\"]\n    resources: [\"nodes\"]\n    verbs: [\"get\", \"list\"]\n</code></pre> <p>In order to be able to list all the Kubernetes Cluster Priority Classes, Hopsworsk requires this cluster role:</p> <pre><code>    - apiGroups: [\"scheduling.k8s.io\"]\n    resources: [\"priorityclasses\"]\n    verbs: [\"get\", \"list\"]\n</code></pre> <p>If the roles above are configured properly (default behaviour), admins can only select values from the drop down menu. If the roles are missing, admins would be required to enter them as free text and should be careful about typos. Any typos here will be propagated in the other configuration and use levels leading to errors or missbehaviour when running computation.</p>"},{"location":"user_guides/projects/scheduling/kube_scheduler/#queues","title":"Queues","text":"<p>Every new project gets automatic access to the default Hopsworks queue. An administrator can define the default queue for projects user jobs and system jobs.</p> <p></p>"},{"location":"user_guides/projects/scheduling/kube_scheduler/#project-configuration","title":"Project Configuration","text":"<p>Hopsworks admins can configure the labels and priority classes that can be used by default within a project. This will be a subset of the ones configured for Hopsworks. In the figure above, in the sub-section <code>Available in Project</code> Hopsworks admins can configure the labels and priority classes available by default in any Hopsworks Project.</p> <p>Hopsworks admins can also override the default project configuration on a per-project basis. That is, Hopsworks admins can make certain labels and priority classes available only to certain projects. This can be achieved from the <code>Cluster Settings -&gt; Project -&gt; &lt;ProjectName&gt; -&gt; edit configuration</code> configuration page:</p> <p></p>"},{"location":"user_guides/projects/scheduling/kube_scheduler/#project-defaults","title":"Project defaults","text":"<p>Within a project, different jobs, Jupyter notebooks and model deployments can run with different labels and/or priority classes. <code>Data Owners</code> in a project can specify the default values from the project settings: The default Label will be used for the default Node Affinity for jobs, notebooks, and model deployments.</p> <p></p>"},{"location":"user_guides/projects/scheduling/kube_scheduler/#configuration-of-jobs-notebooks-and-deployments","title":"Configuration of Jobs, Notebooks, and Deployments","text":"<p>In the advanced configuration sections for job, notebook, and model deployments, users can set affinity, anti affinity and priority class. The Affinity and Anti Affinity can be selected from the list of allowed labels.</p> <p><code>Affinity</code> configures on which nodes this pod can run. If a node has any of the labels present in the Affinity option, the pod can be scheduler to run to run there.</p> <p><code>Anti Affinity</code> configures on which nodes this pod will not run on. If a node has any of the labels present in the Anti Affinity option, the pod will not be scheduler to run there.</p> <p><code>Priority Class</code> specifies with which priority a pod will run.</p> <p></p>"},{"location":"user_guides/projects/scheduling/kueue_details/","title":"Kueue","text":""},{"location":"user_guides/projects/scheduling/kueue_details/#introduction","title":"Introduction","text":"<p>Hopsworks provides the integration with Kueue to provide the additional scheduling abstractions. Hopsworks currently acts only as a \"reader\" to the Kueue abstractions and currently does not manage the lifecycle of Kueue abstraction with the exception of the default localqueue for each namespace. All the other abstractions are expected to be managed by the administrators of Hopsworks, directly on the Kubernetes cluster.</p> <p>However Hopsworks and Kueue integration currently only supports frameworks python and ray for jobs, notebooks and model deployments. The same queues are also used for Hopsworks internal jobs (zipping, git operations, python library installation). Spark is currently not supported, and thus will not be managed by Kueue for scheduling, and instead it will bypass the queues setup (important to note when thinking about queue quotas) and instead are managed directly by the Kubernetes Scheduler.</p>"},{"location":"user_guides/projects/scheduling/kueue_details/#resource-flavors","title":"Resource flavors","text":"<p>When trying to define queues in Kueue, the first abstraction that needs to be defined is a Resource Flavor. The resource flavor defines the resources that a queue will later manage. Hopsworks helm chart installs and uses a default ResourceFlavor</p> <pre><code>apiVersion: kueue.x-k8s.io/v1beta1\nkind: ResourceFlavor\nmetadata:\n  name: default-flavor\nspec:\n  nodeLabels:\n    cloud.provider.com/region: europe\n  topologyName: default\n</code></pre> <p>Node labels filter the available nodes to this resource flavor and is required for topologies</p>"},{"location":"user_guides/projects/scheduling/kueue_details/#cluster-queues","title":"Cluster Queues","text":"<p>Cluster Queues are the actual queues for submitting jobs and model deployments to. The default hopsworks queue looks like:</p> <pre><code>apiVersion: kueue.x-k8s.io/v1beta1\nkind: ClusterQueue\nmetadata:\n  name: other\nspec:\n  cohort: cluster\n  namespaceSelector: {}\n  preemption:\n    borrowWithinCohort:\n      policy: Never\n    reclaimWithinCohort: Never\n    withinClusterQueue: Never\n  queueingStrategy: BestEffortFIFO\n  resourceGroups:\n  - coveredResources:\n    - cpu\n    - memory\n    - pods\n    - nvidia.com/gpu\n    flavors:\n    - name: default-flavor\n      resources:\n      - name: cpu\n        nominalQuota: \"0\"\n      - name: memory\n        nominalQuota: \"0\"\n      - name: pods\n        nominalQuota: \"0\"\n      - name: nvidia.com/gpu\n        nominalQuota: \"0\"\n</code></pre> <p>The preemption and nominal quotas are set to the minimal as this queue is designed to have lowest priority in getting resources allocated. If a cluster is underutilized and there are resources available, it can still borrow up to the maximum resources present in the parent cohort, but by design this queue has no dedicated resources. The presumption is that other, more important queues, defined by the cluster administrator will have higher preference in getting resources.</p>"},{"location":"user_guides/projects/scheduling/kueue_details/#local-queues","title":"Local Queues","text":"<p>Local Queues are the mechanism to provide access to a queue (cluster queue) to a specific project in Hopsworks (Kubernetes namespace).</p> <p>Every new project gets automatic access to the default Hopsworks queue. An administrator can define the default queue for projects user jobs and system jobs.</p> <p></p>"},{"location":"user_guides/projects/scheduling/kueue_details/#cohorts","title":"Cohorts","text":"<p>Cohorts are groupings of cluster queues that have some meaning together and can share resources. Hopsworks defines a default <code>cluster</code> cohort</p> <pre><code>apiVersion: kueue.x-k8s.io/v1alpha1\nkind: Cohort\nmetadata:\n  name: cluster\nspec:\n  resourceGroups:\n  - coveredResources:\n    - cpu\n    - memory\n    - pods\n    - nvidia.com/gpu\n    flavors:\n    - name: default-flavor\n      resources:\n      - name: cpu\n        nominalQuota: 100\n      - name: memory\n        nominalQuota: 200Gi\n      - name: pods\n        nominalQuota: 100\n      - name: nvidia.com/gpu\n        nominalQuota: 50\n</code></pre> <p>Cohorts can contain other cohorts and thus you can create a hierarchy of cohorts. Cohorts can set fair sharing weight where using</p> <pre><code>  fairSharing:\n    weight\n</code></pre> <p>in the definition of a cohort, the user can control a priority towards borrowing resources from other cohorts.</p>"},{"location":"user_guides/projects/scheduling/kueue_details/#topologies","title":"Topologies","text":"<p>Topologies defines a way of grouping together pods belonging to the same job/deployment so that they are colocated within the same topology unit. Hopsworks defines a default topology:</p> <pre><code>apiVersion: kueue.x-k8s.io/v1alpha1\nkind: Topology\nmetadata:\n  name: default\nspec:\n  levels:\n  - nodeLabel: cloud.provider.com/region\n  - nodeLabel: cloud.provider.com/zone\n  - nodeLabel: kubernetes.io/hostname\n</code></pre> <p>The topology is defined in the Resource Flavor used by a Cluster Queue.</p> <p>When creating a new job, the user can select a topology unit for the job to run in and thus decide if all pods of a job should run on the same hostname, in the same zone or in the same region. The user can select the topology for jobs, notebooks and deployments in the <code>Advance configuration -&gt; Scheduler section</code>.</p> <p></p>"},{"location":"user_guides/projects/secrets/create_secret/","title":"How To Create A Secret","text":""},{"location":"user_guides/projects/secrets/create_secret/#introduction","title":"Introduction","text":"<p>A Secret is a key-value pair used to store encrypted information accessible only to the owner of the secret.  Also if you wish to, you can share the same secret API key with all the members of a Project.</p>"},{"location":"user_guides/projects/secrets/create_secret/#ui","title":"UI","text":""},{"location":"user_guides/projects/secrets/create_secret/#step-1-navigate-to-secrets","title":"Step 1: Navigate to Secrets","text":"<p>In the <code>Account Settings</code> page you can find the <code>Secrets</code> section showing a list of all secrets.</p> <p> List of secrets </p>"},{"location":"user_guides/projects/secrets/create_secret/#step-2-create-a-secret","title":"Step 2: Create a Secret","text":"<p>Click <code>New Secret</code> to bring up the dialog for secret creation. Enter a name for the secret to be used for lookup, and the secret value.</p> <p>If the secret should be private to this user, select <code>Private</code>, to share the secret with all members of a project select <code>Project</code> and enter the project name.</p> <p> Create new secret dialog </p>"},{"location":"user_guides/projects/secrets/create_secret/#step-3-secret-created","title":"Step 3: Secret created","text":"<p>Click <code>New Secret</code> to bring up the dialog for secret creation. Enter a name for the secret to be used for lookup, and the secret value.</p> <p>If the secret should be private to this user, select <code>Private</code>, to share the secret with all members of a project select <code>Project</code> and enter the project name.</p> <p> Secret is now created </p>"},{"location":"user_guides/projects/secrets/create_secret/#code","title":"Code","text":""},{"location":"user_guides/projects/secrets/create_secret/#step-1-get-secrets-api","title":"Step 1: Get secrets API","text":"<pre><code>import hopsworks\n\nhopsworks.login()\n\nsecrets_api = hopsworks.get_secrets_api()\n</code></pre>"},{"location":"user_guides/projects/secrets/create_secret/#step-2-create-secret","title":"Step 2: Create secret","text":"<pre><code>secret = secrets_api.create_secret(\"my_secret\", \"Fk3MoPlQXCQvPo\")\n</code></pre>"},{"location":"user_guides/projects/secrets/create_secret/#api-reference","title":"API Reference","text":"<p>Secrets</p>"}]}